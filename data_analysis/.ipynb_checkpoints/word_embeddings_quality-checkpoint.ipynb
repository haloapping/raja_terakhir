{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38fa2f5-8329-4a6e-9052-f31e21ea1a80",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3aac804-7d18-4214-85e6-e23af482c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from umap import UMAP\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8651a-d82b-4ed3-a9b1-b14fb68e1292",
   "metadata": {},
   "source": [
    "# Read dataset and word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8ebb29-da6c-4434-a58b-9dcade7ca2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../datasets/raw/Indonesian_Manually_Tagged_Corpus.tsv\", sep=\"\\t\", header=None, names=[\"token\", \"tag\"], quoting=csv.QUOTE_NONE, skip_blank_lines=False)\n",
    "chars = list(string.ascii_letters + string.digits + string.punctuation)\n",
    "vocabs = set(dataset.token.values.tolist() + chars)\n",
    "vocabs.remove(np.nan)\n",
    "vocabs = list(vocabs)\n",
    "n_vocab = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cba2f6a-532d-4ee4-bf0b-ad35717dc2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embedding.load(\"../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "embeddings.apply_expansion(DigitExpander)\n",
    "embeddings.apply_expansion(CaseExpander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcea6e67-aa88-4d0a-949a-47485ebfc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_oov(vocabs, return_vocabs=False):\n",
    "    counter = 0\n",
    "\n",
    "    if return_vocabs:\n",
    "        oov_tokens = []\n",
    "        for vocab in tqdm(vocabs):\n",
    "            if vocab not in embeddings:\n",
    "                counter += 1\n",
    "                oov_tokens.append(vocab)\n",
    "        \n",
    "        return counter, oov_tokens\n",
    "    else:\n",
    "        for vocab in tqdm(vocabs):\n",
    "            if vocab not in embeddings:\n",
    "                counter += 1        \n",
    "    \n",
    "        return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1020075-6844-4418-9347-6e460b5b122e",
   "metadata": {},
   "source": [
    "# Check character in word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d7f6a9-5974-4298-abe4-d2c8882dcc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62a7bea4b1e4792b1c01431e11b1528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
      "Number of OOV chars: 0\n",
      "OOV chars: []\n"
     ]
    }
   ],
   "source": [
    "n_oov_chars, oov_chars = count_oov(chars, return_vocabs=True)\n",
    "\n",
    "print(f\"Characters: {chars}\")\n",
    "print(f\"Number of OOV chars: {n_oov_chars}\")\n",
    "print(f\"OOV chars: {oov_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a62cd-0f5a-43f4-ad35-0ae00babe157",
   "metadata": {},
   "source": [
    "# Filter vocabs not in words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe073fb-ab23-4975-bb6e-326fe839aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a67a3be4a8f477ea0b1ed46fea2c7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab: 18327\n",
      "\n",
      "Number of OOV vocab: 3748 (20.45%)\n",
      "OOV vocab: ['semaksimal mungkin', 'Perusahaan-perusahaan', 'angkatan darat', 'UKTI', 'Joelianto', 'baru-baru ini', 'Resettlement', 'Al-Hariri', 'berkali-kali', 'partikulir']\n",
      "\n",
      "Number of not OOV vocab: 14579 (79.55%)\n",
      "Not OOV vocab: ['Polri', 'keampuhan', '400', 'menetralkan', 'Fatah', 'tenang', 'system', '17,00', 'alternatif', 'Rahmat']\n"
     ]
    }
   ],
   "source": [
    "n_oov_vocab, oov_vocabs = count_oov(vocabs + chars, return_vocabs=True)\n",
    "n_not_oov_vocab, not_oov_vocabs= len(vocabs) - n_oov_vocab, list(set(vocabs).difference(set(oov_vocabs)))\n",
    "\n",
    "print(f\"Number of vocab: {n_vocab}\\n\")\n",
    "\n",
    "print(f\"Number of OOV vocab: {n_oov_vocab} ({round(n_oov_vocab / n_vocab * 100, 2)}%)\")\n",
    "print(f\"OOV vocab: {oov_vocabs[:10]}\\n\")\n",
    "\n",
    "print(f\"Number of not OOV vocab: {n_not_oov_vocab} ({round(n_not_oov_vocab / n_vocab * 100, 2)}%)\")\n",
    "print(f\"Not OOV vocab: {not_oov_vocabs[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831e2fa-bb01-4875-9b30-d7a483ff9313",
   "metadata": {},
   "source": [
    "# Visualize word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e1289e-5737-435e-a976-f3d05f1a3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = []\n",
    "\n",
    "for vocab in not_oov_vocabs:\n",
    "    word_embeddings.append(embeddings[vocab])\n",
    "\n",
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd5a52-a22d-4a41-8f23-275aa7aa468c",
   "metadata": {},
   "source": [
    "## 2 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c44f5a6-87c3-4f92-b430-4eb398d3b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_2dim = UMAP(n_neighbors=15, n_components=2, n_epochs=500, metric=\"correlation\", random_state=42).fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64b6676-c236-4bf3-9cd9-00cf27043103",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_2dim_df = pd.DataFrame(data={\n",
    "    \"dim_1\": embedding_2dim[:, 0],\n",
    "    \"dim_2\": embedding_2dim[:, 1],\n",
    "    \"vocab\": not_oov_vocabs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c61230-7625-4c6f-9305-810a9dac1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(embedding_2dim_df, x=\"dim_1\", y=\"dim_2\", hover_data=[\"vocab\"], width=800, height=800, title=\"Word Embedding Polyglot Quality in 2 Dimensions\")\n",
    "fig.write_html(\"../images/data_analysis/embedding_2dim.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb1ed8-abfa-434f-a8e2-3619dfc75e11",
   "metadata": {},
   "source": [
    "## 3 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921cd3ac-bfb7-47a1-836e-3c1fcee89af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_3dim = UMAP(n_neighbors=15, n_components=3, n_epochs=500, metric=\"correlation\", random_state=42).fit_transform(word_embeddings)\n",
    "\n",
    "embedding_3dim_df = pd.DataFrame(data={\n",
    "    \"dim_1\": embedding_3dim[:, 0],\n",
    "    \"dim_2\": embedding_3dim[:, 1],\n",
    "    \"dim_3\": embedding_3dim[:, 2],\n",
    "    \"vocab\": not_oov_vocabs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8eed7cd-ab67-4bac-a4b1-3a1840302b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(embedding_3dim_df, x=\"dim_1\", y=\"dim_2\", z=\"dim_3\", hover_data=[\"vocab\"], width=800, height=800, title=\"Word Embedding Polyglot Quality in 3 Dimensions\")\n",
    "fig.write_html(\"../images/data_analysis/embedding_3dim.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf55f69-c2cd-4044-9618-2bd42c549c47",
   "metadata": {},
   "source": [
    "# Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f60cf80-953b-4752-a749-79f724aaf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_not_oov_embeddings = pd.DataFrame(word_embeddings, columns=[\"dim_\" + str(i) for i in range(1, 65)])\n",
    "id_not_oov_embeddings[\"token\"] = not_oov_vocabs\n",
    "id_not_oov_embeddings = id_not_oov_embeddings[[\"token\"] + [\"dim_\" + str(i) for i in range(1, 65)]]\n",
    "id_not_oov_embeddings[id_not_oov_embeddings[\"token\"] == \"p\"]\n",
    "id_not_oov_embeddings.to_csv(\"../datasets/data_analysis/id_not_oov_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ceb61-4b96-44f8-8f42-fecdfd94d865",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notes\n",
    "\n",
    "- Polyglot mengandung 100.004 vocab dengan dimensi embedding 64.\n",
    "- Polyglot mencakup 14.579 kosakata (79.55% dari keseluruhan dataset) dan sisanya 3.748 (20.45%) Out Of Vocabulary.\n",
    "- OOV umumnya berupa, singkatan, nama orang, nama tempat, nama merek (barang dan jasa), mengandung tanda baca (spasi dan kurang) dan bahasa asing.\n",
    "- Semua karakter (tanda baca, digit angka, huruf kapital, dan huruf kecil) berada dalam word embedding polyglot.\n",
    "- Polyglot mengandung semua embedding numerikal positif.\n",
    "- Nilai vektor huruf kapital dan vektor huruf kecil pada Polyglot berbeda.\n",
    "- Word embedding Polyglot memperlihatkan hasil embedding yang cukup baik. Dapat dilihat pada hasil visualisasi sebagian token dikelompokkan sesuai dengan karakteristiknya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f838d9-d9f8-4f08-a3c2-96328f5ab146",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Mekanisme pengujian OOV?\n",
    "\n",
    "- Masiih ada kekeliruan dibagian data preprocessing dan model (classifier dan word embedding).\n",
    "- OOV bisa berada di kalimat lain dengan posisi dan konteks yang berbeda? Cara penanganannya bagaimana? Alternatif gunakan fungsi aggregate,seperti mean atau sum. Apakah efektif??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db023ccf-62ff-44b8-a160-b62e625af280",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44384ec7-016d-4920-93d1-8e44e700877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='correlation', n_components=3, n_epochs=500, random_state=42, verbose=True)\n",
      "Thu Sep 15 03:47:16 2022 Construct fuzzy simplicial set\n",
      "Thu Sep 15 03:47:16 2022 Finding Nearest Neighbors\n",
      "Thu Sep 15 03:47:16 2022 Building RP forest with 21 trees\n",
      "Thu Sep 15 03:47:17 2022 NN descent for 17 iterations\n",
      "\t 1  /  17\n",
      "\t 2  /  17\n",
      "\t 3  /  17\n",
      "\t 4  /  17\n",
      "\t 5  /  17\n",
      "\tStopping threshold met -- exiting after 5 iterations\n",
      "Thu Sep 15 03:47:25 2022 Finished Nearest Neighbor Search\n",
      "Thu Sep 15 03:47:25 2022 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a783dc0704a4f5cbc1f4e42b7fe2c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 15 03:51:08 2022 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "embedding_3dim = UMAP(n_neighbors=15, n_components=3, n_epochs=500, metric=\"correlation\", random_state=42, verbose=True).fit_transform(embeddings.vectors)\n",
    "\n",
    "embedding_3dim_df = pd.DataFrame(data={\n",
    "    \"dim_1\": embedding_3dim[:, 0],\n",
    "    \"dim_2\": embedding_3dim[:, 1],\n",
    "    \"dim_3\": embedding_3dim[:, 2],\n",
    "    \"vocab\": embeddings.words\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(embedding_3dim_df, x=\"dim_1\", y=\"dim_2\", z=\"dim_3\", hover_data=[\"vocab\"], width=800, height=800, title=\"Word Embedding Polyglot Quality in 3 Dimensions\")\n",
    "fig.write_html(\"../images/data_analysis/idn_embedding_polyglot_3dim.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7a0bb-9598-4694-91e7-cd43dbabcb47",
   "metadata": {},
   "source": [
    "# Char Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd28a388-87c4-4337-a93e-a17700241007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chars_embedding_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_194/765756977.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchars_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars_embedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars_embedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mchar_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chars_embedding_df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"../word_embeddings/chars_embedding/char_mimick_glove_d100_c20\") as f:\n",
    "    chars_embedding = f.readlines()\n",
    "\n",
    "chars_embedding = [embedding.split(\"\\n\") for embedding in chars_embedding]\n",
    "chars_embedding = [embedding[0].split(\" \") for embedding in chars_embedding]\n",
    "\n",
    "chars_embedding = np.array(chars_embedding)\n",
    "chars = chars_embedding[:, 0]\n",
    "embeddings = chars_embedding[:, 1:].astype(np.float32)\n",
    "char_embeddings = {char: embedding for char, embedding in zip(chars, embeddings)}\n",
    "char_embeddings[\"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d859b-15cf-4495-b17b-6c58b55ff969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
