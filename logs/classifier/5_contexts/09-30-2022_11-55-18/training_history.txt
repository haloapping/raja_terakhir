HYPERPARAMETERS
------------------------------------------------------------
context_size: 5
input_size: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 24
shuffle: True
lr: 0.005
batch_first: False
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 100
device: cpu

TRAINING PROGRESS
------------------------------------------------------------
EPOCH-1
Batch-50: CrossEntropyLoss=3.0745 | F1Score=0.3341
Batch-100: CrossEntropyLoss=3.0599 | F1Score=0.3398
Batch-150: CrossEntropyLoss=3.0672 | F1Score=0.3472
Batch-200: CrossEntropyLoss=3.0542 | F1Score=0.3531
Batch-226: CrossEntropyLoss=3.0488 | F1Score=0.3542
Batch-26: CrossEntropyLoss=3.0875 | F1Score=0.3568

Training   : Mean CrossEntropyLoss = 3.0725 | Mean F1Score = 0.3294
Validation : Mean CrossEntropyLoss = 3.0597 | Mean F1Score = 0.3549
============================================================

EPOCH-2
Batch-50: CrossEntropyLoss=3.0752 | F1Score=0.3806
Batch-100: CrossEntropyLoss=3.0607 | F1Score=0.3792
Batch-150: CrossEntropyLoss=3.0650 | F1Score=0.3808
Batch-200: CrossEntropyLoss=3.0546 | F1Score=0.3810
Batch-226: CrossEntropyLoss=3.0338 | F1Score=0.3817
Batch-26: CrossEntropyLoss=2.9907 | F1Score=0.3834

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0585 | Mean F1Score = 0.3796
Validation : Mean CrossEntropyLoss = 3.0545 | Mean F1Score = 0.3832
============================================================

EPOCH-3
Batch-50: CrossEntropyLoss=3.0528 | F1Score=0.3779
Batch-100: CrossEntropyLoss=3.0687 | F1Score=0.3840
Batch-150: CrossEntropyLoss=3.0441 | F1Score=0.3910
Batch-200: CrossEntropyLoss=3.0347 | F1Score=0.3923
Batch-226: CrossEntropyLoss=3.0263 | F1Score=0.3945
Batch-26: CrossEntropyLoss=3.0688 | F1Score=0.3970

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0567 | Mean F1Score = 0.3863
Validation : Mean CrossEntropyLoss = 3.0558 | Mean F1Score = 0.3959
============================================================

EPOCH-4
Batch-50: CrossEntropyLoss=3.0497 | F1Score=0.3937
Batch-100: CrossEntropyLoss=3.0408 | F1Score=0.3968
Batch-150: CrossEntropyLoss=3.0669 | F1Score=0.4028
Batch-200: CrossEntropyLoss=3.0465 | F1Score=0.4039
Batch-226: CrossEntropyLoss=3.0456 | F1Score=0.4036
Batch-26: CrossEntropyLoss=2.9575 | F1Score=0.4052

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0562 | Mean F1Score = 0.3976
Validation : Mean CrossEntropyLoss = 3.0517 | Mean F1Score = 0.4038
============================================================

EPOCH-5
Batch-50: CrossEntropyLoss=3.0610 | F1Score=0.4197
Batch-100: CrossEntropyLoss=3.0528 | F1Score=0.4174
Batch-150: CrossEntropyLoss=3.0590 | F1Score=0.4151
Batch-200: CrossEntropyLoss=3.0614 | F1Score=0.4135
Batch-226: CrossEntropyLoss=3.0110 | F1Score=0.4145
Batch-26: CrossEntropyLoss=3.0601 | F1Score=0.4161

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0556 | Mean F1Score = 0.4163
Validation : Mean CrossEntropyLoss = 3.0545 | Mean F1Score = 0.4154
============================================================

EPOCH-6
Batch-50: CrossEntropyLoss=3.0531 | F1Score=0.4172
Batch-100: CrossEntropyLoss=3.0392 | F1Score=0.4183
Batch-150: CrossEntropyLoss=3.0377 | F1Score=0.4195
Batch-200: CrossEntropyLoss=3.0452 | F1Score=0.4214
Batch-226: CrossEntropyLoss=3.0301 | F1Score=0.4198
Batch-26: CrossEntropyLoss=2.9381 | F1Score=0.4206

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0550 | Mean F1Score = 0.4175
Validation : Mean CrossEntropyLoss = 3.0500 | Mean F1Score = 0.4203
============================================================

EPOCH-7
Batch-50: CrossEntropyLoss=3.0582 | F1Score=0.4189
Batch-100: CrossEntropyLoss=3.0393 | F1Score=0.4253
Batch-150: CrossEntropyLoss=3.0629 | F1Score=0.4253
Batch-200: CrossEntropyLoss=3.0466 | F1Score=0.4238
Batch-226: CrossEntropyLoss=3.0146 | F1Score=0.4242
Batch-26: CrossEntropyLoss=2.9760 | F1Score=0.4252

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0546 | Mean F1Score = 0.4250
Validation : Mean CrossEntropyLoss = 3.0517 | Mean F1Score = 0.4252
============================================================

EPOCH-8
Batch-50: CrossEntropyLoss=3.0577 | F1Score=0.4232
Batch-100: CrossEntropyLoss=3.0579 | F1Score=0.4257
Batch-150: CrossEntropyLoss=3.0615 | F1Score=0.4276
Batch-200: CrossEntropyLoss=3.0620 | F1Score=0.4269
Batch-226: CrossEntropyLoss=3.0366 | F1Score=0.4270
Batch-26: CrossEntropyLoss=2.9932 | F1Score=0.4284

Huft üò•! Model not improved.
Training   : Mean CrossEntropyLoss = 3.0547 | Mean F1Score = 0.4233
Validation : Mean CrossEntropyLoss = 3.0513 | Mean F1Score = 0.4285
Patience = 1/100‚ùó
============================================================

EPOCH-9
Batch-50: CrossEntropyLoss=3.0610 | F1Score=0.4282
Batch-100: CrossEntropyLoss=3.0562 | F1Score=0.4330
Batch-150: CrossEntropyLoss=3.0263 | F1Score=0.4290
Batch-200: CrossEntropyLoss=3.0532 | F1Score=0.4302
Batch-226: CrossEntropyLoss=3.0555 | F1Score=0.4307
Batch-26: CrossEntropyLoss=3.0385 | F1Score=0.4320

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0544 | Mean F1Score = 0.4330
Validation : Mean CrossEntropyLoss = 3.0531 | Mean F1Score = 0.4321
============================================================

EPOCH-10
Batch-50: CrossEntropyLoss=3.0500 | F1Score=0.4452
Batch-100: CrossEntropyLoss=3.0555 | F1Score=0.4337
Batch-150: CrossEntropyLoss=3.0588 | F1Score=0.4347
Batch-200: CrossEntropyLoss=3.0423 | F1Score=0.4372
Batch-226: CrossEntropyLoss=3.0388 | F1Score=0.4373
Batch-26: CrossEntropyLoss=3.0087 | F1Score=0.4384

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0541 | Mean F1Score = 0.4375
Validation : Mean CrossEntropyLoss = 3.0517 | Mean F1Score = 0.4379
============================================================

EPOCH-11
Batch-50: CrossEntropyLoss=3.0465 | F1Score=0.4337
Batch-100: CrossEntropyLoss=3.0624 | F1Score=0.4377
Batch-150: CrossEntropyLoss=3.0668 | F1Score=0.4369
Batch-200: CrossEntropyLoss=3.0661 | F1Score=0.4376
Batch-226: CrossEntropyLoss=3.0420 | F1Score=0.4370
Batch-26: CrossEntropyLoss=3.0733 | F1Score=0.4374

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0538 | Mean F1Score = 0.4363
Validation : Mean CrossEntropyLoss = 3.0539 | Mean F1Score = 0.4375
============================================================

EPOCH-12
Batch-50: CrossEntropyLoss=3.0544 | F1Score=0.4356
Batch-100: CrossEntropyLoss=3.0428 | F1Score=0.4393
Batch-150: CrossEntropyLoss=3.0574 | F1Score=0.4441
Batch-200: CrossEntropyLoss=3.0606 | F1Score=0.4413
Batch-226: CrossEntropyLoss=3.0434 | F1Score=0.4429
Batch-26: CrossEntropyLoss=3.1100 | F1Score=0.4433

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0537 | Mean F1Score = 0.4422
Validation : Mean CrossEntropyLoss = 3.0563 | Mean F1Score = 0.4427
============================================================

EPOCH-13
Batch-50: CrossEntropyLoss=3.0517 | F1Score=0.4391
Batch-100: CrossEntropyLoss=3.0406 | F1Score=0.4456
Batch-150: CrossEntropyLoss=3.0613 | F1Score=0.4478
Batch-200: CrossEntropyLoss=3.0475 | F1Score=0.4468
Batch-226: CrossEntropyLoss=3.0451 | F1Score=0.4451
Batch-26: CrossEntropyLoss=3.0114 | F1Score=0.4463

Huft üò•! Model not improved.
Training   : Mean CrossEntropyLoss = 3.0538 | Mean F1Score = 0.4444
Validation : Mean CrossEntropyLoss = 3.0512 | Mean F1Score = 0.4454
Patience = 2/100‚ùó
============================================================

EPOCH-14
Batch-50: CrossEntropyLoss=3.0516 | F1Score=0.4541
Batch-100: CrossEntropyLoss=3.0504 | F1Score=0.4464
Batch-150: CrossEntropyLoss=3.0528 | F1Score=0.4493
Batch-200: CrossEntropyLoss=3.0569 | F1Score=0.4477
Batch-226: CrossEntropyLoss=3.0264 | F1Score=0.4484
Batch-26: CrossEntropyLoss=2.8775 | F1Score=0.4489

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0537 | Mean F1Score = 0.4507
Validation : Mean CrossEntropyLoss = 3.0464 | Mean F1Score = 0.4483
============================================================

EPOCH-15
Batch-50: CrossEntropyLoss=3.0549 | F1Score=0.4477
Batch-100: CrossEntropyLoss=3.0664 | F1Score=0.4506
Batch-150: CrossEntropyLoss=3.0423 | F1Score=0.4498
Batch-200: CrossEntropyLoss=3.0502 | F1Score=0.4487
Batch-226: CrossEntropyLoss=3.0364 | F1Score=0.4503
Batch-26: CrossEntropyLoss=3.0448 | F1Score=0.4516

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0535 | Mean F1Score = 0.4496
Validation : Mean CrossEntropyLoss = 3.0527 | Mean F1Score = 0.4516
============================================================

EPOCH-16
Batch-50: CrossEntropyLoss=3.0463 | F1Score=0.4460
Batch-100: CrossEntropyLoss=3.0631 | F1Score=0.4551
Batch-150: CrossEntropyLoss=3.0633 | F1Score=0.4570
Batch-200: CrossEntropyLoss=3.0525 | F1Score=0.4554
Batch-226: CrossEntropyLoss=3.0355 | F1Score=0.4557
Batch-26: CrossEntropyLoss=3.0641 | F1Score=0.4562

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0534 | Mean F1Score = 0.4534
Validation : Mean CrossEntropyLoss = 3.0541 | Mean F1Score = 0.4560
============================================================

EPOCH-17
Batch-50: CrossEntropyLoss=3.0484 | F1Score=0.4553
Batch-100: CrossEntropyLoss=3.0484 | F1Score=0.4544
Batch-150: CrossEntropyLoss=3.0467 | F1Score=0.4580
Batch-200: CrossEntropyLoss=3.0521 | F1Score=0.4568
Batch-226: CrossEntropyLoss=3.0142 | F1Score=0.4561
Batch-26: CrossEntropyLoss=3.0637 | F1Score=0.4567

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0533 | Mean F1Score = 0.4543
Validation : Mean CrossEntropyLoss = 3.0531 | Mean F1Score = 0.4565
============================================================

EPOCH-18
Batch-50: CrossEntropyLoss=3.0652 | F1Score=0.4529
Batch-100: CrossEntropyLoss=3.0453 | F1Score=0.4623
Batch-150: CrossEntropyLoss=3.0372 | F1Score=0.4610
Batch-200: CrossEntropyLoss=3.0547 | F1Score=0.4597
Batch-226: CrossEntropyLoss=3.0251 | F1Score=0.4593
Batch-26: CrossEntropyLoss=3.0184 | F1Score=0.4599

Huft üò•! Model not improved.
Training   : Mean CrossEntropyLoss = 3.0537 | Mean F1Score = 0.4580
Validation : Mean CrossEntropyLoss = 3.0512 | Mean F1Score = 0.4595
Patience = 3/100‚ùó
============================================================

EPOCH-19
Batch-50: CrossEntropyLoss=3.0469 | F1Score=0.4662
Batch-100: CrossEntropyLoss=3.0688 | F1Score=0.4658
Batch-150: CrossEntropyLoss=3.0395 | F1Score=0.4681
Batch-200: CrossEntropyLoss=3.0492 | F1Score=0.4651
Batch-226: CrossEntropyLoss=3.0271 | F1Score=0.4640
Batch-26: CrossEntropyLoss=2.9771 | F1Score=0.4653

Yeah üéâüòÑ! Model improved.
Training   : Mean CrossEntropyLoss = 3.0530 | Mean F1Score = 0.4694
Validation : Mean CrossEntropyLoss = 3.0506 | Mean F1Score = 0.4644
============================================================

EPOCH-20
Batch-50: CrossEntropyLoss=3.0556 | F1Score=0.4721
Batch-100: CrossEntropyLoss=3.0398 | F1Score=0.4671
Batch-150: CrossEntropyLoss=3.0471 | F1Score=0.4656
Batch-200: CrossEntropyLoss=3.0559 | F1Score=0.4636
Batch-226: CrossEntropyLoss=3.0361 | F1Score=0.4634
Batch-26: CrossEntropyLoss=2.9780 | F1Score=0.4634

Huft üò•! Model not improved.
Training   : Mean CrossEntropyLoss = 3.0534 | Mean F1Score = 0.4703
Validation : Mean CrossEntropyLoss = 3.0505 | Mean F1Score = 0.4638
Patience = 4/100‚ùó
============================================================


TRAINING SUMMARY
------------------------------------------------------------
Best training CrossEntropyLoss			: 3.0530
Best training F1Score			: 0.4694
Best validation CrossEntropyLoss			: 3.0506
Best validation F1Score			: 0.4644
Training duration		: 17.754 minutes.
Training date			: 2022-09-30 11:55:18.675214+08:00
