HYPERPARAMETERS
--------------------------------------------------
context_size: 35
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 20
device: cpu


TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50 : NLLLoss=5.3852 | F1Score=0.2651
Batch-100: NLLLoss=4.5988 | F1Score=0.2913
Batch-150: NLLLoss=5.0409 | F1Score=0.3124
Batch-200: NLLLoss=4.3506 | F1Score=0.3384
Batch-250: NLLLoss=4.7303 | F1Score=0.3537
Batch-300: NLLLoss=4.3363 | F1Score=0.3742
Batch-350: NLLLoss=4.3102 | F1Score=0.3885
Batch-400: NLLLoss=2.8921 | F1Score=0.4041
Batch-450: NLLLoss=3.3365 | F1Score=0.4195
Batch-500: NLLLoss=4.1867 | F1Score=0.4324
Batch-518: NLLLoss=2.7669 | F1Score=0.4375

Mean NLLLoss: 4.4845 | Mean F1Score: 0.3494
==================================================

EPOCH-2
Batch-50 : NLLLoss=2.6364 | F1Score=0.5875
Batch-100: NLLLoss=2.6804 | F1Score=0.5941
Batch-150: NLLLoss=2.5501 | F1Score=0.6008
Batch-200: NLLLoss=2.4848 | F1Score=0.6073
Batch-250: NLLLoss=1.6611 | F1Score=0.6176
Batch-300: NLLLoss=1.9355 | F1Score=0.6203
Batch-350: NLLLoss=2.2339 | F1Score=0.6240
Batch-400: NLLLoss=2.2928 | F1Score=0.6303
Batch-450: NLLLoss=2.6584 | F1Score=0.6385
Batch-500: NLLLoss=2.7978 | F1Score=0.6435
Batch-518: NLLLoss=2.1132 | F1Score=0.6443

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.6542 | Mean F1Score: 0.6158
==================================================

EPOCH-3
Batch-50 : NLLLoss=1.1659 | F1Score=0.7312
Batch-100: NLLLoss=1.5896 | F1Score=0.7359
Batch-150: NLLLoss=1.1244 | F1Score=0.7347
Batch-200: NLLLoss=1.6654 | F1Score=0.7369
Batch-250: NLLLoss=1.9047 | F1Score=0.7352
Batch-300: NLLLoss=1.6596 | F1Score=0.7406
Batch-350: NLLLoss=0.9404 | F1Score=0.7429
Batch-400: NLLLoss=0.7670 | F1Score=0.7441
Batch-450: NLLLoss=1.7406 | F1Score=0.7446
Batch-500: NLLLoss=0.6242 | F1Score=0.7469
Batch-518: NLLLoss=2.5978 | F1Score=0.7479

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7744 | Mean F1Score: 0.7391
==================================================

EPOCH-4
Batch-50 : NLLLoss=0.8688 | F1Score=0.8012
Batch-100: NLLLoss=1.3770 | F1Score=0.8043
Batch-150: NLLLoss=2.0271 | F1Score=0.8123
Batch-200: NLLLoss=0.8331 | F1Score=0.8120
Batch-250: NLLLoss=1.3462 | F1Score=0.8126
Batch-300: NLLLoss=1.1511 | F1Score=0.8127
Batch-350: NLLLoss=1.3272 | F1Score=0.8136
Batch-400: NLLLoss=1.4899 | F1Score=0.8141
Batch-450: NLLLoss=1.6573 | F1Score=0.8146
Batch-500: NLLLoss=1.7607 | F1Score=0.8146
Batch-518: NLLLoss=1.2251 | F1Score=0.8157

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1686 | Mean F1Score: 0.8102
==================================================

EPOCH-5
Batch-50 : NLLLoss=0.8499 | F1Score=0.8934
Batch-100: NLLLoss=0.8156 | F1Score=0.8828
Batch-150: NLLLoss=0.7894 | F1Score=0.8823
Batch-200: NLLLoss=0.4655 | F1Score=0.8786
Batch-250: NLLLoss=0.4575 | F1Score=0.8789
Batch-300: NLLLoss=0.4797 | F1Score=0.8763
Batch-350: NLLLoss=1.0806 | F1Score=0.8746
Batch-400: NLLLoss=0.5188 | F1Score=0.8740
Batch-450: NLLLoss=0.6794 | F1Score=0.8728
Batch-500: NLLLoss=0.2769 | F1Score=0.8721
Batch-518: NLLLoss=0.1767 | F1Score=0.8725

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.6968 | Mean F1Score: 0.8787
==================================================

EPOCH-6
Batch-50 : NLLLoss=0.2475 | F1Score=0.9572
Batch-100: NLLLoss=0.2252 | F1Score=0.9548
Batch-150: NLLLoss=0.3794 | F1Score=0.9528
Batch-200: NLLLoss=0.3035 | F1Score=0.9518
Batch-250: NLLLoss=0.2862 | F1Score=0.9496
Batch-300: NLLLoss=0.3266 | F1Score=0.9465
Batch-350: NLLLoss=0.1859 | F1Score=0.9440
Batch-400: NLLLoss=0.1124 | F1Score=0.9423
Batch-450: NLLLoss=0.2220 | F1Score=0.9403
Batch-500: NLLLoss=0.5185 | F1Score=0.9379
Batch-518: NLLLoss=0.2628 | F1Score=0.9368

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.3464 | Mean F1Score: 0.9488
==================================================

EPOCH-7
Batch-50 : NLLLoss=0.1222 | F1Score=0.9906
Batch-100: NLLLoss=0.0938 | F1Score=0.9900
Batch-150: NLLLoss=0.1024 | F1Score=0.9901
Batch-200: NLLLoss=0.1481 | F1Score=0.9899
Batch-250: NLLLoss=0.1014 | F1Score=0.9899
Batch-300: NLLLoss=0.0838 | F1Score=0.9898
Batch-350: NLLLoss=0.1345 | F1Score=0.9892
Batch-400: NLLLoss=0.0393 | F1Score=0.9888
Batch-450: NLLLoss=0.0655 | F1Score=0.9879
Batch-500: NLLLoss=0.1508 | F1Score=0.9873
Batch-518: NLLLoss=0.0197 | F1Score=0.9868

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.1202 | Mean F1Score: 0.9898
==================================================

EPOCH-8
Batch-50 : NLLLoss=0.0148 | F1Score=0.9987
Batch-100: NLLLoss=0.0114 | F1Score=0.9975
Batch-150: NLLLoss=0.0181 | F1Score=0.9977
Batch-200: NLLLoss=0.0102 | F1Score=0.9981
Batch-250: NLLLoss=0.0159 | F1Score=0.9983
Batch-300: NLLLoss=0.0540 | F1Score=0.9980
Batch-350: NLLLoss=0.0252 | F1Score=0.9981
Batch-400: NLLLoss=0.0075 | F1Score=0.9979
Batch-450: NLLLoss=0.0342 | F1Score=0.9979
Batch-500: NLLLoss=0.0161 | F1Score=0.9981
Batch-518: NLLLoss=0.0095 | F1Score=0.9981

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0352 | Mean F1Score: 0.9979
==================================================

EPOCH-9
Batch-50 : NLLLoss=0.0099 | F1Score=0.9987
Batch-100: NLLLoss=0.0105 | F1Score=0.9994
Batch-150: NLLLoss=0.0047 | F1Score=0.9994
Batch-200: NLLLoss=0.0095 | F1Score=0.9992
Batch-250: NLLLoss=0.0150 | F1Score=0.9990
Batch-300: NLLLoss=0.0102 | F1Score=0.9990
Batch-350: NLLLoss=0.0093 | F1Score=0.9989
Batch-400: NLLLoss=0.0223 | F1Score=0.9990
Batch-450: NLLLoss=0.0144 | F1Score=0.9991
Batch-500: NLLLoss=0.0217 | F1Score=0.9991
Batch-518: NLLLoss=0.0202 | F1Score=0.9992

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0128 | Mean F1Score: 0.9991
==================================================

EPOCH-10
Batch-50 : NLLLoss=0.0065 | F1Score=0.9994
Batch-100: NLLLoss=0.0408 | F1Score=0.9991
Batch-150: NLLLoss=0.0031 | F1Score=0.9994
Batch-200: NLLLoss=0.0062 | F1Score=0.9994
Batch-250: NLLLoss=0.0054 | F1Score=0.9995
Batch-300: NLLLoss=0.0081 | F1Score=0.9994
Batch-350: NLLLoss=0.0066 | F1Score=0.9994
Batch-400: NLLLoss=0.0081 | F1Score=0.9995
Batch-450: NLLLoss=0.0062 | F1Score=0.9995
Batch-500: NLLLoss=0.0022 | F1Score=0.9995
Batch-518: NLLLoss=0.0026 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0077 | Mean F1Score: 0.9994
==================================================

EPOCH-11
Batch-50 : NLLLoss=0.0044 | F1Score=0.9991
Batch-100: NLLLoss=0.0038 | F1Score=0.9994
Batch-150: NLLLoss=0.0027 | F1Score=0.9996
Batch-200: NLLLoss=0.0036 | F1Score=0.9997
Batch-250: NLLLoss=0.0061 | F1Score=0.9997
Batch-300: NLLLoss=0.0020 | F1Score=0.9998
Batch-350: NLLLoss=0.0031 | F1Score=0.9998
Batch-400: NLLLoss=0.0037 | F1Score=0.9998
Batch-450: NLLLoss=0.0041 | F1Score=0.9999
Batch-500: NLLLoss=0.0029 | F1Score=0.9998
Batch-518: NLLLoss=0.0036 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0043 | Mean F1Score: 0.9997
==================================================

EPOCH-12
Batch-50 : NLLLoss=0.0044 | F1Score=1.0000
Batch-100: NLLLoss=0.0037 | F1Score=1.0000
Batch-150: NLLLoss=0.0039 | F1Score=1.0000
Batch-200: NLLLoss=0.0043 | F1Score=1.0000
Batch-250: NLLLoss=0.0032 | F1Score=0.9999
Batch-300: NLLLoss=0.0014 | F1Score=0.9999
Batch-350: NLLLoss=0.0046 | F1Score=0.9999
Batch-400: NLLLoss=0.0018 | F1Score=0.9999
Batch-450: NLLLoss=0.0021 | F1Score=0.9998
Batch-500: NLLLoss=0.0021 | F1Score=0.9998
Batch-518: NLLLoss=0.0031 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0032 | Mean F1Score: 0.9999
==================================================

EPOCH-13
Batch-50 : NLLLoss=0.0015 | F1Score=1.0000
Batch-100: NLLLoss=0.0026 | F1Score=1.0000
Batch-150: NLLLoss=0.0031 | F1Score=0.9999
Batch-200: NLLLoss=0.0032 | F1Score=0.9995
Batch-250: NLLLoss=0.5350 | F1Score=0.9941
Batch-300: NLLLoss=0.2287 | F1Score=0.9773
Batch-350: NLLLoss=0.5054 | F1Score=0.9635
Batch-400: NLLLoss=0.6437 | F1Score=0.9563
Batch-450: NLLLoss=0.3584 | F1Score=0.9522
Batch-500: NLLLoss=0.1354 | F1Score=0.9503
Batch-518: NLLLoss=0.0535 | F1Score=0.9502

Huft üò•! Model not improved.
Mean NLLLoss: 0.2123 | Mean F1Score: 0.9806
Patience = 1/20‚ùó
==================================================

EPOCH-14
Batch-50 : NLLLoss=0.0551 | F1Score=0.9725
Batch-100: NLLLoss=0.0816 | F1Score=0.9756
Batch-150: NLLLoss=0.0799 | F1Score=0.9760
Batch-200: NLLLoss=0.0156 | F1Score=0.9765
Batch-250: NLLLoss=0.0826 | F1Score=0.9777
Batch-300: NLLLoss=0.0870 | F1Score=0.9792
Batch-350: NLLLoss=0.2121 | F1Score=0.9788
Batch-400: NLLLoss=0.0709 | F1Score=0.9795
Batch-450: NLLLoss=0.1431 | F1Score=0.9794
Batch-500: NLLLoss=0.0297 | F1Score=0.9797
Batch-518: NLLLoss=0.0150 | F1Score=0.9800

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0903 | Mean F1Score: 0.9771
==================================================

EPOCH-15
Batch-50 : NLLLoss=0.0101 | F1Score=0.9975
Batch-100: NLLLoss=0.0186 | F1Score=0.9980
Batch-150: NLLLoss=0.0014 | F1Score=0.9984
Batch-200: NLLLoss=0.0020 | F1Score=0.9988
Batch-250: NLLLoss=0.0133 | F1Score=0.9991
Batch-300: NLLLoss=0.0132 | F1Score=0.9991
Batch-350: NLLLoss=0.0253 | F1Score=0.9992
Batch-400: NLLLoss=0.0060 | F1Score=0.9992
Batch-450: NLLLoss=0.0103 | F1Score=0.9992
Batch-500: NLLLoss=0.0063 | F1Score=0.9992
Batch-518: NLLLoss=0.0015 | F1Score=0.9993

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0073 | Mean F1Score: 0.9986
==================================================

EPOCH-16
Batch-50 : NLLLoss=0.0021 | F1Score=1.0000
Batch-100: NLLLoss=0.0026 | F1Score=1.0000
Batch-150: NLLLoss=0.0019 | F1Score=1.0000
Batch-200: NLLLoss=0.0006 | F1Score=1.0000
Batch-250: NLLLoss=0.0023 | F1Score=1.0000
Batch-300: NLLLoss=0.0013 | F1Score=0.9999
Batch-350: NLLLoss=0.0016 | F1Score=1.0000
Batch-400: NLLLoss=0.0012 | F1Score=0.9999
Batch-450: NLLLoss=0.0011 | F1Score=0.9999
Batch-500: NLLLoss=0.0017 | F1Score=0.9999
Batch-518: NLLLoss=0.0015 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0017 | Mean F1Score: 1.0000
==================================================

EPOCH-17
Batch-50 : NLLLoss=0.0014 | F1Score=1.0000
Batch-100: NLLLoss=0.0013 | F1Score=1.0000
Batch-150: NLLLoss=0.0010 | F1Score=1.0000
Batch-200: NLLLoss=0.0014 | F1Score=1.0000
Batch-250: NLLLoss=0.0004 | F1Score=1.0000
Batch-300: NLLLoss=0.0007 | F1Score=0.9999
Batch-350: NLLLoss=0.0014 | F1Score=0.9999
Batch-400: NLLLoss=0.0015 | F1Score=0.9999
Batch-450: NLLLoss=0.0010 | F1Score=0.9999
Batch-500: NLLLoss=0.0009 | F1Score=0.9999
Batch-518: NLLLoss=0.0009 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0011 | Mean F1Score: 1.0000
==================================================

EPOCH-18
Batch-50 : NLLLoss=0.0009 | F1Score=1.0000
Batch-100: NLLLoss=0.0009 | F1Score=1.0000
Batch-150: NLLLoss=0.0008 | F1Score=0.9999
Batch-200: NLLLoss=0.0008 | F1Score=0.9999
Batch-250: NLLLoss=0.0005 | F1Score=0.9999
Batch-300: NLLLoss=0.0005 | F1Score=0.9999
Batch-350: NLLLoss=0.0008 | F1Score=1.0000
Batch-400: NLLLoss=0.0013 | F1Score=0.9999
Batch-450: NLLLoss=0.0015 | F1Score=0.9999
Batch-500: NLLLoss=0.0008 | F1Score=0.9999
Batch-518: NLLLoss=0.0009 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0009 | Mean F1Score: 0.9999
==================================================

EPOCH-19
Batch-50 : NLLLoss=0.0006 | F1Score=1.0000
Batch-100: NLLLoss=0.0008 | F1Score=0.9998
Batch-150: NLLLoss=0.0006 | F1Score=0.9999
Batch-200: NLLLoss=0.0008 | F1Score=0.9999
Batch-250: NLLLoss=0.0007 | F1Score=0.9999
Batch-300: NLLLoss=0.0009 | F1Score=0.9999
Batch-350: NLLLoss=0.0004 | F1Score=1.0000
Batch-400: NLLLoss=0.0009 | F1Score=1.0000
Batch-450: NLLLoss=0.0007 | F1Score=1.0000
Batch-500: NLLLoss=0.0009 | F1Score=0.9999
Batch-518: NLLLoss=0.0004 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0007 | Mean F1Score: 0.9999
==================================================

EPOCH-20
Batch-50 : NLLLoss=0.0005 | F1Score=1.0000
Batch-100: NLLLoss=0.0004 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=1.0000
Batch-200: NLLLoss=0.0007 | F1Score=1.0000
Batch-250: NLLLoss=0.0006 | F1Score=0.9999
Batch-300: NLLLoss=0.0005 | F1Score=0.9999
Batch-350: NLLLoss=0.0007 | F1Score=0.9999
Batch-400: NLLLoss=0.0008 | F1Score=0.9999
Batch-450: NLLLoss=0.0006 | F1Score=0.9999
Batch-500: NLLLoss=0.0005 | F1Score=0.9999
Batch-518: NLLLoss=0.0005 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0006 | Mean F1Score: 1.0000
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss      : 0.0006
Best F1Score      : 1.0000
Training duration : 23.800 minutes.
Training date     : 2022-10-11 13:22:21.935699+08:00
