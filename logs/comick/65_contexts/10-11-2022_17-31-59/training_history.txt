HYPERPARAMETERS
--------------------------------------------------
context_size: 65
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 20
device: cpu


TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50 : NLLLoss=6.1076 | F1Score=0.2700
Batch-100: NLLLoss=4.8941 | F1Score=0.2944
Batch-150: NLLLoss=4.5386 | F1Score=0.3088
Batch-200: NLLLoss=4.8116 | F1Score=0.3375
Batch-250: NLLLoss=4.5884 | F1Score=0.3567
Batch-300: NLLLoss=4.1050 | F1Score=0.3744
Batch-350: NLLLoss=4.6339 | F1Score=0.3897
Batch-400: NLLLoss=3.8761 | F1Score=0.4051
Batch-450: NLLLoss=4.0983 | F1Score=0.4165
Batch-500: NLLLoss=3.8109 | F1Score=0.4310
Batch-518: NLLLoss=3.0280 | F1Score=0.4360

Mean NLLLoss: 4.5275 | Mean F1Score: 0.3509
==================================================

EPOCH-2
Batch-50 : NLLLoss=3.3666 | F1Score=0.5813
Batch-100: NLLLoss=3.2216 | F1Score=0.5903
Batch-150: NLLLoss=1.7423 | F1Score=0.5896
Batch-200: NLLLoss=3.5033 | F1Score=0.6033
Batch-250: NLLLoss=2.4334 | F1Score=0.6048
Batch-300: NLLLoss=2.5413 | F1Score=0.6106
Batch-350: NLLLoss=2.8187 | F1Score=0.6164
Batch-400: NLLLoss=3.0958 | F1Score=0.6233
Batch-450: NLLLoss=2.7586 | F1Score=0.6293
Batch-500: NLLLoss=2.6579 | F1Score=0.6344
Batch-518: NLLLoss=1.3555 | F1Score=0.6368

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.7246 | Mean F1Score: 0.6096
==================================================

EPOCH-3
Batch-50 : NLLLoss=2.1034 | F1Score=0.7294
Batch-100: NLLLoss=1.9588 | F1Score=0.7404
Batch-150: NLLLoss=1.8145 | F1Score=0.7342
Batch-200: NLLLoss=2.0065 | F1Score=0.7319
Batch-250: NLLLoss=2.4355 | F1Score=0.7327
Batch-300: NLLLoss=1.2409 | F1Score=0.7339
Batch-350: NLLLoss=1.0709 | F1Score=0.7394
Batch-400: NLLLoss=2.3279 | F1Score=0.7394
Batch-450: NLLLoss=1.1376 | F1Score=0.7421
Batch-500: NLLLoss=1.8864 | F1Score=0.7429
Batch-518: NLLLoss=1.3983 | F1Score=0.7439

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7986 | Mean F1Score: 0.7378
==================================================

EPOCH-4
Batch-50 : NLLLoss=1.2491 | F1Score=0.8256
Batch-100: NLLLoss=1.1199 | F1Score=0.8176
Batch-150: NLLLoss=1.0702 | F1Score=0.8152
Batch-200: NLLLoss=1.0866 | F1Score=0.8168
Batch-250: NLLLoss=0.8033 | F1Score=0.8149
Batch-300: NLLLoss=1.2970 | F1Score=0.8150
Batch-350: NLLLoss=1.5853 | F1Score=0.8161
Batch-400: NLLLoss=1.3399 | F1Score=0.8165
Batch-450: NLLLoss=2.1013 | F1Score=0.8164
Batch-500: NLLLoss=1.6067 | F1Score=0.8168
Batch-518: NLLLoss=1.0995 | F1Score=0.8172

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1725 | Mean F1Score: 0.8178
==================================================

EPOCH-5
Batch-50 : NLLLoss=0.5284 | F1Score=0.9059
Batch-100: NLLLoss=0.7162 | F1Score=0.8858
Batch-150: NLLLoss=0.7081 | F1Score=0.8778
Batch-200: NLLLoss=0.8127 | F1Score=0.8741
Batch-250: NLLLoss=0.8391 | F1Score=0.8738
Batch-300: NLLLoss=0.6320 | F1Score=0.8728
Batch-350: NLLLoss=0.6851 | F1Score=0.8724
Batch-400: NLLLoss=0.9056 | F1Score=0.8721
Batch-450: NLLLoss=0.6368 | F1Score=0.8730
Batch-500: NLLLoss=0.5950 | F1Score=0.8721
Batch-518: NLLLoss=1.2556 | F1Score=0.8722

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.7067 | Mean F1Score: 0.8789
==================================================

EPOCH-6
Batch-50 : NLLLoss=0.2107 | F1Score=0.9415
Batch-100: NLLLoss=0.2875 | F1Score=0.9461
Batch-150: NLLLoss=0.2298 | F1Score=0.9486
Batch-200: NLLLoss=0.1406 | F1Score=0.9473
Batch-250: NLLLoss=0.4442 | F1Score=0.9439
Batch-300: NLLLoss=0.4598 | F1Score=0.9426
Batch-350: NLLLoss=0.5337 | F1Score=0.9383
Batch-400: NLLLoss=0.3132 | F1Score=0.9364
Batch-450: NLLLoss=0.4146 | F1Score=0.9351
Batch-500: NLLLoss=0.4400 | F1Score=0.9340
Batch-518: NLLLoss=0.2792 | F1Score=0.9337

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.3538 | Mean F1Score: 0.9417
==================================================

EPOCH-7
Batch-50 : NLLLoss=0.1445 | F1Score=0.9878
Batch-100: NLLLoss=0.0980 | F1Score=0.9886
Batch-150: NLLLoss=0.1426 | F1Score=0.9897
Batch-200: NLLLoss=0.1812 | F1Score=0.9895
Batch-250: NLLLoss=0.0993 | F1Score=0.9889
Batch-300: NLLLoss=0.1124 | F1Score=0.9889
Batch-350: NLLLoss=0.1030 | F1Score=0.9879
Batch-400: NLLLoss=0.0482 | F1Score=0.9876
Batch-450: NLLLoss=0.2992 | F1Score=0.9869
Batch-500: NLLLoss=0.1268 | F1Score=0.9859
Batch-518: NLLLoss=0.1334 | F1Score=0.9855

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.1231 | Mean F1Score: 0.9880
==================================================

EPOCH-8
Batch-50 : NLLLoss=0.0276 | F1Score=0.9981
Batch-100: NLLLoss=0.0264 | F1Score=0.9983
Batch-150: NLLLoss=0.0242 | F1Score=0.9984
Batch-200: NLLLoss=0.0324 | F1Score=0.9979
Batch-250: NLLLoss=0.0179 | F1Score=0.9982
Batch-300: NLLLoss=0.0358 | F1Score=0.9983
Batch-350: NLLLoss=0.0215 | F1Score=0.9982
Batch-400: NLLLoss=0.0237 | F1Score=0.9982
Batch-450: NLLLoss=0.0248 | F1Score=0.9983
Batch-500: NLLLoss=0.0213 | F1Score=0.9983
Batch-518: NLLLoss=0.0172 | F1Score=0.9983

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0347 | Mean F1Score: 0.9981
==================================================

EPOCH-9
Batch-50 : NLLLoss=0.0135 | F1Score=0.9987
Batch-100: NLLLoss=0.0092 | F1Score=0.9994
Batch-150: NLLLoss=0.0084 | F1Score=0.9994
Batch-200: NLLLoss=0.0066 | F1Score=0.9995
Batch-250: NLLLoss=0.0065 | F1Score=0.9996
Batch-300: NLLLoss=0.0099 | F1Score=0.9997
Batch-350: NLLLoss=0.0101 | F1Score=0.9996
Batch-400: NLLLoss=0.0091 | F1Score=0.9995
Batch-450: NLLLoss=0.0025 | F1Score=0.9995
Batch-500: NLLLoss=0.0110 | F1Score=0.9995
Batch-518: NLLLoss=0.0100 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0118 | Mean F1Score: 0.9993
==================================================

EPOCH-10
Batch-50 : NLLLoss=0.0055 | F1Score=0.9987
Batch-100: NLLLoss=0.0027 | F1Score=0.9994
Batch-150: NLLLoss=0.0044 | F1Score=0.9996
Batch-200: NLLLoss=0.0079 | F1Score=0.9997
Batch-250: NLLLoss=0.0035 | F1Score=0.9998
Batch-300: NLLLoss=0.0043 | F1Score=0.9996
Batch-350: NLLLoss=0.0039 | F1Score=0.9996
Batch-400: NLLLoss=0.0049 | F1Score=0.9996
Batch-450: NLLLoss=0.0056 | F1Score=0.9996
Batch-500: NLLLoss=0.0066 | F1Score=0.9996
Batch-518: NLLLoss=0.0035 | F1Score=0.9996

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0074 | Mean F1Score: 0.9995
==================================================

EPOCH-11
Batch-50 : NLLLoss=0.0047 | F1Score=0.9987
Batch-100: NLLLoss=0.0019 | F1Score=0.9987
Batch-150: NLLLoss=0.1295 | F1Score=0.9975
Batch-200: NLLLoss=0.1854 | F1Score=0.9930
Batch-250: NLLLoss=0.1770 | F1Score=0.9844
Batch-300: NLLLoss=0.1870 | F1Score=0.9794
Batch-350: NLLLoss=0.5205 | F1Score=0.9747
Batch-400: NLLLoss=0.0427 | F1Score=0.9730
Batch-450: NLLLoss=0.2306 | F1Score=0.9716
Batch-500: NLLLoss=0.0850 | F1Score=0.9717
Batch-518: NLLLoss=0.0828 | F1Score=0.9719

Huft üò•! Model not improved.
Mean NLLLoss: 0.1250 | Mean F1Score: 0.9852
Patience = 1/20‚ùó
==================================================

EPOCH-12
Batch-50 : NLLLoss=0.0324 | F1Score=0.9922
Batch-100: NLLLoss=0.0537 | F1Score=0.9933
Batch-150: NLLLoss=0.0106 | F1Score=0.9937
Batch-200: NLLLoss=0.0107 | F1Score=0.9939
Batch-250: NLLLoss=0.0854 | F1Score=0.9939
Batch-300: NLLLoss=0.0322 | F1Score=0.9942
Batch-350: NLLLoss=0.1441 | F1Score=0.9941
Batch-400: NLLLoss=0.0252 | F1Score=0.9941
Batch-450: NLLLoss=0.0055 | F1Score=0.9944
Batch-500: NLLLoss=0.0026 | F1Score=0.9947
Batch-518: NLLLoss=0.0100 | F1Score=0.9947

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0395 | Mean F1Score: 0.9933
==================================================

EPOCH-13
Batch-50 : NLLLoss=0.0040 | F1Score=0.9987
Batch-100: NLLLoss=0.0036 | F1Score=0.9987
Batch-150: NLLLoss=0.0028 | F1Score=0.9992
Batch-200: NLLLoss=0.0093 | F1Score=0.9986
Batch-250: NLLLoss=0.0028 | F1Score=0.9989
Batch-300: NLLLoss=0.0022 | F1Score=0.9990
Batch-350: NLLLoss=0.0287 | F1Score=0.9985
Batch-400: NLLLoss=0.0082 | F1Score=0.9982
Batch-450: NLLLoss=0.0090 | F1Score=0.9981
Batch-500: NLLLoss=0.0060 | F1Score=0.9982
Batch-518: NLLLoss=0.0170 | F1Score=0.9983

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0114 | Mean F1Score: 0.9986
==================================================

EPOCH-14
Batch-50 : NLLLoss=0.0014 | F1Score=0.9994
Batch-100: NLLLoss=0.0019 | F1Score=0.9989
Batch-150: NLLLoss=0.0163 | F1Score=0.9991
Batch-200: NLLLoss=0.0037 | F1Score=0.9993
Batch-250: NLLLoss=0.0016 | F1Score=0.9994
Batch-300: NLLLoss=0.0031 | F1Score=0.9991
Batch-350: NLLLoss=0.0011 | F1Score=0.9992
Batch-400: NLLLoss=0.0024 | F1Score=0.9992
Batch-450: NLLLoss=0.0024 | F1Score=0.9992
Batch-500: NLLLoss=0.0024 | F1Score=0.9993
Batch-518: NLLLoss=0.0046 | F1Score=0.9993

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0058 | Mean F1Score: 0.9992
==================================================

EPOCH-15
Batch-50 : NLLLoss=0.0018 | F1Score=0.9994
Batch-100: NLLLoss=0.0018 | F1Score=0.9997
Batch-150: NLLLoss=0.0002 | F1Score=0.9996
Batch-200: NLLLoss=0.0009 | F1Score=0.9995
Batch-250: NLLLoss=0.0012 | F1Score=0.9996
Batch-300: NLLLoss=0.0012 | F1Score=0.9996
Batch-350: NLLLoss=0.0006 | F1Score=0.9997
Batch-400: NLLLoss=0.0016 | F1Score=0.9996
Batch-450: NLLLoss=0.0010 | F1Score=0.9996
Batch-500: NLLLoss=0.0018 | F1Score=0.9996
Batch-518: NLLLoss=0.0005 | F1Score=0.9996

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0028 | Mean F1Score: 0.9995
==================================================

EPOCH-16
Batch-50 : NLLLoss=0.0006 | F1Score=1.0000
Batch-100: NLLLoss=0.0014 | F1Score=0.9994
Batch-150: NLLLoss=0.0010 | F1Score=0.9995
Batch-200: NLLLoss=0.0009 | F1Score=0.9996
Batch-250: NLLLoss=0.0015 | F1Score=0.9997
Batch-300: NLLLoss=0.0010 | F1Score=0.9997
Batch-350: NLLLoss=0.0004 | F1Score=0.9998
Batch-400: NLLLoss=0.0018 | F1Score=0.9997
Batch-450: NLLLoss=0.0037 | F1Score=0.9998
Batch-500: NLLLoss=0.0012 | F1Score=0.9998
Batch-518: NLLLoss=0.0009 | F1Score=0.9998

Huft üò•! Model not improved.
Mean NLLLoss: 0.0028 | Mean F1Score: 0.9997
Patience = 2/20‚ùó
==================================================

EPOCH-17
Batch-50 : NLLLoss=0.0008 | F1Score=1.0000
Batch-100: NLLLoss=0.0010 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=1.0000
Batch-200: NLLLoss=0.0044 | F1Score=0.9998
Batch-250: NLLLoss=0.0005 | F1Score=0.9999
Batch-300: NLLLoss=0.0008 | F1Score=0.9998
Batch-350: NLLLoss=0.0010 | F1Score=0.9999
Batch-400: NLLLoss=0.0007 | F1Score=0.9999
Batch-450: NLLLoss=0.0006 | F1Score=0.9999
Batch-500: NLLLoss=0.0009 | F1Score=0.9999
Batch-518: NLLLoss=0.0002 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0016 | Mean F1Score: 0.9999
==================================================

EPOCH-18
Batch-50 : NLLLoss=0.0005 | F1Score=1.0000
Batch-100: NLLLoss=0.0005 | F1Score=0.9998
Batch-150: NLLLoss=0.0006 | F1Score=0.9999
Batch-200: NLLLoss=0.0005 | F1Score=0.9998
Batch-250: NLLLoss=0.0005 | F1Score=0.9999
Batch-300: NLLLoss=0.0007 | F1Score=0.9999
Batch-350: NLLLoss=0.0009 | F1Score=0.9999
Batch-400: NLLLoss=0.0013 | F1Score=0.9998
Batch-450: NLLLoss=0.0016 | F1Score=0.9998
Batch-500: NLLLoss=0.0191 | F1Score=0.9996
Batch-518: NLLLoss=0.0136 | F1Score=0.9995

Huft üò•! Model not improved.
Mean NLLLoss: 0.0045 | Mean F1Score: 0.9999
Patience = 3/20‚ùó
==================================================

EPOCH-19
Batch-50 : NLLLoss=0.0858 | F1Score=0.9937
Batch-100: NLLLoss=0.2885 | F1Score=0.9806
Batch-150: NLLLoss=0.1943 | F1Score=0.9563
Batch-200: NLLLoss=0.1169 | F1Score=0.9438
Batch-250: NLLLoss=0.5961 | F1Score=0.9392
Batch-300: NLLLoss=0.1467 | F1Score=0.9401
Batch-350: NLLLoss=0.1566 | F1Score=0.9416
Batch-400: NLLLoss=0.2437 | F1Score=0.9437
Batch-450: NLLLoss=0.1068 | F1Score=0.9465
Batch-500: NLLLoss=0.0317 | F1Score=0.9488
Batch-518: NLLLoss=0.2553 | F1Score=0.9496

Huft üò•! Model not improved.
Mean NLLLoss: 0.1928 | Mean F1Score: 0.9553
Patience = 4/20‚ùó
==================================================

EPOCH-20
Batch-50 : NLLLoss=0.0285 | F1Score=0.9925
Batch-100: NLLLoss=0.0046 | F1Score=0.9944
Batch-150: NLLLoss=0.0092 | F1Score=0.9948
Batch-200: NLLLoss=0.0882 | F1Score=0.9950
Batch-250: NLLLoss=0.0115 | F1Score=0.9954
Batch-300: NLLLoss=0.0133 | F1Score=0.9957
Batch-350: NLLLoss=0.0028 | F1Score=0.9961
Batch-400: NLLLoss=0.0019 | F1Score=0.9963
Batch-450: NLLLoss=0.0150 | F1Score=0.9964
Batch-500: NLLLoss=0.0076 | F1Score=0.9965
Batch-518: NLLLoss=0.0018 | F1Score=0.9965

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0183 | Mean F1Score: 0.9950
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss      : 0.0183
Best F1Score      : 0.9950
Training duration : 30.621 minutes.
Training date     : 2022-10-11 17:31:59.987034+08:00
