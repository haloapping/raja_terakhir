HYPERPARAMETERS
--------------------------------------------------
context_size: 5
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 3
device: cpu

TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50: NLLLoss=6.5263 | F1Score=0.2837
Batch-100: NLLLoss=5.4040 | F1Score=0.3059
Batch-150: NLLLoss=5.7614 | F1Score=0.3200
Batch-200: NLLLoss=5.5270 | F1Score=0.3397
Batch-250: NLLLoss=4.6353 | F1Score=0.3565
Batch-300: NLLLoss=4.0126 | F1Score=0.3759
Batch-350: NLLLoss=1.9967 | F1Score=0.3882
Batch-400: NLLLoss=3.6330 | F1Score=0.4013
Batch-450: NLLLoss=2.4446 | F1Score=0.4154
Batch-500: NLLLoss=2.3087 | F1Score=0.4290
Batch-518: NLLLoss=3.5344 | F1Score=0.4329

Mean NLLLoss: 4.5342 | Mean F1Score: 0.3539
==================================================

EPOCH-2
Batch-50: NLLLoss=3.3760 | F1Score=0.5744
Batch-100: NLLLoss=3.3712 | F1Score=0.5866
Batch-150: NLLLoss=3.4926 | F1Score=0.5902
Batch-200: NLLLoss=4.2378 | F1Score=0.5903
Batch-250: NLLLoss=2.5979 | F1Score=0.5981
Batch-300: NLLLoss=2.9041 | F1Score=0.6070
Batch-350: NLLLoss=2.9711 | F1Score=0.6155
Batch-400: NLLLoss=3.0327 | F1Score=0.6228
Batch-450: NLLLoss=3.2152 | F1Score=0.6290
Batch-500: NLLLoss=2.1808 | F1Score=0.6354
Batch-518: NLLLoss=3.0474 | F1Score=0.6373

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.7087 | Mean F1Score: 0.6046
==================================================

EPOCH-3
Batch-50: NLLLoss=1.3988 | F1Score=0.7131
Batch-100: NLLLoss=2.4824 | F1Score=0.7262
Batch-150: NLLLoss=1.5049 | F1Score=0.7304
Batch-200: NLLLoss=0.9896 | F1Score=0.7367
Batch-250: NLLLoss=1.6257 | F1Score=0.7415
Batch-300: NLLLoss=1.3798 | F1Score=0.7414
Batch-350: NLLLoss=1.6527 | F1Score=0.7439
Batch-400: NLLLoss=1.4147 | F1Score=0.7462
Batch-450: NLLLoss=1.3446 | F1Score=0.7473
Batch-500: NLLLoss=0.6732 | F1Score=0.7497
Batch-518: NLLLoss=1.3963 | F1Score=0.7494

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7533 | Mean F1Score: 0.7363
==================================================

EPOCH-4
Batch-50: NLLLoss=0.9104 | F1Score=0.8190
Batch-100: NLLLoss=0.9707 | F1Score=0.8158
Batch-150: NLLLoss=1.3226 | F1Score=0.8190
Batch-200: NLLLoss=1.4440 | F1Score=0.8198
Batch-250: NLLLoss=1.7784 | F1Score=0.8164
Batch-300: NLLLoss=0.8779 | F1Score=0.8171
Batch-350: NLLLoss=0.7761 | F1Score=0.8178
Batch-400: NLLLoss=0.9556 | F1Score=0.8181
Batch-450: NLLLoss=0.7659 | F1Score=0.8192
Batch-500: NLLLoss=1.1638 | F1Score=0.8197
Batch-518: NLLLoss=0.2249 | F1Score=0.8217

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1020 | Mean F1Score: 0.8186
==================================================

EPOCH-5
Batch-50: NLLLoss=0.7149 | F1Score=0.9025
Batch-100: NLLLoss=0.5599 | F1Score=0.8997
Batch-150: NLLLoss=0.6685 | F1Score=0.8974
Batch-200: NLLLoss=0.8269 | F1Score=0.8971
Batch-250: NLLLoss=0.8656 | F1Score=0.8949
Batch-300: NLLLoss=0.8305 | F1Score=0.8930
Batch-350: NLLLoss=0.7230 | F1Score=0.8925
Batch-400: NLLLoss=0.6661 | F1Score=0.8919
Batch-450: NLLLoss=0.8265 | F1Score=0.8912
Batch-500: NLLLoss=0.6827 | F1Score=0.8900
Batch-518: NLLLoss=0.4753 | F1Score=0.8894

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.6045 | Mean F1Score: 0.8953
==================================================

EPOCH-6
Batch-50: NLLLoss=0.2169 | F1Score=0.9712
Batch-100: NLLLoss=0.3109 | F1Score=0.9712
Batch-150: NLLLoss=0.1810 | F1Score=0.9733
Batch-200: NLLLoss=0.0728 | F1Score=0.9714
Batch-250: NLLLoss=0.2298 | F1Score=0.9696
Batch-300: NLLLoss=0.3358 | F1Score=0.9690
Batch-350: NLLLoss=0.5092 | F1Score=0.9650
Batch-400: NLLLoss=0.2589 | F1Score=0.9638
Batch-450: NLLLoss=0.3284 | F1Score=0.9637
Batch-500: NLLLoss=0.1101 | F1Score=0.9627
Batch-518: NLLLoss=0.2564 | F1Score=0.9625

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.2407 | Mean F1Score: 0.9680
==================================================

EPOCH-7
Batch-50: NLLLoss=0.0489 | F1Score=0.9931
Batch-100: NLLLoss=0.0175 | F1Score=0.9955
Batch-150: NLLLoss=0.0430 | F1Score=0.9955
Batch-200: NLLLoss=0.0340 | F1Score=0.9957
Batch-250: NLLLoss=0.1616 | F1Score=0.9948
Batch-300: NLLLoss=0.0539 | F1Score=0.9942
Batch-350: NLLLoss=0.0560 | F1Score=0.9936
Batch-400: NLLLoss=0.0568 | F1Score=0.9935
Batch-450: NLLLoss=0.0197 | F1Score=0.9937
Batch-500: NLLLoss=0.0463 | F1Score=0.9938
Batch-518: NLLLoss=0.0196 | F1Score=0.9937

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0683 | Mean F1Score: 0.9944
==================================================

EPOCH-8
Batch-50: NLLLoss=0.2802 | F1Score=0.9975
Batch-100: NLLLoss=0.0369 | F1Score=0.9978
Batch-150: NLLLoss=0.0095 | F1Score=0.9983
Batch-200: NLLLoss=0.0117 | F1Score=0.9984
Batch-250: NLLLoss=0.0205 | F1Score=0.9986
Batch-300: NLLLoss=0.0122 | F1Score=0.9989
Batch-350: NLLLoss=0.0098 | F1Score=0.9987
Batch-400: NLLLoss=0.0131 | F1Score=0.9986
Batch-450: NLLLoss=0.0171 | F1Score=0.9985
Batch-500: NLLLoss=0.0168 | F1Score=0.9984
Batch-518: NLLLoss=0.0381 | F1Score=0.9985

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0215 | Mean F1Score: 0.9985
==================================================

EPOCH-9
Batch-50: NLLLoss=0.0057 | F1Score=1.0000
Batch-100: NLLLoss=0.0062 | F1Score=1.0000
Batch-150: NLLLoss=0.0069 | F1Score=0.9998
Batch-200: NLLLoss=0.0060 | F1Score=0.9995
Batch-250: NLLLoss=0.0087 | F1Score=0.9994
Batch-300: NLLLoss=0.0089 | F1Score=0.9994
Batch-350: NLLLoss=0.0054 | F1Score=0.9994
Batch-400: NLLLoss=0.0040 | F1Score=0.9994
Batch-450: NLLLoss=0.0034 | F1Score=0.9994
Batch-500: NLLLoss=0.0057 | F1Score=0.9995
Batch-518: NLLLoss=0.0079 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0090 | Mean F1Score: 0.9996
==================================================

EPOCH-10
Batch-50: NLLLoss=0.0029 | F1Score=1.0000
Batch-100: NLLLoss=0.0026 | F1Score=0.9998
Batch-150: NLLLoss=0.0034 | F1Score=0.9993
Batch-200: NLLLoss=0.0050 | F1Score=0.9995
Batch-250: NLLLoss=0.0059 | F1Score=0.9993
Batch-300: NLLLoss=0.0063 | F1Score=0.9993
Batch-350: NLLLoss=0.0074 | F1Score=0.9994
Batch-400: NLLLoss=0.0032 | F1Score=0.9994
Batch-450: NLLLoss=0.0043 | F1Score=0.9994
Batch-500: NLLLoss=0.0036 | F1Score=0.9994
Batch-518: NLLLoss=0.0048 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0068 | Mean F1Score: 0.9995
==================================================

EPOCH-11
Batch-50: NLLLoss=0.0028 | F1Score=0.9994
Batch-100: NLLLoss=0.0027 | F1Score=0.9997
Batch-150: NLLLoss=0.0030 | F1Score=0.9994
Batch-200: NLLLoss=0.0035 | F1Score=0.9995
Batch-250: NLLLoss=0.0025 | F1Score=0.9996
Batch-300: NLLLoss=0.0025 | F1Score=0.9996
Batch-350: NLLLoss=0.0037 | F1Score=0.9996
Batch-400: NLLLoss=0.0050 | F1Score=0.9996
Batch-450: NLLLoss=0.0061 | F1Score=0.9996
Batch-500: NLLLoss=0.0042 | F1Score=0.9996
Batch-518: NLLLoss=0.0039 | F1Score=0.9996

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0043 | Mean F1Score: 0.9996
==================================================

EPOCH-12
Batch-50: NLLLoss=0.5065 | F1Score=0.9825
Batch-100: NLLLoss=0.4235 | F1Score=0.9394
Batch-150: NLLLoss=0.3134 | F1Score=0.9335
Batch-200: NLLLoss=0.2746 | F1Score=0.9336
Batch-250: NLLLoss=0.1442 | F1Score=0.9378
Batch-300: NLLLoss=0.2206 | F1Score=0.9436
Batch-350: NLLLoss=0.1380 | F1Score=0.9484
Batch-400: NLLLoss=0.0670 | F1Score=0.9516
Batch-450: NLLLoss=0.1380 | F1Score=0.9542
Batch-500: NLLLoss=0.0223 | F1Score=0.9561
Batch-518: NLLLoss=0.1436 | F1Score=0.9569

Huft üò•! Model not improved.
Mean NLLLoss: 0.1918 | Mean F1Score: 0.9504
Patience = 1/3‚ùó
==================================================

EPOCH-13
Batch-50: NLLLoss=0.0229 | F1Score=0.9956
Batch-100: NLLLoss=0.0034 | F1Score=0.9972
Batch-150: NLLLoss=0.0356 | F1Score=0.9973
Batch-200: NLLLoss=0.0171 | F1Score=0.9973
Batch-250: NLLLoss=0.0137 | F1Score=0.9976
Batch-300: NLLLoss=0.0040 | F1Score=0.9973
Batch-350: NLLLoss=0.0196 | F1Score=0.9971
Batch-400: NLLLoss=0.0128 | F1Score=0.9971
Batch-450: NLLLoss=0.0060 | F1Score=0.9972
Batch-500: NLLLoss=0.0018 | F1Score=0.9970
Batch-518: NLLLoss=0.0009 | F1Score=0.9970

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0200 | Mean F1Score: 0.9971
==================================================

EPOCH-14
Batch-50: NLLLoss=0.0030 | F1Score=0.9994
Batch-100: NLLLoss=0.0018 | F1Score=0.9997
Batch-150: NLLLoss=0.0052 | F1Score=0.9996
Batch-200: NLLLoss=0.0180 | F1Score=0.9995
Batch-250: NLLLoss=0.0005 | F1Score=0.9995
Batch-300: NLLLoss=0.0016 | F1Score=0.9995
Batch-350: NLLLoss=0.0015 | F1Score=0.9996
Batch-400: NLLLoss=0.0029 | F1Score=0.9996
Batch-450: NLLLoss=0.0021 | F1Score=0.9996
Batch-500: NLLLoss=0.0011 | F1Score=0.9995
Batch-518: NLLLoss=0.0006 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0047 | Mean F1Score: 0.9996
==================================================

EPOCH-15
Batch-50: NLLLoss=0.0218 | F1Score=1.0000
Batch-100: NLLLoss=0.0021 | F1Score=1.0000
Batch-150: NLLLoss=0.0015 | F1Score=1.0000
Batch-200: NLLLoss=0.0013 | F1Score=1.0000
Batch-250: NLLLoss=0.0012 | F1Score=0.9999
Batch-300: NLLLoss=0.0015 | F1Score=0.9999
Batch-350: NLLLoss=0.0012 | F1Score=0.9999
Batch-400: NLLLoss=0.0008 | F1Score=0.9999
Batch-450: NLLLoss=0.0011 | F1Score=0.9999
Batch-500: NLLLoss=0.0014 | F1Score=0.9999
Batch-518: NLLLoss=0.0004 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0017 | Mean F1Score: 0.9999
==================================================

EPOCH-16
Batch-50: NLLLoss=0.0010 | F1Score=1.0000
Batch-100: NLLLoss=0.0007 | F1Score=0.9998
Batch-150: NLLLoss=0.0016 | F1Score=0.9999
Batch-200: NLLLoss=0.0007 | F1Score=0.9999
Batch-250: NLLLoss=0.0007 | F1Score=0.9999
Batch-300: NLLLoss=0.0011 | F1Score=0.9999
Batch-350: NLLLoss=0.0008 | F1Score=1.0000
Batch-400: NLLLoss=0.0007 | F1Score=0.9999
Batch-450: NLLLoss=0.0011 | F1Score=0.9999
Batch-500: NLLLoss=0.0012 | F1Score=0.9999
Batch-518: NLLLoss=0.0010 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0009 | Mean F1Score: 0.9999
==================================================

EPOCH-17
Batch-50: NLLLoss=0.0006 | F1Score=1.0000
Batch-100: NLLLoss=0.0003 | F1Score=1.0000
Batch-150: NLLLoss=0.0007 | F1Score=1.0000
Batch-200: NLLLoss=0.0006 | F1Score=1.0000
Batch-250: NLLLoss=0.0005 | F1Score=0.9999
Batch-300: NLLLoss=0.0008 | F1Score=0.9999
Batch-350: NLLLoss=0.0003 | F1Score=0.9999
Batch-400: NLLLoss=0.0011 | F1Score=0.9999
Batch-450: NLLLoss=0.0005 | F1Score=0.9999
Batch-500: NLLLoss=0.0011 | F1Score=0.9999
Batch-518: NLLLoss=0.0005 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0006 | Mean F1Score: 1.0000
==================================================

EPOCH-18
Batch-50: NLLLoss=0.0006 | F1Score=1.0000
Batch-100: NLLLoss=0.0003 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=1.0000
Batch-200: NLLLoss=0.0006 | F1Score=1.0000
Batch-250: NLLLoss=0.0004 | F1Score=1.0000
Batch-300: NLLLoss=0.0005 | F1Score=1.0000
Batch-350: NLLLoss=0.0014 | F1Score=1.0000
Batch-400: NLLLoss=0.0003 | F1Score=1.0000
Batch-450: NLLLoss=0.0006 | F1Score=0.9999
Batch-500: NLLLoss=0.0005 | F1Score=0.9999
Batch-518: NLLLoss=0.0009 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0005 | Mean F1Score: 1.0000
==================================================

EPOCH-19
Batch-50: NLLLoss=0.0004 | F1Score=1.0000
Batch-100: NLLLoss=0.0007 | F1Score=1.0000
Batch-150: NLLLoss=0.0010 | F1Score=0.9999
Batch-200: NLLLoss=0.0005 | F1Score=0.9999
Batch-250: NLLLoss=0.0002 | F1Score=0.9999
Batch-300: NLLLoss=0.0004 | F1Score=0.9999
Batch-350: NLLLoss=0.0004 | F1Score=1.0000
Batch-400: NLLLoss=0.0004 | F1Score=0.9999
Batch-450: NLLLoss=0.0005 | F1Score=0.9999
Batch-500: NLLLoss=0.0004 | F1Score=0.9999
Batch-518: NLLLoss=0.0008 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0004 | Mean F1Score: 0.9999
==================================================

EPOCH-20
Batch-50: NLLLoss=0.0004 | F1Score=1.0000
Batch-100: NLLLoss=0.0002 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=0.9998
Batch-200: NLLLoss=0.0005 | F1Score=0.9998
Batch-250: NLLLoss=0.0002 | F1Score=0.9999
Batch-300: NLLLoss=0.0003 | F1Score=0.9999
Batch-350: NLLLoss=0.0003 | F1Score=0.9999
Batch-400: NLLLoss=0.0003 | F1Score=0.9999
Batch-450: NLLLoss=0.0003 | F1Score=0.9999
Batch-500: NLLLoss=0.0002 | F1Score=0.9999
Batch-518: NLLLoss=0.0002 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0003 | Mean F1Score: 0.9999
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss			: 0.0003
Best F1Score			: 0.9999
Training duration		: 24.601 minutes.
Training date			: 2022-09-30 11:29:11.870168+08:00
