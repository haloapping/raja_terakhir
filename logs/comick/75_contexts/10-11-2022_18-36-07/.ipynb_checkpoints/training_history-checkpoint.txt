HYPERPARAMETERS
--------------------------------------------------
context_size: 75
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 20
device: cpu


TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50 : NLLLoss=6.1964 | F1Score=0.2569
Batch-100: NLLLoss=4.7256 | F1Score=0.2972
Batch-150: NLLLoss=5.7057 | F1Score=0.3106
Batch-200: NLLLoss=3.9199 | F1Score=0.3322
Batch-250: NLLLoss=3.1171 | F1Score=0.3548
Batch-300: NLLLoss=3.5555 | F1Score=0.3745
Batch-350: NLLLoss=3.7680 | F1Score=0.3912
Batch-400: NLLLoss=3.4508 | F1Score=0.4046
Batch-450: NLLLoss=4.9516 | F1Score=0.4172
Batch-500: NLLLoss=2.6373 | F1Score=0.4302
Batch-518: NLLLoss=1.4986 | F1Score=0.4357

Mean NLLLoss: 4.5231 | Mean F1Score: 0.3466
==================================================

EPOCH-2
Batch-50 : NLLLoss=4.3062 | F1Score=0.5969
Batch-100: NLLLoss=3.4311 | F1Score=0.5963
Batch-150: NLLLoss=2.3704 | F1Score=0.6032
Batch-200: NLLLoss=2.1314 | F1Score=0.6130
Batch-250: NLLLoss=1.9218 | F1Score=0.6219
Batch-300: NLLLoss=2.7350 | F1Score=0.6264
Batch-350: NLLLoss=4.0043 | F1Score=0.6298
Batch-400: NLLLoss=2.4300 | F1Score=0.6349
Batch-450: NLLLoss=2.6664 | F1Score=0.6380
Batch-500: NLLLoss=3.3792 | F1Score=0.6423
Batch-518: NLLLoss=1.8223 | F1Score=0.6440

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.6799 | Mean F1Score: 0.6194
==================================================

EPOCH-3
Batch-50 : NLLLoss=2.2479 | F1Score=0.7331
Batch-100: NLLLoss=1.8120 | F1Score=0.7341
Batch-150: NLLLoss=2.3277 | F1Score=0.7299
Batch-200: NLLLoss=2.1023 | F1Score=0.7333
Batch-250: NLLLoss=1.1819 | F1Score=0.7338
Batch-300: NLLLoss=1.1156 | F1Score=0.7390
Batch-350: NLLLoss=1.6171 | F1Score=0.7426
Batch-400: NLLLoss=1.7566 | F1Score=0.7445
Batch-450: NLLLoss=2.0020 | F1Score=0.7478
Batch-500: NLLLoss=1.6255 | F1Score=0.7491
Batch-518: NLLLoss=1.8228 | F1Score=0.7501

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7674 | Mean F1Score: 0.7388
==================================================

EPOCH-4
Batch-50 : NLLLoss=0.8565 | F1Score=0.8306
Batch-100: NLLLoss=0.6488 | F1Score=0.8138
Batch-150: NLLLoss=1.3516 | F1Score=0.8163
Batch-200: NLLLoss=0.4816 | F1Score=0.8152
Batch-250: NLLLoss=0.9638 | F1Score=0.8167
Batch-300: NLLLoss=0.8787 | F1Score=0.8177
Batch-350: NLLLoss=1.0396 | F1Score=0.8207
Batch-400: NLLLoss=1.1555 | F1Score=0.8211
Batch-450: NLLLoss=0.8277 | F1Score=0.8197
Batch-500: NLLLoss=1.3204 | F1Score=0.8206
Batch-518: NLLLoss=1.0045 | F1Score=0.8206

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1491 | Mean F1Score: 0.8188
==================================================

EPOCH-5
Batch-50 : NLLLoss=0.4795 | F1Score=0.9019
Batch-100: NLLLoss=0.4740 | F1Score=0.8930
Batch-150: NLLLoss=0.8050 | F1Score=0.8836
Batch-200: NLLLoss=0.8604 | F1Score=0.8810
Batch-250: NLLLoss=0.5603 | F1Score=0.8804
Batch-300: NLLLoss=1.0023 | F1Score=0.8780
Batch-350: NLLLoss=0.7605 | F1Score=0.8759
Batch-400: NLLLoss=0.6782 | F1Score=0.8764
Batch-450: NLLLoss=0.5919 | F1Score=0.8775
Batch-500: NLLLoss=0.9391 | F1Score=0.8747
Batch-518: NLLLoss=0.7334 | F1Score=0.8751

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.6854 | Mean F1Score: 0.8832
==================================================

EPOCH-6
Batch-50 : NLLLoss=0.3874 | F1Score=0.9559
Batch-100: NLLLoss=0.1527 | F1Score=0.9551
Batch-150: NLLLoss=0.3510 | F1Score=0.9530
Batch-200: NLLLoss=0.2589 | F1Score=0.9504
Batch-250: NLLLoss=0.5146 | F1Score=0.9464
Batch-300: NLLLoss=0.4397 | F1Score=0.9463
Batch-350: NLLLoss=0.4288 | F1Score=0.9460
Batch-400: NLLLoss=0.8414 | F1Score=0.9441
Batch-450: NLLLoss=0.5383 | F1Score=0.9414
Batch-500: NLLLoss=0.2849 | F1Score=0.9396
Batch-518: NLLLoss=0.4429 | F1Score=0.9394

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.3320 | Mean F1Score: 0.9488
==================================================

EPOCH-7
Batch-50 : NLLLoss=0.1837 | F1Score=0.9906
Batch-100: NLLLoss=0.0673 | F1Score=0.9925
Batch-150: NLLLoss=0.1574 | F1Score=0.9933
Batch-200: NLLLoss=0.1165 | F1Score=0.9931
Batch-250: NLLLoss=0.0806 | F1Score=0.9912
Batch-300: NLLLoss=0.2765 | F1Score=0.9887
Batch-350: NLLLoss=0.0485 | F1Score=0.9883
Batch-400: NLLLoss=0.0999 | F1Score=0.9882
Batch-450: NLLLoss=0.1927 | F1Score=0.9878
Batch-500: NLLLoss=0.0809 | F1Score=0.9873
Batch-518: NLLLoss=0.0965 | F1Score=0.9873

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.1108 | Mean F1Score: 0.9905
==================================================

EPOCH-8
Batch-50 : NLLLoss=0.0470 | F1Score=0.9981
Batch-100: NLLLoss=0.0477 | F1Score=0.9987
Batch-150: NLLLoss=0.0118 | F1Score=0.9987
Batch-200: NLLLoss=0.0266 | F1Score=0.9986
Batch-250: NLLLoss=0.0312 | F1Score=0.9989
Batch-300: NLLLoss=0.0530 | F1Score=0.9986
Batch-350: NLLLoss=0.0115 | F1Score=0.9988
Batch-400: NLLLoss=0.0173 | F1Score=0.9988
Batch-450: NLLLoss=0.0118 | F1Score=0.9985
Batch-500: NLLLoss=0.0142 | F1Score=0.9984
Batch-518: NLLLoss=0.0213 | F1Score=0.9983

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0307 | Mean F1Score: 0.9987
==================================================

EPOCH-9
Batch-50 : NLLLoss=0.0156 | F1Score=0.9987
Batch-100: NLLLoss=0.0036 | F1Score=0.9984
Batch-150: NLLLoss=0.0059 | F1Score=0.9987
Batch-200: NLLLoss=0.0115 | F1Score=0.9989
Batch-250: NLLLoss=0.0205 | F1Score=0.9990
Batch-300: NLLLoss=0.0060 | F1Score=0.9990
Batch-350: NLLLoss=0.0101 | F1Score=0.9990
Batch-400: NLLLoss=0.0044 | F1Score=0.9991
Batch-450: NLLLoss=0.0080 | F1Score=0.9992
Batch-500: NLLLoss=0.0051 | F1Score=0.9991
Batch-518: NLLLoss=0.0131 | F1Score=0.9992

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0125 | Mean F1Score: 0.9989
==================================================

EPOCH-10
Batch-50 : NLLLoss=0.0047 | F1Score=0.9997
Batch-100: NLLLoss=0.0058 | F1Score=0.9995
Batch-150: NLLLoss=0.0050 | F1Score=0.9996
Batch-200: NLLLoss=0.0070 | F1Score=0.9992
Batch-250: NLLLoss=0.0068 | F1Score=0.9992
Batch-300: NLLLoss=0.0057 | F1Score=0.9994
Batch-350: NLLLoss=0.0091 | F1Score=0.9994
Batch-400: NLLLoss=0.0038 | F1Score=0.9993
Batch-450: NLLLoss=0.0068 | F1Score=0.9992
Batch-500: NLLLoss=0.0071 | F1Score=0.9992
Batch-518: NLLLoss=0.0027 | F1Score=0.9992

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0092 | Mean F1Score: 0.9994
==================================================

EPOCH-11
Batch-50 : NLLLoss=0.0039 | F1Score=1.0000
Batch-100: NLLLoss=0.0041 | F1Score=0.9997
Batch-150: NLLLoss=0.0042 | F1Score=0.9994
Batch-200: NLLLoss=0.0037 | F1Score=0.9995
Batch-250: NLLLoss=0.0037 | F1Score=0.9995
Batch-300: NLLLoss=0.0034 | F1Score=0.9995
Batch-350: NLLLoss=0.0029 | F1Score=0.9995
Batch-400: NLLLoss=0.0090 | F1Score=0.9995
Batch-450: NLLLoss=0.0036 | F1Score=0.9995
Batch-500: NLLLoss=0.0044 | F1Score=0.9995
Batch-518: NLLLoss=0.0042 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0053 | Mean F1Score: 0.9996
==================================================

EPOCH-12
Batch-50 : NLLLoss=0.0043 | F1Score=1.0000
Batch-100: NLLLoss=0.0028 | F1Score=0.9997
Batch-150: NLLLoss=0.0047 | F1Score=0.9998
Batch-200: NLLLoss=0.0027 | F1Score=0.9998
Batch-250: NLLLoss=0.0031 | F1Score=0.9997
Batch-300: NLLLoss=0.0034 | F1Score=0.9997
Batch-350: NLLLoss=0.0062 | F1Score=0.9997
Batch-400: NLLLoss=0.0036 | F1Score=0.9998
Batch-450: NLLLoss=0.0022 | F1Score=0.9997
Batch-500: NLLLoss=0.0021 | F1Score=0.9997
Batch-518: NLLLoss=0.0022 | F1Score=0.9997

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0038 | Mean F1Score: 0.9998
==================================================

EPOCH-13
Batch-50 : NLLLoss=0.0035 | F1Score=1.0000
Batch-100: NLLLoss=0.0022 | F1Score=1.0000
Batch-150: NLLLoss=0.0034 | F1Score=0.9998
Batch-200: NLLLoss=0.0010 | F1Score=0.9998
Batch-250: NLLLoss=0.0082 | F1Score=0.9997
Batch-300: NLLLoss=0.0019 | F1Score=0.9997
Batch-350: NLLLoss=0.0022 | F1Score=0.9997
Batch-400: NLLLoss=0.0014 | F1Score=0.9998
Batch-450: NLLLoss=0.0016 | F1Score=0.9997
Batch-500: NLLLoss=0.0025 | F1Score=0.9997
Batch-518: NLLLoss=0.0056 | F1Score=0.9997

Huft üò•! Model not improved.
Mean NLLLoss: 0.0044 | Mean F1Score: 0.9998
Patience = 1/20‚ùó
==================================================

EPOCH-14
Batch-50 : NLLLoss=0.0046 | F1Score=0.9994
Batch-100: NLLLoss=0.0023 | F1Score=0.9997
Batch-150: NLLLoss=0.0023 | F1Score=0.9996
Batch-200: NLLLoss=0.0016 | F1Score=0.9997
Batch-250: NLLLoss=0.0015 | F1Score=0.9996
Batch-300: NLLLoss=0.0014 | F1Score=0.9996
Batch-350: NLLLoss=0.0013 | F1Score=0.9997
Batch-400: NLLLoss=0.0014 | F1Score=0.9997
Batch-450: NLLLoss=0.0028 | F1Score=0.9996
Batch-500: NLLLoss=0.0032 | F1Score=0.9996
Batch-518: NLLLoss=0.0179 | F1Score=0.9995

Huft üò•! Model not improved.
Mean NLLLoss: 0.0044 | Mean F1Score: 0.9995
Patience = 2/20‚ùó
==================================================

EPOCH-15
Batch-50 : NLLLoss=0.7368 | F1Score=0.9812
Batch-100: NLLLoss=0.2141 | F1Score=0.9248
Batch-150: NLLLoss=0.2077 | F1Score=0.8988
Batch-200: NLLLoss=0.1011 | F1Score=0.8959
Batch-250: NLLLoss=0.2707 | F1Score=0.8997
Batch-300: NLLLoss=0.3018 | F1Score=0.9021
Batch-350: NLLLoss=0.1961 | F1Score=0.9046
Batch-400: NLLLoss=0.2056 | F1Score=0.9090
Batch-450: NLLLoss=0.1702 | F1Score=0.9135
Batch-500: NLLLoss=0.0454 | F1Score=0.9181
Batch-518: NLLLoss=0.1730 | F1Score=0.9188

Huft üò•! Model not improved.
Mean NLLLoss: 0.3261 | Mean F1Score: 0.9181
Patience = 3/20‚ùó
==================================================

EPOCH-16
Batch-50 : NLLLoss=0.0130 | F1Score=0.9937
Batch-100: NLLLoss=0.0173 | F1Score=0.9942
Batch-150: NLLLoss=0.1021 | F1Score=0.9950
Batch-200: NLLLoss=0.0326 | F1Score=0.9947
Batch-250: NLLLoss=0.0276 | F1Score=0.9949
Batch-300: NLLLoss=0.0079 | F1Score=0.9953
Batch-350: NLLLoss=0.0203 | F1Score=0.9952
Batch-400: NLLLoss=0.0319 | F1Score=0.9955
Batch-450: NLLLoss=0.0051 | F1Score=0.9956
Batch-500: NLLLoss=0.0934 | F1Score=0.9958
Batch-518: NLLLoss=0.0033 | F1Score=0.9958

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0253 | Mean F1Score: 0.9948
==================================================

EPOCH-17
Batch-50 : NLLLoss=0.0017 | F1Score=0.9994
Batch-100: NLLLoss=0.0018 | F1Score=0.9997
Batch-150: NLLLoss=0.0020 | F1Score=0.9998
Batch-200: NLLLoss=0.0022 | F1Score=0.9998
Batch-250: NLLLoss=0.0025 | F1Score=0.9999
Batch-300: NLLLoss=0.0011 | F1Score=0.9999
Batch-350: NLLLoss=0.0017 | F1Score=0.9998
Batch-400: NLLLoss=0.0016 | F1Score=0.9998
Batch-450: NLLLoss=0.0019 | F1Score=0.9998
Batch-500: NLLLoss=0.0029 | F1Score=0.9998
Batch-518: NLLLoss=0.0004 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0030 | Mean F1Score: 0.9998
==================================================

EPOCH-18
Batch-50 : NLLLoss=0.0010 | F1Score=1.0000
Batch-100: NLLLoss=0.0005 | F1Score=1.0000
Batch-150: NLLLoss=0.0013 | F1Score=0.9999
Batch-200: NLLLoss=0.0011 | F1Score=0.9998
Batch-250: NLLLoss=0.0009 | F1Score=0.9998
Batch-300: NLLLoss=0.0009 | F1Score=0.9998
Batch-350: NLLLoss=0.0016 | F1Score=0.9999
Batch-400: NLLLoss=0.0025 | F1Score=0.9999
Batch-450: NLLLoss=0.0014 | F1Score=0.9998
Batch-500: NLLLoss=0.0022 | F1Score=0.9997
Batch-518: NLLLoss=0.0007 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0025 | Mean F1Score: 0.9999
==================================================

EPOCH-19
Batch-50 : NLLLoss=0.0013 | F1Score=0.9994
Batch-100: NLLLoss=0.0008 | F1Score=0.9994
Batch-150: NLLLoss=0.0009 | F1Score=0.9996
Batch-200: NLLLoss=0.0016 | F1Score=0.9997
Batch-250: NLLLoss=0.0018 | F1Score=0.9998
Batch-300: NLLLoss=0.0007 | F1Score=0.9997
Batch-350: NLLLoss=0.0008 | F1Score=0.9998
Batch-400: NLLLoss=0.0009 | F1Score=0.9997
Batch-450: NLLLoss=0.0013 | F1Score=0.9998
Batch-500: NLLLoss=0.0010 | F1Score=0.9997
Batch-518: NLLLoss=0.0008 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0016 | Mean F1Score: 0.9997
==================================================

EPOCH-20
Batch-50 : NLLLoss=0.0003 | F1Score=1.0000
Batch-100: NLLLoss=0.0009 | F1Score=1.0000
Batch-150: NLLLoss=0.0009 | F1Score=1.0000
Batch-200: NLLLoss=0.0009 | F1Score=1.0000
Batch-250: NLLLoss=0.0004 | F1Score=1.0000
Batch-300: NLLLoss=0.0011 | F1Score=1.0000
Batch-350: NLLLoss=0.0008 | F1Score=1.0000
Batch-400: NLLLoss=0.0009 | F1Score=1.0000
Batch-450: NLLLoss=0.0005 | F1Score=1.0000
Batch-500: NLLLoss=0.0008 | F1Score=0.9999
Batch-518: NLLLoss=0.0006 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0008 | Mean F1Score: 1.0000
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss      : 0.0008
Best F1Score      : 1.0000
Training duration : 34.174 minutes.
Training date     : 2022-10-11 18:36:07.451122+08:00
