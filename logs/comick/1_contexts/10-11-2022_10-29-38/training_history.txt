HYPERPARAMETERS
--------------------------------------------------
context_size: 1
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 20
device: cpu


TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50 : NLLLoss=5.8824 | F1Score=0.2725
Batch-100: NLLLoss=5.5511 | F1Score=0.2837
Batch-150: NLLLoss=5.0628 | F1Score=0.3165
Batch-200: NLLLoss=4.4808 | F1Score=0.3436
Batch-250: NLLLoss=4.4597 | F1Score=0.3691
Batch-300: NLLLoss=4.3766 | F1Score=0.3833
Batch-350: NLLLoss=3.2468 | F1Score=0.3963
Batch-400: NLLLoss=3.3948 | F1Score=0.4109
Batch-450: NLLLoss=3.2871 | F1Score=0.4265
Batch-500: NLLLoss=3.2360 | F1Score=0.4384
Batch-518: NLLLoss=3.2195 | F1Score=0.4424

Mean NLLLoss: 4.4493 | Mean F1Score: 0.3550
==================================================

EPOCH-2
Batch-50 : NLLLoss=2.4593 | F1Score=0.5644
Batch-100: NLLLoss=1.8496 | F1Score=0.5913
Batch-150: NLLLoss=3.3346 | F1Score=0.6044
Batch-200: NLLLoss=2.9763 | F1Score=0.6135
Batch-250: NLLLoss=1.4838 | F1Score=0.6230
Batch-300: NLLLoss=2.7566 | F1Score=0.6277
Batch-350: NLLLoss=2.5987 | F1Score=0.6316
Batch-400: NLLLoss=2.4289 | F1Score=0.6383
Batch-450: NLLLoss=2.8322 | F1Score=0.6423
Batch-500: NLLLoss=2.9857 | F1Score=0.6452
Batch-518: NLLLoss=1.4275 | F1Score=0.6459

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.6649 | Mean F1Score: 0.6162
==================================================

EPOCH-3
Batch-50 : NLLLoss=1.8222 | F1Score=0.7075
Batch-100: NLLLoss=2.1684 | F1Score=0.7122
Batch-150: NLLLoss=1.6876 | F1Score=0.7220
Batch-200: NLLLoss=1.9579 | F1Score=0.7246
Batch-250: NLLLoss=2.1274 | F1Score=0.7280
Batch-300: NLLLoss=1.7430 | F1Score=0.7340
Batch-350: NLLLoss=1.8182 | F1Score=0.7388
Batch-400: NLLLoss=1.1374 | F1Score=0.7412
Batch-450: NLLLoss=0.5167 | F1Score=0.7455
Batch-500: NLLLoss=2.0997 | F1Score=0.7489
Batch-518: NLLLoss=0.9352 | F1Score=0.7494

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7644 | Mean F1Score: 0.7293
==================================================

EPOCH-4
Batch-50 : NLLLoss=1.1854 | F1Score=0.8100
Batch-100: NLLLoss=1.1647 | F1Score=0.8103
Batch-150: NLLLoss=1.3367 | F1Score=0.8144
Batch-200: NLLLoss=1.6109 | F1Score=0.8134
Batch-250: NLLLoss=1.6613 | F1Score=0.8160
Batch-300: NLLLoss=0.8710 | F1Score=0.8152
Batch-350: NLLLoss=1.3256 | F1Score=0.8155
Batch-400: NLLLoss=0.7884 | F1Score=0.8171
Batch-450: NLLLoss=1.3265 | F1Score=0.8184
Batch-500: NLLLoss=0.9425 | F1Score=0.8190
Batch-518: NLLLoss=1.1719 | F1Score=0.8188

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1532 | Mean F1Score: 0.8146
==================================================

EPOCH-5
Batch-50 : NLLLoss=0.5392 | F1Score=0.8806
Batch-100: NLLLoss=0.5182 | F1Score=0.8766
Batch-150: NLLLoss=0.9550 | F1Score=0.8752
Batch-200: NLLLoss=0.8032 | F1Score=0.8733
Batch-250: NLLLoss=1.0626 | F1Score=0.8719
Batch-300: NLLLoss=0.6195 | F1Score=0.8712
Batch-350: NLLLoss=1.3231 | F1Score=0.8696
Batch-400: NLLLoss=1.3704 | F1Score=0.8686
Batch-450: NLLLoss=0.8648 | F1Score=0.8687
Batch-500: NLLLoss=0.6431 | F1Score=0.8688
Batch-518: NLLLoss=0.4483 | F1Score=0.8693

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.7050 | Mean F1Score: 0.8742
==================================================

EPOCH-6
Batch-50 : NLLLoss=0.3025 | F1Score=0.9500
Batch-100: NLLLoss=0.6605 | F1Score=0.9461
Batch-150: NLLLoss=0.3666 | F1Score=0.9395
Batch-200: NLLLoss=0.2369 | F1Score=0.9373
Batch-250: NLLLoss=0.6647 | F1Score=0.9376
Batch-300: NLLLoss=0.2748 | F1Score=0.9348
Batch-350: NLLLoss=0.6261 | F1Score=0.9330
Batch-400: NLLLoss=0.3212 | F1Score=0.9312
Batch-450: NLLLoss=0.4031 | F1Score=0.9295
Batch-500: NLLLoss=0.1937 | F1Score=0.9288
Batch-518: NLLLoss=0.8106 | F1Score=0.9283

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.3752 | Mean F1Score: 0.9375
==================================================

EPOCH-7
Batch-50 : NLLLoss=0.2668 | F1Score=0.9794
Batch-100: NLLLoss=0.1096 | F1Score=0.9806
Batch-150: NLLLoss=0.2218 | F1Score=0.9819
Batch-200: NLLLoss=0.1090 | F1Score=0.9810
Batch-250: NLLLoss=0.1786 | F1Score=0.9813
Batch-300: NLLLoss=0.1015 | F1Score=0.9807
Batch-350: NLLLoss=0.2802 | F1Score=0.9800
Batch-400: NLLLoss=0.0584 | F1Score=0.9790
Batch-450: NLLLoss=0.1446 | F1Score=0.9776
Batch-500: NLLLoss=0.1903 | F1Score=0.9769
Batch-518: NLLLoss=0.1844 | F1Score=0.9766

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.1588 | Mean F1Score: 0.9803
==================================================

EPOCH-8
Batch-50 : NLLLoss=0.0286 | F1Score=0.9950
Batch-100: NLLLoss=0.0728 | F1Score=0.9950
Batch-150: NLLLoss=0.0894 | F1Score=0.9946
Batch-200: NLLLoss=0.0543 | F1Score=0.9952
Batch-250: NLLLoss=0.0345 | F1Score=0.9949
Batch-300: NLLLoss=0.0238 | F1Score=0.9944
Batch-350: NLLLoss=0.0826 | F1Score=0.9939
Batch-400: NLLLoss=0.0790 | F1Score=0.9939
Batch-450: NLLLoss=0.0302 | F1Score=0.9933
Batch-500: NLLLoss=0.0811 | F1Score=0.9930
Batch-518: NLLLoss=0.1795 | F1Score=0.9930

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0617 | Mean F1Score: 0.9938
==================================================

EPOCH-9
Batch-50 : NLLLoss=0.0123 | F1Score=0.9969
Batch-100: NLLLoss=0.0171 | F1Score=0.9978
Batch-150: NLLLoss=0.0035 | F1Score=0.9976
Batch-200: NLLLoss=0.0185 | F1Score=0.9968
Batch-250: NLLLoss=0.0187 | F1Score=0.9968
Batch-300: NLLLoss=0.0106 | F1Score=0.9971
Batch-350: NLLLoss=0.0215 | F1Score=0.9972
Batch-400: NLLLoss=0.0386 | F1Score=0.9973
Batch-450: NLLLoss=0.0253 | F1Score=0.9973
Batch-500: NLLLoss=0.0080 | F1Score=0.9974
Batch-518: NLLLoss=0.0394 | F1Score=0.9974

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0267 | Mean F1Score: 0.9971
==================================================

EPOCH-10
Batch-50 : NLLLoss=0.0046 | F1Score=0.9987
Batch-100: NLLLoss=0.0107 | F1Score=0.9981
Batch-150: NLLLoss=0.0111 | F1Score=0.9983
Batch-200: NLLLoss=0.0153 | F1Score=0.9986
Batch-250: NLLLoss=0.0110 | F1Score=0.9984
Batch-300: NLLLoss=0.0352 | F1Score=0.9972
Batch-350: NLLLoss=0.0421 | F1Score=0.9962
Batch-400: NLLLoss=0.0686 | F1Score=0.9949
Batch-450: NLLLoss=0.0361 | F1Score=0.9937
Batch-500: NLLLoss=0.1789 | F1Score=0.9926
Batch-518: NLLLoss=0.1188 | F1Score=0.9923

Huft üò•! Model not improved.
Mean NLLLoss: 0.0483 | Mean F1Score: 0.9969
Patience = 1/20‚ùó
==================================================

EPOCH-11
Batch-50 : NLLLoss=0.0428 | F1Score=0.9878
Batch-100: NLLLoss=0.1145 | F1Score=0.9858
Batch-150: NLLLoss=0.0379 | F1Score=0.9864
Batch-200: NLLLoss=0.0760 | F1Score=0.9886
Batch-250: NLLLoss=0.0236 | F1Score=0.9894
Batch-300: NLLLoss=0.0136 | F1Score=0.9905
Batch-350: NLLLoss=0.0366 | F1Score=0.9909
Batch-400: NLLLoss=0.0647 | F1Score=0.9913
Batch-450: NLLLoss=0.0574 | F1Score=0.9918
Batch-500: NLLLoss=0.0747 | F1Score=0.9923
Batch-518: NLLLoss=0.1203 | F1Score=0.9923

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0457 | Mean F1Score: 0.9894
==================================================

EPOCH-12
Batch-50 : NLLLoss=0.0050 | F1Score=0.9987
Batch-100: NLLLoss=0.0137 | F1Score=0.9984
Batch-150: NLLLoss=0.0110 | F1Score=0.9985
Batch-200: NLLLoss=0.0109 | F1Score=0.9986
Batch-250: NLLLoss=0.0048 | F1Score=0.9986
Batch-300: NLLLoss=0.0046 | F1Score=0.9987
Batch-350: NLLLoss=0.0022 | F1Score=0.9987
Batch-400: NLLLoss=0.0082 | F1Score=0.9986
Batch-450: NLLLoss=0.0226 | F1Score=0.9984
Batch-500: NLLLoss=0.0126 | F1Score=0.9985
Batch-518: NLLLoss=0.0034 | F1Score=0.9985

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0139 | Mean F1Score: 0.9985
==================================================

EPOCH-13
Batch-50 : NLLLoss=0.0062 | F1Score=0.9987
Batch-100: NLLLoss=0.0027 | F1Score=0.9994
Batch-150: NLLLoss=0.0026 | F1Score=0.9994
Batch-200: NLLLoss=0.0025 | F1Score=0.9995
Batch-250: NLLLoss=0.0034 | F1Score=0.9994
Batch-300: NLLLoss=0.0028 | F1Score=0.9994
Batch-350: NLLLoss=0.0076 | F1Score=0.9993
Batch-400: NLLLoss=0.0041 | F1Score=0.9992
Batch-450: NLLLoss=0.0136 | F1Score=0.9985
Batch-500: NLLLoss=0.0385 | F1Score=0.9981
Batch-518: NLLLoss=0.0048 | F1Score=0.9980

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0126 | Mean F1Score: 0.9991
==================================================

EPOCH-14
Batch-50 : NLLLoss=0.0065 | F1Score=0.9969
Batch-100: NLLLoss=0.0096 | F1Score=0.9969
Batch-150: NLLLoss=0.0076 | F1Score=0.9956
Batch-200: NLLLoss=0.0647 | F1Score=0.9941
Batch-250: NLLLoss=0.0100 | F1Score=0.9942
Batch-300: NLLLoss=0.0890 | F1Score=0.9931
Batch-350: NLLLoss=0.0532 | F1Score=0.9913
Batch-400: NLLLoss=0.0896 | F1Score=0.9894
Batch-450: NLLLoss=0.1617 | F1Score=0.9889
Batch-500: NLLLoss=0.0528 | F1Score=0.9884
Batch-518: NLLLoss=0.0108 | F1Score=0.9885

Huft üò•! Model not improved.
Mean NLLLoss: 0.0534 | Mean F1Score: 0.9933
Patience = 2/20‚ùó
==================================================

EPOCH-15
Batch-50 : NLLLoss=0.0180 | F1Score=0.9906
Batch-100: NLLLoss=0.0506 | F1Score=0.9895
Batch-150: NLLLoss=0.0297 | F1Score=0.9909
Batch-200: NLLLoss=0.0038 | F1Score=0.9916
Batch-250: NLLLoss=0.0137 | F1Score=0.9922
Batch-300: NLLLoss=0.0069 | F1Score=0.9927
Batch-350: NLLLoss=0.0268 | F1Score=0.9933
Batch-400: NLLLoss=0.0054 | F1Score=0.9934
Batch-450: NLLLoss=0.0087 | F1Score=0.9938
Batch-500: NLLLoss=0.0238 | F1Score=0.9941
Batch-518: NLLLoss=0.0136 | F1Score=0.9942

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0282 | Mean F1Score: 0.9923
==================================================

EPOCH-16
Batch-50 : NLLLoss=0.0021 | F1Score=0.9987
Batch-100: NLLLoss=0.0035 | F1Score=0.9984
Batch-150: NLLLoss=0.0155 | F1Score=0.9981
Batch-200: NLLLoss=0.0020 | F1Score=0.9981
Batch-250: NLLLoss=0.0038 | F1Score=0.9985
Batch-300: NLLLoss=0.0024 | F1Score=0.9984
Batch-350: NLLLoss=0.0008 | F1Score=0.9983
Batch-400: NLLLoss=0.0040 | F1Score=0.9984
Batch-450: NLLLoss=0.0011 | F1Score=0.9984
Batch-500: NLLLoss=0.0028 | F1Score=0.9984
Batch-518: NLLLoss=0.0168 | F1Score=0.9984

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0086 | Mean F1Score: 0.9985
==================================================

EPOCH-17
Batch-50 : NLLLoss=0.0007 | F1Score=0.9975
Batch-100: NLLLoss=0.0042 | F1Score=0.9984
Batch-150: NLLLoss=0.0013 | F1Score=0.9990
Batch-200: NLLLoss=0.0015 | F1Score=0.9991
Batch-250: NLLLoss=0.0006 | F1Score=0.9991
Batch-300: NLLLoss=0.0007 | F1Score=0.9991
Batch-350: NLLLoss=0.0009 | F1Score=0.9992
Batch-400: NLLLoss=0.0012 | F1Score=0.9993
Batch-450: NLLLoss=0.0006 | F1Score=0.9994
Batch-500: NLLLoss=0.0008 | F1Score=0.9994
Batch-518: NLLLoss=0.0006 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0035 | Mean F1Score: 0.9988
==================================================

EPOCH-18
Batch-50 : NLLLoss=0.0006 | F1Score=0.9994
Batch-100: NLLLoss=0.0009 | F1Score=0.9997
Batch-150: NLLLoss=0.0006 | F1Score=0.9998
Batch-200: NLLLoss=0.0009 | F1Score=0.9998
Batch-250: NLLLoss=0.0007 | F1Score=0.9999
Batch-300: NLLLoss=0.0007 | F1Score=0.9999
Batch-350: NLLLoss=0.0002 | F1Score=0.9998
Batch-400: NLLLoss=0.0017 | F1Score=0.9998
Batch-450: NLLLoss=0.0014 | F1Score=0.9999
Batch-500: NLLLoss=0.0005 | F1Score=0.9999
Batch-518: NLLLoss=0.0003 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0010 | Mean F1Score: 0.9998
==================================================

EPOCH-19
Batch-50 : NLLLoss=0.0003 | F1Score=0.9997
Batch-100: NLLLoss=0.0004 | F1Score=0.9997
Batch-150: NLLLoss=0.0004 | F1Score=0.9998
Batch-200: NLLLoss=0.0004 | F1Score=0.9998
Batch-250: NLLLoss=0.0002 | F1Score=0.9999
Batch-300: NLLLoss=0.0004 | F1Score=0.9999
Batch-350: NLLLoss=0.0002 | F1Score=0.9999
Batch-400: NLLLoss=0.0004 | F1Score=0.9999
Batch-450: NLLLoss=0.0005 | F1Score=0.9999
Batch-500: NLLLoss=0.0004 | F1Score=0.9999
Batch-518: NLLLoss=0.0004 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0005 | Mean F1Score: 0.9998
==================================================

EPOCH-20
Batch-50 : NLLLoss=0.0003 | F1Score=1.0000
Batch-100: NLLLoss=0.0006 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=1.0000
Batch-200: NLLLoss=0.0002 | F1Score=1.0000
Batch-250: NLLLoss=0.0003 | F1Score=0.9999
Batch-300: NLLLoss=0.0004 | F1Score=0.9999
Batch-350: NLLLoss=0.0002 | F1Score=0.9999
Batch-400: NLLLoss=0.0004 | F1Score=0.9999
Batch-450: NLLLoss=0.0005 | F1Score=0.9999
Batch-500: NLLLoss=0.0004 | F1Score=0.9999
Batch-518: NLLLoss=0.0008 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0004 | Mean F1Score: 1.0000
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss      : 0.0004
Best F1Score      : 1.0000
Training duration : 15.418 minutes.
Training date     : 2022-10-11 10:29:38.207216+08:00
