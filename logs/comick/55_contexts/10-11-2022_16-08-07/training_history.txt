HYPERPARAMETERS
--------------------------------------------------
context_size: 55
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 20
device: cpu


TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50 : NLLLoss=6.6898 | F1Score=0.2831
Batch-100: NLLLoss=5.7229 | F1Score=0.3097
Batch-150: NLLLoss=5.7369 | F1Score=0.3306
Batch-200: NLLLoss=4.4095 | F1Score=0.3500
Batch-250: NLLLoss=3.8921 | F1Score=0.3717
Batch-300: NLLLoss=4.4559 | F1Score=0.3844
Batch-350: NLLLoss=4.4180 | F1Score=0.3978
Batch-400: NLLLoss=3.0422 | F1Score=0.4092
Batch-450: NLLLoss=4.0458 | F1Score=0.4211
Batch-500: NLLLoss=2.1492 | F1Score=0.4328
Batch-518: NLLLoss=4.7859 | F1Score=0.4369

Mean NLLLoss: 4.5272 | Mean F1Score: 0.3571
==================================================

EPOCH-2
Batch-50 : NLLLoss=2.8761 | F1Score=0.5900
Batch-100: NLLLoss=2.3925 | F1Score=0.5947
Batch-150: NLLLoss=2.6063 | F1Score=0.5890
Batch-200: NLLLoss=2.7851 | F1Score=0.5991
Batch-250: NLLLoss=3.3482 | F1Score=0.6072
Batch-300: NLLLoss=2.3620 | F1Score=0.6193
Batch-350: NLLLoss=2.1349 | F1Score=0.6231
Batch-400: NLLLoss=2.5970 | F1Score=0.6279
Batch-450: NLLLoss=2.1390 | F1Score=0.6328
Batch-500: NLLLoss=1.8613 | F1Score=0.6363
Batch-518: NLLLoss=2.8440 | F1Score=0.6387

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.7032 | Mean F1Score: 0.6087
==================================================

EPOCH-3
Batch-50 : NLLLoss=2.1285 | F1Score=0.7300
Batch-100: NLLLoss=1.6368 | F1Score=0.7322
Batch-150: NLLLoss=1.7694 | F1Score=0.7238
Batch-200: NLLLoss=2.8020 | F1Score=0.7293
Batch-250: NLLLoss=1.8479 | F1Score=0.7274
Batch-300: NLLLoss=2.2908 | F1Score=0.7273
Batch-350: NLLLoss=1.6132 | F1Score=0.7290
Batch-400: NLLLoss=1.4833 | F1Score=0.7352
Batch-450: NLLLoss=0.5174 | F1Score=0.7404
Batch-500: NLLLoss=2.0886 | F1Score=0.7430
Batch-518: NLLLoss=2.0708 | F1Score=0.7440

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7863 | Mean F1Score: 0.7325
==================================================

EPOCH-4
Batch-50 : NLLLoss=1.5221 | F1Score=0.8231
Batch-100: NLLLoss=1.4136 | F1Score=0.8147
Batch-150: NLLLoss=1.0058 | F1Score=0.8169
Batch-200: NLLLoss=1.2895 | F1Score=0.8148
Batch-250: NLLLoss=0.9902 | F1Score=0.8145
Batch-300: NLLLoss=1.4562 | F1Score=0.8162
Batch-350: NLLLoss=0.8650 | F1Score=0.8156
Batch-400: NLLLoss=1.3459 | F1Score=0.8163
Batch-450: NLLLoss=0.9655 | F1Score=0.8187
Batch-500: NLLLoss=1.1633 | F1Score=0.8179
Batch-518: NLLLoss=0.9565 | F1Score=0.8176

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1601 | Mean F1Score: 0.8182
==================================================

EPOCH-5
Batch-50 : NLLLoss=0.7781 | F1Score=0.8869
Batch-100: NLLLoss=0.7336 | F1Score=0.8866
Batch-150: NLLLoss=1.0223 | F1Score=0.8834
Batch-200: NLLLoss=0.6934 | F1Score=0.8801
Batch-250: NLLLoss=0.4934 | F1Score=0.8801
Batch-300: NLLLoss=0.5678 | F1Score=0.8780
Batch-350: NLLLoss=0.5300 | F1Score=0.8748
Batch-400: NLLLoss=0.5632 | F1Score=0.8735
Batch-450: NLLLoss=1.1457 | F1Score=0.8743
Batch-500: NLLLoss=0.2405 | F1Score=0.8749
Batch-518: NLLLoss=1.2864 | F1Score=0.8745

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.6915 | Mean F1Score: 0.8781
==================================================

EPOCH-6
Batch-50 : NLLLoss=0.5446 | F1Score=0.9488
Batch-100: NLLLoss=0.1697 | F1Score=0.9478
Batch-150: NLLLoss=0.2215 | F1Score=0.9467
Batch-200: NLLLoss=0.3147 | F1Score=0.9468
Batch-250: NLLLoss=0.4385 | F1Score=0.9459
Batch-300: NLLLoss=0.2305 | F1Score=0.9424
Batch-350: NLLLoss=0.4862 | F1Score=0.9408
Batch-400: NLLLoss=0.2013 | F1Score=0.9399
Batch-450: NLLLoss=0.3517 | F1Score=0.9393
Batch-500: NLLLoss=0.2968 | F1Score=0.9377
Batch-518: NLLLoss=0.6110 | F1Score=0.9371

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.3431 | Mean F1Score: 0.9447
==================================================

EPOCH-7
Batch-50 : NLLLoss=0.1828 | F1Score=0.9919
Batch-100: NLLLoss=0.0255 | F1Score=0.9925
Batch-150: NLLLoss=0.2095 | F1Score=0.9928
Batch-200: NLLLoss=0.0240 | F1Score=0.9913
Batch-250: NLLLoss=0.0840 | F1Score=0.9902
Batch-300: NLLLoss=0.1404 | F1Score=0.9899
Batch-350: NLLLoss=0.2597 | F1Score=0.9896
Batch-400: NLLLoss=0.2111 | F1Score=0.9883
Batch-450: NLLLoss=0.0698 | F1Score=0.9879
Batch-500: NLLLoss=0.0290 | F1Score=0.9876
Batch-518: NLLLoss=0.2910 | F1Score=0.9872

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.1144 | Mean F1Score: 0.9905
==================================================

EPOCH-8
Batch-50 : NLLLoss=0.0187 | F1Score=0.9994
Batch-100: NLLLoss=0.0254 | F1Score=0.9991
Batch-150: NLLLoss=0.0821 | F1Score=0.9994
Batch-200: NLLLoss=0.0141 | F1Score=0.9988
Batch-250: NLLLoss=0.0227 | F1Score=0.9989
Batch-300: NLLLoss=0.0223 | F1Score=0.9991
Batch-350: NLLLoss=0.0239 | F1Score=0.9991
Batch-400: NLLLoss=0.0508 | F1Score=0.9992
Batch-450: NLLLoss=0.0318 | F1Score=0.9991
Batch-500: NLLLoss=0.0251 | F1Score=0.9989
Batch-518: NLLLoss=0.0276 | F1Score=0.9990

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0286 | Mean F1Score: 0.9991
==================================================

EPOCH-9
Batch-50 : NLLLoss=0.0063 | F1Score=0.9997
Batch-100: NLLLoss=0.0166 | F1Score=0.9998
Batch-150: NLLLoss=0.0127 | F1Score=0.9995
Batch-200: NLLLoss=0.0170 | F1Score=0.9994
Batch-250: NLLLoss=0.0168 | F1Score=0.9995
Batch-300: NLLLoss=0.0173 | F1Score=0.9996
Batch-350: NLLLoss=0.0050 | F1Score=0.9996
Batch-400: NLLLoss=0.0102 | F1Score=0.9995
Batch-450: NLLLoss=0.0304 | F1Score=0.9995
Batch-500: NLLLoss=0.0086 | F1Score=0.9995
Batch-518: NLLLoss=0.0084 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0105 | Mean F1Score: 0.9995
==================================================

EPOCH-10
Batch-50 : NLLLoss=0.0077 | F1Score=0.9994
Batch-100: NLLLoss=0.0059 | F1Score=0.9987
Batch-150: NLLLoss=0.1637 | F1Score=0.9990
Batch-200: NLLLoss=0.0067 | F1Score=0.9990
Batch-250: NLLLoss=0.0069 | F1Score=0.9992
Batch-300: NLLLoss=0.0035 | F1Score=0.9993
Batch-350: NLLLoss=0.0077 | F1Score=0.9993
Batch-400: NLLLoss=0.0050 | F1Score=0.9994
Batch-450: NLLLoss=0.0056 | F1Score=0.9995
Batch-500: NLLLoss=0.0035 | F1Score=0.9995
Batch-518: NLLLoss=0.0035 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0076 | Mean F1Score: 0.9992
==================================================

EPOCH-11
Batch-50 : NLLLoss=0.0044 | F1Score=1.0000
Batch-100: NLLLoss=0.0025 | F1Score=1.0000
Batch-150: NLLLoss=0.0025 | F1Score=0.9999
Batch-200: NLLLoss=0.0041 | F1Score=0.9998
Batch-250: NLLLoss=0.0038 | F1Score=0.9999
Batch-300: NLLLoss=0.0034 | F1Score=0.9999
Batch-350: NLLLoss=0.0036 | F1Score=0.9999
Batch-400: NLLLoss=0.0031 | F1Score=0.9998
Batch-450: NLLLoss=0.0018 | F1Score=0.9998
Batch-500: NLLLoss=0.0047 | F1Score=0.9997
Batch-518: NLLLoss=0.0035 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0048 | Mean F1Score: 0.9999
==================================================

EPOCH-12
Batch-50 : NLLLoss=0.0035 | F1Score=0.9997
Batch-100: NLLLoss=0.0029 | F1Score=0.9998
Batch-150: NLLLoss=0.0025 | F1Score=0.9999
Batch-200: NLLLoss=0.0039 | F1Score=0.9998
Batch-250: NLLLoss=0.0024 | F1Score=0.9999
Batch-300: NLLLoss=0.0011 | F1Score=0.9999
Batch-350: NLLLoss=0.0029 | F1Score=0.9998
Batch-400: NLLLoss=0.0039 | F1Score=0.9996
Batch-450: NLLLoss=0.0099 | F1Score=0.9994
Batch-500: NLLLoss=0.0043 | F1Score=0.9995
Batch-518: NLLLoss=0.0028 | F1Score=0.9995

Huft üò•! Model not improved.
Mean NLLLoss: 0.0058 | Mean F1Score: 0.9997
Patience = 1/20‚ùó
==================================================

EPOCH-13
Batch-50 : NLLLoss=0.2077 | F1Score=0.9984
Batch-100: NLLLoss=0.0069 | F1Score=0.9989
Batch-150: NLLLoss=0.0029 | F1Score=0.9989
Batch-200: NLLLoss=0.0074 | F1Score=0.9991
Batch-250: NLLLoss=0.0040 | F1Score=0.9992
Batch-300: NLLLoss=0.0075 | F1Score=0.9993
Batch-350: NLLLoss=0.0041 | F1Score=0.9994
Batch-400: NLLLoss=0.0158 | F1Score=0.9995
Batch-450: NLLLoss=0.0399 | F1Score=0.9994
Batch-500: NLLLoss=0.7900 | F1Score=0.9912
Batch-518: NLLLoss=0.5055 | F1Score=0.9876

Huft üò•! Model not improved.
Mean NLLLoss: 0.0594 | Mean F1Score: 0.9986
Patience = 2/20‚ùó
==================================================

EPOCH-14
Batch-50 : NLLLoss=0.7197 | F1Score=0.8865
Batch-100: NLLLoss=0.4517 | F1Score=0.9064
Batch-150: NLLLoss=0.2416 | F1Score=0.9118
Batch-200: NLLLoss=0.2324 | F1Score=0.9234
Batch-250: NLLLoss=0.1667 | F1Score=0.9281
Batch-300: NLLLoss=0.2671 | F1Score=0.9322
Batch-350: NLLLoss=0.0958 | F1Score=0.9367
Batch-400: NLLLoss=0.0954 | F1Score=0.9387
Batch-450: NLLLoss=0.0715 | F1Score=0.9408
Batch-500: NLLLoss=0.0776 | F1Score=0.9435
Batch-518: NLLLoss=0.1189 | F1Score=0.9441

Huft üò•! Model not improved.
Mean NLLLoss: 0.2308 | Mean F1Score: 0.9220
Patience = 3/20‚ùó
==================================================

EPOCH-15
Batch-50 : NLLLoss=0.0522 | F1Score=0.9925
Batch-100: NLLLoss=0.0218 | F1Score=0.9934
Batch-150: NLLLoss=0.0079 | F1Score=0.9946
Batch-200: NLLLoss=0.0065 | F1Score=0.9953
Batch-250: NLLLoss=0.0031 | F1Score=0.9962
Batch-300: NLLLoss=0.0097 | F1Score=0.9967
Batch-350: NLLLoss=0.0156 | F1Score=0.9971
Batch-400: NLLLoss=0.0028 | F1Score=0.9974
Batch-450: NLLLoss=0.0304 | F1Score=0.9973
Batch-500: NLLLoss=0.0182 | F1Score=0.9976
Batch-518: NLLLoss=0.0051 | F1Score=0.9976

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0154 | Mean F1Score: 0.9955
==================================================

EPOCH-16
Batch-50 : NLLLoss=0.0011 | F1Score=0.9987
Batch-100: NLLLoss=0.0018 | F1Score=0.9994
Batch-150: NLLLoss=0.0017 | F1Score=0.9994
Batch-200: NLLLoss=0.0008 | F1Score=0.9994
Batch-250: NLLLoss=0.0021 | F1Score=0.9995
Batch-300: NLLLoss=0.0018 | F1Score=0.9995
Batch-350: NLLLoss=0.0015 | F1Score=0.9995
Batch-400: NLLLoss=0.0018 | F1Score=0.9995
Batch-450: NLLLoss=0.0015 | F1Score=0.9996
Batch-500: NLLLoss=0.0019 | F1Score=0.9996
Batch-518: NLLLoss=0.0003 | F1Score=0.9996

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0039 | Mean F1Score: 0.9994
==================================================

EPOCH-17
Batch-50 : NLLLoss=0.0021 | F1Score=0.9997
Batch-100: NLLLoss=0.0016 | F1Score=0.9998
Batch-150: NLLLoss=0.0016 | F1Score=0.9999
Batch-200: NLLLoss=0.0020 | F1Score=0.9999
Batch-250: NLLLoss=0.0010 | F1Score=0.9999
Batch-300: NLLLoss=0.0016 | F1Score=0.9998
Batch-350: NLLLoss=0.0013 | F1Score=0.9998
Batch-400: NLLLoss=0.0007 | F1Score=0.9998
Batch-450: NLLLoss=0.0012 | F1Score=0.9998
Batch-500: NLLLoss=0.0005 | F1Score=0.9998
Batch-518: NLLLoss=0.0018 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0015 | Mean F1Score: 0.9998
==================================================

EPOCH-18
Batch-50 : NLLLoss=0.0010 | F1Score=0.9997
Batch-100: NLLLoss=0.0007 | F1Score=0.9997
Batch-150: NLLLoss=0.0006 | F1Score=0.9998
Batch-200: NLLLoss=0.0013 | F1Score=0.9998
Batch-250: NLLLoss=0.0015 | F1Score=0.9999
Batch-300: NLLLoss=0.0012 | F1Score=0.9998
Batch-350: NLLLoss=0.0005 | F1Score=0.9998
Batch-400: NLLLoss=0.0008 | F1Score=0.9998
Batch-450: NLLLoss=0.0009 | F1Score=0.9998
Batch-500: NLLLoss=0.0007 | F1Score=0.9998
Batch-518: NLLLoss=0.0013 | F1Score=0.9998

Huft üò•! Model not improved.
Mean NLLLoss: 0.0017 | Mean F1Score: 0.9998
Patience = 4/20‚ùó
==================================================

EPOCH-19
Batch-50 : NLLLoss=0.0006 | F1Score=1.0000
Batch-100: NLLLoss=0.0008 | F1Score=1.0000
Batch-150: NLLLoss=0.0011 | F1Score=0.9999
Batch-200: NLLLoss=0.0009 | F1Score=0.9999
Batch-250: NLLLoss=0.0008 | F1Score=0.9998
Batch-300: NLLLoss=0.0009 | F1Score=0.9998
Batch-350: NLLLoss=0.0003 | F1Score=0.9998
Batch-400: NLLLoss=0.0013 | F1Score=0.9998
Batch-450: NLLLoss=0.0010 | F1Score=0.9998
Batch-500: NLLLoss=0.0004 | F1Score=0.9998
Batch-518: NLLLoss=0.0013 | F1Score=0.9998

Huft üò•! Model not improved.
Mean NLLLoss: 0.0018 | Mean F1Score: 0.9999
Patience = 5/20‚ùó
==================================================

EPOCH-20
Batch-50 : NLLLoss=0.0007 | F1Score=1.0000
Batch-100: NLLLoss=0.0005 | F1Score=1.0000
Batch-150: NLLLoss=0.0015 | F1Score=0.9997
Batch-200: NLLLoss=0.0006 | F1Score=0.9998
Batch-250: NLLLoss=0.0007 | F1Score=0.9998
Batch-300: NLLLoss=0.0007 | F1Score=0.9998
Batch-350: NLLLoss=0.0007 | F1Score=0.9998
Batch-400: NLLLoss=0.0006 | F1Score=0.9998
Batch-450: NLLLoss=0.0003 | F1Score=0.9998
Batch-500: NLLLoss=0.0006 | F1Score=0.9997
Batch-518: NLLLoss=0.0551 | F1Score=0.9997

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0012 | Mean F1Score: 0.9998
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss      : 0.0012
Best F1Score      : 0.9998
Training duration : 27.812 minutes.
Training date     : 2022-10-11 16:08:07.936452+08:00
