HYPERPARAMETERS
--------------------------------------------------
context_size: 20
input_size_left_context: 64
input_size_oov_context: 20
input_size_right_context: 64
batch_size: 32
num_hidden_layer: 1
hidden_size: 128
output_size: 3611
shuffle: True
lr: 0.001
batch_first: True
bidirectional: True
init_wb_with_kaiming_normal: True
n_epoch: 20
patience: 20
device: cpu


TRAINING PROGRESS
--------------------------------------------------
EPOCH-1
Batch-50 : NLLLoss=4.3955 | F1Score=0.2763
Batch-100: NLLLoss=5.5535 | F1Score=0.2875
Batch-150: NLLLoss=4.9265 | F1Score=0.3023
Batch-200: NLLLoss=4.7751 | F1Score=0.3271
Batch-250: NLLLoss=3.1236 | F1Score=0.3532
Batch-300: NLLLoss=4.4950 | F1Score=0.3720
Batch-350: NLLLoss=5.0200 | F1Score=0.3871
Batch-400: NLLLoss=3.5484 | F1Score=0.3999
Batch-450: NLLLoss=3.1183 | F1Score=0.4152
Batch-500: NLLLoss=3.9979 | F1Score=0.4303
Batch-518: NLLLoss=2.8809 | F1Score=0.4347

Mean NLLLoss: 4.4998 | Mean F1Score: 0.3450
==================================================

EPOCH-2
Batch-50 : NLLLoss=2.6012 | F1Score=0.6000
Batch-100: NLLLoss=2.3296 | F1Score=0.6050
Batch-150: NLLLoss=3.9705 | F1Score=0.6088
Batch-200: NLLLoss=1.6539 | F1Score=0.6097
Batch-250: NLLLoss=2.4428 | F1Score=0.6113
Batch-300: NLLLoss=3.1977 | F1Score=0.6152
Batch-350: NLLLoss=2.8563 | F1Score=0.6240
Batch-400: NLLLoss=3.2789 | F1Score=0.6296
Batch-450: NLLLoss=2.1281 | F1Score=0.6340
Batch-500: NLLLoss=1.6332 | F1Score=0.6398
Batch-518: NLLLoss=2.7613 | F1Score=0.6412

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 2.6987 | Mean F1Score: 0.6164
==================================================

EPOCH-3
Batch-50 : NLLLoss=1.8852 | F1Score=0.7150
Batch-100: NLLLoss=1.6940 | F1Score=0.7272
Batch-150: NLLLoss=1.6992 | F1Score=0.7292
Batch-200: NLLLoss=2.0828 | F1Score=0.7269
Batch-250: NLLLoss=2.4494 | F1Score=0.7290
Batch-300: NLLLoss=2.1653 | F1Score=0.7342
Batch-350: NLLLoss=1.5881 | F1Score=0.7366
Batch-400: NLLLoss=1.7584 | F1Score=0.7381
Batch-450: NLLLoss=2.0288 | F1Score=0.7417
Batch-500: NLLLoss=2.8514 | F1Score=0.7444
Batch-518: NLLLoss=1.6888 | F1Score=0.7446

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.7877 | Mean F1Score: 0.7302
==================================================

EPOCH-4
Batch-50 : NLLLoss=0.8594 | F1Score=0.8306
Batch-100: NLLLoss=0.7579 | F1Score=0.8213
Batch-150: NLLLoss=0.7341 | F1Score=0.8184
Batch-200: NLLLoss=1.3693 | F1Score=0.8185
Batch-250: NLLLoss=1.0412 | F1Score=0.8139
Batch-300: NLLLoss=1.4440 | F1Score=0.8105
Batch-350: NLLLoss=1.3702 | F1Score=0.8144
Batch-400: NLLLoss=0.8197 | F1Score=0.8147
Batch-450: NLLLoss=0.9357 | F1Score=0.8168
Batch-500: NLLLoss=1.3106 | F1Score=0.8168
Batch-518: NLLLoss=0.8287 | F1Score=0.8169

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 1.1618 | Mean F1Score: 0.8158
==================================================

EPOCH-5
Batch-50 : NLLLoss=0.9553 | F1Score=0.8859
Batch-100: NLLLoss=0.7529 | F1Score=0.8867
Batch-150: NLLLoss=0.3564 | F1Score=0.8891
Batch-200: NLLLoss=0.5915 | F1Score=0.8851
Batch-250: NLLLoss=0.3411 | F1Score=0.8839
Batch-300: NLLLoss=0.8736 | F1Score=0.8803
Batch-350: NLLLoss=0.4582 | F1Score=0.8773
Batch-400: NLLLoss=0.6173 | F1Score=0.8762
Batch-450: NLLLoss=1.2344 | F1Score=0.8758
Batch-500: NLLLoss=0.3545 | F1Score=0.8753
Batch-518: NLLLoss=1.3853 | F1Score=0.8746

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.6822 | Mean F1Score: 0.8820
==================================================

EPOCH-6
Batch-50 : NLLLoss=0.2062 | F1Score=0.9550
Batch-100: NLLLoss=0.4009 | F1Score=0.9556
Batch-150: NLLLoss=0.3510 | F1Score=0.9521
Batch-200: NLLLoss=0.1695 | F1Score=0.9505
Batch-250: NLLLoss=0.2732 | F1Score=0.9493
Batch-300: NLLLoss=0.3856 | F1Score=0.9471
Batch-350: NLLLoss=0.4976 | F1Score=0.9470
Batch-400: NLLLoss=0.2082 | F1Score=0.9452
Batch-450: NLLLoss=0.3190 | F1Score=0.9451
Batch-500: NLLLoss=0.0796 | F1Score=0.9439
Batch-518: NLLLoss=0.3234 | F1Score=0.9434

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.3252 | Mean F1Score: 0.9498
==================================================

EPOCH-7
Batch-50 : NLLLoss=0.0296 | F1Score=0.9866
Batch-100: NLLLoss=0.0391 | F1Score=0.9895
Batch-150: NLLLoss=0.1193 | F1Score=0.9885
Batch-200: NLLLoss=0.1073 | F1Score=0.9889
Batch-250: NLLLoss=0.1467 | F1Score=0.9887
Batch-300: NLLLoss=0.0936 | F1Score=0.9896
Batch-350: NLLLoss=0.2076 | F1Score=0.9901
Batch-400: NLLLoss=0.1069 | F1Score=0.9891
Batch-450: NLLLoss=0.2137 | F1Score=0.9888
Batch-500: NLLLoss=0.1209 | F1Score=0.9886
Batch-518: NLLLoss=0.1424 | F1Score=0.9882

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.1084 | Mean F1Score: 0.9888
==================================================

EPOCH-8
Batch-50 : NLLLoss=0.0579 | F1Score=0.9984
Batch-100: NLLLoss=0.0319 | F1Score=0.9980
Batch-150: NLLLoss=0.0279 | F1Score=0.9979
Batch-200: NLLLoss=0.1093 | F1Score=0.9981
Batch-250: NLLLoss=0.0120 | F1Score=0.9984
Batch-300: NLLLoss=0.0136 | F1Score=0.9985
Batch-350: NLLLoss=0.0277 | F1Score=0.9986
Batch-400: NLLLoss=0.0572 | F1Score=0.9986
Batch-450: NLLLoss=0.0333 | F1Score=0.9987
Batch-500: NLLLoss=0.0072 | F1Score=0.9988
Batch-518: NLLLoss=0.0053 | F1Score=0.9987

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0292 | Mean F1Score: 0.9983
==================================================

EPOCH-9
Batch-50 : NLLLoss=0.0071 | F1Score=1.0000
Batch-100: NLLLoss=0.0141 | F1Score=0.9994
Batch-150: NLLLoss=0.0054 | F1Score=0.9996
Batch-200: NLLLoss=0.0094 | F1Score=0.9997
Batch-250: NLLLoss=0.0083 | F1Score=0.9996
Batch-300: NLLLoss=0.0112 | F1Score=0.9996
Batch-350: NLLLoss=0.0066 | F1Score=0.9995
Batch-400: NLLLoss=0.0062 | F1Score=0.9995
Batch-450: NLLLoss=0.0085 | F1Score=0.9995
Batch-500: NLLLoss=0.0045 | F1Score=0.9994
Batch-518: NLLLoss=0.0244 | F1Score=0.9993

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0108 | Mean F1Score: 0.9996
==================================================

EPOCH-10
Batch-50 : NLLLoss=0.0072 | F1Score=0.9997
Batch-100: NLLLoss=0.0079 | F1Score=0.9998
Batch-150: NLLLoss=0.0061 | F1Score=0.9999
Batch-200: NLLLoss=0.0059 | F1Score=0.9999
Batch-250: NLLLoss=0.0031 | F1Score=0.9997
Batch-300: NLLLoss=0.0104 | F1Score=0.9997
Batch-350: NLLLoss=0.0054 | F1Score=0.9998
Batch-400: NLLLoss=0.0072 | F1Score=0.9996
Batch-450: NLLLoss=0.0051 | F1Score=0.9996
Batch-500: NLLLoss=0.0027 | F1Score=0.9995
Batch-518: NLLLoss=0.0068 | F1Score=0.9995

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0073 | Mean F1Score: 0.9997
==================================================

EPOCH-11
Batch-50 : NLLLoss=0.0040 | F1Score=1.0000
Batch-100: NLLLoss=0.0036 | F1Score=0.9994
Batch-150: NLLLoss=0.0050 | F1Score=0.9990
Batch-200: NLLLoss=0.0094 | F1Score=0.9989
Batch-250: NLLLoss=0.0042 | F1Score=0.9991
Batch-300: NLLLoss=0.0030 | F1Score=0.9990
Batch-350: NLLLoss=0.0062 | F1Score=0.9989
Batch-400: NLLLoss=0.0141 | F1Score=0.9986
Batch-450: NLLLoss=0.2654 | F1Score=0.9939
Batch-500: NLLLoss=0.1820 | F1Score=0.9857
Batch-518: NLLLoss=0.1023 | F1Score=0.9836

Huft üò•! Model not improved.
Mean NLLLoss: 0.0776 | Mean F1Score: 0.9975
Patience = 1/20‚ùó
==================================================

EPOCH-12
Batch-50 : NLLLoss=0.3117 | F1Score=0.9472
Batch-100: NLLLoss=0.0538 | F1Score=0.9517
Batch-150: NLLLoss=0.1324 | F1Score=0.9562
Batch-200: NLLLoss=0.0566 | F1Score=0.9598
Batch-250: NLLLoss=0.0620 | F1Score=0.9614
Batch-300: NLLLoss=0.0795 | F1Score=0.9634
Batch-350: NLLLoss=0.1344 | F1Score=0.9637
Batch-400: NLLLoss=0.0171 | F1Score=0.9655
Batch-450: NLLLoss=0.1525 | F1Score=0.9662
Batch-500: NLLLoss=0.0631 | F1Score=0.9677
Batch-518: NLLLoss=0.0671 | F1Score=0.9681

Huft üò•! Model not improved.
Mean NLLLoss: 0.1418 | Mean F1Score: 0.9601
Patience = 2/20‚ùó
==================================================

EPOCH-13
Batch-50 : NLLLoss=0.0115 | F1Score=0.9981
Batch-100: NLLLoss=0.0157 | F1Score=0.9981
Batch-150: NLLLoss=0.0068 | F1Score=0.9985
Batch-200: NLLLoss=0.0043 | F1Score=0.9987
Batch-250: NLLLoss=0.0063 | F1Score=0.9983
Batch-300: NLLLoss=0.0206 | F1Score=0.9982
Batch-350: NLLLoss=0.0278 | F1Score=0.9983
Batch-400: NLLLoss=0.0147 | F1Score=0.9985
Batch-450: NLLLoss=0.0084 | F1Score=0.9985
Batch-500: NLLLoss=0.0291 | F1Score=0.9987
Batch-518: NLLLoss=0.0073 | F1Score=0.9987

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0122 | Mean F1Score: 0.9984
==================================================

EPOCH-14
Batch-50 : NLLLoss=0.0028 | F1Score=0.9984
Batch-100: NLLLoss=0.0025 | F1Score=0.9989
Batch-150: NLLLoss=0.0031 | F1Score=0.9993
Batch-200: NLLLoss=0.0022 | F1Score=0.9995
Batch-250: NLLLoss=0.0019 | F1Score=0.9994
Batch-300: NLLLoss=0.0013 | F1Score=0.9995
Batch-350: NLLLoss=0.0017 | F1Score=0.9994
Batch-400: NLLLoss=0.0015 | F1Score=0.9995
Batch-450: NLLLoss=0.0019 | F1Score=0.9994
Batch-500: NLLLoss=0.0017 | F1Score=0.9994
Batch-518: NLLLoss=0.0014 | F1Score=0.9994

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0036 | Mean F1Score: 0.9993
==================================================

EPOCH-15
Batch-50 : NLLLoss=0.0023 | F1Score=1.0000
Batch-100: NLLLoss=0.0011 | F1Score=0.9997
Batch-150: NLLLoss=0.0017 | F1Score=0.9997
Batch-200: NLLLoss=0.0021 | F1Score=0.9995
Batch-250: NLLLoss=0.0017 | F1Score=0.9996
Batch-300: NLLLoss=0.0015 | F1Score=0.9997
Batch-350: NLLLoss=0.0011 | F1Score=0.9997
Batch-400: NLLLoss=0.0016 | F1Score=0.9998
Batch-450: NLLLoss=0.0015 | F1Score=0.9998
Batch-500: NLLLoss=0.0005 | F1Score=0.9997
Batch-518: NLLLoss=0.0005 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0023 | Mean F1Score: 0.9997
==================================================

EPOCH-16
Batch-50 : NLLLoss=0.0013 | F1Score=1.0000
Batch-100: NLLLoss=0.0008 | F1Score=0.9992
Batch-150: NLLLoss=0.0013 | F1Score=0.9994
Batch-200: NLLLoss=0.0016 | F1Score=0.9995
Batch-250: NLLLoss=0.0007 | F1Score=0.9996
Batch-300: NLLLoss=0.0008 | F1Score=0.9996
Batch-350: NLLLoss=0.0014 | F1Score=0.9996
Batch-400: NLLLoss=0.0010 | F1Score=0.9997
Batch-450: NLLLoss=0.0011 | F1Score=0.9997
Batch-500: NLLLoss=0.0010 | F1Score=0.9997
Batch-518: NLLLoss=0.0005 | F1Score=0.9997

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0021 | Mean F1Score: 0.9996
==================================================

EPOCH-17
Batch-50 : NLLLoss=0.0008 | F1Score=1.0000
Batch-100: NLLLoss=0.0006 | F1Score=0.9998
Batch-150: NLLLoss=0.0007 | F1Score=0.9999
Batch-200: NLLLoss=0.0007 | F1Score=0.9999
Batch-250: NLLLoss=0.0007 | F1Score=0.9999
Batch-300: NLLLoss=0.0007 | F1Score=0.9998
Batch-350: NLLLoss=0.0009 | F1Score=0.9999
Batch-400: NLLLoss=0.0005 | F1Score=0.9999
Batch-450: NLLLoss=0.0011 | F1Score=0.9999
Batch-500: NLLLoss=0.0009 | F1Score=0.9999
Batch-518: NLLLoss=0.0005 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0011 | Mean F1Score: 0.9999
==================================================

EPOCH-18
Batch-50 : NLLLoss=0.0004 | F1Score=1.0000
Batch-100: NLLLoss=0.0007 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=1.0000
Batch-200: NLLLoss=0.0004 | F1Score=0.9998
Batch-250: NLLLoss=0.0008 | F1Score=0.9999
Batch-300: NLLLoss=0.0005 | F1Score=0.9998
Batch-350: NLLLoss=0.0004 | F1Score=0.9999
Batch-400: NLLLoss=0.0004 | F1Score=0.9999
Batch-450: NLLLoss=0.0009 | F1Score=0.9999
Batch-500: NLLLoss=0.1344 | F1Score=0.9998
Batch-518: NLLLoss=0.0004 | F1Score=0.9998

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0011 | Mean F1Score: 0.9999
==================================================

EPOCH-19
Batch-50 : NLLLoss=0.0006 | F1Score=1.0000
Batch-100: NLLLoss=0.0005 | F1Score=1.0000
Batch-150: NLLLoss=0.0005 | F1Score=1.0000
Batch-200: NLLLoss=0.0005 | F1Score=0.9999
Batch-250: NLLLoss=0.0004 | F1Score=0.9999
Batch-300: NLLLoss=0.0004 | F1Score=0.9999
Batch-350: NLLLoss=0.0006 | F1Score=0.9999
Batch-400: NLLLoss=0.0010 | F1Score=0.9999
Batch-450: NLLLoss=0.0002 | F1Score=0.9999
Batch-500: NLLLoss=0.0008 | F1Score=0.9999
Batch-518: NLLLoss=0.0004 | F1Score=0.9999

Yeah üéâüòÑ! Model improved.
Mean NLLLoss: 0.0007 | Mean F1Score: 0.9999
==================================================

EPOCH-20
Batch-50 : NLLLoss=0.0004 | F1Score=1.0000
Batch-100: NLLLoss=0.0004 | F1Score=1.0000
Batch-150: NLLLoss=0.0004 | F1Score=1.0000
Batch-200: NLLLoss=0.0004 | F1Score=0.9998
Batch-250: NLLLoss=0.0006 | F1Score=0.9997
Batch-300: NLLLoss=0.0006 | F1Score=0.9998
Batch-350: NLLLoss=0.0003 | F1Score=0.9998
Batch-400: NLLLoss=0.0003 | F1Score=0.9998
Batch-450: NLLLoss=0.0064 | F1Score=0.9998
Batch-500: NLLLoss=0.0049 | F1Score=0.9995
Batch-518: NLLLoss=0.0124 | F1Score=0.9993

Huft üò•! Model not improved.
Mean NLLLoss: 0.0043 | Mean F1Score: 0.9999
Patience = 3/20‚ùó
==================================================


TRAINING SUMMARY
--------------------------------------------------
Best NLLLoss      : 0.0007
Best F1Score      : 0.9999
Training duration : 15.589 minutes.
Training date     : 2022-10-11 11:58:43.365288+08:00
