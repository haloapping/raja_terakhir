{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8319205c-12ab-4314-8558-cfae2ca2d1bb",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f1e2d8-ccca-4f8b-97e8-4ab8bf6c5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from einops import rearrange\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassConfusionMatrix\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015ead4-c615-4dbc-aa8c-c7e611c3b76e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecab9839-baf2-480b-a00d-53dc81c66ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=65,\n",
    "        fold=1,\n",
    "        max_seq_len=82,\n",
    "        input_size=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        dropout=0,\n",
    "        bias=True,\n",
    "        output_size=24,\n",
    "        shuffle=True,\n",
    "        lr=0.005,\n",
    "        batch_first=False,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=50,\n",
    "        patience=50,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.fold = fold\n",
    "        self.input_size = input_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "\n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651614b-b4a1-4968-88a1-ac38e4dc8323",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d70aa1f-c03c-4ec8-b720-0eca1d63020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pos_tag = pd.read_csv(\"../../../datasets/raw/Indonesian_Manually_Tagged_Corpus.tsv\", sep=\"\\t\", header=None, names=[\"token\", \"tag\"], quoting=csv.QUOTE_NONE, skip_blank_lines=False) \n",
    "train = pd.read_csv(f\"../../../datasets/raw/cv/train/train.0{hyperparams.fold}.tsv\", sep=\"\\t\", header=None, names=[\"token\", \"tag\"], quoting=csv.QUOTE_NONE, skip_blank_lines=False)\n",
    "val = pd.read_csv(f\"../../../datasets/raw/cv/val/val.0{hyperparams.fold}.tsv\", sep=\"\\t\", header=None, names=[\"token\", \"tag\"], quoting=csv.QUOTE_NONE, skip_blank_lines=False)\n",
    "test = pd.read_csv(f\"../../../datasets/raw/cv/test/test.0{hyperparams.fold}.tsv\", sep=\"\\t\", header=None, names=[\"token\", \"tag\"], quoting=csv.QUOTE_NONE, skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ce54d-7e46-4d41-aa8e-1dd7f33b8fa4",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c6717c-8302-45dd-acfc-625c92530a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../logs/comick/65_contexts/10-11-2022_12-21-42/oov_embedding_dict.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5374/1056228644.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_expansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCaseExpander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moov_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../../../logs/comick/{hyperparams.context_size}_contexts/10-11-2022_12-21-42/oov_embedding_dict.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../logs/comick/65_contexts/10-11-2022_12-21-42/oov_embedding_dict.pkl'"
     ]
    }
   ],
   "source": [
    "embeddings = Embedding.load(\"../../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "embeddings.apply_expansion(DigitExpander)\n",
    "embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "oov_embeddings = pickle.load(open(f\"../../../logs/comick/{hyperparams.context_size}_contexts/10-11-2022_17-31-59/oov_embedding_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ed87f-c18d-4a59-a47b-c4e874b951bb",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29837c-9113-4d5d-b835-351bce316e07",
   "metadata": {},
   "source": [
    "## Add OOV Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aeabff-e02c-4d83-bdee-ca428d401eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_oov_flag(tokens, embeddings):\n",
    "    oov_flags = []\n",
    "    \n",
    "    for token in tqdm(tokens):\n",
    "        try:\n",
    "            if token not in embeddings:\n",
    "                oov_flags.append(True)\n",
    "            else:\n",
    "                oov_flags.append(False)\n",
    "        except:\n",
    "            oov_flags.append(False)\n",
    "        \n",
    "    return pd.DataFrame(oov_flags, columns=[\"is_oov\"])\n",
    "\n",
    "id_pos_tag_oov_flags = add_oov_flag(id_pos_tag[\"token\"].values, embeddings)\n",
    "train_oov_flags = add_oov_flag(train[\"token\"].values, embeddings)\n",
    "val_oov_flags = add_oov_flag(val[\"token\"].values, embeddings)\n",
    "test_oov_flags = add_oov_flag(test[\"token\"].values, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a6631-74de-483c-b03d-a4176880b8af",
   "metadata": {},
   "source": [
    "## Concate OOV Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cddb08-c462-4fd0-b348-44f5464cbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pos_tag_df = pd.concat([id_pos_tag, id_pos_tag_oov_flags], axis=1)\n",
    "train_df = pd.concat([train, train_oov_flags], axis=1)\n",
    "val_df = pd.concat([val, val_oov_flags], axis=1)\n",
    "test_df = pd.concat([test, test_oov_flags], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183be89-5b99-42f8-bedf-8061a990d8b1",
   "metadata": {},
   "source": [
    "## Lowercase OOV Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d0127-b999-49e3-ae3d-73d01c5cb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pos_tag_df['token'] = np.where(id_pos_tag_df['is_oov'] == True, id_pos_tag_df['token'].str.lower(), id_pos_tag_df['token'])\n",
    "train_df['token'] = np.where(train_df['is_oov'] == True, train_df['token'].str.lower(), train_df['token'])\n",
    "val_df['token'] = np.where(val_df['is_oov'] == True, val_df['token'].str.lower(), val_df['token'])\n",
    "test_df['token'] = np.where(test_df['is_oov'] == True, test_df['token'].str.lower(), test_df['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d866169-d378-495d-9af7-c1939feaae40",
   "metadata": {},
   "source": [
    "## Embedding Dict, Index to Token and Token to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130fc5d-3e7e-480d-9f47-b5af4b2ce3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(id_pos_tag_df[[\"token\", \"tag\", \"is_oov\"]].itertuples(index=False, name=None))\n",
    "\n",
    "def embedding_dict(tokens, embeddings, oov_embeddings):\n",
    "    embedding = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token[0] is not np.nan:\n",
    "            if token[2] == False or token[0] in embeddings:\n",
    "                embedding[token[0]] = embeddings[token[0]]\n",
    "            else:\n",
    "                embedding[token[0]] = oov_embeddings[token[0].lower()]\n",
    "\n",
    "    return embedding\n",
    "\n",
    "embedding_dict = embedding_dict(tokens, embeddings, oov_embeddings)\n",
    "embedding_dict[\"<PAD>\"] = embeddings[\"<PAD>\"]\n",
    "word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(embedding_dict.values()))), padding_idx=list(embedding_dict.keys()).index(\"<PAD>\"), freeze=True)\n",
    "\n",
    "idx_to_token = {idx: token for idx, token in enumerate(list(embedding_dict.keys()))}\n",
    "token_to_idx = {token: idx for idx, token in enumerate(list(embedding_dict.keys()))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab92441-64b9-430e-a8d6-4b4df859cefc",
   "metadata": {},
   "source": [
    "## Token to Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3703497-9ee3-43b9-9705-fcfd2a3f1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(tokens, max_length_sentence=hyperparams.max_seq_len):\n",
    "    sentence = []\n",
    "    sentences = []\n",
    "\n",
    "    for token in tqdm(tokens):\n",
    "        if token[0] is not np.nan:\n",
    "            sentence.append(token)\n",
    "        else:\n",
    "            sentences.append(sentence[:max_length_sentence])\n",
    "            sentence = []\n",
    "\n",
    "    return sentences\n",
    "\n",
    "all_sentence = make_sentence(list(id_pos_tag_df[[\"token\", \"tag\", \"is_oov\"]].itertuples(index=False, name=None)))\n",
    "train_sentences = make_sentence(list(train_df[[\"token\", \"tag\", \"is_oov\"]].itertuples(index=False, name=None)))\n",
    "val_sentences = make_sentence(list(test_df[[\"token\", \"tag\", \"is_oov\"]].itertuples(index=False, name=None)))\n",
    "test_sentences = make_sentence(list(val_df[[\"token\", \"tag\", \"is_oov\"]].itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e44ae-dae4-421f-b35e-97e23727e90c",
   "metadata": {},
   "source": [
    "## Word Token, Padding, and Word token to Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0cf3b-695c-4abe-b462-f5aed832efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docs(docs, idx_token):\n",
    "    new_sentence = []\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in tqdm(docs):\n",
    "        for token in sentence:\n",
    "            new_sentence.append(token[idx_token])\n",
    "        sentences.append(new_sentence)\n",
    "        new_sentence = []\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def padding(docs, max_seq_len=hyperparams.max_seq_len, mode=\"post\", val_pad=\"<PAD>\"):\n",
    "    docs = deepcopy(docs)\n",
    "    doc_with_pad = []\n",
    "    docs_with_pad = []\n",
    "\n",
    "    for doc in tqdm(docs):\n",
    "        if mode == \"pre\":\n",
    "            for _ in range(max_seq_len - len(doc)):\n",
    "                doc.insert(0, val_pad)\n",
    "        elif mode == \"post\":\n",
    "            for _ in range(max_seq_len - len(doc)):\n",
    "                doc.append(val_pad)\n",
    "        else:\n",
    "            return f\"Mode {mode} is not available, use instead 'pre' or 'post'.\"\n",
    "        \n",
    "        docs_with_pad.append(doc)\n",
    "        \n",
    "    return np.array(docs_with_pad)\n",
    "\n",
    "def sent_to_idx(docs, token_to_idx):\n",
    "    new_sentence = []\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in tqdm(docs):\n",
    "        for token in sentence:\n",
    "            new_sentence.append(token_to_idx[token])\n",
    "        sentences.append(new_sentence)\n",
    "        new_sentence = []\n",
    "\n",
    "    return np.array(sentences)\n",
    "\n",
    "def convert_feature_to_idx(docs, idx_token, token_to_idx, max_seq_len):\n",
    "    sentences = tokenize_docs(docs, idx_token)\n",
    "    sentences_with_pad = padding(sentences, max_seq_len=max_seq_len, mode=\"post\", val_pad=\"<PAD>\")\n",
    "    sentences_to_idx = sent_to_idx(sentences_with_pad, token_to_idx)\n",
    "    \n",
    "    return sentences_to_idx\n",
    "\n",
    "train_sentence_idxs = convert_feature_to_idx(train_sentences, 0, token_to_idx, hyperparams.max_seq_len)\n",
    "val_sentence_idxs = convert_feature_to_idx(val_sentences, 0, token_to_idx, hyperparams.max_seq_len)\n",
    "test_sentence_idxs = convert_feature_to_idx(test_sentences, 0, token_to_idx, hyperparams.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2d150-0276-4ea8-a844-1a06b0b911c6",
   "metadata": {},
   "source": [
    "## Encode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9be34-f098-4b1e-8a4e-d8d06e43e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(np.delete(id_pos_tag_df[\"tag\"].unique(), 3))\n",
    "idx_to_label = {idx: label for idx, label in enumerate(classes + [\"<PAD>\"])}\n",
    "label_to_idx = {label: idx for idx, label in enumerate(classes + [\"<PAD>\"])}\n",
    "\n",
    "def encode_class(docs, label_to_idx, seq_len=hyperparams.max_seq_len):\n",
    "    class_idxs = []\n",
    "    classes = []\n",
    "    \n",
    "    for sentence in tqdm(docs):\n",
    "        for token in sentence:\n",
    "            class_idxs.append(label_to_idx[token[1]])\n",
    "            \n",
    "        for _ in range(seq_len - len(sentence)):\n",
    "            class_idxs.append(label_to_idx[\"<PAD>\"])\n",
    "                \n",
    "        classes.append(class_idxs)\n",
    "        class_idxs = []\n",
    "\n",
    "    return np.array(classes)\n",
    "        \n",
    "train_class_idxs = encode_class(train_sentences, label_to_idx)\n",
    "val_class_idxs = encode_class(val_sentences, label_to_idx)\n",
    "test_class_idxs = encode_class(test_sentences, label_to_idx)\n",
    "\n",
    "print(f\"Training shape   : {train_sentence_idxs.shape, train_class_idxs.shape}\")\n",
    "print(f\"validation shape : {val_class_idxs.shape, val_class_idxs.shape}\")\n",
    "print(f\"Test shape       : {test_sentence_idxs.shape, test_class_idxs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6d808-0ee8-4979-bcee-43b5020ac96f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Note\n",
    "\n",
    "- Number of sentences = 10030\n",
    "- Train : 72% (7222)\n",
    "- Val   : 8% (802)\n",
    "- Test  : 20% (2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33573647-a8e8-474e-9c88-beee03332ec6",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f0ccc-e2ca-4aa7-b1c3-d782696a3fab",
   "metadata": {},
   "source": [
    "## Feature and Actual Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927d980-8ee5-4a78-870a-7bef385dbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = torch.LongTensor(train_sentence_idxs)\n",
    "val_feature = torch.LongTensor(val_sentence_idxs)\n",
    "test_feature = torch.LongTensor(test_sentence_idxs)\n",
    "\n",
    "train_class = torch.LongTensor(train_class_idxs)\n",
    "val_class = torch.LongTensor(val_class_idxs)\n",
    "test_class = torch.LongTensor(test_class_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf817-b692-48d7-b7ac-3e96f4fcc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature.shape, val_feature.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd249179-151f-4197-b90b-3b3cd4bcb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class.shape, val_class.shape, test_class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c681e53-f708-4c1f-864d-ab93f19412c2",
   "metadata": {},
   "source": [
    "## Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae72c8a-6bac-4736-9148-75953831a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_feature, train_class)\n",
    "val_dataset = TensorDataset(val_feature, val_class)\n",
    "test_dataset = TensorDataset(test_feature, test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a20687-dcce-432c-bcab-eda007eb35c7",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a5731-663c-4776-b855-ae1b6be2cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b71ac-f59f-45d3-a06a-54602ded7e95",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc8230-8e64-4f59-b284-da8cced683f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=hyperparams.input_size,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        dropout=hyperparams.dropout,\n",
    "        bias=hyperparams.bias,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(POSTagger, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "                \n",
    "        self.feature = nn.LSTM(\n",
    "            input_size = self.input_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            bias = self.bias,\n",
    "            dropout = self.dropout,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * self.hidden_size if hyperparams.bidirectional else self.hidden_size, out_features=self.output_size, bias=self.bias),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "\n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "        \n",
    "    def forward(self, feature, hidden=None):\n",
    "        output, (hidden, memory) = self.feature(feature, None)\n",
    "        prob = self.classifier(output)\n",
    "\n",
    "        return prob\n",
    "    \n",
    "model = POSTagger().to(hyperparams.device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72bd6-f2e6-4f44-8b36-702e5acede59",
   "metadata": {},
   "source": [
    "## Optimizer, Criterion, and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc4414-fa93-4518-a4bf-872de25c47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.CrossEntropyLoss().to(hyperparams.device)\n",
    "train_metric = MulticlassF1Score(average=\"micro\", num_classes=24, mdmc_average=\"global\", multiclass=True, ignore_index=label_to_idx[\"<PAD>\"]).to(hyperparams.device)\n",
    "val_metric = MulticlassF1Score(average=\"micro\", num_classes=24, mdmc_average=\"global\", multiclass=True, ignore_index=label_to_idx[\"<PAD>\"]).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", verbose=True, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591ed5b-e957-43aa-a3d5-4735852fac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5bdeea-d9c3-41ca-95c0-c2752215f2d8",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9bdc1-f6c2-41b1-8d3f-e4bfadd2aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (feature, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        feature = rearrange(feature, \"n s -> s n\")\n",
    "        actual_label = rearrange(actual_label, \"n s -> s n\")\n",
    "        embedding = word_embeddings(feature)\n",
    "        \n",
    "        prob = model(\n",
    "            embedding.to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "        \n",
    "        prob = prob.reshape(-1, prob.shape[-1])\n",
    "        actual_label = actual_label.reshape(-1)\n",
    "\n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "                \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if len(dataloader) < 10:\n",
    "            if batch % 1 == 0 or batch == len(dataloader):\n",
    "                batch_name = \"Batch-\" + str(batch)\n",
    "                print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "                with open(f\"../../../logs/classifier/{hyperparams.context_size}_contexts/{root_path}/training_history.txt\", \"a\") as f:\n",
    "                    f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "        else:\n",
    "            if batch % 15 == 0 or batch == len(dataloader):\n",
    "                batch_name = \"Batch-\" + str(batch)\n",
    "                print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "                with open(f\"../../../logs/classifier/{hyperparams.context_size}_contexts/{root_path}/training_history.txt\", \"a\") as f:\n",
    "                    f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "            \n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ead5e-da1c-44ec-ac87-48c4891f19cb",
   "metadata": {},
   "source": [
    "## Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0310cd28-75cb-433e-938b-ed4e4989eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(dataloader, model, criterion, metric):\n",
    "    model.eval()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (feature, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "            # Forward Propagation\n",
    "            feature = rearrange(feature, \"n s -> s n\")\n",
    "            actual_label = rearrange(actual_label, \"n s -> s n\")\n",
    "            embedding = word_embeddings(feature)\n",
    "\n",
    "            prob = model(\n",
    "                embedding.to(hyperparams.device),\n",
    "                actual_label.to(hyperparams.device)\n",
    "            )\n",
    "\n",
    "            prob = prob.reshape(-1, prob.shape[-1])\n",
    "            actual_label = actual_label.reshape(-1)\n",
    "\n",
    "            loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "            metric_score = metric(prob, actual_label.to(hyperparams.device))\n",
    "            metric_score = metric.compute()\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "            batch_metric_scores.append(metric_score)\n",
    "            \n",
    "            if len(dataloader) < 10 and (batch % 1 == 0 or batch == len(dataloader)):\n",
    "                batch_name = \"Batch-\" + str(batch)\n",
    "                print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "                with open(f\"../../../logs/classifier/{hyperparams.context_size}_contexts/{root_path}/training_history.txt\", \"a\") as f:\n",
    "                    f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "            else:\n",
    "                if batch % 15 == 0 or batch == len(dataloader):\n",
    "                    batch_name = \"Batch-\" + str(batch)\n",
    "                    print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "                    with open(f\"../../../logs/classifier/{hyperparams.context_size}_contexts/{root_path}/training_history.txt\", \"a\") as f:\n",
    "                        f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e1189-3368-496b-83ea-a9a98a30826e",
   "metadata": {},
   "source": [
    "## Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929b35d-23fc-411b-9352-187dbb4ca310",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "root_path = f\"../../../logs/classifier/{hyperparams.context_size}_contexts/fold_0{hyperparams.fold}/{path_name}\"\n",
    "os.makedirs(root_path)\n",
    "\n",
    "def looping_step(train_dataloader, val_dataloader, model, optimizer, criterion, train_metric, val_metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_training_losses = []\n",
    "    epoch_training_metric_scores = []\n",
    "    epoch_val_losses = []\n",
    "    epoch_val_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"{root_path}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 80}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 80}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"{root_path}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "            f.write(f\"Training Step\\n\")\n",
    "            \n",
    "        # Training Step\n",
    "        print(\"Training Step\")\n",
    "        batch_training_losses, batch_training_metric_scores = training_step(train_dataloader, model, optimizer, criterion, train_metric, scheduler=None)\n",
    "        epoch_training_loss = torch.mean(torch.FloatTensor(batch_training_losses))\n",
    "\n",
    "        epoch_training_loss = torch.mean(torch.FloatTensor(batch_training_losses))\n",
    "        epoch_training_losses.append(epoch_training_loss.item())\n",
    "\n",
    "        epoch_training_metric_score = torch.mean(torch.FloatTensor(batch_training_metric_scores))\n",
    "        epoch_training_metric_scores.append(epoch_training_metric_score.item())\n",
    "        \n",
    "        # Validation Step\n",
    "        with open(f\"{root_path}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"\\nValidation Step\\n\")\n",
    "            \n",
    "        print(\"\\nValidation Step\")\n",
    "        batch_val_losses, batch_val_metric_scores = validation_step(val_dataloader, model, criterion, val_metric)\n",
    "        epoch_val_loss = torch.mean(torch.FloatTensor(batch_val_losses))\n",
    "\n",
    "        epoch_val_loss = torch.mean(torch.FloatTensor(batch_val_losses))\n",
    "        epoch_val_losses.append(epoch_val_loss.item())\n",
    "\n",
    "        epoch_val_metric_score = torch.mean(torch.FloatTensor(batch_val_metric_scores))\n",
    "        epoch_val_metric_scores.append(epoch_val_metric_score.item())\n",
    "        \n",
    "        with open(f\"{root_path}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_training_loss = epoch_training_loss\n",
    "                    best_training_metric = epoch_training_metric_score\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    best_val_metric = epoch_val_metric_score\n",
    "                    \n",
    "                    print(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\")\n",
    "                    print(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\\n\")\n",
    "                elif epoch_training_losses[-1] < epoch_training_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_training_loss = epoch_training_loss\n",
    "                    best_training_metric = epoch_training_metric_score\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    best_val_metric = epoch_val_metric_score\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\")\n",
    "                    print(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\")\n",
    "                    print(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_training_loss = epoch_training_loss\n",
    "                    best_training_metric = epoch_training_metric_score\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    best_val_metric = epoch_val_metric_score\n",
    "                    \n",
    "                    print(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\")\n",
    "                    print(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\\n\")\n",
    "                elif epoch_training_metric_scores[-1] > epoch_training_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_training_loss = epoch_training_loss\n",
    "                    best_training_metric = epoch_training_metric_score\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    best_val_metric = epoch_val_metric_score\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\")\n",
    "                    print(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\")\n",
    "                    print(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"\\nTraining   : Mean {str(criterion).split('(')[0]} = {(epoch_training_loss):.4f} | Mean {str(train_metric).split('(')[0]} = {(epoch_training_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Validation : Mean {str(criterion).split('(')[0]} = {(epoch_val_loss):.4f} | Mean {str(val_metric).split('(')[0]} = {(epoch_val_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "            print(\"=\" * 80, end=\"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"{'=' * 80}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                \n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                break\n",
    "        \n",
    "        train_metric.reset()\n",
    "        val_metric.reset()\n",
    "        \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_training_losses, \"green\", label=\"Training\")\n",
    "    ax_loss.plot(epoch_val_losses, \"orange\", label=\"Validation\")\n",
    "    ax_loss.legend()\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_training_metric_scores, \"green\", label=\"Training\")\n",
    "    ax_metric_score.plot(epoch_val_metric_scores, \"orange\", label=\"Validation\")\n",
    "    ax_metric_score.legend()\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"{root_path}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    name_best_training_loss = f\"Best {str(criterion).split('(')[0]} training\".ljust(34)\n",
    "    name_best_training_metric = f\"Best {str(train_metric).split('(')[0]} validation\".ljust(34)\n",
    "    name_best_validation_loss = f\"Best {str(criterion).split('(')[0]} training\".ljust(34)\n",
    "    name_best_validation_metric = f\"Best {str(val_metric).split('(')[0]} validation\".ljust(34)\n",
    "    name_training_time = f\"Training duration\".ljust(34)\n",
    "    name_training_date = f\"Training date\".ljust(34)\n",
    "    \n",
    "    print(f\"{name_best_training_loss}: {best_training_loss:.4f}\")\n",
    "    print(f\"{name_best_validation_loss}: {best_val_loss:.4f}\")\n",
    "    print(f\"{name_best_training_metric}: {best_training_metric:.4f}\")\n",
    "    print(f\"{name_best_validation_metric}: {best_val_metric:.4f}\")\n",
    "    print(f\"{name_training_time}: {((finish_time - start_time) / 60):.4f} minutes.\")\n",
    "    print(f\"{name_training_date}: {now}\")\n",
    "    \n",
    "    with open(f\"{root_path}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 80}\\n\")\n",
    "        f.write(f\"{name_best_training_loss}: {best_training_loss:.4f}\\n\")\n",
    "        f.write(f\"{name_best_validation_loss}: {best_val_loss:.4f}\\n\")\n",
    "        f.write(f\"{name_best_training_metric}: {best_training_metric:.4f}\\n\")\n",
    "        f.write(f\"{name_best_validation_metric}: {best_val_metric:.4f}\\n\")\n",
    "        f.write(f\"{name_training_time}: {((finish_time - start_time) / 60):.4f} minutes.\\n\")\n",
    "        f.write(f\"{name_training_date}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    pd.DataFrame({\n",
    "        \"epoch\": list(range(1, hyperparams.n_epoch + 1)),\n",
    "        \"loss\": epoch_training_losses\n",
    "    }).to_csv(f\"{root_path}/training_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        \"epoch\": list(range(1, hyperparams.n_epoch + 1)),\n",
    "        \"f1_score\": epoch_training_metric_scores\n",
    "    }).to_csv(f\"{root_path}/training_metric_scores.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        \"epoch\": list(range(1, hyperparams.n_epoch + 1)),\n",
    "        \"loss\": epoch_val_losses\n",
    "    }).to_csv(f\"{root_path}/val_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        \"epoch\": list(range(1, hyperparams.n_epoch + 1)),\n",
    "        \"f1_score\": epoch_val_metric_scores\n",
    "    }).to_csv(f\"{root_path}/val_metric_scores.csv\", index=False)    \n",
    "    \n",
    "    filename_model_params = f\"{root_path}/model_params.pth\"\n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    \n",
    "    return epoch_training_losses, epoch_training_metric_scores, epoch_val_losses, epoch_val_metric_scores\n",
    "\n",
    "epoch_training_losses, epoch_training_metric_scores, epoch_val_losses, epoch_val_metric_scores = looping_step(train_dataloader, val_dataloader, model, optimizer, criterion, train_metric, val_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1ab00-060c-492c-b731-b5b78bc10bc0",
   "metadata": {},
   "source": [
    "## End Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d19df-0c5b-42c0-9619-f3409774cf4b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa68954-9917-497d-9be1-15bafefe7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader, model, criterion, metric):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (feature, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "            # Forward Propagation\n",
    "            feature = rearrange(feature, \"n s -> s n\")\n",
    "            actual_label = rearrange(actual_label, \"n s -> s n\")\n",
    "            embedding = word_embeddings(feature)\n",
    "\n",
    "            prob = model(\n",
    "                embedding.to(hyperparams.device),\n",
    "                actual_label.to(hyperparams.device)\n",
    "            )\n",
    "\n",
    "            prob = prob.reshape(-1, prob.shape[-1])\n",
    "            actual_label = actual_label.reshape(-1)\n",
    "            pred = rearrange(prob.argmax(dim=1).reshape(feature.shape[0], len(dataloader.dataset)), \"s n -> n s\")\n",
    "\n",
    "            loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "            metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "            metric_score = metric.compute()\n",
    "\n",
    "    return loss, metric_score, pred\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(hyperparams.device)\n",
    "metric = MulticlassF1Score(average=\"micro\", num_classes=24, mdmc_average=\"samplewise\", multiclass=True, ignore_index=label_to_idx[\"<PAD>\"]).to(hyperparams.device)\n",
    "train_loss, train_f1_score, train_pred_label = evaluation(train_dataloader, model, criterion, metric)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(hyperparams.device)\n",
    "metric = MulticlassF1Score(average=\"micro\", num_classes=24, mdmc_average=\"samplewise\", multiclass=True, ignore_index=label_to_idx[\"<PAD>\"]).to(hyperparams.device)\n",
    "val_loss, val_f1_score, val_pred_label = evaluation(val_dataloader, model, criterion, metric)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(hyperparams.device)\n",
    "metric = MulticlassF1Score(average=\"micro\", num_classes=24, mdmc_average=\"samplewise\", multiclass=True, ignore_index=label_to_idx[\"<PAD>\"]).to(hyperparams.device)\n",
    "test_loss, test_f1_score, test_pred_label = evaluation(test_dataloader, model, criterion, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf283357-3263-43eb-88e0-b3726c41fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f1_score, val_f1_score, test_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d776c1-f3d8-48ce-9092-d5919ef6c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f1_score, val_f1_score, test_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee85a3-646f-4e0b-8dfa-8b89f3e0a473",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948cb64-711c-4407-bdd6-78bf02161505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def viz_evaluation(pred_label, actual_label, title):\n",
    "    plt.figure(figsize=(15, 7), dpi=200)\n",
    "    sns.heatmap(pred_label.detach().cpu() == actual_label, cbar=True, cmap=\"binary_r\")\n",
    "    plt.xlabel(\"Sentence Length\")\n",
    "    plt.ylabel(\"Sentence\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"{root_path}/heatmap_{title.lower()}.jpg\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "train_actual_label = train_dataloader.dataset.tensors[1]\n",
    "val_actual_label = val_dataloader.dataset.tensors[1]\n",
    "test_actual_label = test_dataloader.dataset.tensors[1]\n",
    "\n",
    "viz_evaluation(train_pred_label, train_actual_label, f\"Training (F1-Score: {(train_f1_score):.4f})\")\n",
    "viz_evaluation(val_pred_label, val_actual_label, f\"Validation (F1-Score: {(val_f1_score):.4f})\")\n",
    "viz_evaluation(test_pred_label, test_actual_label, f\"Test (F1-Score: {(test_f1_score):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a7dca-8191-47cc-a4be-45abd1a9b1d1",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68452f-f3ad-4899-868b-8aff317e0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(pred_class, actual_class, title, normalize=None):\n",
    "    conf_mat = MulticlassConfusionMatrix(num_classes=24, normalize=normalize)\n",
    "    plt.figure(figsize=(20, 10), dpi=200)\n",
    "    sns.heatmap(conf_mat(pred_class.detach().cpu(), actual_class), annot=True, fmt=\".2g\", xticklabels=list(label_to_idx.keys()), yticklabels=list(label_to_idx.keys()));\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Actual Class\")\n",
    "    plt.ylabel(\"Predict Class\")\n",
    "    plt.savefig(f\"{root_path}/conf_matrix_{title.lower()}.jpg\", dpi=200)\n",
    "\n",
    "confusion_matrix(train_pred_label, train_actual_label, \"Training\", \"pred\")\n",
    "confusion_matrix(val_pred_label,val_actual_label, \"Validation\", \"pred\")\n",
    "confusion_matrix(test_pred_label, test_actual_label, \"Test\", \"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada17c56-345a-4eba-914a-b093ce956d27",
   "metadata": {},
   "source": [
    "## Prediction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467f514-d6a6-46e4-b9bb-295686c5a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_wrong_pred_each_class(pred_class, actual_class):\n",
    "    classes, count_class = actual_class.flatten().unique(return_counts=True)\n",
    "    conf_mat = MulticlassConfusionMatrix(num_classes=24, normalize=\"none\")\n",
    "    correct_pred_count_class = conf_mat(pred_class.detach().cpu(), actual_class).diag()\n",
    "    \n",
    "    correct_pred = {idx_to_label[label.item()]: count.item() for label, count in zip(classes, (correct_pred_count_class))}\n",
    "    wrong_pred = {idx_to_label[label.item()]: count.item() for label, count in zip(classes, (count_class - correct_pred_count_class))}\n",
    "    \n",
    "    return correct_pred, wrong_pred\n",
    "\n",
    "correct_pred, wrong_pred = number_wrong_pred_each_class(train_pred_label, train_actual_label)\n",
    "print(f\"| {'Number class'.ljust(14)} | {'Correct prediction'.ljust(12)} | {'Wrong prediction'.ljust(12)} |\")\n",
    "for (label_correct_pred, count_correct_pred), (label_wrong_pred, count_wrong_pred) in zip(correct_pred.items(), wrong_pred.items()):\n",
    "    print(f\"| {str(label_correct_pred).ljust(6)}: {str(count_correct_pred + count_wrong_pred).ljust(7)}| {label_correct_pred.ljust(6)}: {str(count_correct_pred).ljust(10)} | {label_wrong_pred.ljust(6)}: {str(count_wrong_pred).ljust(8)} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2b172-d261-42d3-b9c3-69f67cd0d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actual_label[308], train_pred_label[308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c3a3f-2f10-44f8-b394-bedaf19e2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actual_label[308].detach().cpu() == train_pred_label[308].detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f44bd5-cd73-4184-9078-63d2b5718078",
   "metadata": {},
   "source": [
    "## OOV prediction is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4df60-901f-49d3-8dfb-b37fab4773be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_flag_token(sentences, max_seq_len=hyperparams.max_seq_len):\n",
    "    sent_copy = deepcopy(sentences)\n",
    "    sent = []\n",
    "    oov_flag = []\n",
    "    \n",
    "    for sentence in tqdm(sent_copy):\n",
    "        for token in sentence:\n",
    "            sent.append(token[2])\n",
    "        \n",
    "        for _ in range(max_seq_len- len(sentence)):\n",
    "            sent.append(False)\n",
    "        \n",
    "        oov_flag.append(sent)\n",
    "        sent = []\n",
    "        \n",
    "    return np.array(oov_flag)\n",
    "\n",
    "train_oov_flag = oov_flag_token(train_sentences, max_seq_len=hyperparams.max_seq_len)\n",
    "val_oov_flag = oov_flag_token(val_sentences, max_seq_len=hyperparams.max_seq_len)\n",
    "test_oov_flag = oov_flag_token(test_sentences, max_seq_len=hyperparams.max_seq_len)\n",
    "\n",
    "def pencentage_oov_pred_correct(pred_oov_flag, oov_flag_tensor):\n",
    "    oov_correct_counter = 0\n",
    "    oov_wrong_counter = 0\n",
    "\n",
    "    for pred, oov_label in tqdm(zip(pred_oov_flag, oov_flag_tensor)):\n",
    "        if pred == True and oov_label == True:\n",
    "            oov_correct_counter += 1\n",
    "        elif pred == False and oov_label == True:\n",
    "            oov_wrong_counter += 1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return oov_correct_counter, oov_wrong_counter\n",
    "\n",
    "train_pred_oov_flag = (train_actual_label == train_pred_label.detach().cpu()).flatten()\n",
    "val_pred_oov_flag = (val_actual_label == val_pred_label.detach().cpu()).flatten()\n",
    "test_pred_oov_flag = (test_actual_label == test_pred_label.detach().cpu()).flatten()\n",
    "train_oov_flag_tensor = torch.tensor(train_oov_flag).flatten()\n",
    "val_oov_flag_tensor = torch.tensor(val_oov_flag).flatten()\n",
    "test_oov_flag_tensor = torch.tensor(test_oov_flag).flatten()\n",
    "\n",
    "train_oov_correct_counter, train_oov_wrong_counter = pencentage_oov_pred_correct(train_pred_oov_flag, train_oov_flag_tensor)\n",
    "val_oov_correct_counter, val_oov_wrong_counter = pencentage_oov_pred_correct(val_pred_oov_flag, val_oov_flag_tensor)\n",
    "test_oov_correct_counter, test_oov_wrong_counter = pencentage_oov_pred_correct(test_pred_oov_flag, test_oov_flag_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59490d-6b9f-4c4d-833b-28eaf624d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_percentage_pred_oov(title, oov_correct_counter, oov_wrong_counter):\n",
    "    print(f\"{title}\")\n",
    "    print(f\"Number OOV token            : {oov_correct_counter + oov_wrong_counter}\")\n",
    "    print(f\"Correct prediction          : {oov_correct_counter}\")\n",
    "    print(f\"Wrong prediction            : {oov_wrong_counter}\")\n",
    "    print(f\"Percentage correct oov pred : {oov_correct_counter / (oov_correct_counter + oov_wrong_counter) * 100}\\n\")\n",
    "    \n",
    "    with open(f\"{root_path}/oov_summary.txt\", \"a\") as f:\n",
    "        f.write(f\"{title}\\n\")\n",
    "        f.write(f\"Number OOV token            : {oov_correct_counter + oov_wrong_counter}\\n\")\n",
    "        f.write(f\"Correct prediction          : {oov_correct_counter}\\n\")\n",
    "        f.write(f\"Wrong prediction            : {oov_wrong_counter}\\n\")\n",
    "        f.write(f\"Percentage correct oov pred : {oov_correct_counter / (oov_correct_counter + oov_wrong_counter) * 100}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cda2e3-0629-4e3c-8459-4324b513ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_percentage_pred_oov(\"Training\", train_oov_correct_counter, train_oov_wrong_counter)\n",
    "print_percentage_pred_oov(\"Validation\", val_oov_correct_counter, val_oov_wrong_counter)\n",
    "print_percentage_pred_oov(\"Test\", test_oov_correct_counter, test_oov_wrong_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efce585-5e19-45eb-8781-6818b8369819",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(train_oov_flag, cmap=plt.cm.binary_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97963f81-d9bf-4239-842d-ef30df0dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(val_oov_flag, cmap=plt.cm.binary_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674a992-1715-4d50-892d-bb7df050aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(test_oov_flag, cmap=plt.cm.binary_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc95b3-4ee0-4cc1-b7d9-40a3e8150c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4828bfc-586d-4f3f-999f-cea4c114d2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
