{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=70,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 65)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 68)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.5925378e-11,  4.5763605e-41, -8.5925378e-11, ...,\n",
       "         4.5763605e-41,  3.5532771e-21,  4.5763605e-41],\n",
       "       [ 1.1265459e-40,  0.0000000e+00,  3.0430165e-20, ...,\n",
       "         0.0000000e+00,  3.0431199e-20,  4.5763605e-41],\n",
       "       [ 3.5535873e-21,  4.5763605e-41,  1.1267000e-40, ...,\n",
       "         4.5763605e-41,  1.1268401e-40,  0.0000000e+00],\n",
       "       ...,\n",
       "       [ 1.0994448e-40,  0.0000000e+00,  2.6582128e-21, ...,\n",
       "         0.0000000e+00,  2.6582774e-21,  4.5763605e-41],\n",
       "       [ 3.5040857e-21,  4.5763605e-41,  1.0995989e-40, ...,\n",
       "         4.5763605e-41,  1.0997390e-40,  0.0000000e+00],\n",
       "       [ 2.6583485e-21,  4.5763605e-41,  3.5043701e-21, ...,\n",
       "         4.5763605e-41,  3.5046285e-21,  4.5763605e-41]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f6cac23824ff582d127e59fc5f005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=5.9626 | F1Score=0.2888\n",
      "Batch-100: NLLLoss=4.7250 | F1Score=0.3044\n",
      "Batch-150: NLLLoss=5.9060 | F1Score=0.3176\n",
      "Batch-200: NLLLoss=2.8798 | F1Score=0.3411\n",
      "Batch-250: NLLLoss=4.0345 | F1Score=0.3527\n",
      "Batch-300: NLLLoss=4.3362 | F1Score=0.3745\n",
      "Batch-350: NLLLoss=4.9097 | F1Score=0.3868\n",
      "Batch-400: NLLLoss=3.1116 | F1Score=0.4007\n",
      "Batch-450: NLLLoss=3.7858 | F1Score=0.4154\n",
      "Batch-500: NLLLoss=3.7133 | F1Score=0.4265\n",
      "Batch-518: NLLLoss=2.9188 | F1Score=0.4316\n",
      "\n",
      "Mean NLLLoss: 4.5346 | Mean F1Score: 0.3515\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736f023a54784f319ba53d17bdc61e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.5968 | F1Score=0.5658\n",
      "Batch-100: NLLLoss=3.0534 | F1Score=0.5846\n",
      "Batch-150: NLLLoss=3.7616 | F1Score=0.5914\n",
      "Batch-200: NLLLoss=4.1978 | F1Score=0.5974\n",
      "Batch-250: NLLLoss=2.1446 | F1Score=0.6053\n",
      "Batch-300: NLLLoss=3.3228 | F1Score=0.6111\n",
      "Batch-350: NLLLoss=2.6238 | F1Score=0.6195\n",
      "Batch-400: NLLLoss=3.1342 | F1Score=0.6204\n",
      "Batch-450: NLLLoss=3.2092 | F1Score=0.6261\n",
      "Batch-500: NLLLoss=1.7970 | F1Score=0.6306\n",
      "Batch-518: NLLLoss=3.1688 | F1Score=0.6328\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 2.7275 | Mean F1Score: 0.6025\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dda8c23a064f7eb7c7c0f89fffd2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.2814 | F1Score=0.7300\n",
      "Batch-100: NLLLoss=1.3455 | F1Score=0.7300\n",
      "Batch-150: NLLLoss=2.0479 | F1Score=0.7243\n",
      "Batch-200: NLLLoss=1.6524 | F1Score=0.7209\n",
      "Batch-250: NLLLoss=1.4019 | F1Score=0.7230\n",
      "Batch-300: NLLLoss=1.7378 | F1Score=0.7301\n",
      "Batch-350: NLLLoss=2.3604 | F1Score=0.7310\n",
      "Batch-400: NLLLoss=2.7076 | F1Score=0.7368\n",
      "Batch-450: NLLLoss=1.4116 | F1Score=0.7405\n",
      "Batch-500: NLLLoss=2.1996 | F1Score=0.7429\n",
      "Batch-518: NLLLoss=1.3633 | F1Score=0.7444\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 1.7968 | Mean F1Score: 0.7311\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438540dfe7d24118be8fcd5afa89820b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.8774 | F1Score=0.8096\n",
      "Batch-100: NLLLoss=1.0098 | F1Score=0.8111\n",
      "Batch-150: NLLLoss=0.9803 | F1Score=0.8172\n",
      "Batch-200: NLLLoss=1.7663 | F1Score=0.8154\n",
      "Batch-250: NLLLoss=0.9683 | F1Score=0.8108\n",
      "Batch-300: NLLLoss=1.3250 | F1Score=0.8116\n",
      "Batch-350: NLLLoss=0.6573 | F1Score=0.8116\n",
      "Batch-400: NLLLoss=1.0251 | F1Score=0.8133\n",
      "Batch-450: NLLLoss=0.8068 | F1Score=0.8129\n",
      "Batch-500: NLLLoss=1.5752 | F1Score=0.8155\n",
      "Batch-518: NLLLoss=1.1776 | F1Score=0.8157\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 1.1748 | Mean F1Score: 0.8138\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e7ecbc176843d08fc807ae9fb82037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.4882 | F1Score=0.8853\n",
      "Batch-100: NLLLoss=1.0238 | F1Score=0.8873\n",
      "Batch-150: NLLLoss=0.4002 | F1Score=0.8870\n",
      "Batch-200: NLLLoss=0.5830 | F1Score=0.8843\n",
      "Batch-250: NLLLoss=0.1815 | F1Score=0.8813\n",
      "Batch-300: NLLLoss=0.8239 | F1Score=0.8795\n",
      "Batch-350: NLLLoss=0.6210 | F1Score=0.8785\n",
      "Batch-400: NLLLoss=0.5364 | F1Score=0.8751\n",
      "Batch-450: NLLLoss=0.6446 | F1Score=0.8733\n",
      "Batch-500: NLLLoss=1.3465 | F1Score=0.8724\n",
      "Batch-518: NLLLoss=0.4671 | F1Score=0.8717\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.7033 | Mean F1Score: 0.8801\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05c1e0301b54c7fa1d798b6d158285d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.3053 | F1Score=0.9569\n",
      "Batch-100: NLLLoss=0.4790 | F1Score=0.9506\n",
      "Batch-150: NLLLoss=0.2583 | F1Score=0.9488\n",
      "Batch-200: NLLLoss=0.4055 | F1Score=0.9470\n",
      "Batch-250: NLLLoss=0.2226 | F1Score=0.9461\n",
      "Batch-300: NLLLoss=0.2738 | F1Score=0.9432\n",
      "Batch-350: NLLLoss=0.2780 | F1Score=0.9413\n",
      "Batch-400: NLLLoss=0.5942 | F1Score=0.9396\n",
      "Batch-450: NLLLoss=0.6006 | F1Score=0.9386\n",
      "Batch-500: NLLLoss=0.1123 | F1Score=0.9374\n",
      "Batch-518: NLLLoss=0.1956 | F1Score=0.9370\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.3452 | Mean F1Score: 0.9458\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f833a8dc12a4941b629a896d592f44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0852 | F1Score=0.9931\n",
      "Batch-100: NLLLoss=0.1307 | F1Score=0.9903\n",
      "Batch-150: NLLLoss=0.2328 | F1Score=0.9898\n",
      "Batch-200: NLLLoss=0.0761 | F1Score=0.9903\n",
      "Batch-250: NLLLoss=0.1801 | F1Score=0.9889\n",
      "Batch-300: NLLLoss=0.2080 | F1Score=0.9884\n",
      "Batch-350: NLLLoss=0.1787 | F1Score=0.9878\n",
      "Batch-400: NLLLoss=0.1492 | F1Score=0.9879\n",
      "Batch-450: NLLLoss=0.1452 | F1Score=0.9872\n",
      "Batch-500: NLLLoss=0.0827 | F1Score=0.9871\n",
      "Batch-518: NLLLoss=0.1367 | F1Score=0.9869\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.1171 | Mean F1Score: 0.9894\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b63de5081f04fc4b150444ed9f8b997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0311 | F1Score=0.9981\n",
      "Batch-100: NLLLoss=0.0557 | F1Score=0.9991\n",
      "Batch-150: NLLLoss=0.0135 | F1Score=0.9987\n",
      "Batch-200: NLLLoss=0.0129 | F1Score=0.9989\n",
      "Batch-250: NLLLoss=0.0274 | F1Score=0.9991\n",
      "Batch-300: NLLLoss=0.0295 | F1Score=0.9988\n",
      "Batch-350: NLLLoss=0.0196 | F1Score=0.9987\n",
      "Batch-400: NLLLoss=0.0498 | F1Score=0.9988\n",
      "Batch-450: NLLLoss=0.0414 | F1Score=0.9987\n",
      "Batch-500: NLLLoss=0.0483 | F1Score=0.9987\n",
      "Batch-518: NLLLoss=0.0476 | F1Score=0.9986\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0341 | Mean F1Score: 0.9988\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f829cb70eadb4657b89559852f892522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0118 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0292 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0172 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0070 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0084 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0248 | F1Score=0.9995\n",
      "Batch-350: NLLLoss=0.0045 | F1Score=0.9995\n",
      "Batch-400: NLLLoss=0.0123 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0125 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0044 | F1Score=0.9995\n",
      "Batch-518: NLLLoss=0.0046 | F1Score=0.9995\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0125 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d251ee97a24fed8e09be0e00c30aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0270 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0053 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0036 | F1Score=0.9997\n",
      "Batch-200: NLLLoss=0.0041 | F1Score=0.9996\n",
      "Batch-250: NLLLoss=0.0071 | F1Score=0.9994\n",
      "Batch-300: NLLLoss=0.0088 | F1Score=0.9993\n",
      "Batch-350: NLLLoss=0.0085 | F1Score=0.9994\n",
      "Batch-400: NLLLoss=0.0100 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0075 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0041 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0082 | F1Score=0.9995\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0084 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4e53f0231940d095a971c51e5f673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0051 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0018 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0053 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0046 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0031 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0046 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0166 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0130 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0048 | F1Score=0.9995\n",
      "Batch-500: NLLLoss=0.0034 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0056 | F1Score=0.9996\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0078 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7870d4b27f74ed484d06ca58b6f3cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0056 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0099 | F1Score=0.9987\n",
      "Batch-150: NLLLoss=0.1428 | F1Score=0.9946\n",
      "Batch-200: NLLLoss=0.6221 | F1Score=0.9731\n",
      "Batch-250: NLLLoss=0.2044 | F1Score=0.9625\n",
      "Batch-300: NLLLoss=0.2808 | F1Score=0.9586\n",
      "Batch-350: NLLLoss=0.1051 | F1Score=0.9563\n",
      "Batch-400: NLLLoss=0.1113 | F1Score=0.9557\n",
      "Batch-450: NLLLoss=0.0915 | F1Score=0.9574\n",
      "Batch-500: NLLLoss=0.1041 | F1Score=0.9586\n",
      "Batch-518: NLLLoss=0.1985 | F1Score=0.9586\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.1768 | Mean F1Score: 0.9731\n",
      "Patience = 1/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75b38da3da42d88d48ec6b1ebde1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0071 | F1Score=0.9862\n",
      "Batch-100: NLLLoss=0.0361 | F1Score=0.9862\n",
      "Batch-150: NLLLoss=0.0469 | F1Score=0.9886\n",
      "Batch-200: NLLLoss=0.0132 | F1Score=0.9891\n",
      "Batch-250: NLLLoss=0.0051 | F1Score=0.9904\n",
      "Batch-300: NLLLoss=0.0053 | F1Score=0.9911\n",
      "Batch-350: NLLLoss=0.0125 | F1Score=0.9916\n",
      "Batch-400: NLLLoss=0.0716 | F1Score=0.9922\n",
      "Batch-450: NLLLoss=0.0183 | F1Score=0.9919\n",
      "Batch-500: NLLLoss=0.0303 | F1Score=0.9922\n",
      "Batch-518: NLLLoss=0.0633 | F1Score=0.9922\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0440 | Mean F1Score: 0.9898\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd88161fdee442892d25d85ddb535b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0016 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0054 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0038 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0053 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0048 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0018 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0025 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0027 | F1Score=0.9996\n",
      "Batch-500: NLLLoss=0.0056 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0045 | F1Score=0.9996\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0061 | Mean F1Score: 0.9997\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdc21488e204d29ba70588ee5169195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0025 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0014 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0019 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0020 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0018 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0015 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0010 | F1Score=0.9996\n",
      "Batch-450: NLLLoss=0.0007 | F1Score=0.9997\n",
      "Batch-500: NLLLoss=0.0013 | F1Score=0.9997\n",
      "Batch-518: NLLLoss=0.0024 | F1Score=0.9997\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0022 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78363049ffd4cb5b9938997184bd5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0009 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0012 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0007 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0022 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0009 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0007 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0007 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0011 | F1Score=0.9997\n",
      "Batch-450: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0018 | F1Score=0.9997\n",
      "Batch-518: NLLLoss=0.0009 | F1Score=0.9998\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0018 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbedca19831e4cd7afe8ca0569374c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0686 | F1Score=0.9997\n",
      "Batch-200: NLLLoss=0.0005 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0005 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0016 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0014 | F1Score=0.9993\n",
      "Batch-400: NLLLoss=0.0037 | F1Score=0.9987\n",
      "Batch-450: NLLLoss=0.0227 | F1Score=0.9973\n",
      "Batch-500: NLLLoss=0.2186 | F1Score=0.9960\n",
      "Batch-518: NLLLoss=0.0206 | F1Score=0.9957\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0206 | Mean F1Score: 0.9991\n",
      "Patience = 2/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12209b3b2024600a61b28a3036d865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0778 | F1Score=0.9875\n",
      "Batch-100: NLLLoss=0.3295 | F1Score=0.9853\n",
      "Batch-150: NLLLoss=0.0494 | F1Score=0.9843\n",
      "Batch-200: NLLLoss=0.3279 | F1Score=0.9820\n",
      "Batch-250: NLLLoss=0.2062 | F1Score=0.9796\n",
      "Batch-300: NLLLoss=0.0441 | F1Score=0.9793\n",
      "Batch-350: NLLLoss=0.2688 | F1Score=0.9787\n",
      "Batch-400: NLLLoss=0.0784 | F1Score=0.9790\n",
      "Batch-450: NLLLoss=0.0086 | F1Score=0.9793\n",
      "Batch-500: NLLLoss=0.0295 | F1Score=0.9799\n",
      "Batch-518: NLLLoss=0.2217 | F1Score=0.9801\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0801 | Mean F1Score: 0.9818\n",
      "Patience = 3/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f40d42358f4ca0b8f636901dc8e0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0314 | F1Score=0.9984\n",
      "Batch-100: NLLLoss=0.0086 | F1Score=0.9977\n",
      "Batch-150: NLLLoss=0.0072 | F1Score=0.9970\n",
      "Batch-200: NLLLoss=0.0038 | F1Score=0.9973\n",
      "Batch-250: NLLLoss=0.0354 | F1Score=0.9977\n",
      "Batch-300: NLLLoss=0.0099 | F1Score=0.9979\n",
      "Batch-350: NLLLoss=0.0010 | F1Score=0.9980\n",
      "Batch-400: NLLLoss=0.0068 | F1Score=0.9979\n",
      "Batch-450: NLLLoss=0.0012 | F1Score=0.9979\n",
      "Batch-500: NLLLoss=0.0177 | F1Score=0.9981\n",
      "Batch-518: NLLLoss=0.0004 | F1Score=0.9981\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0132 | Mean F1Score: 0.9978\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e748c3cd19984e3bab8399bcec056c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0015 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0016 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0017 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0035 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0010 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0011 | F1Score=0.9999\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0013 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0013\n",
      "Best F1Score      : 0.9999\n",
      "Training duration : 31.798 minutes.\n",
      "Training date     : 2022-10-11 18:03:50.490284+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16818544,  2.1862051 ,  1.3241848 , ..., -0.3507055 ,\n",
       "        -4.335224  , -3.066726  ],\n",
       "       [ 6.412116  , -1.8693992 ,  3.9892633 , ..., -1.8746115 ,\n",
       "         0.76640475,  8.432886  ],\n",
       "       [ 6.1087055 ,  0.46522278, -1.8878435 , ..., -2.519249  ,\n",
       "         2.4431005 ,  6.155076  ],\n",
       "       ...,\n",
       "       [ 0.40316278, -2.659848  , -4.500704  , ..., -9.384507  ,\n",
       "         1.0302902 ,  1.4961867 ],\n",
       "       [-3.481931  ,  7.5845604 , -0.9713888 , ..., -5.489715  ,\n",
       "        -5.605214  , -1.0845299 ],\n",
       "       [ 2.4219918 , -5.320081  , -2.748453  , ..., -3.3779361 ,\n",
       "        -1.2179054 , -1.0573279 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.1752, -0.2253,  0.2140,  ..., -0.3710,  0.2593, -0.0005],\n",
       "                      [ 0.1128, -0.1407, -0.1047,  ..., -0.3076,  0.4449,  0.0471],\n",
       "                      [ 0.0812,  0.1805,  0.2146,  ..., -0.1118,  0.0690, -0.4765],\n",
       "                      ...,\n",
       "                      [ 0.1076, -0.2078,  0.0775,  ..., -0.1524,  0.0861, -0.2193],\n",
       "                      [ 0.4088, -0.1101, -0.4411,  ..., -0.0962,  0.2110,  0.1047],\n",
       "                      [ 0.3475,  0.0076,  0.2233,  ..., -0.2455,  0.2049, -0.2490]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[-2.2124e-02, -1.1768e-01, -2.6057e-01,  ..., -2.0322e-01,\n",
       "                       -2.1703e-02,  2.2001e-01],\n",
       "                      [-4.1550e-02,  4.5393e-03,  8.8242e-03,  ..., -7.2486e-02,\n",
       "                       -1.4707e-01,  1.3819e-02],\n",
       "                      [-1.5200e-03,  7.4562e-02, -1.2125e-02,  ..., -1.8781e-01,\n",
       "                       -1.0808e-01,  2.9672e-01],\n",
       "                      ...,\n",
       "                      [ 9.0287e-02,  3.1688e-01,  5.7888e-02,  ..., -1.1885e-01,\n",
       "                       -2.1910e-05,  2.7682e-01],\n",
       "                      [ 6.6998e-02, -1.1753e-01, -1.9567e-01,  ..., -1.3919e-01,\n",
       "                       -2.0487e-01,  1.3935e-01],\n",
       "                      [ 8.0682e-02,  8.8477e-02, -9.0421e-02,  ..., -5.9716e-02,\n",
       "                       -9.2458e-02,  3.2806e-02]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-1.3876e-01, -4.5187e-02, -3.2737e-01, -9.1366e-02, -1.6998e-01,\n",
       "                      -1.5622e-01, -5.5590e-02, -2.2132e-01, -2.7596e-01, -2.2591e-01,\n",
       "                      -1.5711e-01, -1.0333e-01, -1.4148e-01, -1.8443e-01, -6.7420e-02,\n",
       "                      -1.7647e-01, -2.5740e-02, -1.6560e-01, -1.1162e-01, -2.9898e-01,\n",
       "                      -1.2557e-01, -1.5848e-01, -9.6419e-02, -1.5645e-01, -1.3317e-01,\n",
       "                      -1.7315e-01, -1.2900e-01, -9.3749e-02, -1.0013e-01, -3.2636e-03,\n",
       "                      -1.8628e-01, -1.0622e-01, -1.3575e-01, -2.0032e-01, -3.9052e-02,\n",
       "                      -8.6317e-02, -1.3599e-01, -1.7899e-01, -1.1701e-01, -5.7060e-03,\n",
       "                      -2.2557e-01, -1.0630e-01, -2.0849e-01, -1.8295e-01, -1.6863e-01,\n",
       "                      -1.7375e-01, -1.2364e-01, -2.9808e-02, -2.0342e-01, -2.5684e-01,\n",
       "                      -2.6754e-01, -1.2234e-01, -1.6374e-02, -1.9395e-01, -8.2191e-02,\n",
       "                      -1.5270e-01, -2.4472e-01, -1.2863e-01, -9.7100e-02, -1.8807e-01,\n",
       "                      -2.2368e-02, -2.6865e-01, -1.9677e-01, -2.1831e-01, -2.4113e-01,\n",
       "                      -1.6599e-01, -2.9592e-01, -3.2864e-01, -2.3518e-01, -1.3522e-01,\n",
       "                      -3.3417e-02, -1.4360e-01, -9.6153e-02, -1.5236e-01, -3.1282e-01,\n",
       "                      -2.6371e-01, -1.7041e-01, -1.0949e-01, -1.7857e-01, -2.3094e-01,\n",
       "                      -1.9589e-01, -1.5768e-01, -1.0208e-01,  2.0560e-02, -2.4541e-01,\n",
       "                      -1.3969e-01, -1.8724e-01, -2.1017e-01, -2.4386e-01, -2.7093e-01,\n",
       "                      -2.8561e-01, -1.1122e-01, -2.2194e-01, -1.9837e-01, -1.4070e-01,\n",
       "                      -2.3294e-01, -1.7304e-01, -1.2708e-01, -2.5475e-01, -2.5974e-02,\n",
       "                      -2.1904e-01, -3.4214e-01, -1.9336e-01, -2.3184e-01, -2.5986e-01,\n",
       "                      -2.6481e-01, -1.5488e-01, -1.5737e-01, -2.6293e-01, -1.4618e-02,\n",
       "                      -3.3790e-01, -1.8612e-01, -1.7192e-01, -1.2984e-01, -3.1857e-01,\n",
       "                      -1.2291e-01, -1.6161e-01, -9.7620e-02, -8.2333e-02, -2.2584e-01,\n",
       "                      -1.4682e-01, -1.7179e-01, -8.1396e-02, -1.7099e-01, -1.9744e-01,\n",
       "                      -1.4344e-01, -2.3007e-01, -1.8888e-01, -8.9161e-02, -1.2156e-01,\n",
       "                      -2.0107e-01, -1.2808e-01, -2.8542e-01, -2.4177e-02, -1.8271e-01,\n",
       "                      -2.1799e-01, -6.6157e-02, -1.0303e-01, -1.9242e-01, -4.8328e-02,\n",
       "                      -3.9051e-02, -1.4792e-01, -1.0956e-01, -8.2950e-03, -2.2982e-01,\n",
       "                      -2.3304e-01, -2.2924e-02, -2.6022e-01, -1.4410e-02, -2.8114e-01,\n",
       "                      -8.4119e-02, -1.3132e-01, -9.1933e-02, -1.2602e-01, -7.3082e-02,\n",
       "                      -5.4798e-02,  1.4582e-02, -1.2500e-01, -2.0137e-01, -1.5969e-01,\n",
       "                      -2.1857e-01, -1.5806e-01, -5.8165e-02, -6.0608e-02, -9.4091e-02,\n",
       "                      -1.3596e-01, -2.3416e-01, -5.4603e-02, -7.4743e-03, -1.5875e-01,\n",
       "                      -1.9158e-01, -8.0312e-02, -9.6503e-02, -6.6672e-02, -1.1135e-01,\n",
       "                       8.2161e-02, -8.3523e-02, -9.3124e-02, -1.3002e-01, -5.4322e-02,\n",
       "                      -6.4521e-02, -2.0162e-01, -1.9746e-01, -2.4278e-01, -9.2962e-02,\n",
       "                      -1.7390e-01, -3.1983e-01, -9.0272e-02, -1.2846e-01, -2.1074e-01,\n",
       "                      -1.9969e-02, -6.9884e-02, -1.3133e-01, -1.0049e-01, -1.0256e-01,\n",
       "                      -2.3140e-01, -8.4955e-02, -8.3827e-02, -4.0597e-02, -8.0032e-02,\n",
       "                       1.1028e-02, -2.5668e-01, -3.0332e-01, -1.3200e-01, -1.3208e-01,\n",
       "                      -4.2041e-03, -1.1328e-01, -1.0322e-01, -4.5146e-02, -1.6665e-01,\n",
       "                      -4.9456e-02, -7.4613e-03, -2.0550e-01, -1.3300e-01, -1.5096e-01,\n",
       "                      -1.0776e-01, -2.5522e-01, -1.6881e-01, -1.1200e-01,  2.6799e-02,\n",
       "                      -7.5123e-02, -8.9056e-02, -4.6595e-02, -2.5569e-01, -1.6018e-01,\n",
       "                      -1.0925e-01, -1.3353e-01, -1.9075e-03, -6.5984e-03, -1.7133e-01,\n",
       "                      -2.2618e-01, -1.0469e-01, -1.9773e-01, -2.0433e-01, -1.0048e-01,\n",
       "                      -2.0388e-01, -1.0611e-01, -1.3179e-01, -2.4331e-01, -9.4842e-02,\n",
       "                      -1.8802e-01, -2.4031e-01, -1.1708e-01, -2.2369e-01, -7.8970e-03,\n",
       "                      -1.7698e-01, -9.1030e-02, -1.4272e-01, -8.6132e-02, -1.7785e-01,\n",
       "                      -1.6445e-01, -1.7265e-01, -4.4291e-02, -2.2703e-01, -1.6778e-01,\n",
       "                      -1.9456e-01,  6.7403e-02, -3.7935e-02,  6.2568e-03, -1.4940e-03,\n",
       "                      -8.9568e-03, -7.1701e-03, -6.4330e-03,  9.0345e-02, -5.4102e-02,\n",
       "                      -3.2939e-04,  6.5155e-02,  7.1748e-02, -1.6791e-01,  3.5562e-02,\n",
       "                       6.9131e-02, -3.0359e-03, -1.0683e-02, -6.6635e-02,  5.2803e-02,\n",
       "                       2.9886e-02,  5.7558e-02, -1.0737e-02,  2.0522e-02,  1.0823e-02,\n",
       "                       7.6340e-02,  1.3952e-02, -7.9671e-02,  2.3942e-02,  2.4761e-02,\n",
       "                      -2.9563e-02,  4.5032e-02,  2.2419e-02,  3.7976e-02,  1.8465e-02,\n",
       "                       5.7312e-02, -5.1683e-03, -6.5513e-02,  4.9401e-02, -3.1878e-02,\n",
       "                       6.6798e-02,  6.4167e-02, -1.6541e-01,  3.5824e-02,  1.2982e-01,\n",
       "                      -2.7967e-02, -1.3200e-02, -7.2563e-03,  2.3364e-02, -5.8358e-02,\n",
       "                       8.5017e-03,  3.0759e-02,  2.6130e-02,  4.0191e-02,  3.6365e-02,\n",
       "                      -1.0168e-02,  1.1959e-02,  5.4408e-02, -5.3780e-02,  1.3454e-02,\n",
       "                       2.1192e-02, -1.2619e-02, -1.6436e-02,  2.6257e-02,  4.6114e-02,\n",
       "                       1.9881e-02, -9.4196e-02,  6.4506e-02, -3.2466e-02, -6.7364e-03,\n",
       "                      -6.6731e-02, -1.0738e-02,  4.1612e-02,  6.4849e-02, -1.5833e-02,\n",
       "                       1.5915e-02, -3.5597e-03,  6.0735e-02, -5.5633e-02, -1.3414e-01,\n",
       "                      -9.5998e-03,  6.1489e-02,  7.8997e-02,  2.1093e-03, -2.6338e-03,\n",
       "                      -6.8573e-02, -1.2246e-01,  4.8541e-02,  3.9521e-02, -3.9540e-02,\n",
       "                       3.9642e-02,  1.1145e-01, -1.1123e-02, -7.1241e-03,  8.0714e-02,\n",
       "                      -5.0559e-02, -6.0461e-02, -4.4128e-02, -1.5999e-01,  2.6318e-02,\n",
       "                       5.7152e-02, -8.8490e-03,  9.3209e-02, -4.6680e-02,  1.4571e-02,\n",
       "                      -3.9313e-02,  1.6009e-02, -3.8159e-02, -1.0744e-01, -5.4415e-03,\n",
       "                       9.0052e-02,  7.6444e-02,  5.7770e-02, -2.0561e-02,  5.3549e-03,\n",
       "                      -5.2623e-03,  6.4293e-02,  9.9906e-02, -5.2227e-02, -8.7761e-03,\n",
       "                      -8.3210e-02,  1.5934e-01,  1.6174e-04,  4.5019e-02,  6.6068e-02,\n",
       "                       1.1780e-02, -6.1050e-03, -6.8307e-02,  3.5829e-02, -1.6686e-01,\n",
       "                      -2.7700e-01, -1.5824e-01, -1.0817e-01, -1.4837e-01, -2.0306e-01,\n",
       "                      -1.1253e-01, -2.0751e-01, -1.8969e-01, -1.4330e-01, -2.3397e-01,\n",
       "                      -9.6731e-02, -2.3658e-01, -2.7964e-01, -9.4703e-02, -1.1736e-01,\n",
       "                      -1.0305e-01, -1.5000e-01, -1.2543e-01, -2.7048e-01, -8.0327e-02,\n",
       "                      -2.5973e-01, -7.5739e-02, -1.3458e-01, -6.0540e-02, -1.5209e-01,\n",
       "                      -2.2311e-01, -1.3419e-01, -1.8222e-01, -1.7696e-02, -7.8136e-02,\n",
       "                      -5.9203e-02, -2.4907e-01, -1.3132e-01, -1.1321e-01, -2.1975e-01,\n",
       "                       1.8991e-02, -1.0545e-01, -1.3566e-01, -1.3153e-01, -5.3950e-02,\n",
       "                      -1.3610e-01, -2.0447e-01, -2.5861e-02, -2.3202e-01, -2.5965e-01,\n",
       "                      -1.8229e-01, -1.3142e-01, -2.0087e-01, -1.1250e-01, -2.1604e-01,\n",
       "                      -1.5240e-01, -5.9517e-02, -2.0378e-01, -1.1992e-01, -6.3087e-02,\n",
       "                      -1.2207e-01, -3.4813e-01, -6.0291e-02, -1.3021e-01, -2.2642e-01,\n",
       "                      -2.7020e-01, -1.1663e-01, -1.6669e-01, -3.0423e-01, -1.5377e-01,\n",
       "                      -4.8449e-02, -3.6347e-01, -3.3104e-01, -1.8135e-01, -1.4486e-01,\n",
       "                      -7.3065e-02, -1.9095e-02, -2.2395e-01, -3.4261e-01, -2.1885e-01,\n",
       "                      -1.1950e-01, -6.6411e-02, -1.0484e-01, -1.6872e-01, -1.1548e-01,\n",
       "                      -1.1576e-01, -1.3812e-01, -5.0087e-02, -2.0018e-01, -1.1148e-01,\n",
       "                      -2.4704e-01, -1.7312e-01, -2.5997e-01, -2.7647e-01, -1.9856e-01,\n",
       "                      -1.7014e-01, -1.5270e-01, -1.6841e-01, -1.4691e-01, -1.7796e-01,\n",
       "                      -2.8312e-02, -2.7463e-01, -2.4749e-01, -2.1944e-01, -2.4377e-01,\n",
       "                      -1.3985e-01, -1.8432e-01, -2.4585e-01, -2.3873e-01, -2.2761e-01,\n",
       "                      -8.5689e-02, -9.0796e-02, -1.4834e-01,  6.1053e-02, -2.0613e-01,\n",
       "                      -2.0360e-01, -2.3356e-01, -1.6287e-01, -2.7839e-01, -2.7253e-01,\n",
       "                      -1.4735e-01, -7.7636e-02, -1.8720e-01, -9.9303e-02, -2.1818e-01,\n",
       "                      -1.8530e-01, -9.9639e-02, -1.5055e-01, -1.3851e-01, -1.0594e-02,\n",
       "                      -1.9298e-01, -1.1326e-01])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([-1.1471e-01, -1.9845e-01, -1.2633e-01, -1.8505e-01, -1.7888e-01,\n",
       "                      -1.1697e-01, -1.3770e-01, -1.9004e-01, -2.7268e-01, -1.5455e-01,\n",
       "                      -2.2479e-01, -2.2958e-01, -2.6038e-01, -2.7495e-01, -7.0575e-02,\n",
       "                      -8.3229e-02, -8.8962e-02, -1.8721e-01, -1.1990e-01, -1.9995e-01,\n",
       "                      -4.4643e-02, -1.5802e-01, -5.0024e-03, -1.9155e-01, -1.9187e-01,\n",
       "                      -1.2143e-01, -2.3141e-01, -1.8662e-01, -1.6557e-01, -6.1497e-02,\n",
       "                      -1.0749e-01, -1.5153e-01, -2.5533e-01, -3.6156e-02, -1.6372e-01,\n",
       "                      -1.1630e-01, -6.1804e-02, -2.1261e-01, -6.5751e-02, -5.2135e-02,\n",
       "                      -4.8843e-02, -9.9051e-02, -1.5918e-01, -2.3701e-01, -1.0057e-01,\n",
       "                      -9.4296e-02, -8.9186e-02, -1.1764e-01, -1.2738e-01, -1.6218e-01,\n",
       "                      -3.4894e-01, -4.9985e-02, -4.2785e-02, -1.7201e-01, -1.5136e-01,\n",
       "                      -2.2650e-01, -1.8700e-01, -1.4339e-01, -2.2452e-01, -2.7695e-01,\n",
       "                      -2.2239e-01, -2.1863e-01, -3.0233e-01, -1.4518e-01, -2.1825e-01,\n",
       "                      -1.4315e-01, -3.3067e-01, -3.6181e-01, -2.8854e-01, -1.5863e-01,\n",
       "                      -1.4564e-01, -1.4878e-01, -1.7258e-01, -1.0230e-01, -3.1990e-01,\n",
       "                      -2.5196e-01, -1.8925e-01, -3.6486e-02, -1.8308e-01, -1.0181e-01,\n",
       "                      -1.7015e-01, -7.9456e-02, -1.6300e-01, -1.4586e-01, -2.7779e-01,\n",
       "                      -2.5542e-01, -2.5474e-01, -1.7844e-01, -2.9964e-01, -2.5852e-01,\n",
       "                      -1.0017e-01, -1.7314e-01, -9.5897e-02, -1.5382e-01, -8.4118e-02,\n",
       "                      -8.8033e-02, -1.4083e-01, -1.3566e-01, -1.8826e-01, -9.7228e-02,\n",
       "                      -2.7779e-01, -1.8068e-01, -2.2449e-01,  6.6615e-03, -3.2898e-01,\n",
       "                      -2.2489e-01, -1.1516e-02, -2.2804e-01, -1.2830e-01, -1.2090e-01,\n",
       "                      -2.8184e-01, -2.7301e-01, -1.6433e-01, -1.8775e-01, -2.8379e-01,\n",
       "                      -2.1096e-01, -4.4243e-02, -9.8403e-02, -2.0524e-02, -2.2184e-01,\n",
       "                      -1.3614e-01, -1.3309e-01, -1.2683e-01, -8.5556e-02, -1.1324e-01,\n",
       "                      -4.7331e-02, -2.6993e-01, -7.5993e-02, -1.2220e-01, -2.4641e-01,\n",
       "                      -1.3572e-01, -1.6782e-01, -2.1299e-01, -1.1648e-01, -2.7124e-01,\n",
       "                      -8.1880e-02, -1.2075e-01, -2.5194e-01, -1.8760e-01, -4.3620e-02,\n",
       "                      -2.0090e-01, -1.9152e-01, -3.3303e-02, -1.5649e-01, -1.5905e-02,\n",
       "                      -1.4804e-01, -6.3545e-02, -1.0238e-01, -2.5274e-01, -1.6589e-01,\n",
       "                      -1.7503e-01, -2.2955e-01, -6.1791e-02, -8.6290e-04, -1.1048e-01,\n",
       "                      -1.4062e-01, -5.4084e-02, -1.2863e-01, -6.2566e-02, -1.0021e-01,\n",
       "                      -2.0491e-01, -2.2973e-01, -6.0336e-02, -9.9600e-02,  5.5114e-03,\n",
       "                      -2.9478e-01, -4.4711e-02, -3.4096e-02, -9.8373e-02, -1.5053e-02,\n",
       "                      -7.1000e-02, -1.1051e-01, -2.3341e-01, -9.8048e-02, -1.3685e-01,\n",
       "                      -3.2913e-02, -1.0305e-01, -1.2233e-01, -2.0703e-01, -1.3974e-01,\n",
       "                       6.2729e-02, -1.7390e-01, -1.8576e-01, -2.8547e-01, -1.3964e-01,\n",
       "                      -1.5859e-01, -6.8755e-02, -2.0427e-01, -8.5667e-02, -1.0645e-01,\n",
       "                      -1.2688e-01, -1.8170e-01, -1.3740e-01, -7.8482e-02, -7.3321e-02,\n",
       "                      -2.1096e-01, -1.6996e-01, -2.8292e-01, -1.0855e-01, -1.3129e-01,\n",
       "                      -3.2296e-02, -1.2374e-01, -1.8085e-01, -2.9277e-02, -1.9886e-01,\n",
       "                      -5.6054e-02, -6.1259e-02,  1.5972e-02, -3.2957e-02, -1.0225e-01,\n",
       "                      -8.7563e-02, -1.1908e-02, -2.6433e-01, -1.1995e-01, -1.6717e-01,\n",
       "                      -5.2755e-02, -2.2624e-01, -1.6540e-01, -1.2295e-01, -1.7136e-01,\n",
       "                      -1.3328e-01, -1.2860e-01, -3.1919e-02, -1.9174e-01, -1.0446e-01,\n",
       "                      -2.0048e-01, -2.2260e-01, -9.6635e-02, -1.9650e-01, -1.6927e-01,\n",
       "                      -2.0722e-01, -2.0743e-02, -1.7285e-01, -1.8575e-01, -4.7899e-02,\n",
       "                      -2.3504e-01, -1.0530e-01, -9.1892e-02, -1.6509e-01, -1.7882e-01,\n",
       "                      -1.5694e-01, -1.0644e-01, -2.4940e-01, -1.9841e-01, -6.5986e-02,\n",
       "                      -9.3659e-02, -1.2026e-01, -1.2924e-01, -1.2995e-01, -8.6849e-02,\n",
       "                      -2.0417e-01, -1.5733e-01, -1.3579e-01, -3.6705e-02, -2.2835e-01,\n",
       "                      -1.9512e-01, -7.5364e-03,  7.3682e-02,  9.7183e-04, -3.8223e-02,\n",
       "                       9.2244e-03,  7.1929e-02,  6.1977e-02, -9.9604e-02, -5.6419e-02,\n",
       "                       7.6303e-02,  5.0435e-02, -7.2998e-02, -1.8509e-02,  2.8402e-02,\n",
       "                       1.0702e-01,  2.9627e-02,  5.8305e-02, -6.4019e-05,  6.5280e-02,\n",
       "                       6.8398e-02, -2.2405e-02, -8.9337e-02,  1.3488e-01,  7.0319e-02,\n",
       "                       6.7981e-03, -3.8945e-02,  7.3384e-02, -5.8399e-02, -6.2143e-02,\n",
       "                      -5.3429e-02, -3.8357e-02,  8.6446e-02,  2.2823e-02,  1.1758e-01,\n",
       "                      -2.6357e-02,  1.0990e-01, -5.5018e-02,  2.8532e-02, -2.7047e-03,\n",
       "                       7.1251e-02,  1.9574e-02,  1.9029e-02, -1.9593e-02, -4.5099e-02,\n",
       "                       3.6532e-02,  2.8787e-02, -2.3872e-02, -3.8251e-02, -3.1461e-03,\n",
       "                       1.1428e-01,  3.4616e-02, -7.8901e-02, -1.1889e-03, -4.2218e-02,\n",
       "                      -1.8629e-02, -3.8522e-02, -1.1680e-01, -3.1736e-02,  7.2468e-02,\n",
       "                       4.5771e-02,  8.2858e-02,  1.1393e-02, -7.7763e-02,  4.3627e-02,\n",
       "                      -2.0606e-02,  5.8260e-02,  5.3294e-02, -4.1721e-02,  2.8381e-02,\n",
       "                      -2.1002e-02, -1.6918e-02,  4.4942e-03,  6.6808e-02,  7.5030e-02,\n",
       "                      -7.8941e-03,  2.3396e-02, -8.7979e-02,  1.0645e-01,  2.6986e-02,\n",
       "                      -7.7287e-02,  1.2485e-04,  1.2456e-02, -6.8189e-03,  3.3420e-02,\n",
       "                       6.1470e-02, -3.3323e-02, -3.5572e-03,  1.0012e-01, -8.6794e-02,\n",
       "                       4.6462e-02, -6.6586e-02, -5.7932e-02, -1.8624e-02,  3.3924e-02,\n",
       "                      -1.1352e-01, -9.0887e-02, -4.0429e-02,  4.2110e-02, -2.4396e-02,\n",
       "                       6.5295e-02,  6.3716e-02, -1.1089e-02, -9.4325e-02, -1.1062e-02,\n",
       "                       9.5444e-02,  4.0077e-02,  1.2154e-01, -9.3798e-02,  1.4538e-02,\n",
       "                       1.0429e-01,  7.4521e-02,  3.7719e-03, -1.1552e-02,  6.7449e-02,\n",
       "                       1.0128e-02,  2.0970e-02,  1.8488e-02, -6.0780e-03, -1.1292e-02,\n",
       "                      -1.2748e-01, -4.5366e-02,  1.8560e-02,  2.2289e-03,  5.2138e-02,\n",
       "                      -1.7410e-02,  1.0363e-01, -1.4029e-02, -1.7810e-02, -1.7953e-01,\n",
       "                      -1.2913e-01, -1.5001e-01, -6.3017e-02, -2.4172e-01, -8.7680e-02,\n",
       "                      -2.9421e-01, -2.4519e-01, -2.0586e-01, -1.8074e-01, -1.6495e-01,\n",
       "                      -1.3094e-01, -2.7966e-01, -3.1635e-01, -1.8051e-02, -1.3742e-01,\n",
       "                      -2.3041e-01, -1.9691e-01, -4.9806e-02, -2.8333e-01, -2.8959e-02,\n",
       "                      -1.4143e-01, -1.3120e-01, -1.8423e-01, -1.1118e-01, -1.9206e-01,\n",
       "                      -2.7700e-01, -1.7072e-01, -1.6677e-01, -1.4694e-01, -2.3134e-01,\n",
       "                      -1.8181e-01, -2.2847e-01, -1.2108e-01, -1.5791e-01, -1.5535e-01,\n",
       "                      -1.6213e-01, -1.2560e-01, -1.4207e-01, -7.8115e-02, -9.0841e-02,\n",
       "                      -2.3866e-01, -2.0538e-01, -2.9549e-02, -1.4076e-01, -1.9177e-01,\n",
       "                      -2.1335e-01, -1.7742e-01, -1.8583e-01, -7.5507e-02, -2.0847e-01,\n",
       "                      -9.2403e-02, -5.9189e-02, -2.1409e-01, -9.7139e-02, -1.7744e-01,\n",
       "                      -2.0429e-01, -1.8313e-01, -2.5781e-01, -2.7963e-01, -1.5160e-01,\n",
       "                      -3.0542e-01, -1.5366e-01, -2.2848e-01, -1.2033e-01, -1.9394e-01,\n",
       "                      -8.9060e-02, -1.7887e-01, -2.2713e-01, -1.7292e-01, -2.0395e-01,\n",
       "                      -1.4098e-01, -6.9651e-02, -2.2257e-01, -3.3123e-01, -1.8302e-01,\n",
       "                      -2.3389e-01, -3.3820e-02, -1.2066e-01, -8.6233e-02, -7.3259e-02,\n",
       "                      -9.1225e-02, -1.6600e-01, -2.3977e-01, -1.3385e-01, -2.1036e-01,\n",
       "                      -1.4137e-01, -1.7848e-01, -2.6090e-01, -3.1525e-01, -2.1995e-01,\n",
       "                      -2.2488e-01, -1.0442e-01, -1.0107e-01, -1.6045e-01, -1.7852e-01,\n",
       "                      -1.6384e-01, -2.0869e-01, -1.4455e-01, -1.5534e-01, -2.1087e-01,\n",
       "                      -1.6815e-01, -1.4797e-01, -1.5060e-01, -2.6356e-01, -1.6383e-01,\n",
       "                      -9.2744e-02, -1.4035e-01, -2.4230e-01,  4.0524e-02, -3.3184e-01,\n",
       "                      -9.6545e-02, -1.8054e-01, -1.4244e-01, -2.3108e-01, -2.6397e-01,\n",
       "                      -8.1657e-02, -3.4172e-02, -1.7708e-01, -1.1262e-01, -1.9708e-01,\n",
       "                      -1.6840e-01, -1.3930e-01,  7.4238e-03, -1.2137e-01, -1.0141e-01,\n",
       "                      -3.1242e-01, -8.6767e-03])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.1264,  0.0936,  0.0350,  ..., -0.0063,  0.2065, -0.6377],\n",
       "                      [ 0.1161, -0.1953, -0.2260,  ..., -0.2529,  0.1858, -0.2169],\n",
       "                      [ 0.1854, -0.5069, -0.2117,  ...,  0.0059,  0.1124, -0.1017],\n",
       "                      ...,\n",
       "                      [ 0.2525,  0.4732, -0.1690,  ...,  0.0892,  0.0119,  0.0062],\n",
       "                      [ 0.1279, -0.1617,  0.5836,  ..., -0.3443, -0.0681, -0.0485],\n",
       "                      [ 0.0659,  0.0024,  0.1424,  ...,  0.3952,  0.1862,  0.1460]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1533,  0.1418, -0.1590,  ...,  0.0779, -0.1219,  0.1060],\n",
       "                      [ 0.0640,  0.0464, -0.0495,  ...,  0.0018,  0.0007,  0.3530],\n",
       "                      [ 0.0919, -0.0698, -0.0191,  ...,  0.3746,  0.0146, -0.1915],\n",
       "                      ...,\n",
       "                      [ 0.1045,  0.0214,  0.3623,  ...,  0.0362, -0.2157, -0.0913],\n",
       "                      [ 0.2144,  0.1629, -0.0810,  ..., -0.0514, -0.0947, -0.0687],\n",
       "                      [-0.0020, -0.1804,  0.0109,  ...,  0.0885, -0.0330,  0.1288]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([-9.7515e-02,  1.9793e-01, -1.3982e-01,  4.4683e-02, -3.9477e-03,\n",
       "                       1.8170e-01,  1.5685e-01,  1.3355e-01, -2.6274e-02, -3.2828e-02,\n",
       "                       1.3111e-02, -5.2891e-02, -1.3446e-02,  1.9165e-02, -4.0948e-02,\n",
       "                      -2.5724e-02,  7.6825e-02,  1.2430e-01,  1.2117e-01,  1.6703e-01,\n",
       "                       4.0750e-02,  8.5101e-02,  9.2229e-02, -2.8107e-02, -5.1327e-02,\n",
       "                      -2.0062e-01,  1.7110e-03,  1.2576e-01,  9.4736e-02,  1.1843e-02,\n",
       "                       1.3149e-02,  1.9963e-02, -1.0801e-02, -6.8716e-02,  3.7795e-02,\n",
       "                       7.4799e-02,  1.4460e-01,  1.4330e-02,  1.4078e-01,  1.1502e-01,\n",
       "                      -6.1486e-02, -1.1131e-02,  6.4181e-02,  1.1730e-01,  3.1692e-03,\n",
       "                       2.0317e-02, -5.4278e-02,  1.6565e-01,  3.0770e-02,  9.0126e-02,\n",
       "                       1.6777e-01,  1.4753e-02,  4.3688e-03,  8.1234e-02,  2.1640e-01,\n",
       "                       5.9036e-02,  8.4517e-04,  1.1474e-01, -4.1306e-02,  1.8472e-02,\n",
       "                       7.9856e-03, -8.5040e-03,  3.1864e-03, -9.6129e-03, -4.5175e-02,\n",
       "                       1.1576e-01, -7.8072e-02, -1.3084e-01, -9.0233e-02,  5.3814e-02,\n",
       "                       4.2855e-02,  1.6104e-01,  2.1582e-02, -3.7054e-02,  7.5067e-02,\n",
       "                      -6.2450e-02,  2.6740e-02,  3.9755e-02,  2.5432e-02,  1.4466e-01,\n",
       "                      -1.2825e-01,  4.9830e-02, -2.0812e-02,  9.1142e-02, -1.2775e-02,\n",
       "                       1.5800e-01,  5.0552e-03,  8.8404e-02, -1.1947e-03,  2.9268e-02,\n",
       "                       1.3068e-01,  1.0029e-01,  7.5716e-02, -6.3433e-02,  1.2337e-01,\n",
       "                       1.6964e-01,  5.9698e-02,  6.5629e-02,  8.4314e-02,  8.2744e-02,\n",
       "                       1.1039e-01, -5.4879e-02,  1.0364e-01,  6.8046e-02,  3.7598e-03,\n",
       "                       9.1514e-02, -4.2318e-02, -4.8901e-02, -8.7231e-02, -1.3222e-02,\n",
       "                       2.6654e-02,  9.6359e-02,  1.0339e-01, -1.5346e-02, -2.7753e-02,\n",
       "                      -1.2227e-02,  5.4556e-02, -2.6033e-02,  1.2298e-01, -7.0613e-02,\n",
       "                      -1.3678e-01,  7.7145e-02,  1.4896e-01,  3.0162e-02,  7.7403e-02,\n",
       "                       4.6755e-02, -2.2976e-02, -2.9187e-02,  4.9434e-02, -1.3473e-01,\n",
       "                      -1.9550e-01, -1.1148e-02, -1.5416e-01, -4.7413e-02, -3.9182e-02,\n",
       "                      -1.5244e-02,  1.3841e-03, -1.4095e-03,  1.9969e-03, -1.2220e-01,\n",
       "                      -2.6876e-02, -2.1851e-02, -3.4648e-03,  6.9449e-02,  1.0235e-02,\n",
       "                      -1.7332e-01, -5.6764e-02, -1.5611e-01, -2.7153e-01, -3.6892e-02,\n",
       "                      -5.1533e-02, -8.2594e-02, -7.4608e-02, -8.1670e-03, -1.3501e-01,\n",
       "                      -4.6032e-02,  2.4786e-02, -9.2097e-02, -1.9471e-01, -1.1509e-01,\n",
       "                       8.2784e-02,  1.1278e-01,  3.0946e-02, -1.8826e-01, -1.2730e-01,\n",
       "                       8.0482e-02, -1.1095e-01, -8.7060e-04,  2.0231e-02,  1.1059e-01,\n",
       "                      -5.5984e-02, -7.1854e-03,  1.0625e-02, -1.3471e-01, -6.3659e-02,\n",
       "                      -7.7887e-02,  3.4515e-02,  2.0918e-02, -2.2601e-01, -1.9392e-01,\n",
       "                       2.5379e-03,  3.2277e-02, -1.9931e-03,  5.1143e-02,  5.8806e-02,\n",
       "                       5.9777e-02,  1.7961e-01, -5.8581e-02, -8.9126e-02, -3.1078e-02,\n",
       "                      -6.6166e-03,  7.2946e-04, -1.8587e-02, -4.8135e-02,  1.7101e-02,\n",
       "                      -9.7111e-02,  5.8885e-02,  9.3524e-02, -4.0038e-02, -1.9319e-01,\n",
       "                       1.7421e-02, -1.4300e-01, -1.3202e-01, -7.1060e-02, -1.6170e-01,\n",
       "                       5.3405e-03, -2.7817e-02, -5.4173e-02, -2.4463e-01,  1.1257e-03,\n",
       "                      -1.1825e-01, -5.3801e-02,  5.7899e-02,  1.3707e-02, -6.3982e-02,\n",
       "                      -5.4739e-03, -1.3732e-01, -5.2279e-02, -1.5291e-01,  6.1623e-02,\n",
       "                       2.4936e-01, -6.2560e-02, -8.6778e-02, -1.2235e-01, -5.5767e-02,\n",
       "                      -6.6516e-02, -5.3004e-02, -5.2030e-02, -1.3071e-01, -8.5412e-02,\n",
       "                      -2.1987e-01, -1.6851e-01, -8.9583e-02, -9.8088e-02, -7.0098e-02,\n",
       "                      -1.0474e-01, -1.8060e-01, -7.6848e-02, -6.8972e-02,  3.8954e-03,\n",
       "                       1.5436e-02, -6.6561e-02, -6.7469e-02,  4.6718e-02, -1.1598e-01,\n",
       "                       6.8977e-03, -3.2595e-03,  3.4802e-03, -9.9517e-02, -8.1557e-02,\n",
       "                      -1.9907e-02,  7.4948e-02, -1.5321e-01, -8.6966e-02, -1.9869e-01,\n",
       "                      -2.1282e-01,  9.9987e-02, -2.6316e-02, -7.4223e-03, -6.7845e-03,\n",
       "                      -2.1693e-02,  6.0541e-02,  8.4466e-02, -9.7337e-03, -4.0471e-02,\n",
       "                      -7.7185e-02,  1.5298e-02,  7.0408e-02, -7.6933e-03, -3.7887e-02,\n",
       "                      -6.2190e-02, -2.3598e-02,  8.0507e-03, -1.2247e-01, -6.9092e-02,\n",
       "                      -6.8098e-02, -1.8967e-01,  7.5066e-02, -5.3380e-02,  2.5904e-02,\n",
       "                      -5.2637e-02,  7.2388e-02, -6.9895e-02,  8.2878e-02, -1.5261e-01,\n",
       "                       1.4795e-01,  2.6190e-02, -1.3143e-02,  2.8663e-02,  1.2856e-01,\n",
       "                      -9.5539e-02,  3.7237e-02, -1.9394e-02, -4.1703e-03,  1.1172e-02,\n",
       "                      -4.5502e-02, -1.9340e-02, -1.5574e-01,  5.4936e-02, -4.8632e-02,\n",
       "                       1.9414e-03,  1.4666e-02,  2.6415e-02, -7.0412e-02,  3.0165e-02,\n",
       "                       5.5058e-02,  7.9246e-02,  1.0913e-01,  1.0642e-02,  1.3242e-02,\n",
       "                       6.0534e-02, -3.5213e-02,  3.2937e-02, -5.8370e-02,  5.1108e-02,\n",
       "                      -4.4399e-02,  4.8271e-02, -5.3265e-02, -3.2065e-02, -5.6922e-04,\n",
       "                       1.2190e-02,  1.0626e-01, -1.8890e-02,  4.5796e-02, -7.5394e-02,\n",
       "                      -1.1455e-02, -1.3835e-02,  6.0856e-02,  7.2004e-02,  9.5494e-02,\n",
       "                      -2.6905e-03, -7.2585e-02, -3.2577e-02,  1.9160e-02, -1.1751e-01,\n",
       "                      -2.1773e-02, -4.6869e-02, -4.6691e-02,  5.6707e-03, -3.3129e-02,\n",
       "                      -1.4917e-01,  3.3434e-02,  3.6355e-02,  2.5648e-02,  3.4844e-02,\n",
       "                       8.9269e-02, -1.0755e-01,  6.0969e-02, -2.4134e-02,  5.0466e-02,\n",
       "                       3.7503e-02, -8.5410e-02, -4.5446e-02, -8.3203e-03, -9.5253e-02,\n",
       "                      -1.1967e-01, -2.0541e-04,  1.5200e-01,  6.8067e-02,  6.3064e-03,\n",
       "                       1.8545e-01,  5.0226e-02,  1.3817e-01, -7.6956e-02,  3.7768e-02,\n",
       "                      -3.8376e-02, -5.5115e-02, -6.8707e-02,  6.5863e-02, -2.0045e-03,\n",
       "                      -3.6191e-02,  4.7302e-02,  2.2755e-02,  4.2941e-02,  1.9567e-02,\n",
       "                       2.8853e-02,  2.5734e-02, -1.8094e-01, -2.9083e-02,  1.4366e-02,\n",
       "                      -2.6475e-02,  1.4089e-03, -8.3012e-02,  2.3535e-04, -2.0279e-02,\n",
       "                       1.6016e-01, -8.2144e-03,  1.7156e-01,  4.5563e-02,  5.6717e-02,\n",
       "                      -7.4899e-02,  1.8720e-01,  1.2632e-02, -4.1565e-02,  1.1558e-02,\n",
       "                      -1.4581e-02,  1.8367e-01,  1.4984e-01,  3.1296e-02, -3.9185e-02,\n",
       "                       1.1158e-01,  8.3004e-02,  1.6413e-01,  1.2505e-01,  6.2957e-02,\n",
       "                       1.0499e-01,  5.4921e-02,  1.6921e-02,  1.0869e-01,  4.7979e-03,\n",
       "                      -2.7046e-03,  1.3098e-01,  2.3804e-01,  5.0356e-02,  1.5119e-02,\n",
       "                       7.7605e-03,  4.2598e-02, -3.8234e-02,  4.1202e-02,  1.7181e-01,\n",
       "                       1.4762e-01,  1.9790e-01, -4.9674e-03,  8.6689e-02, -7.2137e-02,\n",
       "                       1.3394e-01,  5.1319e-02,  1.4339e-01,  1.2460e-01, -1.4257e-01,\n",
       "                       6.1864e-02,  1.3100e-01,  1.1566e-01,  1.3212e-01,  1.1493e-01,\n",
       "                       2.9965e-02,  6.3618e-02,  1.0555e-01,  2.5259e-02,  7.1261e-02,\n",
       "                       2.6187e-01,  6.9733e-02,  6.0417e-02,  1.2894e-01,  3.6990e-02,\n",
       "                       1.4118e-01,  1.1961e-01,  2.2016e-01,  1.5967e-01,  1.3838e-01,\n",
       "                       1.1560e-02, -8.2152e-02, -7.7441e-03,  7.0128e-02, -7.9143e-02,\n",
       "                       4.1013e-02,  1.6768e-02, -6.4854e-02,  8.2691e-02, -9.3899e-02,\n",
       "                       4.1524e-02,  1.2998e-02,  7.6061e-02,  1.9276e-01, -3.8356e-02,\n",
       "                       3.0871e-02, -1.0305e-03,  1.5699e-01,  8.9432e-03,  1.0238e-01,\n",
       "                      -1.1309e-01,  1.2744e-01,  2.2962e-02,  9.5715e-02,  8.6117e-02,\n",
       "                       1.2694e-01, -1.0893e-01, -7.5708e-02,  6.7070e-02,  1.0275e-01,\n",
       "                       1.9879e-02,  1.3185e-01,  4.7077e-02,  3.1202e-02,  9.9664e-02,\n",
       "                      -4.0472e-02, -4.1476e-02,  6.1650e-02,  2.4332e-02,  1.2010e-01,\n",
       "                       4.2260e-02, -2.6413e-02,  8.4883e-02,  3.3241e-02, -5.1410e-02,\n",
       "                       6.8750e-02,  2.9519e-01, -4.9179e-02,  6.3193e-02, -1.6114e-02,\n",
       "                      -2.0369e-02,  6.4517e-02,  1.9403e-01, -8.0210e-02,  1.8607e-01,\n",
       "                       5.3493e-02,  1.7731e-01,  2.5608e-03,  1.9597e-01, -7.7282e-02,\n",
       "                       3.0965e-02,  1.7763e-01])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-5.0929e-02,  1.0761e-01, -1.6120e-01,  1.9572e-01,  6.1245e-02,\n",
       "                       5.8345e-02,  1.2170e-01,  9.5777e-02, -1.3506e-01,  8.8039e-02,\n",
       "                       1.1033e-01,  8.5460e-02,  3.4561e-02,  1.0589e-01, -6.9478e-03,\n",
       "                      -5.4522e-02, -8.4250e-02,  9.8478e-02,  1.6830e-01,  2.0923e-01,\n",
       "                       7.1323e-02,  7.6525e-02,  3.8221e-02,  1.3832e-01,  9.0653e-03,\n",
       "                      -7.8665e-02, -2.7750e-02,  2.3350e-02,  2.2647e-02, -7.1271e-02,\n",
       "                       3.9578e-02, -3.9014e-03, -1.4244e-02,  9.4055e-03,  9.9125e-02,\n",
       "                       5.2056e-02,  1.2559e-01,  2.2897e-01,  5.6222e-02,  7.6346e-02,\n",
       "                       2.8574e-02,  2.9076e-02,  8.7379e-03,  9.0951e-02,  9.5134e-02,\n",
       "                       5.9995e-02, -1.3619e-01,  1.5582e-01,  7.3319e-02, -8.2192e-03,\n",
       "                       7.5074e-02, -5.1711e-02, -9.7750e-02,  4.6381e-02,  1.2547e-01,\n",
       "                       7.5467e-02,  3.8166e-02,  3.5990e-02, -3.3321e-02,  5.0104e-02,\n",
       "                      -6.6519e-02,  1.7410e-02,  1.0712e-01,  9.7608e-02,  1.0404e-01,\n",
       "                       9.8654e-02,  7.6138e-02,  3.0679e-02, -8.7764e-02,  5.7875e-02,\n",
       "                      -5.1925e-02,  7.1795e-02, -2.8136e-02, -1.7999e-02,  5.4472e-02,\n",
       "                      -6.6366e-02, -4.6761e-02, -6.9617e-03,  5.2463e-02,  4.8666e-02,\n",
       "                      -1.0061e-01,  6.1198e-02, -1.4076e-01,  1.1108e-01,  4.8881e-02,\n",
       "                       8.3525e-02, -1.7847e-01, -3.9951e-02,  9.6408e-02,  2.6177e-02,\n",
       "                       2.9191e-02,  9.4332e-02,  5.8756e-02, -2.8727e-02,  4.9239e-02,\n",
       "                       1.2559e-01,  6.1491e-02,  1.0512e-01,  1.0055e-01,  1.1147e-01,\n",
       "                      -1.8210e-03,  6.9887e-02,  3.8155e-02,  1.1219e-01,  2.3760e-02,\n",
       "                       1.1696e-01, -7.1321e-02, -1.1025e-01, -4.1861e-02,  6.3401e-02,\n",
       "                       2.3740e-02,  4.5810e-02,  5.5038e-02, -3.1485e-02,  4.8924e-02,\n",
       "                       1.1356e-02, -1.6195e-01,  8.0219e-02,  2.1242e-01, -1.2475e-01,\n",
       "                       1.1566e-01,  2.2004e-01,  4.8818e-02, -5.5201e-02, -8.6609e-04,\n",
       "                       1.7910e-02, -3.9826e-02,  2.0707e-02,  7.5377e-02, -1.2834e-01,\n",
       "                      -1.3628e-01,  1.1892e-02, -9.2741e-02, -4.7761e-02, -5.3691e-02,\n",
       "                      -3.4798e-02,  6.7947e-02,  6.6800e-02, -1.1114e-01, -4.1722e-02,\n",
       "                      -8.5190e-02, -2.9630e-02,  7.1418e-02, -1.3096e-01, -7.7136e-03,\n",
       "                      -1.5841e-01, -3.4876e-02, -3.8568e-02, -2.7753e-01,  1.1983e-02,\n",
       "                       8.3554e-03, -4.4710e-02, -1.2051e-01, -3.8169e-02,  4.6291e-02,\n",
       "                      -4.1412e-02,  3.2937e-02,  1.4183e-01,  2.9628e-02,  2.3140e-02,\n",
       "                      -6.6530e-03,  1.4931e-01, -1.2629e-01, -1.6540e-01, -1.2198e-01,\n",
       "                       4.4812e-02, -1.2947e-01,  2.9976e-02,  9.7704e-03,  7.0846e-02,\n",
       "                      -3.5259e-02,  2.8777e-02, -6.8446e-02, -6.4270e-02, -7.1768e-02,\n",
       "                      -1.4781e-01,  1.0830e-01,  2.0319e-02, -1.7950e-01, -2.0912e-01,\n",
       "                      -9.5846e-02, -7.9734e-02,  6.7573e-02,  5.6553e-02, -1.4318e-01,\n",
       "                      -4.7861e-02,  6.2152e-02, -1.2170e-01, -1.7466e-02,  6.3410e-02,\n",
       "                      -1.0813e-02,  9.1775e-03, -3.9059e-02, -5.0616e-02, -5.7688e-02,\n",
       "                      -8.0041e-02, -1.4668e-02,  4.1315e-02, -4.4694e-02, -8.8673e-02,\n",
       "                      -3.6382e-02, -1.1792e-01, -3.4550e-02, -6.6270e-02, -7.6765e-02,\n",
       "                      -1.9837e-02,  1.4032e-02, -5.8874e-02, -3.4789e-02, -8.1586e-02,\n",
       "                      -2.0949e-01, -3.3038e-02,  7.9853e-03, -6.3540e-02,  3.1099e-04,\n",
       "                       6.8338e-02, -1.0726e-01, -6.2388e-02, -1.4773e-01,  7.0745e-02,\n",
       "                      -6.2142e-03, -6.1006e-02, -7.7575e-02, -1.9613e-02, -3.5557e-03,\n",
       "                      -8.4749e-02,  6.6624e-02,  5.3797e-02, -2.6857e-01, -8.1601e-02,\n",
       "                      -2.0698e-01, -8.6080e-02, -2.1088e-01, -1.4822e-02,  1.7557e-02,\n",
       "                      -1.9532e-01, -2.1355e-01, -6.7666e-02, -1.6426e-02, -1.1781e-01,\n",
       "                      -6.9947e-02, -1.1096e-02, -2.2117e-01,  5.0225e-02, -1.3726e-01,\n",
       "                      -3.6846e-02, -4.6756e-02, -6.6005e-02,  3.7379e-02, -8.3037e-02,\n",
       "                       1.5236e-01, -1.6322e-02, -7.0828e-02, -6.1002e-02, -5.5411e-02,\n",
       "                      -1.1354e-01,  7.2122e-02,  1.1249e-01, -1.0440e-01,  2.0453e-02,\n",
       "                      -1.1117e-01, -1.1073e-01,  2.5800e-02, -1.5645e-03, -2.7460e-02,\n",
       "                       4.7213e-02,  9.4410e-02,  3.6437e-03,  5.0415e-02, -1.2201e-01,\n",
       "                      -1.6089e-01,  4.3898e-02,  3.7114e-02, -4.8018e-03,  2.8946e-02,\n",
       "                      -3.6207e-02,  8.7353e-03,  1.0781e-01, -4.7206e-02,  8.2011e-02,\n",
       "                      -2.9021e-02,  5.8638e-02, -6.3159e-02, -7.7089e-02, -1.2840e-02,\n",
       "                       8.0087e-02, -4.3682e-02, -5.1970e-02,  1.2909e-01,  8.8343e-02,\n",
       "                       6.2587e-02,  2.5382e-02, -3.0258e-02,  6.1094e-03, -1.2519e-02,\n",
       "                      -2.3910e-03, -4.2889e-03, -1.0095e-01, -1.4484e-01, -3.8354e-02,\n",
       "                       4.1048e-02, -1.4799e-03, -3.9602e-02, -1.0872e-01, -5.3524e-02,\n",
       "                       1.1066e-02, -6.2150e-02, -2.6612e-02,  6.7134e-02,  3.9677e-02,\n",
       "                      -4.1411e-02, -7.3689e-02,  6.5371e-02, -5.3937e-02, -1.2649e-02,\n",
       "                      -1.7149e-01,  9.7496e-02,  9.2304e-02,  2.8562e-02, -1.0093e-02,\n",
       "                       2.4269e-02,  2.6224e-02, -7.6738e-03, -7.6987e-02,  7.7083e-02,\n",
       "                       1.2466e-01,  4.9265e-02,  2.6208e-02,  1.4044e-01,  1.2995e-02,\n",
       "                       1.7724e-02,  4.4844e-02, -6.4700e-02, -3.9261e-02,  1.8895e-02,\n",
       "                      -6.4361e-02, -2.5721e-02, -1.0407e-01,  2.2888e-02, -4.4972e-02,\n",
       "                      -1.4877e-01, -5.9438e-02,  1.2946e-01,  9.5526e-02,  7.3860e-02,\n",
       "                      -9.6146e-02, -3.2003e-02, -6.5454e-02,  4.6896e-02, -6.1120e-02,\n",
       "                      -1.8625e-02, -1.8854e-02,  7.4903e-02,  8.6681e-02,  8.3087e-02,\n",
       "                       2.9008e-02,  4.0651e-02, -3.0694e-02,  3.1654e-02,  1.0478e-02,\n",
       "                       1.4711e-04, -4.5780e-03, -4.0250e-02, -3.0683e-02,  3.8246e-02,\n",
       "                      -1.0716e-01, -4.9469e-02, -2.5805e-03, -1.8107e-02,  2.2805e-02,\n",
       "                      -3.7089e-02,  1.5644e-03,  3.3988e-02,  1.1380e-01,  1.6925e-02,\n",
       "                       7.4075e-02,  3.3677e-02, -3.9504e-02, -3.2800e-03,  2.0887e-02,\n",
       "                       5.0549e-02, -8.1589e-02,  1.4858e-01,  8.8505e-03,  3.9950e-02,\n",
       "                       1.9329e-01, -5.1329e-02,  1.4316e-01,  8.8413e-02,  1.2573e-01,\n",
       "                       6.6750e-02,  1.9860e-01, -3.8348e-02,  2.9003e-02,  7.5385e-02,\n",
       "                       4.0557e-02,  1.1244e-01,  5.1411e-02, -3.7989e-02, -3.6464e-02,\n",
       "                       6.1790e-02,  1.0513e-01,  2.0727e-01,  1.1995e-01,  1.0115e-01,\n",
       "                       8.3999e-02,  6.2514e-02,  8.1423e-02,  2.3742e-02, -5.6619e-02,\n",
       "                      -1.7355e-04,  4.9664e-02,  2.1985e-01,  1.5813e-01, -1.2742e-01,\n",
       "                      -3.2737e-03,  9.1990e-02,  4.1927e-02,  2.7068e-02,  6.7163e-02,\n",
       "                       1.6465e-01,  1.8437e-01,  1.8997e-02,  1.0137e-01, -2.1863e-02,\n",
       "                      -1.5325e-03,  1.0292e-01,  2.2919e-01,  7.7899e-02,  4.9913e-02,\n",
       "                       9.5790e-02,  6.7877e-02,  8.9670e-02,  9.4558e-02,  3.5365e-02,\n",
       "                      -1.1724e-01,  1.3375e-02, -5.0399e-02,  1.1385e-01,  1.2204e-02,\n",
       "                       1.5277e-01, -5.2942e-02,  2.0981e-01,  1.1437e-01, -4.0520e-02,\n",
       "                       4.1868e-02,  3.1419e-02,  9.6607e-02,  1.3508e-01,  1.0562e-01,\n",
       "                       8.4998e-02, -5.7570e-02, -2.2202e-02,  1.2715e-01,  8.9261e-02,\n",
       "                       1.2940e-01,  7.7283e-02,  5.9564e-02,  1.1575e-01, -6.8059e-02,\n",
       "                       1.4939e-01,  4.3464e-02,  1.2286e-01,  2.1336e-01, -6.4886e-02,\n",
       "                      -2.0961e-01, -1.1170e-01,  5.5963e-02,  8.7360e-02, -1.0823e-01,\n",
       "                      -1.0231e-01, -6.7849e-03,  1.2403e-01,  5.4286e-02,  1.1703e-01,\n",
       "                       4.1159e-02,  8.8118e-02,  1.0305e-01,  1.0224e-01, -2.1726e-02,\n",
       "                       1.3611e-01,  5.1480e-02, -2.5673e-02, -1.1262e-01,  1.3393e-01,\n",
       "                       1.1395e-01, -1.8652e-02,  2.0322e-01,  9.1912e-02,  2.0050e-01,\n",
       "                       1.1982e-01, -5.7084e-02,  1.3449e-01, -1.0945e-01,  4.7279e-02,\n",
       "                       5.7503e-02,  3.7122e-01, -7.5213e-03,  7.1987e-02,  5.6130e-02,\n",
       "                      -6.8414e-03,  9.5459e-02,  7.0179e-02, -2.8586e-02,  1.4072e-01,\n",
       "                       4.3658e-02,  1.1678e-01,  1.0501e-01,  1.7840e-01,  5.0585e-02,\n",
       "                      -4.8957e-02,  5.8811e-04])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[-3.2620e-01,  4.0247e-01,  2.3072e-01,  ...,  2.0201e-01,\n",
       "                        2.5956e-01, -3.6443e-03],\n",
       "                      [ 4.7042e-01,  5.3086e-02, -5.2819e-02,  ..., -2.4820e-01,\n",
       "                       -3.3051e-02,  4.8704e-02],\n",
       "                      [ 3.6540e-01,  5.0955e-01, -8.1819e-02,  ...,  1.6793e-01,\n",
       "                       -2.6084e-01, -8.5725e-01],\n",
       "                      ...,\n",
       "                      [ 1.3928e-01,  3.5394e-01,  6.0512e-01,  ...,  1.9950e-04,\n",
       "                        4.0206e-01, -2.5378e-01],\n",
       "                      [-1.5114e-01,  2.9450e-01, -2.1459e-01,  ...,  2.7172e-01,\n",
       "                       -1.5725e-01,  1.6848e-01],\n",
       "                      [ 2.9238e-01,  5.5871e-02,  4.7722e-01,  ..., -2.4756e-01,\n",
       "                        3.5473e-02, -1.0800e-01]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.3209,  0.3768,  0.2911,  ..., -0.0826,  0.1109,  0.1893],\n",
       "                      [ 0.3458,  0.0176,  0.2262,  ..., -0.0421,  0.2346, -0.0010],\n",
       "                      [ 0.0045,  0.2639,  0.1276,  ...,  0.0614, -0.1893, -0.0170],\n",
       "                      ...,\n",
       "                      [ 0.0258,  0.1536,  0.1928,  ...,  0.1966, -0.2166, -0.0359],\n",
       "                      [-0.0694, -0.1780,  0.0606,  ...,  0.2586,  0.0206, -0.0029],\n",
       "                      [ 0.0402, -0.0650,  0.2191,  ...,  0.1914, -0.1326, -0.1676]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-0.0528,  0.0371, -0.1152, -0.1404, -0.0283, -0.0735, -0.2790, -0.1205,\n",
       "                      -0.2184, -0.3066, -0.2326, -0.0500, -0.1420,  0.0333, -0.2117, -0.2840,\n",
       "                      -0.0692,  0.0014, -0.1021, -0.2002, -0.2091, -0.1005, -0.2215, -0.1760,\n",
       "                       0.0438, -0.2109, -0.0079, -0.2759, -0.1585, -0.0189,  0.1931, -0.1038,\n",
       "                      -0.0834, -0.2729, -0.2381, -0.1053, -0.1316,  0.0024, -0.1328, -0.2142,\n",
       "                      -0.1716, -0.1213, -0.1400, -0.1274,  0.0239, -0.1889, -0.0895, -0.1327,\n",
       "                      -0.1790, -0.1042, -0.2180, -0.2042, -0.0107, -0.1164,  0.0065, -0.2467,\n",
       "                      -0.1941, -0.1492, -0.2333, -0.2176, -0.0269, -0.1023, -0.0384, -0.2295,\n",
       "                      -0.0782, -0.2900,  0.0625, -0.1508, -0.1037, -0.0074, -0.0884, -0.1238,\n",
       "                      -0.1473, -0.0662, -0.0453, -0.1896, -0.0791, -0.0586, -0.1356, -0.0815,\n",
       "                      -0.0366, -0.1328, -0.0688, -0.1753,  0.0575, -0.1285, -0.2798, -0.1618,\n",
       "                      -0.1050,  0.0130, -0.2333, -0.1100, -0.0999, -0.0830, -0.1391, -0.0459,\n",
       "                      -0.1531,  0.0081, -0.0787, -0.0486, -0.1247, -0.1281, -0.1119, -0.1415,\n",
       "                      -0.2198, -0.1518, -0.0993, -0.0229, -0.0960, -0.0006, -0.0892, -0.1258,\n",
       "                      -0.1365, -0.0642, -0.0756, -0.0598, -0.1572, -0.0563, -0.0079, -0.1441,\n",
       "                      -0.2580, -0.2148, -0.0064, -0.0684, -0.0153, -0.1901, -0.0927,  0.0584,\n",
       "                      -0.0527, -0.0705, -0.0067, -0.1651, -0.1887,  0.0291, -0.0181, -0.0567,\n",
       "                      -0.1784, -0.1350,  0.0292, -0.1013,  0.0263,  0.0375, -0.2089, -0.1021,\n",
       "                      -0.0743,  0.0433, -0.0244, -0.0517, -0.0718, -0.0301, -0.0730, -0.2201,\n",
       "                      -0.1218, -0.1707, -0.0945, -0.0471, -0.0946, -0.1456, -0.1064, -0.1180,\n",
       "                       0.0287, -0.2114, -0.0969, -0.0699,  0.0089, -0.1223, -0.0930, -0.0137,\n",
       "                      -0.0626, -0.0590, -0.1564, -0.0621,  0.0623, -0.1301,  0.0827,  0.0025,\n",
       "                      -0.1299,  0.1756, -0.1813, -0.1083, -0.0481, -0.0973,  0.0936, -0.2214,\n",
       "                       0.0467, -0.1180,  0.0311, -0.1621, -0.1288, -0.1256, -0.0787, -0.1221,\n",
       "                      -0.0073, -0.1245, -0.0478, -0.1227, -0.0244,  0.0773, -0.0166, -0.0379,\n",
       "                       0.1066, -0.1338, -0.0553, -0.0472,  0.0014, -0.0487,  0.0378, -0.0896,\n",
       "                      -0.0864, -0.0611, -0.0010, -0.1087, -0.2070, -0.1757, -0.1107, -0.0961,\n",
       "                      -0.1054, -0.0319, -0.1130, -0.0422, -0.0880, -0.0791, -0.0060, -0.1996,\n",
       "                      -0.0542, -0.0473, -0.1282, -0.0421,  0.0413, -0.1544, -0.0288, -0.1805,\n",
       "                      -0.0892, -0.1340, -0.1642, -0.1349,  0.0458, -0.0574,  0.0309, -0.0955,\n",
       "                      -0.0291, -0.1637, -0.0929,  0.0966, -0.0104, -0.1198, -0.0975, -0.0015,\n",
       "                      -0.0928, -0.1695,  0.0032,  0.0822, -0.0832, -0.0783, -0.0718,  0.1161,\n",
       "                       0.0974,  0.0407,  0.0424, -0.0515,  0.0434,  0.0198, -0.1894,  0.1932,\n",
       "                       0.0241,  0.0133,  0.0112, -0.0146,  0.0503,  0.0621,  0.0169,  0.0917,\n",
       "                       0.0721,  0.0752,  0.0427,  0.0176, -0.0267,  0.0631, -0.0360, -0.0134,\n",
       "                      -0.0551, -0.0543,  0.0083,  0.0789, -0.0024, -0.0358,  0.0833, -0.0390,\n",
       "                       0.0010,  0.0716,  0.0368, -0.0429,  0.1059,  0.0292, -0.1011, -0.0177,\n",
       "                      -0.0270, -0.0750,  0.0211,  0.0569, -0.0610, -0.1291,  0.0404,  0.0232,\n",
       "                       0.0473, -0.0073, -0.1219,  0.0723, -0.0665, -0.0275, -0.0868, -0.0934,\n",
       "                       0.0591, -0.0334, -0.0882, -0.1375, -0.0216, -0.0923,  0.0090, -0.0399,\n",
       "                       0.0353, -0.0087,  0.0019, -0.1416,  0.0507, -0.0171, -0.0247,  0.0791,\n",
       "                      -0.0697,  0.0280,  0.0521,  0.0475,  0.0010,  0.0350,  0.0155,  0.0132,\n",
       "                       0.0021, -0.0611,  0.0635,  0.0248, -0.1067,  0.0051,  0.0818,  0.0276,\n",
       "                      -0.1230,  0.0204,  0.0618, -0.0473,  0.0746, -0.0065,  0.0142, -0.0626,\n",
       "                      -0.0846, -0.0593,  0.0799, -0.0925, -0.0627,  0.1180, -0.0737,  0.0534,\n",
       "                       0.1524, -0.0623, -0.0123, -0.0435,  0.1087,  0.0521,  0.1545, -0.1563,\n",
       "                       0.0680,  0.0833, -0.0366, -0.0356,  0.0090,  0.0701,  0.0363,  0.0568,\n",
       "                      -0.0701,  0.0250,  0.0057,  0.0623, -0.0768,  0.0771, -0.0645,  0.0218,\n",
       "                      -0.0506,  0.0215, -0.0418, -0.1920, -0.1143, -0.0664, -0.0229, -0.0025,\n",
       "                      -0.2876, -0.1014,  0.0237, -0.0865,  0.1604,  0.1733, -0.2334, -0.0928,\n",
       "                      -0.0597, -0.0409, -0.0925, -0.0693, -0.2207, -0.1692,  0.0089, -0.2294,\n",
       "                      -0.0402, -0.1766, -0.0658, -0.2105, -0.0176, -0.0041,  0.0105, -0.0539,\n",
       "                      -0.1414, -0.0361, -0.0329, -0.0556,  0.0046, -0.0325, -0.0253, -0.1647,\n",
       "                      -0.0955, -0.2485, -0.2127, -0.0974,  0.0117, -0.1440,  0.0192, -0.0973,\n",
       "                      -0.1546, -0.0173, -0.1288, -0.1112, -0.0430, -0.0621,  0.0025, -0.1837,\n",
       "                      -0.0967, -0.0906, -0.1139, -0.0691, -0.1558, -0.0624, -0.2274, -0.1942,\n",
       "                      -0.0535, -0.0865, -0.1927, -0.0712, -0.0630, -0.0846, -0.1471, -0.0921,\n",
       "                       0.0151, -0.0624, -0.0656, -0.1531, -0.1159, -0.0467, -0.0624, -0.0797,\n",
       "                      -0.0317, -0.2241,  0.0572, -0.1195, -0.1241, -0.2473, -0.1416,  0.0005,\n",
       "                      -0.0703, -0.2212, -0.2789, -0.1407, -0.3024, -0.0437, -0.0854, -0.1821,\n",
       "                      -0.0415,  0.0845, -0.2186,  0.0959, -0.1639, -0.1420, -0.0095, -0.0032,\n",
       "                      -0.1201, -0.0521, -0.1137, -0.0373, -0.0349,  0.0834, -0.2566, -0.1484,\n",
       "                      -0.0601,  0.0106, -0.0199,  0.0712,  0.0329,  0.0065, -0.1428, -0.1751,\n",
       "                      -0.1699, -0.2753,  0.0107, -0.0942, -0.1925, -0.1354, -0.0738,  0.0695])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-2.2685e-02, -3.9847e-02, -4.5802e-02, -7.7291e-02, -1.4952e-01,\n",
       "                       5.9792e-02, -1.6142e-01, -1.1739e-01, -2.4109e-01, -1.6951e-01,\n",
       "                       7.2348e-03, -1.6658e-01, -2.6802e-01,  2.4235e-02, -1.0496e-01,\n",
       "                      -9.3382e-02, -7.1864e-02,  1.6036e-02,  4.3193e-02, -1.4035e-01,\n",
       "                      -8.1388e-02, -7.2831e-02, -7.7400e-02, -1.6353e-01, -2.7357e-02,\n",
       "                      -1.4826e-01, -1.5995e-01, -1.7125e-01, -1.8485e-01, -6.3145e-02,\n",
       "                       1.4581e-01, -1.8088e-01, -1.5199e-02, -2.5508e-01, -2.9968e-02,\n",
       "                       1.2909e-02, -1.8928e-01, -7.0206e-02, -1.4006e-01, -6.7092e-03,\n",
       "                      -1.8717e-01, -2.1433e-01, -1.1576e-01, -1.3384e-01, -2.4963e-02,\n",
       "                      -1.6115e-01, -7.7080e-02, -1.3876e-01, -1.9839e-01, -1.3383e-01,\n",
       "                      -2.1306e-01, -1.7574e-01,  4.2236e-02, -2.0987e-01, -1.6045e-01,\n",
       "                      -1.7580e-01, -4.1679e-02, -1.6612e-01, -1.8834e-01, -1.9399e-01,\n",
       "                      -3.1066e-02,  2.6608e-02, -1.8809e-01, -2.0276e-01,  1.8703e-02,\n",
       "                      -9.6682e-02,  3.3573e-02, -7.4803e-02, -1.0949e-01, -8.1898e-02,\n",
       "                      -6.8142e-02, -1.5990e-01, -1.9435e-02,  4.4886e-03,  9.4712e-02,\n",
       "                      -3.2867e-01, -9.3802e-02, -1.5052e-01, -1.7298e-01, -8.0279e-02,\n",
       "                      -5.0474e-02, -6.5138e-02, -1.1532e-01, -2.4504e-02, -1.7509e-01,\n",
       "                      -1.6622e-01, -4.3623e-02, -6.1101e-02, -1.2798e-01, -6.4683e-02,\n",
       "                      -8.5472e-02, -1.3191e-01, -7.0503e-02, -9.4538e-02, -2.1127e-01,\n",
       "                      -1.5279e-01, -1.9346e-01, -2.0749e-02, -2.2931e-01, -1.7262e-01,\n",
       "                      -1.5050e-01, -6.5946e-02, -1.4343e-01, -6.5858e-02,  3.7466e-02,\n",
       "                      -3.8763e-01, -1.6995e-01, -4.8057e-02, -1.5000e-01,  8.4724e-03,\n",
       "                      -6.3308e-02, -2.0279e-01, -2.0340e-01, -1.6669e-01, -6.0746e-02,\n",
       "                      -9.0337e-02, -1.8343e-01, -6.4624e-02, -1.0479e-01, -1.1963e-01,\n",
       "                      -2.6239e-01, -1.2242e-01, -1.1207e-01,  4.4861e-04, -1.9634e-01,\n",
       "                      -1.7114e-01,  1.1033e-01, -1.6133e-01, -5.0242e-02, -7.6428e-02,\n",
       "                      -3.4249e-02,  1.0042e-02, -1.3811e-01,  5.9096e-02, -6.8703e-02,\n",
       "                      -1.0504e-01, -8.8576e-02, -1.6715e-01,  9.7241e-02, -1.6430e-01,\n",
       "                      -5.4619e-02,  1.3168e-01, -1.3344e-01, -5.1913e-04, -4.4564e-02,\n",
       "                      -8.3465e-02, -7.0045e-02, -1.1123e-01, -1.5657e-01,  1.9739e-02,\n",
       "                      -4.0953e-02, -2.2090e-01, -1.0721e-01, -1.7455e-01, -8.9419e-02,\n",
       "                      -8.9498e-02, -1.0344e-01, -4.1955e-02, -7.0240e-02, -1.1401e-01,\n",
       "                       1.1561e-01, -6.2243e-03, -5.5984e-02,  8.1772e-02,  1.3574e-01,\n",
       "                       7.0467e-03, -4.7517e-02, -1.0412e-01, -6.8746e-02, -3.2671e-02,\n",
       "                      -1.3278e-01, -7.2889e-02,  1.2329e-01,  4.7393e-02,  2.0004e-01,\n",
       "                      -4.3466e-02, -1.8070e-01, -6.9374e-02, -3.9191e-02, -1.5885e-01,\n",
       "                      -2.0665e-01, -1.4512e-01,  2.4495e-03, -1.4225e-01, -3.5529e-02,\n",
       "                      -2.4281e-01,  1.1390e-02,  7.5209e-02, -1.4895e-01,  3.4030e-02,\n",
       "                       1.1990e-01, -5.5870e-02, -8.1531e-02, -1.5607e-01, -2.9711e-02,\n",
       "                      -1.1694e-01, -1.2286e-02, -4.4568e-02, -1.1381e-01,  1.1138e-01,\n",
       "                       4.8946e-02, -9.8791e-02,  1.3267e-02,  1.0227e-02,  6.1236e-03,\n",
       "                      -2.1390e-01, -6.4530e-02, -1.0336e-01, -1.7460e-01, -9.8594e-02,\n",
       "                      -1.6818e-01, -1.3518e-01, -1.9168e-01, -1.2394e-01, -6.7641e-02,\n",
       "                       3.6820e-02, -4.4374e-02, -2.3321e-03, -3.5917e-02,  9.6654e-02,\n",
       "                      -1.9368e-01,  4.3506e-05, -2.2504e-02, -1.4630e-02, -1.3133e-01,\n",
       "                       4.8457e-02, -1.8019e-01, -8.5061e-02,  2.4062e-02, -6.0824e-02,\n",
       "                       6.6996e-04, -1.0134e-02, -1.4328e-01, -8.1652e-02, -1.6676e-02,\n",
       "                      -8.1191e-02,  1.3817e-01,  1.6148e-01,  6.5023e-02, -8.3338e-02,\n",
       "                       8.6383e-03, -1.3146e-01, -1.1722e-01,  9.8742e-02,  7.6064e-02,\n",
       "                      -1.8260e-01, -2.5229e-03, -1.3260e-01, -2.2049e-02, -1.9654e-01,\n",
       "                      -6.1221e-02,  7.5004e-02, -1.7672e-01, -7.8000e-02, -3.9716e-02,\n",
       "                       1.3519e-03,  7.5888e-02, -2.2943e-02,  4.2914e-02,  2.6167e-03,\n",
       "                      -1.0967e-01, -6.0992e-02,  1.8580e-02,  1.1306e-01, -7.5667e-02,\n",
       "                      -1.1700e-02,  2.0900e-01, -1.6275e-01, -2.5614e-03,  7.1711e-02,\n",
       "                       3.4557e-02, -2.4868e-02,  4.6213e-02,  1.0457e-01,  4.2789e-02,\n",
       "                      -7.6305e-02, -1.9059e-02,  1.0915e-01, -3.8186e-02, -6.5534e-02,\n",
       "                      -1.9144e-02,  2.4865e-02,  3.9636e-02,  5.4706e-02, -1.4091e-02,\n",
       "                       4.8936e-03,  1.0437e-01,  9.8055e-02,  2.2869e-02, -5.5145e-02,\n",
       "                      -9.6940e-02, -4.0540e-02,  1.6422e-01,  4.7196e-02, -3.6598e-02,\n",
       "                      -5.6108e-02,  9.5597e-02,  4.4640e-02, -5.8737e-03, -8.8558e-03,\n",
       "                      -6.0235e-02, -1.1309e-01,  3.2252e-02, -1.2938e-01, -4.2397e-02,\n",
       "                       7.9841e-03,  1.3258e-01, -4.0184e-02, -9.4803e-02, -8.9293e-02,\n",
       "                      -1.6466e-02, -1.2869e-01, -4.1490e-02,  1.2370e-01, -4.3819e-02,\n",
       "                      -1.0259e-02,  1.2527e-02, -3.4683e-02,  1.0534e-01, -9.4809e-02,\n",
       "                      -1.8175e-02,  5.9083e-02,  8.5827e-02, -2.0014e-01,  1.3986e-02,\n",
       "                      -1.8130e-02,  7.6222e-02,  4.3090e-02,  8.3412e-02, -3.4704e-02,\n",
       "                      -1.0220e-02,  9.4254e-02,  8.0841e-02, -4.7928e-02, -5.5134e-02,\n",
       "                      -6.9694e-02, -5.9785e-02,  1.7434e-02,  1.0874e-01, -8.6420e-02,\n",
       "                      -1.6766e-02, -2.3895e-02,  4.4598e-02, -1.2925e-02, -1.8909e-02,\n",
       "                       1.3494e-02, -7.1043e-02,  1.2244e-01,  6.2062e-03,  6.0933e-03,\n",
       "                      -1.1559e-01,  2.2875e-02, -5.3772e-02,  4.5804e-02,  3.7387e-02,\n",
       "                      -7.9079e-02, -3.3331e-02,  1.8080e-01,  5.9592e-02,  4.0003e-03,\n",
       "                       6.0814e-02, -3.9375e-02,  1.1178e-01, -9.4746e-03,  4.9823e-02,\n",
       "                       8.1273e-02,  1.0689e-02, -1.8289e-01, -5.3361e-03,  7.8146e-02,\n",
       "                       7.0455e-02, -8.8838e-02,  9.5234e-02,  5.8576e-02,  3.7996e-02,\n",
       "                       9.9108e-02, -7.4342e-02, -2.6050e-02, -4.4143e-02,  1.8678e-02,\n",
       "                       1.4569e-02,  1.4916e-01, -4.6053e-02,  5.8654e-02, -7.7117e-02,\n",
       "                      -7.2148e-02, -2.1382e-02, -1.0464e-01, -2.1637e-01, -4.7950e-02,\n",
       "                       5.7158e-02, -5.5852e-02, -2.1471e-01, -2.0844e-01,  2.8335e-02,\n",
       "                      -1.3946e-01,  1.9676e-01,  1.0293e-01, -1.2989e-01, -9.0054e-02,\n",
       "                      -1.0150e-02, -5.7694e-02,  3.1445e-02, -1.1944e-01, -1.2683e-01,\n",
       "                      -6.4866e-02,  1.0335e-02, -1.6257e-01, -1.4777e-01, -1.7318e-01,\n",
       "                      -1.8536e-02, -1.6181e-01, -8.4986e-02, -5.9377e-02,  1.1019e-01,\n",
       "                      -1.8131e-01, -3.1439e-02, -7.7886e-02, -1.0831e-01, -7.1625e-03,\n",
       "                      -1.4879e-01,  1.0956e-02, -5.3747e-02,  4.9426e-03, -1.6723e-01,\n",
       "                      -1.9364e-01, -2.5680e-01, -1.0673e-01,  2.9287e-03, -1.6929e-01,\n",
       "                       1.2405e-01, -1.7931e-01, -2.5048e-01, -1.6749e-01, -5.2459e-02,\n",
       "                      -7.9391e-02, -1.0582e-02, -2.3764e-01,  3.5581e-02, -5.1246e-02,\n",
       "                      -1.2442e-01, -1.5904e-01, -1.4302e-02, -1.1948e-01, -9.6428e-02,\n",
       "                      -3.6852e-02, -1.4671e-01, -1.4376e-01, -4.8836e-02, -1.4764e-01,\n",
       "                       5.8031e-03, -1.3603e-01, -6.3694e-03, -6.2351e-02, -6.2337e-02,\n",
       "                      -1.1327e-01, -2.3866e-02,  1.8533e-02,  8.1420e-02, -1.5260e-01,\n",
       "                      -4.0875e-02, -1.6782e-01, -1.0971e-01, -7.5344e-02, -1.2437e-01,\n",
       "                      -1.1524e-01, -1.2744e-01, -1.8635e-01, -5.3830e-02, -1.8586e-01,\n",
       "                      -4.1868e-02, -1.7894e-01, -1.6303e-01, -2.3374e-01, -7.2197e-02,\n",
       "                      -1.1021e-01, -1.1422e-01, -6.5774e-02, -5.2639e-02, -1.8889e-01,\n",
       "                      -1.0511e-01,  6.2195e-02, -1.8197e-01, -3.2128e-02, -5.8161e-02,\n",
       "                      -1.1017e-01, -1.0401e-01, -1.9332e-01, -4.2506e-03, -8.2979e-02,\n",
       "                      -1.9669e-01, -1.0080e-01, -5.2615e-02, -3.7055e-02, -1.5597e-01,\n",
       "                      -1.6269e-01, -4.3964e-02, -1.3546e-01, -8.4910e-02,  2.6801e-02,\n",
       "                      -1.5016e-01, -5.7811e-03, -4.9149e-03, -9.5050e-02, -2.4519e-01,\n",
       "                      -1.1815e-01, -1.4147e-01, -1.8812e-01, -5.2396e-02,  3.3140e-02,\n",
       "                      -8.0911e-02, -1.2114e-01])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0793, -0.3287, -0.0446,  ..., -0.2212,  0.2572, -0.0068],\n",
       "                      [ 0.0981,  0.0592,  0.0695,  ...,  0.0605,  0.4990,  0.0167],\n",
       "                      [-0.1746,  0.0312,  0.6176,  ...,  0.0829, -0.4412,  0.3176],\n",
       "                      ...,\n",
       "                      [ 0.0771, -0.0168, -0.2700,  ...,  0.3136,  0.0825, -0.4030],\n",
       "                      [-0.0150,  0.3218,  0.3049,  ..., -0.3444,  0.1610,  0.0624],\n",
       "                      [-0.2889,  0.0761, -0.3955,  ..., -0.3344,  0.0816,  0.0869]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0464, -0.3748,  0.0418,  ...,  0.2291,  0.0348,  0.2659],\n",
       "                      [ 0.1525, -0.1556, -0.0548,  ...,  0.1942,  0.0980,  0.2504],\n",
       "                      [-0.2666, -0.1116,  0.0557,  ..., -0.0090,  0.0027,  0.0116],\n",
       "                      ...,\n",
       "                      [-0.2450,  0.2675,  0.2359,  ..., -0.3007,  0.1680, -0.3566],\n",
       "                      [ 0.0617, -0.2236, -0.0120,  ...,  0.1273,  0.0033, -0.1965],\n",
       "                      [ 0.0648,  0.0379, -0.1796,  ...,  0.1091,  0.1815, -0.0115]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 1.6291e-01,  1.6057e-01,  2.0475e-01,  1.9370e-02,  1.2883e-01,\n",
       "                       1.4048e-01,  1.6467e-01,  9.0583e-02,  7.0829e-02,  1.4472e-01,\n",
       "                       3.1279e-01,  1.2287e-01,  1.3603e-01,  1.4288e-01, -9.4242e-02,\n",
       "                       1.9434e-01,  4.7865e-02,  1.2731e-01, -5.6499e-03,  1.8530e-01,\n",
       "                       1.8439e-01,  1.4910e-01, -2.2356e-02,  8.9433e-02,  2.4940e-02,\n",
       "                       1.1277e-02, -5.4664e-02,  2.1807e-02,  2.2075e-01,  2.2502e-01,\n",
       "                       1.8956e-01,  2.1836e-01,  3.8227e-01,  9.5372e-02,  2.6569e-01,\n",
       "                       9.1728e-02,  8.0786e-02,  1.5917e-01,  7.8418e-02,  8.9692e-02,\n",
       "                      -6.1591e-02,  5.5391e-02,  2.3982e-01,  2.2059e-01,  1.6653e-01,\n",
       "                       1.0506e-01, -6.8576e-03,  5.0433e-02,  9.8093e-02,  5.4911e-02,\n",
       "                       1.1200e-01,  9.0919e-02,  3.1023e-02, -2.5350e-02, -1.0712e-03,\n",
       "                       1.6815e-01,  1.2635e-01,  1.6750e-02,  1.7214e-01,  1.2942e-01,\n",
       "                       1.6810e-01,  1.5423e-01,  1.1758e-01,  7.8749e-02,  2.0918e-01,\n",
       "                       2.7329e-01,  1.4682e-01,  3.0412e-01,  1.9263e-01,  1.9220e-01,\n",
       "                      -6.9499e-02,  1.5891e-01,  4.5467e-02, -5.5998e-02,  8.5799e-02,\n",
       "                       3.2900e-02,  2.4362e-01,  2.7295e-01,  2.0292e-02,  1.4060e-01,\n",
       "                       1.5155e-01,  8.4632e-02,  1.6984e-01,  2.4143e-01,  1.5374e-01,\n",
       "                       7.7254e-02,  2.7202e-01,  1.0027e-01,  9.4620e-02,  2.7334e-01,\n",
       "                       1.7561e-01, -3.3728e-02,  1.3914e-01, -2.1171e-02,  2.2447e-01,\n",
       "                      -1.8655e-02,  1.9990e-02,  7.8500e-02,  2.6612e-01,  1.6445e-01,\n",
       "                       8.3348e-02,  1.0530e-01,  8.9356e-02,  1.7700e-01,  1.4618e-02,\n",
       "                       1.3027e-01,  2.2575e-01,  4.8081e-02,  1.4705e-01, -6.7886e-03,\n",
       "                       2.8254e-01,  1.1183e-01,  3.8947e-02,  2.5006e-01,  1.8806e-01,\n",
       "                       1.7043e-01,  2.1098e-01,  1.1208e-01,  4.6362e-03,  2.5477e-01,\n",
       "                       2.7554e-01,  1.7844e-01,  7.8772e-02,  1.5111e-01,  1.7828e-01,\n",
       "                      -1.2865e-01, -1.7626e-02,  3.7335e-01,  1.3390e-02,  1.7741e-02,\n",
       "                      -7.7888e-02, -9.1776e-02,  2.3236e-01, -3.1586e-02,  1.7524e-01,\n",
       "                       5.6269e-02,  7.1784e-02,  7.7150e-02,  1.2681e-01,  7.6077e-02,\n",
       "                       6.0187e-02,  7.0543e-02,  4.1727e-02, -5.4611e-02, -3.8627e-02,\n",
       "                       9.0816e-02,  1.5013e-01,  1.2724e-01,  2.1248e-01, -7.1779e-03,\n",
       "                       8.9554e-02, -6.6129e-02,  5.9967e-02, -1.6718e-01, -6.1506e-02,\n",
       "                       1.3243e-01,  9.5314e-02, -4.0741e-03, -8.9849e-02,  4.8581e-02,\n",
       "                      -1.9986e-02, -1.0129e-02,  4.8377e-02,  1.5158e-01,  2.1113e-02,\n",
       "                       1.7372e-02,  5.6366e-03,  5.7044e-02,  7.9589e-02,  3.0675e-02,\n",
       "                       8.6579e-02,  1.2110e-01, -3.8013e-03, -3.1180e-02, -1.9650e-02,\n",
       "                      -8.1268e-02, -9.6766e-02,  2.9660e-02, -2.6452e-02, -1.2287e-01,\n",
       "                       1.0743e-01,  1.1816e-01,  8.7651e-02,  1.0035e-01, -7.5000e-02,\n",
       "                       6.2491e-02,  1.0595e-01,  3.3304e-02,  1.0506e-01, -3.3468e-02,\n",
       "                       5.3796e-02,  4.6382e-02, -9.4281e-02, -7.8076e-02,  1.2007e-01,\n",
       "                       4.4586e-02,  1.0022e-01,  4.1418e-02,  1.9012e-01,  1.8415e-02,\n",
       "                       4.4874e-02,  6.1927e-02, -1.0932e-01,  3.6356e-03,  5.1777e-02,\n",
       "                       8.6470e-02,  2.3854e-02,  4.5598e-02,  1.2974e-01, -2.8094e-02,\n",
       "                       1.0054e-01, -6.2922e-02, -3.0536e-02,  6.4825e-02,  1.3135e-01,\n",
       "                       6.0806e-02,  4.5694e-02, -1.6750e-01, -1.9826e-02,  1.2983e-02,\n",
       "                       4.0979e-02,  4.7087e-02,  1.3261e-01,  1.0624e-02,  3.1267e-02,\n",
       "                       5.2137e-02,  8.1240e-02, -9.8947e-02,  1.1528e-01,  2.6782e-02,\n",
       "                       9.0102e-02, -2.8258e-02,  1.0111e-01,  2.3594e-02,  8.2501e-04,\n",
       "                       2.6820e-02,  6.4870e-04, -6.0199e-02,  8.7062e-02,  1.8337e-01,\n",
       "                      -3.2649e-02,  3.1046e-02,  6.1762e-02,  1.0241e-01, -5.4690e-02,\n",
       "                       1.3687e-01,  2.5365e-02,  6.7890e-02,  7.7145e-02,  3.8872e-02,\n",
       "                       3.9697e-02,  6.0026e-02,  3.1019e-02,  9.1064e-02, -5.0615e-02,\n",
       "                       1.1547e-01, -4.2666e-02,  4.3931e-02, -2.1907e-02,  1.0824e-01,\n",
       "                       9.5822e-02,  1.3011e-01,  3.7688e-04,  1.2107e-01, -6.1793e-02,\n",
       "                      -1.1451e-02, -3.4518e-02,  7.0040e-02, -3.2593e-02, -2.4398e-01,\n",
       "                      -9.6296e-02, -1.6415e-01, -3.9075e-02,  5.6072e-02,  6.2901e-03,\n",
       "                       3.6113e-02, -1.9664e-02, -7.3167e-02, -5.0845e-02,  1.8801e-02,\n",
       "                       1.4854e-02,  8.0262e-02,  3.1037e-02,  2.4873e-02, -6.0110e-02,\n",
       "                       1.5669e-02,  4.5789e-02,  5.8831e-02, -1.4815e-02,  5.8106e-02,\n",
       "                       2.4430e-02,  1.3224e-01, -6.6073e-02, -2.5764e-02,  1.8287e-02,\n",
       "                       2.9079e-03, -5.4807e-02,  5.8905e-02,  2.2287e-02,  4.0763e-02,\n",
       "                       1.6503e-01, -3.6047e-02,  1.2699e-02, -7.9949e-02, -4.1788e-02,\n",
       "                      -1.9783e-02, -1.9181e-02,  2.2783e-02,  9.6866e-02, -5.2017e-02,\n",
       "                       6.5851e-03, -1.2494e-01, -4.4049e-03,  1.0028e-01, -5.1780e-02,\n",
       "                      -1.7191e-01,  5.7059e-02, -8.4439e-02, -9.6912e-02,  2.4041e-02,\n",
       "                      -8.8865e-02,  6.5992e-03,  6.9587e-02, -1.8527e-02,  6.4551e-02,\n",
       "                       1.5578e-01,  1.4283e-02,  4.1824e-04,  1.3455e-01, -1.4352e-01,\n",
       "                       4.7835e-02,  2.9572e-02, -1.1444e-01,  5.4305e-02, -6.6879e-02,\n",
       "                       1.7383e-01, -5.2362e-02, -1.7530e-03,  6.4708e-02, -5.1526e-02,\n",
       "                      -8.5214e-02, -7.9637e-02, -3.7952e-02, -9.7358e-03, -1.9780e-02,\n",
       "                       1.1105e-01, -9.1485e-02,  5.7728e-02,  7.9215e-02, -9.4156e-02,\n",
       "                       1.4957e-02,  7.0115e-02, -6.2060e-02, -3.4596e-02, -1.7136e-02,\n",
       "                       1.4290e-01,  8.4692e-02,  3.2129e-02, -4.1222e-02, -1.6921e-01,\n",
       "                       9.2319e-02,  1.9032e-02,  4.1965e-02, -3.5480e-02,  8.0474e-02,\n",
       "                       2.5119e-02,  3.1206e-02,  9.8158e-02, -3.4022e-02, -3.3863e-03,\n",
       "                       1.3065e-02,  9.7951e-02, -3.8648e-02, -1.8653e-03, -1.2151e-02,\n",
       "                       3.0942e-03, -3.8803e-02,  1.2104e-01, -8.3731e-02, -2.7485e-02,\n",
       "                      -8.5804e-04,  5.8249e-02,  1.4077e-02, -3.0809e-02,  4.3482e-01,\n",
       "                       9.9365e-02, -1.4958e-02,  6.4572e-02,  9.7640e-02,  2.2273e-01,\n",
       "                       1.7715e-01,  2.0818e-01,  4.2077e-03,  2.5599e-01, -2.6662e-02,\n",
       "                       1.5008e-01,  2.0892e-01, -1.3442e-01,  7.6829e-02,  1.6828e-01,\n",
       "                       1.9077e-01,  1.3934e-01,  1.9321e-02,  1.4570e-01,  3.2918e-01,\n",
       "                       1.3643e-01,  1.3979e-01,  1.7188e-01,  6.9211e-02, -1.4096e-02,\n",
       "                       1.1667e-01,  1.5634e-01,  2.9555e-01,  3.3887e-01,  4.0059e-02,\n",
       "                       1.4928e-01,  3.2785e-01, -2.4981e-02,  8.4445e-02,  1.0058e-01,\n",
       "                       2.7741e-01,  6.6608e-02, -4.7920e-03,  8.6088e-02, -5.6254e-02,\n",
       "                      -4.6031e-02,  2.1832e-01,  1.2739e-01,  2.6299e-01,  1.6276e-01,\n",
       "                      -1.6410e-02, -3.5906e-02,  2.2374e-01,  5.6254e-02,  6.3456e-02,\n",
       "                       4.8106e-02,  4.0956e-02,  1.0074e-01, -1.7069e-03,  1.6321e-01,\n",
       "                       1.5196e-01,  2.1111e-01,  5.0891e-01,  1.9203e-01,  2.6242e-01,\n",
       "                       1.7597e-01,  1.7462e-01,  2.5963e-01,  7.1216e-02,  2.6649e-01,\n",
       "                       8.2265e-02,  2.6366e-01,  1.4823e-01,  1.6515e-01,  2.7722e-01,\n",
       "                       9.7098e-02,  1.1496e-01, -2.4807e-02,  1.4118e-01,  1.0398e-01,\n",
       "                       3.4595e-01,  3.0494e-01,  9.1120e-02,  2.6690e-02,  7.9880e-02,\n",
       "                       1.8362e-01, -2.8035e-02, -5.7676e-02,  8.6372e-02,  2.2008e-01,\n",
       "                       4.1027e-02,  1.1616e-01,  1.2114e-01,  2.3767e-01,  1.7862e-01,\n",
       "                       5.2846e-02,  2.5585e-01,  1.3236e-01,  2.6250e-01,  7.7496e-02,\n",
       "                       1.4448e-01,  1.8079e-01,  2.1153e-01,  1.1488e-01,  1.5041e-01,\n",
       "                       1.2403e-01,  5.6777e-02,  1.5277e-01,  2.8626e-03,  1.6528e-02,\n",
       "                       2.9130e-01,  2.8772e-01,  1.5071e-01,  5.0807e-02,  2.0595e-01,\n",
       "                      -4.8356e-02, -5.2865e-02, -5.0210e-02,  6.4912e-03,  9.2223e-02,\n",
       "                       1.5544e-01,  1.6274e-01,  5.2976e-02,  1.8394e-01,  2.5143e-01,\n",
       "                       1.7870e-01,  1.6426e-02,  2.7035e-01,  1.0950e-01, -1.9723e-02,\n",
       "                      -9.8404e-02,  1.3188e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 5.3470e-02,  1.1718e-01,  2.8668e-02,  7.3104e-02,  2.1004e-02,\n",
       "                       2.0750e-01,  1.9966e-01,  1.7441e-01, -5.6201e-02,  1.8957e-01,\n",
       "                       3.4457e-01,  1.3687e-01,  7.3227e-02, -1.2908e-02, -1.2582e-01,\n",
       "                       6.2410e-03,  3.8920e-02,  2.3207e-01,  9.0009e-02,  5.6765e-02,\n",
       "                       2.7187e-01,  2.0022e-01, -1.9470e-02,  1.3788e-01,  9.8540e-02,\n",
       "                       4.5300e-02,  9.8921e-03,  7.9715e-02,  7.4818e-02,  2.1364e-01,\n",
       "                       1.6985e-01,  1.3692e-01,  2.3660e-01, -1.6302e-02,  2.6562e-01,\n",
       "                       1.3668e-01,  3.9758e-02,  1.6925e-01, -3.2132e-02,  1.1888e-01,\n",
       "                      -1.2496e-01, -5.1269e-02,  2.9449e-01,  1.8933e-01,  2.0092e-01,\n",
       "                       8.1587e-02,  1.0692e-01,  2.9093e-02,  1.4679e-01, -1.4815e-01,\n",
       "                       1.0168e-01,  7.1798e-02,  1.5274e-02,  2.3291e-01,  1.4526e-01,\n",
       "                       2.4313e-01,  3.8442e-02,  9.8555e-02,  9.0967e-02,  2.2437e-01,\n",
       "                       3.8364e-02,  1.0498e-01,  1.5304e-01,  8.1045e-02,  2.8622e-01,\n",
       "                       2.2727e-01,  2.7020e-01,  2.1711e-01, -6.9868e-03,  1.4160e-01,\n",
       "                       1.3726e-02,  2.0611e-01,  1.0372e-01,  5.3146e-03,  7.7109e-02,\n",
       "                       1.9566e-02,  3.9545e-01,  2.3228e-01, -5.0655e-02,  2.1144e-01,\n",
       "                       4.1472e-02,  1.1138e-01,  1.0914e-01,  1.4222e-01,  1.0961e-01,\n",
       "                       3.7880e-02,  1.9918e-01, -3.1030e-02,  1.5955e-01,  2.2800e-01,\n",
       "                       9.3167e-02,  5.5214e-02,  5.5797e-02,  1.6022e-02,  1.4219e-01,\n",
       "                      -2.0094e-02, -6.5521e-02, -5.8329e-03,  3.2268e-01,  7.7064e-02,\n",
       "                       9.3214e-03,  1.6874e-01,  9.4976e-02,  1.2278e-01, -4.3911e-02,\n",
       "                       1.6594e-01,  1.5454e-01,  1.5255e-01,  3.8679e-02,  1.3495e-02,\n",
       "                       2.7119e-01,  4.0444e-02, -1.0085e-01,  3.2933e-01,  2.3280e-01,\n",
       "                       1.2712e-01,  2.0796e-01,  2.1414e-01,  9.8733e-02,  6.1632e-02,\n",
       "                       1.3452e-01,  2.6292e-01,  9.9211e-02,  1.9151e-01,  1.3435e-01,\n",
       "                      -7.5927e-02,  3.2993e-02,  3.0364e-01, -2.0136e-02,  1.5027e-01,\n",
       "                      -6.9693e-03, -4.7813e-02,  1.2997e-01, -3.3419e-02,  1.3746e-01,\n",
       "                       1.3309e-01, -2.3549e-02,  4.5195e-02,  1.6294e-01,  4.7152e-02,\n",
       "                      -2.0483e-02,  1.1440e-01,  1.2590e-01, -9.6093e-02,  9.6278e-02,\n",
       "                      -5.5200e-03,  7.8715e-02,  5.4091e-02,  1.8107e-01, -4.2025e-02,\n",
       "                       1.5392e-01, -8.0090e-02,  8.1265e-02,  1.6690e-02,  8.8906e-02,\n",
       "                       7.8187e-03,  8.8035e-02,  1.2181e-01, -4.3304e-02, -4.6675e-02,\n",
       "                       1.6138e-01,  9.8094e-03, -2.7357e-02,  7.0127e-02, -7.6814e-02,\n",
       "                       5.7763e-02,  1.3668e-01,  1.1123e-01,  8.6312e-02, -4.4168e-02,\n",
       "                       1.2684e-01,  8.8767e-02, -1.0110e-01, -2.0702e-02,  4.1153e-02,\n",
       "                      -4.4597e-02,  4.6540e-02,  8.4943e-02, -5.4319e-02,  1.7323e-02,\n",
       "                       1.6816e-01,  4.3239e-02,  7.8040e-02, -9.0604e-02,  5.3314e-02,\n",
       "                       1.6272e-01, -6.4344e-02,  9.6203e-02,  1.1443e-01, -1.4458e-03,\n",
       "                      -1.0978e-01,  3.6712e-02,  4.3394e-02, -7.2933e-03, -7.9344e-03,\n",
       "                       6.0404e-02,  1.3569e-01,  8.4919e-02,  8.9810e-02, -8.9255e-02,\n",
       "                       9.1578e-02,  7.6403e-02, -2.5030e-03,  3.3142e-02,  1.3372e-01,\n",
       "                      -4.8977e-02,  8.9174e-02,  8.3302e-02,  1.4858e-01, -1.6306e-01,\n",
       "                       1.3663e-01, -7.4166e-02, -1.0915e-02,  2.2052e-02, -5.6708e-02,\n",
       "                       1.0514e-01,  6.5863e-02, -4.8535e-02, -6.3565e-02, -2.3874e-02,\n",
       "                       1.6747e-01,  3.4234e-02,  2.0480e-01,  1.2155e-01,  2.2343e-02,\n",
       "                      -5.5565e-02,  1.1784e-01, -1.6293e-01,  4.2560e-02,  1.4927e-02,\n",
       "                      -5.8724e-02,  1.3392e-01,  8.3913e-02,  1.1559e-02,  5.9632e-02,\n",
       "                       2.3041e-02, -6.8417e-03,  3.5131e-02,  2.8579e-02,  4.4536e-02,\n",
       "                       2.2322e-02,  1.0687e-02,  6.1937e-02, -1.0326e-01,  6.6899e-02,\n",
       "                       7.3955e-02,  7.7078e-02,  1.4286e-01,  6.9113e-02, -4.0544e-02,\n",
       "                       1.1884e-01,  1.6046e-01,  1.0452e-01,  1.5290e-01,  1.5687e-01,\n",
       "                       1.9146e-01, -9.9443e-03, -4.9139e-02,  1.2547e-01,  1.7572e-02,\n",
       "                       5.7756e-02, -4.0195e-02,  4.3458e-02, -5.4649e-02,  1.8671e-02,\n",
       "                       1.0728e-01, -4.1402e-02,  1.0827e-01, -8.9912e-02,  6.4707e-02,\n",
       "                       3.3228e-02, -5.5547e-02, -4.7888e-02,  9.1645e-02,  5.2624e-02,\n",
       "                       5.0886e-03, -1.1088e-01,  4.1857e-02, -2.0246e-02,  7.6646e-04,\n",
       "                       2.8400e-03,  1.1433e-01, -4.9234e-02, -4.0818e-02,  2.4322e-04,\n",
       "                       4.7925e-03, -6.3438e-02,  8.2440e-02, -2.9180e-02, -4.1834e-02,\n",
       "                       5.3039e-02,  6.2986e-02, -1.7836e-02,  5.7044e-02, -7.1590e-02,\n",
       "                      -1.1054e-01,  1.3133e-01,  4.3668e-02, -9.8406e-02,  7.0714e-02,\n",
       "                       1.0475e-01, -4.1118e-02,  5.8915e-02, -1.6256e-02,  4.6658e-03,\n",
       "                       2.0137e-02, -1.2183e-01,  7.2617e-02, -3.5526e-02,  7.9438e-03,\n",
       "                       5.3937e-02,  9.6527e-02, -1.2439e-01,  1.0532e-01, -1.6045e-01,\n",
       "                      -3.0957e-02, -1.7483e-02,  9.9420e-02, -3.3913e-02, -1.0093e-01,\n",
       "                      -5.1586e-02,  5.1246e-02, -5.2004e-02,  5.1378e-02, -1.2205e-02,\n",
       "                       5.1675e-02, -7.8757e-02,  4.2560e-02,  5.3529e-02, -9.1793e-02,\n",
       "                      -3.4409e-02,  1.7965e-01, -5.7572e-02, -6.3669e-03,  3.2622e-02,\n",
       "                      -3.8466e-02, -6.1012e-02,  1.6643e-02,  1.5026e-01,  3.6699e-02,\n",
       "                      -3.3149e-02,  1.4743e-01, -1.3381e-02,  1.0389e-01, -5.5398e-02,\n",
       "                       1.9355e-02, -1.4118e-01, -2.2104e-02, -1.2894e-01, -2.1310e-02,\n",
       "                      -2.2199e-02,  2.2643e-02,  6.8265e-02, -3.6223e-03, -6.4744e-03,\n",
       "                       8.7897e-02,  4.8861e-02,  8.3658e-02, -6.8625e-02, -6.7788e-02,\n",
       "                       8.2787e-02, -2.1412e-02,  5.5054e-02,  1.2470e-02, -2.8030e-02,\n",
       "                       1.6592e-01, -2.1327e-02, -5.2573e-04,  8.4684e-02,  5.6869e-02,\n",
       "                      -1.2441e-01, -8.5159e-02, -3.4170e-02,  3.5150e-02, -1.2549e-01,\n",
       "                       1.1234e-04, -1.1485e-01,  2.9705e-03, -8.0643e-02,  1.6639e-02,\n",
       "                      -1.0593e-01,  2.5339e-02,  5.6876e-02, -9.5714e-03,  2.8853e-01,\n",
       "                       2.0118e-04,  1.6182e-02,  1.7637e-01,  1.2575e-01,  2.1793e-01,\n",
       "                       1.2528e-01,  1.2360e-01,  3.6963e-02,  2.1403e-01, -3.2197e-02,\n",
       "                       1.2772e-01,  2.6736e-01, -5.5483e-03,  2.8174e-02,  5.3887e-02,\n",
       "                       2.1418e-01,  1.0395e-01,  2.7990e-02,  6.6873e-02,  3.4781e-01,\n",
       "                       8.7800e-02,  1.6682e-01,  6.2824e-02,  2.2064e-01,  3.7960e-03,\n",
       "                       1.5146e-01, -3.6558e-02,  6.3481e-02,  7.9714e-02, -3.3351e-02,\n",
       "                       1.4434e-01,  3.1067e-01, -1.5889e-02,  8.8764e-02,  9.0953e-02,\n",
       "                       2.6726e-01, -1.2468e-02, -8.3596e-03, -3.6484e-02, -1.2512e-01,\n",
       "                      -1.3074e-01,  1.1071e-01, -3.6963e-02,  2.9862e-01,  1.2320e-01,\n",
       "                      -2.4224e-02,  2.1083e-02,  2.4033e-01,  2.9424e-02,  1.8887e-01,\n",
       "                       9.0736e-02, -1.3981e-02,  3.6778e-03,  5.7935e-02,  2.1452e-01,\n",
       "                       2.1419e-01,  1.7355e-01,  3.8058e-01,  1.1516e-01,  1.5216e-01,\n",
       "                       1.4170e-01,  1.7643e-01,  2.3110e-01,  6.2332e-02,  3.5004e-01,\n",
       "                       6.8674e-02,  1.4994e-01,  5.6591e-02,  2.3223e-01,  2.0986e-01,\n",
       "                       3.9938e-02,  2.6811e-03, -5.4710e-02,  1.1810e-01,  3.0938e-01,\n",
       "                       3.9302e-01,  2.2007e-01,  1.5802e-01, -2.5607e-03,  1.8591e-01,\n",
       "                       2.6730e-01,  2.1850e-02, -3.6813e-02,  1.0576e-01,  2.4963e-01,\n",
       "                       6.8241e-02,  2.4250e-01,  1.5099e-01,  2.6445e-01,  3.2086e-01,\n",
       "                       1.4727e-02,  2.9538e-01,  3.0457e-02,  1.7069e-01,  3.1394e-02,\n",
       "                       1.8812e-01,  1.0564e-01,  1.3484e-01,  5.2710e-02,  9.2844e-02,\n",
       "                      -7.2728e-02,  1.5419e-01,  1.3482e-01, -3.9124e-02,  1.4861e-01,\n",
       "                       3.6677e-01,  2.3365e-01,  1.5292e-01, -4.1717e-02,  8.0146e-02,\n",
       "                       5.6028e-02, -8.0819e-02, -1.2469e-01,  8.9462e-02,  1.1329e-01,\n",
       "                       2.6096e-01,  2.9055e-01, -5.9882e-02,  1.9714e-01,  1.3541e-01,\n",
       "                       3.1065e-01,  6.5180e-02,  8.2272e-02,  1.2766e-01, -6.6329e-02,\n",
       "                      -1.4408e-01,  1.9606e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0609, -0.4507,  0.1150,  ..., -0.5287,  0.2154,  0.2402],\n",
       "                      [ 0.4446, -0.5148, -0.1590,  ..., -0.5978,  0.0592, -0.1434],\n",
       "                      [ 0.1324,  0.0604,  0.2242,  ..., -0.4705, -0.0671, -0.2734],\n",
       "                      ...,\n",
       "                      [ 0.2680, -0.0985,  0.1937,  ..., -0.2134,  0.3565,  0.1412],\n",
       "                      [ 0.2740,  0.1000,  0.1566,  ..., -0.2160,  0.2388, -0.2535],\n",
       "                      [ 0.2161, -0.2401, -0.1565,  ..., -0.0930,  0.1418, -0.2347]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0759, -0.0810, -0.0097,  ..., -0.2214,  0.1340, -0.1524],\n",
       "                      [ 0.0491, -0.2153,  0.1028,  ..., -0.2565,  0.2963,  0.0865],\n",
       "                      [-0.0903,  0.0651, -0.1262,  ..., -0.0695, -0.0471, -0.2025],\n",
       "                      ...,\n",
       "                      [-0.1254, -0.0828, -0.0684,  ...,  0.1194, -0.0124, -0.1864],\n",
       "                      [-0.1935,  0.1091,  0.0523,  ..., -0.0172,  0.0603, -0.0136],\n",
       "                      [ 0.0408, -0.1701, -0.0183,  ..., -0.1196,  0.0828,  0.0357]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([-0.1007, -0.1274, -0.1388, -0.2076, -0.2457, -0.2552, -0.1654, -0.1829,\n",
       "                      -0.3071, -0.0502, -0.1745, -0.1976, -0.2372, -0.1798, -0.3091, -0.0534,\n",
       "                      -0.2129, -0.3216, -0.0522, -0.2241, -0.1540, -0.3279, -0.1981, -0.1385,\n",
       "                      -0.1068, -0.1269, -0.2562, -0.1843, -0.1623, -0.1374, -0.2429, -0.2686,\n",
       "                      -0.1936, -0.1559, -0.0773, -0.0584, -0.0819, -0.1814, -0.1017, -0.1203,\n",
       "                      -0.2967, -0.2453, -0.2242, -0.0840, -0.2756, -0.2803, -0.1095, -0.0772,\n",
       "                      -0.2460, -0.2990, -0.0797, -0.1060, -0.0863, -0.2310, -0.2430, -0.1719,\n",
       "                      -0.2072, -0.0992, -0.2625, -0.1107, -0.1980, -0.2565, -0.1856, -0.1998,\n",
       "                       0.0023, -0.1138, -0.1598, -0.1581, -0.1320, -0.2480, -0.1384, -0.1893,\n",
       "                      -0.1254, -0.1346, -0.2128, -0.2936, -0.0926, -0.1229, -0.1248, -0.2542,\n",
       "                      -0.2086, -0.0938, -0.2388, -0.0615, -0.2913, -0.0929, -0.2971, -0.2145,\n",
       "                      -0.2193, -0.2462, -0.1829, -0.1974, -0.2392, -0.2217, -0.2493, -0.1184,\n",
       "                      -0.0222, -0.1461,  0.0400, -0.1619, -0.2131, -0.1166, -0.1193, -0.1618,\n",
       "                      -0.0914, -0.1427, -0.1559, -0.1145, -0.2747, -0.1150, -0.1785, -0.1680,\n",
       "                      -0.1392, -0.0161, -0.1535, -0.2158, -0.1878, -0.0681, -0.1514, -0.1713,\n",
       "                      -0.0637, -0.1334, -0.2898, -0.1546, -0.1624, -0.3043, -0.1917, -0.1343,\n",
       "                      -0.1418, -0.2919, -0.2639, -0.0276, -0.2267, -0.1139,  0.0094, -0.1118,\n",
       "                      -0.1318, -0.0879, -0.1654, -0.1359, -0.0970, -0.0880, -0.1977, -0.0924,\n",
       "                      -0.1407, -0.0461, -0.0843, -0.1670, -0.0938, -0.2278, -0.0938, -0.2478,\n",
       "                      -0.1910, -0.1015, -0.1682, -0.2530, -0.2768, -0.1615, -0.1356, -0.1232,\n",
       "                      -0.1498, -0.0851, -0.1564, -0.1063, -0.1612, -0.1564, -0.1367,  0.0316,\n",
       "                      -0.0516, -0.0029, -0.1923, -0.0179, -0.1494, -0.1456, -0.2148, -0.1858,\n",
       "                      -0.1716, -0.0504, -0.1442, -0.0904, -0.1514, -0.1147, -0.0084, -0.0859,\n",
       "                      -0.1363, -0.2479, -0.1325, -0.1922, -0.1060, -0.1724, -0.0512, -0.1265,\n",
       "                      -0.0138, -0.0994, -0.1531, -0.1563, -0.0441, -0.0546, -0.1549, -0.1279,\n",
       "                      -0.0767, -0.1398, -0.2109, -0.0883, -0.2524, -0.0665, -0.1350, -0.0555,\n",
       "                      -0.1220, -0.1294, -0.1825, -0.1932, -0.0339, -0.1479, -0.1591, -0.1433,\n",
       "                      -0.0926, -0.1282, -0.1748, -0.1066, -0.2117, -0.1599, -0.1652, -0.1496,\n",
       "                      -0.1664, -0.0141, -0.1080, -0.0703, -0.2194, -0.1157, -0.1014, -0.2201,\n",
       "                      -0.3080, -0.1068, -0.1074, -0.1260, -0.1060, -0.2022, -0.1884, -0.1100,\n",
       "                      -0.2542,  0.0030, -0.1243, -0.1290, -0.1259, -0.1498, -0.1446, -0.0368,\n",
       "                       0.0623, -0.1708, -0.1802, -0.2109, -0.1028, -0.1992, -0.2996, -0.0455,\n",
       "                      -0.0785, -0.0119, -0.0452,  0.1209, -0.0416, -0.0810,  0.0322, -0.0113,\n",
       "                      -0.1054,  0.0076, -0.0243, -0.0269,  0.0297,  0.0143, -0.0497, -0.1231,\n",
       "                      -0.0382,  0.0007,  0.0342,  0.0332,  0.0713, -0.0947, -0.0670, -0.0498,\n",
       "                       0.0599,  0.0560, -0.0449,  0.0451, -0.0611, -0.1331,  0.0712, -0.0084,\n",
       "                      -0.0291, -0.0311, -0.0535, -0.0160,  0.0427,  0.0186, -0.0586, -0.0342,\n",
       "                       0.0116,  0.0355,  0.0079, -0.0201, -0.0301,  0.0755, -0.0141,  0.0325,\n",
       "                       0.0332,  0.0334, -0.0445, -0.0353, -0.0764, -0.0784,  0.0652,  0.0717,\n",
       "                       0.0217,  0.0905, -0.0214, -0.0695,  0.1424, -0.0981,  0.0305,  0.0830,\n",
       "                       0.0380,  0.0146, -0.0148, -0.0372,  0.0311, -0.0786,  0.0304, -0.0262,\n",
       "                       0.0869,  0.0728,  0.1130, -0.0230,  0.0561, -0.0980, -0.0732, -0.0406,\n",
       "                       0.0401,  0.0473,  0.0806,  0.0236,  0.0321,  0.0156,  0.0136,  0.0062,\n",
       "                      -0.0988, -0.0716,  0.1142,  0.0219, -0.0225,  0.0686,  0.0029, -0.0289,\n",
       "                      -0.0294, -0.0230,  0.0620, -0.0513, -0.0963,  0.0433, -0.0777, -0.0415,\n",
       "                       0.0645,  0.0201, -0.0077,  0.0168, -0.0837,  0.0790, -0.0450, -0.0490,\n",
       "                       0.0323, -0.0293,  0.0470, -0.0399, -0.0624, -0.0146,  0.0157,  0.0497,\n",
       "                       0.1153,  0.0051, -0.0562,  0.0671, -0.0617,  0.0056, -0.0381,  0.0398,\n",
       "                      -0.1796, -0.0543, -0.2578, -0.2404, -0.2845, -0.1821, -0.2168, -0.1875,\n",
       "                      -0.2356, -0.1633, -0.2298, -0.1663, -0.1553, -0.1203, -0.2389, -0.0526,\n",
       "                      -0.2715, -0.1250, -0.1533, -0.2000, -0.1166, -0.1493, -0.2539, -0.2968,\n",
       "                      -0.0936, -0.1640, -0.2510, -0.0939, -0.2430, -0.1695, -0.2822, -0.1956,\n",
       "                      -0.1856, -0.1482, -0.2602, -0.0091, -0.1089, -0.1813, -0.2360, -0.1552,\n",
       "                      -0.3104, -0.1704, -0.2804, -0.0903, -0.2504, -0.1533, -0.1857, -0.0990,\n",
       "                      -0.1959, -0.1358, -0.1564, -0.0289, -0.1088, -0.1233, -0.1962, -0.2164,\n",
       "                      -0.2714, -0.1499, -0.1229, -0.2557, -0.2502, -0.2277, -0.1735, -0.1577,\n",
       "                      -0.1660, -0.2750, -0.2096, -0.2322, -0.1833, -0.0868, -0.0315, -0.2257,\n",
       "                      -0.1040, -0.0977, -0.1586, -0.1386, -0.1437, -0.0340, -0.1283, -0.1287,\n",
       "                      -0.1400, -0.1930, -0.2893, -0.0329, -0.1385, -0.0331, -0.1914, -0.2645,\n",
       "                      -0.0859, -0.1865, -0.1438, -0.1427, -0.3267, -0.1829, -0.3356, -0.1896,\n",
       "                      -0.2033, -0.0945, -0.0895, -0.0402, -0.1952, -0.1084, -0.1961, -0.0765,\n",
       "                      -0.1417, -0.2086, -0.1820, -0.1250, -0.1276, -0.1995, -0.1206, -0.1362,\n",
       "                      -0.2057, -0.0723, -0.2978, -0.1390, -0.1568, -0.1298, -0.0083, -0.1825,\n",
       "                      -0.1526, -0.1039, -0.1807, -0.1140, -0.1225, -0.1244, -0.1205, -0.1988])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-0.1325, -0.2244, -0.2611, -0.1711, -0.1929, -0.2287, -0.1370, -0.1847,\n",
       "                      -0.1824, -0.0817, -0.1664, -0.1416, -0.2388, -0.2026, -0.2353, -0.0457,\n",
       "                      -0.2731, -0.1346, -0.1513, -0.0518, -0.1190, -0.2464, -0.1028, -0.2447,\n",
       "                      -0.1481, -0.1704, -0.2317, -0.1459, -0.0949, -0.1209, -0.2019, -0.2269,\n",
       "                      -0.2048, -0.1909, -0.1617, -0.1316, -0.1382, -0.1498, -0.0993, -0.1221,\n",
       "                      -0.0903, -0.2234, -0.1448, -0.1226, -0.0981, -0.2766, -0.2404, -0.2686,\n",
       "                      -0.3279, -0.3453, -0.1616, -0.0255, -0.1887, -0.1262, -0.0938, -0.1536,\n",
       "                      -0.1199, -0.1154, -0.1946, -0.2224, -0.1992, -0.1381, -0.2167, -0.2634,\n",
       "                      -0.0453, -0.2338, -0.0907, -0.2083, -0.1592, -0.0843, -0.1499, -0.1171,\n",
       "                      -0.1743, -0.0815, -0.2703, -0.1919, -0.1355, -0.1234, -0.1062, -0.1055,\n",
       "                      -0.1679, -0.1666, -0.2035, -0.0949, -0.2086, -0.1592, -0.1622, -0.1633,\n",
       "                      -0.1476, -0.1460, -0.1548, -0.2096, -0.2462, -0.1770, -0.2224, -0.2439,\n",
       "                      -0.0390, -0.1770, -0.1273, -0.0242, -0.0270, -0.0499, -0.2629, -0.0694,\n",
       "                      -0.1306, -0.1297, -0.1636, -0.0887, -0.1822, -0.2511, -0.2844, -0.0766,\n",
       "                      -0.1490, -0.1264, -0.2672, -0.0966, -0.1622, -0.2711, -0.1221, -0.0537,\n",
       "                      -0.1735, -0.2990, -0.2222, -0.1780, -0.1882, -0.1109, -0.2144, -0.2573,\n",
       "                      -0.1548, -0.1845, -0.0508, -0.1344, -0.0384, -0.0862, -0.2282, -0.0371,\n",
       "                      -0.0845, -0.1926, -0.0885, -0.1380, -0.0410, -0.1227, -0.1209, -0.1771,\n",
       "                      -0.0412, -0.0326, -0.0916, -0.1824, -0.0339, -0.1411, -0.1155, -0.1372,\n",
       "                      -0.1393, -0.0564, -0.1190, -0.2140, -0.0644,  0.0015, -0.2596, -0.2118,\n",
       "                      -0.0982, -0.2056, -0.1222, -0.1283, -0.1858, -0.1646,  0.0202, -0.1820,\n",
       "                      -0.2598, -0.0548, -0.2174, -0.0831, -0.1010, -0.3318, -0.1112, -0.1168,\n",
       "                      -0.1025, -0.2867, -0.2155, -0.0766, -0.1109, -0.1822, -0.1186, -0.1468,\n",
       "                       0.0123, -0.1950, -0.0649, -0.1431, -0.1617, -0.2105, -0.1175, -0.1642,\n",
       "                      -0.1367, -0.1408, -0.0644, -0.0466, -0.1199, -0.2108, -0.2569, -0.1088,\n",
       "                      -0.2190, -0.1208, -0.2056, -0.0684,  0.0271, -0.1110, -0.1461, -0.0849,\n",
       "                      -0.2176, -0.1231, -0.1941,  0.0125, -0.1309, -0.1184, -0.2132, -0.2087,\n",
       "                      -0.1700, -0.0007, -0.1933, -0.0954, -0.1641, -0.1729, -0.2064, -0.0726,\n",
       "                      -0.1722, -0.1217, -0.0628, -0.1119, -0.0801, -0.0691, -0.1547, -0.1130,\n",
       "                      -0.1120, -0.1262, -0.2492, -0.0419,  0.0064, -0.0331, -0.1595, -0.1245,\n",
       "                      -0.2101, -0.1411, -0.1954, -0.1138, -0.2310, -0.2042, -0.0611, -0.1137,\n",
       "                      -0.1352,  0.0344, -0.1458, -0.1964, -0.1560, -0.0665, -0.2033, -0.2751,\n",
       "                      -0.1121,  0.0282, -0.0295, -0.1320,  0.0128,  0.1665,  0.0108,  0.0244,\n",
       "                      -0.0205,  0.0653, -0.0185, -0.0013,  0.0158, -0.0079, -0.1305, -0.0520,\n",
       "                       0.0464,  0.0387,  0.0160,  0.0488,  0.0777,  0.0267,  0.0145,  0.1174,\n",
       "                      -0.0824, -0.0471,  0.0225,  0.0144,  0.0454, -0.0551, -0.0891, -0.0810,\n",
       "                      -0.0333, -0.0013,  0.0006, -0.0260,  0.0175,  0.0184,  0.0446, -0.0297,\n",
       "                       0.0361, -0.0830, -0.0251, -0.0994,  0.0232, -0.0643, -0.0126,  0.0664,\n",
       "                      -0.0210,  0.0225, -0.0141, -0.0487, -0.0471,  0.0122,  0.0412, -0.0030,\n",
       "                       0.0025, -0.0706,  0.1048, -0.1199, -0.0725,  0.0849,  0.0200, -0.0388,\n",
       "                       0.0394, -0.0337, -0.0150, -0.0151,  0.0030,  0.0378, -0.0141,  0.0454,\n",
       "                      -0.0314,  0.0116, -0.0765, -0.0384,  0.0387,  0.0884,  0.0294, -0.0351,\n",
       "                      -0.0657,  0.0397,  0.1236, -0.0144, -0.0702,  0.0791,  0.0532, -0.0153,\n",
       "                       0.0884,  0.0319,  0.0773, -0.0165,  0.0030, -0.0246,  0.0167, -0.0456,\n",
       "                      -0.0156,  0.1199,  0.0834,  0.0991, -0.0170, -0.0371,  0.0648, -0.1584,\n",
       "                      -0.0178, -0.0348, -0.0534,  0.0293,  0.0472,  0.1583,  0.0037, -0.0191,\n",
       "                      -0.0666,  0.0478,  0.0070,  0.0248,  0.0243, -0.0322, -0.1746, -0.0148,\n",
       "                       0.1046, -0.0781, -0.0814,  0.0612, -0.0393, -0.0182, -0.0421, -0.0924,\n",
       "                      -0.1166, -0.1413, -0.1377, -0.1311, -0.1924, -0.2229, -0.1675, -0.2100,\n",
       "                      -0.2502, -0.0921, -0.1504, -0.2333, -0.1234, -0.2055, -0.2135, -0.1004,\n",
       "                      -0.2052, -0.1798, -0.0740, -0.1908, -0.2629, -0.1898, -0.1686, -0.1936,\n",
       "                      -0.2011, -0.0838, -0.2338, -0.2817, -0.1212, -0.2015, -0.3242, -0.0822,\n",
       "                      -0.1208, -0.3020, -0.0454, -0.1559, -0.1160, -0.0320, -0.1206, -0.1669,\n",
       "                      -0.2827, -0.1275, -0.1490, -0.0117, -0.2803, -0.2088, -0.1295, -0.1851,\n",
       "                      -0.1297, -0.3256, -0.2351, -0.1132, -0.1676, -0.0997, -0.0583, -0.1613,\n",
       "                      -0.1167, -0.1195, -0.2795, -0.2465, -0.2019, -0.1992, -0.1018, -0.2167,\n",
       "                      -0.1425, -0.0826, -0.0390, -0.1900, -0.1058, -0.0398, -0.0795, -0.1564,\n",
       "                      -0.1006, -0.1780, -0.2664, -0.0781, -0.1363, -0.0894, -0.1294, -0.2313,\n",
       "                      -0.2063, -0.0856, -0.2135, -0.1058, -0.2148, -0.1586, -0.2244, -0.0691,\n",
       "                      -0.2042, -0.1582, -0.1300, -0.2293, -0.3441, -0.0367, -0.1241, -0.0815,\n",
       "                      -0.0955, -0.1766, -0.1888,  0.0379, -0.0984, -0.1945, -0.2681, -0.0780,\n",
       "                      -0.1796, -0.1437, -0.1993, -0.1837, -0.2352, -0.2411, -0.1443, -0.1969,\n",
       "                      -0.1524, -0.1365, -0.2293, -0.1114, -0.1928, -0.2361, -0.1419, -0.0221,\n",
       "                      -0.1175, -0.1507, -0.2421, -0.0848, -0.1668, -0.2040, -0.1378, -0.3618])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0576, -0.2272,  0.1693,  ...,  0.2567, -0.3845, -0.0561],\n",
       "                      [-0.0112, -0.1078,  0.0303,  ...,  0.0398,  0.0180, -0.0966],\n",
       "                      [ 0.2730,  0.0574,  0.1914,  ..., -0.0404,  0.0893,  0.0654],\n",
       "                      ...,\n",
       "                      [-0.1084, -0.0660, -0.1066,  ..., -0.0874,  0.5127,  0.1476],\n",
       "                      [-0.3088, -0.4224,  0.1889,  ...,  0.1752, -0.2710, -0.0744],\n",
       "                      [-0.2416,  0.1257,  0.0269,  ...,  0.0285,  0.3465, -0.0668]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-1.3632e-01, -1.7788e-01, -5.6560e-02,  ..., -1.2950e-01,\n",
       "                        3.5127e-01,  5.3915e-05],\n",
       "                      [-3.2825e-01, -1.6689e-01,  3.2374e-01,  ..., -2.2880e-01,\n",
       "                       -1.2055e-01, -9.2096e-02],\n",
       "                      [-2.5270e-01,  6.9136e-02, -1.2331e-01,  ..., -1.4281e-01,\n",
       "                       -1.4908e-01,  2.0874e-02],\n",
       "                      ...,\n",
       "                      [-6.0944e-02,  5.5666e-02, -7.6457e-03,  ...,  4.0945e-02,\n",
       "                       -3.3812e-02, -8.9572e-02],\n",
       "                      [ 3.2732e-01, -1.0520e-01,  1.5271e-01,  ..., -2.0036e-01,\n",
       "                        9.0496e-02, -1.2943e-02],\n",
       "                      [-1.5707e-01,  7.0369e-02,  1.4095e-01,  ...,  7.3734e-02,\n",
       "                       -1.5956e-01, -1.8694e-01]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 1.5850e-01, -1.4547e-01,  3.0665e-02,  6.3653e-02, -5.0722e-02,\n",
       "                       7.7596e-02,  1.3910e-01,  2.2388e-01, -9.6219e-02,  1.5504e-01,\n",
       "                       1.9080e-01,  2.8491e-01,  6.1375e-02,  2.1014e-02,  7.4573e-02,\n",
       "                       6.1723e-03, -9.6526e-02, -6.8094e-02,  5.9577e-02, -3.3659e-02,\n",
       "                       3.1313e-02,  9.7661e-02, -1.6307e-02,  1.3522e-02,  4.5520e-02,\n",
       "                      -4.5872e-02,  3.4614e-02,  5.5000e-02, -4.4982e-02,  7.6439e-02,\n",
       "                      -3.9059e-02, -7.8013e-02,  4.2845e-02,  7.5529e-02, -5.9633e-02,\n",
       "                      -2.8202e-02,  1.7671e-01, -1.1235e-02,  2.5123e-01,  6.3909e-03,\n",
       "                      -5.8368e-03,  2.1788e-01,  9.1460e-03, -1.1680e-01, -6.9380e-03,\n",
       "                      -9.1018e-02,  1.7181e-01,  1.9781e-01,  3.4330e-02,  6.8940e-02,\n",
       "                       1.0323e-01,  2.6144e-01,  8.9075e-02,  1.9016e-01, -7.1410e-02,\n",
       "                       2.4226e-02,  1.7246e-01,  1.8338e-04,  3.3568e-03, -4.9804e-02,\n",
       "                       2.0950e-02,  2.3138e-02,  3.4770e-02,  1.4031e-01, -3.2945e-02,\n",
       "                      -1.5541e-01,  8.3190e-02, -5.5798e-02, -5.9227e-02,  7.9863e-02,\n",
       "                       2.6632e-01,  9.4386e-02,  5.7152e-02, -5.4061e-02,  3.2266e-02,\n",
       "                       1.0572e-01,  2.3661e-01, -9.4379e-02,  1.0243e-01,  1.2595e-01,\n",
       "                       1.1215e-01,  4.9269e-02,  5.0838e-02, -1.5751e-01, -2.2711e-02,\n",
       "                       3.3260e-02,  8.4792e-02,  2.9699e-04, -8.0481e-02, -1.8753e-02,\n",
       "                       1.2303e-01, -1.5860e-01, -1.0657e-02,  6.6013e-02,  8.7991e-02,\n",
       "                       7.2458e-03, -5.8061e-02, -7.5423e-02,  7.5830e-02,  5.7508e-02,\n",
       "                       2.6182e-01,  2.1002e-02, -2.9683e-02, -8.8337e-02,  1.1274e-02,\n",
       "                      -4.3275e-03, -2.5509e-02,  9.0059e-02,  1.3266e-02,  2.9657e-02,\n",
       "                      -4.3931e-02, -4.7445e-02,  1.3312e-01,  1.6195e-01,  9.3528e-02,\n",
       "                      -7.2310e-02,  6.6595e-03,  8.8801e-02,  9.8820e-02, -1.2263e-01,\n",
       "                       1.1961e-01,  2.8653e-01, -8.4271e-03, -6.9041e-02,  1.9347e-02,\n",
       "                      -4.7845e-02, -5.8974e-02,  8.5279e-02, -8.8408e-02, -6.7880e-02,\n",
       "                      -1.1028e-02,  1.3518e-01,  1.8584e-03, -1.1073e-01, -5.9368e-02,\n",
       "                      -4.6023e-02,  2.9059e-03,  9.4898e-02, -3.2422e-02, -1.7173e-01,\n",
       "                      -1.3299e-02,  4.9368e-02,  6.4050e-02, -1.2948e-01, -9.8590e-03,\n",
       "                      -1.4845e-01, -1.8916e-02,  9.3307e-02,  5.7390e-02, -2.5056e-01,\n",
       "                      -1.3728e-01,  7.4405e-02,  9.2532e-02, -6.0212e-02, -1.5556e-01,\n",
       "                      -3.8201e-03, -2.1533e-01,  2.5851e-02,  3.8033e-02, -6.0832e-02,\n",
       "                      -1.0169e-01,  8.2202e-02, -1.5796e-01, -2.0161e-01, -5.7518e-02,\n",
       "                       9.5707e-02, -1.0899e-01,  5.3082e-02, -6.5076e-02, -2.2053e-02,\n",
       "                       3.9558e-02, -1.0596e-01, -2.9504e-02, -1.9646e-01,  1.5864e-02,\n",
       "                      -2.7553e-02, -1.7764e-01, -3.6293e-02, -7.6348e-02, -7.7166e-02,\n",
       "                      -1.0631e-01, -6.5549e-02, -1.3175e-01, -4.3514e-02, -1.0633e-01,\n",
       "                       6.6613e-02,  2.4094e-02, -1.2284e-02, -8.4785e-02, -1.5750e-01,\n",
       "                      -1.2918e-01, -6.4456e-03, -9.6270e-02, -3.8471e-02, -6.8766e-02,\n",
       "                      -9.0699e-02, -4.4587e-02, -8.6881e-03, -5.3382e-02,  1.6838e-03,\n",
       "                      -7.8809e-02, -1.3197e-01, -5.1987e-02, -7.3405e-02, -4.5336e-02,\n",
       "                      -2.5242e-02, -1.7286e-01,  4.5974e-03,  2.5214e-02, -1.5986e-01,\n",
       "                       2.8889e-02,  5.7437e-03, -1.4167e-01, -6.6602e-02, -1.1011e-01,\n",
       "                       7.0754e-03, -1.0590e-02,  8.3398e-02, -3.1233e-02, -1.5000e-02,\n",
       "                      -1.0801e-01, -2.1910e-01, -1.4638e-01, -9.6989e-02, -8.9871e-02,\n",
       "                      -3.7146e-02, -6.8460e-02, -1.9828e-02, -3.3194e-03,  4.7967e-02,\n",
       "                       1.1043e-01, -6.9328e-02,  3.8654e-02,  2.8266e-02, -1.7235e-01,\n",
       "                      -6.0034e-02, -2.8281e-02, -1.3248e-01, -2.4182e-01, -1.4626e-01,\n",
       "                      -4.4803e-02, -8.7820e-02, -3.6883e-02, -1.7346e-01,  3.1298e-03,\n",
       "                       2.8562e-02, -9.7264e-02, -1.5066e-01, -7.8767e-02,  4.5564e-02,\n",
       "                      -3.8870e-02, -5.0105e-02,  4.8204e-03, -2.0260e-02,  1.0462e-02,\n",
       "                      -5.2982e-02,  1.1796e-01,  9.1240e-02,  7.8171e-02, -3.5834e-02,\n",
       "                       8.4325e-02, -2.5703e-02, -5.9545e-03, -7.6287e-02, -2.0283e-02,\n",
       "                       1.0487e-02, -1.3111e-01, -5.3543e-02, -9.8316e-03, -7.6219e-03,\n",
       "                      -3.9873e-02, -4.9869e-02, -3.3386e-02,  3.4177e-02,  1.2471e-01,\n",
       "                       6.4905e-02, -1.1038e-01,  2.3667e-02,  4.5918e-02, -5.0108e-02,\n",
       "                      -1.4260e-01,  2.4397e-02,  5.0424e-02,  4.6161e-02,  8.5051e-02,\n",
       "                      -7.1092e-03, -1.9168e-03, -2.0364e-02, -1.7591e-01, -1.0904e-02,\n",
       "                      -9.4591e-02, -4.3782e-02,  2.1396e-02,  1.6962e-02, -4.4020e-02,\n",
       "                      -1.0268e-01,  6.9769e-02, -1.1582e-02,  1.6235e-01,  7.3872e-02,\n",
       "                      -3.9057e-02, -7.9029e-02, -4.5021e-02,  4.5959e-02, -2.7908e-02,\n",
       "                       8.0085e-02, -2.9216e-02,  3.6350e-02, -1.4531e-01,  8.9079e-02,\n",
       "                       3.8797e-02, -1.0144e-01, -2.7136e-02, -1.1644e-02,  5.2602e-02,\n",
       "                      -9.4163e-03, -6.0556e-03, -1.2073e-01,  8.7021e-02, -1.2306e-02,\n",
       "                       4.0529e-02, -6.2240e-02,  7.2408e-02, -2.4108e-02, -1.0186e-01,\n",
       "                       4.8874e-03,  6.5301e-02, -2.9953e-02, -8.5316e-02, -6.5544e-02,\n",
       "                      -2.8122e-02, -1.0256e-02, -3.1002e-02, -6.8755e-03,  8.8434e-02,\n",
       "                      -3.1496e-02,  1.4385e-01,  6.6078e-02,  4.8968e-02,  1.5537e-02,\n",
       "                       3.2927e-02, -1.7071e-02,  3.9559e-03, -2.7231e-02, -9.2322e-02,\n",
       "                       2.4656e-02, -1.0276e-01, -4.6231e-02,  4.2339e-02,  7.8136e-03,\n",
       "                       9.2844e-02,  8.1279e-02, -8.5339e-02,  8.8940e-02,  1.2001e-01,\n",
       "                       2.6494e-02, -4.7088e-02, -8.2056e-02, -1.4233e-02,  5.5108e-02,\n",
       "                       6.3796e-02, -2.6277e-02, -1.7503e-01, -7.9446e-02,  9.5311e-02,\n",
       "                      -2.3848e-02,  4.2609e-02,  6.4995e-02, -8.2437e-02, -5.6861e-03,\n",
       "                       2.7685e-02, -9.8350e-02, -5.6981e-02, -1.2693e-02,  7.4031e-02,\n",
       "                      -9.0829e-02,  2.5027e-02, -4.0666e-02, -1.1300e-01,  1.2022e-01,\n",
       "                       9.9211e-02,  4.2304e-02,  1.5331e-01,  9.7067e-03,  8.1539e-02,\n",
       "                       2.2454e-02,  4.9169e-02, -4.3604e-02,  2.3021e-01,  8.0887e-02,\n",
       "                       7.9610e-02, -1.8083e-03,  4.8405e-02,  1.6163e-01,  9.2152e-02,\n",
       "                       3.5527e-01,  2.0193e-01, -1.0341e-02,  2.4395e-02,  8.5881e-02,\n",
       "                       1.1033e-01,  5.8167e-02,  1.0238e-01,  8.1759e-04,  1.9896e-01,\n",
       "                       2.0114e-01,  3.4152e-02,  3.8955e-02,  2.0260e-01,  4.3166e-02,\n",
       "                       4.6922e-03,  8.0447e-02,  3.4095e-02,  1.6808e-02,  5.7718e-02,\n",
       "                       1.3714e-01, -1.4939e-03,  1.4486e-01, -1.1228e-01, -2.9619e-03,\n",
       "                       1.1499e-01,  7.4391e-02,  1.4538e-01,  6.5869e-02,  3.1893e-02,\n",
       "                       3.0691e-02,  1.2027e-01,  5.1535e-02, -1.4757e-01,  4.8540e-04,\n",
       "                       1.9243e-01,  7.0926e-02, -3.9709e-02,  9.0671e-02,  6.5911e-03,\n",
       "                       1.5030e-01,  8.0848e-02,  1.3270e-01, -5.0096e-02, -8.7851e-02,\n",
       "                       2.5918e-01,  7.0992e-02,  9.2197e-04, -8.7565e-02,  6.9551e-03,\n",
       "                       8.0826e-02, -1.1064e-02,  7.2473e-03, -7.2149e-02, -8.4395e-03,\n",
       "                      -6.3445e-03, -1.8835e-01, -3.1076e-02,  6.5788e-02,  3.1659e-01,\n",
       "                       3.1133e-01,  1.9812e-01,  1.1200e-01,  6.0528e-02,  1.0028e-01,\n",
       "                       1.1774e-01,  1.4303e-01,  3.7398e-02,  5.0511e-02,  1.1280e-01,\n",
       "                       7.8244e-02,  7.2639e-02,  3.3980e-02, -4.4461e-02,  1.0265e-01,\n",
       "                       2.7417e-01,  2.9726e-01,  3.4379e-02,  6.5185e-02,  6.8635e-02,\n",
       "                      -4.9687e-02, -1.9189e-02,  1.6669e-02,  1.2398e-01,  4.2565e-02,\n",
       "                       4.0971e-02, -6.6663e-02,  2.0944e-02, -2.1366e-02,  2.1034e-01,\n",
       "                       1.1040e-01,  4.9575e-02,  1.1224e-02,  6.8221e-02, -1.9851e-03,\n",
       "                      -7.0216e-03, -6.0578e-02, -1.0877e-02,  7.8669e-02,  1.6135e-01,\n",
       "                       1.3708e-01,  6.2110e-02,  1.7198e-01, -5.1044e-02,  3.4290e-02,\n",
       "                       2.0635e-01,  1.7119e-01,  2.5825e-03, -3.4881e-02,  9.4363e-02,\n",
       "                       1.8775e-01,  6.7051e-02, -4.5172e-02,  1.6955e-01,  7.2763e-02,\n",
       "                      -3.2180e-02, -2.6986e-02])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 1.4411e-01, -7.2175e-02,  5.3521e-02, -3.9918e-04,  1.2038e-01,\n",
       "                       1.3025e-01, -2.2533e-02,  1.6171e-01,  5.8587e-02,  1.4835e-01,\n",
       "                       5.1965e-02,  2.6850e-01,  1.4930e-01,  4.9144e-02,  6.0788e-03,\n",
       "                      -3.0076e-03, -3.2628e-02,  1.6423e-01,  6.4543e-02, -2.1079e-02,\n",
       "                       1.3682e-01,  1.0783e-01,  3.4095e-03, -6.5994e-02,  8.8360e-02,\n",
       "                       5.5554e-02,  5.6796e-02,  1.6586e-01,  6.2516e-02,  2.2829e-01,\n",
       "                       1.8414e-01, -1.0655e-01, -6.1180e-02,  8.8938e-02, -4.6728e-02,\n",
       "                       8.5343e-02,  1.5870e-01,  2.7634e-02,  3.1833e-01,  5.8881e-02,\n",
       "                       6.5051e-02,  3.0612e-02,  6.6357e-02, -6.0559e-03,  3.6122e-02,\n",
       "                       8.7700e-03,  8.7622e-02,  2.1905e-01,  1.6617e-02,  1.6832e-01,\n",
       "                       7.4491e-03,  1.7145e-01, -4.0341e-02,  2.0801e-01,  4.3449e-02,\n",
       "                       9.9517e-02,  7.7212e-02,  4.1838e-02, -3.9285e-02,  2.9858e-02,\n",
       "                       7.8753e-02, -3.8534e-02,  1.4633e-01, -9.6387e-02, -5.7500e-02,\n",
       "                      -5.5701e-03,  4.9818e-02, -5.7648e-02, -3.7581e-02,  4.2313e-02,\n",
       "                       2.7583e-01,  2.2096e-01,  1.2260e-01,  1.1622e-01, -1.0735e-01,\n",
       "                       1.8351e-01,  2.9786e-01, -3.2024e-02, -2.6469e-02,  1.6791e-01,\n",
       "                       9.9383e-02,  2.6808e-02,  5.2704e-02, -1.6496e-01,  5.8716e-04,\n",
       "                       1.7573e-03,  1.7287e-01,  1.4864e-01, -6.3791e-02,  7.6726e-02,\n",
       "                       2.7458e-02,  5.4963e-02, -4.2101e-02,  1.3163e-01,  1.6069e-01,\n",
       "                      -6.0627e-02,  9.9730e-03,  8.4228e-03,  1.0176e-01, -1.2022e-01,\n",
       "                       2.9201e-01,  2.3309e-01,  1.1413e-01, -8.4923e-02, -6.0533e-02,\n",
       "                      -8.7826e-02,  5.7556e-02,  8.7934e-02,  3.2827e-05,  3.5295e-02,\n",
       "                       5.9780e-03,  2.4973e-02,  5.9848e-02,  8.1559e-02,  2.2891e-02,\n",
       "                      -3.8884e-02,  8.6031e-02, -2.5595e-02,  7.4500e-02, -1.1622e-01,\n",
       "                       1.2238e-01,  2.6410e-01, -1.2052e-01,  1.2584e-02,  1.1586e-01,\n",
       "                       2.6818e-02,  1.5426e-02,  3.2067e-02, -2.2853e-02,  5.1684e-02,\n",
       "                      -1.2017e-01, -3.7415e-02, -4.1398e-02, -5.7664e-02, -2.2384e-02,\n",
       "                      -4.8534e-02,  1.6404e-01,  1.1291e-02, -8.7751e-02, -1.1572e-01,\n",
       "                      -1.9287e-01,  1.9429e-02, -7.4574e-02, -4.8194e-03, -5.8356e-02,\n",
       "                      -8.2458e-02, -7.2942e-02, -1.7048e-02, -2.6881e-02, -1.1398e-01,\n",
       "                      -3.7856e-02, -4.0389e-02, -6.4973e-03, -2.9439e-02, -2.5121e-01,\n",
       "                      -9.8718e-02, -1.3146e-01, -8.2933e-02, -2.6438e-02, -1.1731e-01,\n",
       "                      -3.3287e-02, -2.9604e-02, -1.7084e-01, -2.0494e-01, -1.3282e-01,\n",
       "                       1.3588e-03, -1.3593e-01,  8.8060e-02,  2.2352e-02, -1.7224e-01,\n",
       "                       4.8142e-02,  7.1282e-02, -8.7752e-02, -3.0530e-01, -9.9317e-03,\n",
       "                      -7.7022e-02, -1.9108e-01, -7.5878e-02, -1.6369e-01, -1.0915e-01,\n",
       "                      -1.4366e-01, -4.2043e-02, -1.5649e-01, -3.2324e-02, -1.8985e-01,\n",
       "                       7.6947e-03, -6.4681e-02,  9.9351e-03, -2.8946e-02, -1.0305e-01,\n",
       "                       3.5957e-02, -8.0402e-02, -3.7983e-02, -9.6098e-02,  5.0840e-03,\n",
       "                      -6.3506e-02, -1.3026e-01, -1.8872e-02, -1.5650e-01,  7.2138e-02,\n",
       "                      -1.1221e-01, -1.2900e-01, -1.1133e-01, -3.6663e-02, -7.2378e-02,\n",
       "                       1.1931e-01, -2.0627e-01,  1.4156e-01,  9.7803e-02, -1.1699e-01,\n",
       "                       5.2000e-02,  1.0074e-01,  2.3685e-02, -9.5531e-02, -8.9937e-02,\n",
       "                       3.0868e-03,  6.7112e-02,  4.4864e-02,  4.3541e-02, -1.9699e-01,\n",
       "                      -5.0829e-02, -2.1935e-01, -8.7201e-02, -4.2273e-03, -1.0894e-01,\n",
       "                      -1.7095e-01, -7.2461e-02,  1.2434e-02, -6.4359e-02,  2.6689e-02,\n",
       "                       3.3610e-02, -8.3058e-02, -1.0891e-03,  1.8175e-01, -1.3238e-01,\n",
       "                       1.1645e-01, -5.7555e-02, -1.4199e-01, -9.2243e-03, -1.0396e-01,\n",
       "                      -6.7218e-02,  7.7804e-03, -1.4263e-01, -1.1800e-01, -4.0786e-02,\n",
       "                      -1.2806e-01, -6.2536e-02,  8.4733e-03, -8.8782e-02,  3.3879e-02,\n",
       "                      -1.5655e-01, -7.1646e-02, -1.3478e-01, -7.8003e-02, -5.5088e-02,\n",
       "                      -1.4106e-01, -2.8767e-02, -5.3350e-02,  3.1041e-02, -1.2204e-01,\n",
       "                       2.8982e-02,  4.4453e-03, -9.7025e-02,  5.2935e-02,  6.0620e-02,\n",
       "                      -5.2907e-02, -9.7667e-02,  4.8534e-02, -4.5034e-03,  1.3796e-01,\n",
       "                      -6.9343e-02, -2.6030e-02,  6.0627e-02,  2.3542e-02,  5.6131e-02,\n",
       "                       7.4162e-02,  8.4926e-02,  8.3729e-02, -1.2702e-02,  3.8574e-03,\n",
       "                      -9.8227e-02, -6.3115e-02,  3.3489e-02,  3.5480e-04, -8.6500e-02,\n",
       "                       5.3282e-02,  7.6648e-02, -8.1491e-02, -9.4895e-02,  4.7633e-02,\n",
       "                       8.5189e-02, -3.4359e-02, -1.7005e-01, -4.2559e-02, -9.3724e-02,\n",
       "                      -1.9104e-02,  2.7262e-02, -3.3872e-02,  1.0212e-01, -5.2442e-02,\n",
       "                       1.0943e-01,  4.2613e-02,  1.3509e-02,  8.4020e-02,  3.5752e-02,\n",
       "                      -4.0887e-03, -2.8158e-02, -4.2322e-02, -2.9948e-02,  1.6146e-02,\n",
       "                      -2.8339e-02,  4.0753e-02,  2.9051e-02,  5.8097e-02,  1.8275e-02,\n",
       "                       1.2195e-01,  4.9450e-03, -2.0746e-02,  5.6194e-02,  2.7311e-02,\n",
       "                       2.3614e-02, -2.1487e-02,  1.7964e-01, -1.1063e-02, -4.0009e-02,\n",
       "                       7.4765e-02,  7.4130e-02,  1.1590e-02, -2.7869e-02,  1.1061e-02,\n",
       "                       3.5684e-02, -4.2040e-02, -1.4919e-01,  1.6576e-01,  5.0107e-02,\n",
       "                       3.7370e-02,  3.5360e-02, -5.5148e-02,  7.5125e-02,  1.0107e-01,\n",
       "                      -5.7508e-02, -1.2658e-01,  1.9573e-03,  9.2297e-02, -6.8781e-02,\n",
       "                      -7.3570e-02, -6.6890e-02, -2.0423e-02, -7.4099e-02, -5.8064e-02,\n",
       "                       3.8607e-02,  6.9325e-02, -4.0398e-02,  9.2673e-02,  1.1545e-01,\n",
       "                      -2.7390e-02, -2.2840e-03, -1.3690e-02, -5.0112e-02,  6.5533e-02,\n",
       "                      -8.2613e-03,  2.5021e-02, -3.6824e-02,  5.8326e-02,  3.5779e-02,\n",
       "                      -9.1246e-02,  2.0012e-02, -1.0464e-01, -5.4898e-02, -1.3855e-02,\n",
       "                       2.9856e-02, -8.6761e-02, -9.8717e-02, -3.4809e-02, -4.6400e-02,\n",
       "                       1.3445e-01, -1.2149e-01,  6.9574e-02, -4.4246e-02, -1.8132e-02,\n",
       "                      -6.4377e-03,  9.7570e-02, -1.5557e-02, -4.1022e-05,  1.2339e-01,\n",
       "                      -1.9665e-02,  1.4087e-01,  6.5673e-02,  1.9834e-01,  1.1077e-01,\n",
       "                       4.5341e-02,  5.2586e-02,  1.9995e-02,  8.7381e-02,  7.4062e-02,\n",
       "                       2.6238e-01,  3.2228e-01,  1.2302e-02,  7.4993e-02,  8.6035e-03,\n",
       "                      -6.0635e-02,  1.4535e-01,  1.6854e-01,  4.1248e-02,  1.3415e-01,\n",
       "                       1.4123e-01,  5.8448e-02, -1.3668e-01, -1.7961e-02,  1.1435e-01,\n",
       "                       2.2955e-02,  1.6579e-01,  1.5461e-01,  1.3129e-02,  7.0461e-02,\n",
       "                      -7.6117e-02,  1.0194e-03,  9.5476e-02, -1.8231e-01,  3.0997e-02,\n",
       "                       1.6793e-01,  4.9861e-02,  2.3826e-01,  1.3063e-01,  5.9727e-02,\n",
       "                       9.1364e-02,  2.0099e-01,  2.3928e-02, -8.5155e-02,  7.5363e-03,\n",
       "                       8.4252e-02,  4.3277e-02, -6.3371e-02, -1.4325e-02,  1.2586e-01,\n",
       "                       1.2647e-01,  1.8786e-01,  7.4728e-02, -8.4718e-02,  4.0697e-02,\n",
       "                       1.6686e-01,  9.5496e-02, -8.1536e-02,  7.9708e-02,  3.8746e-02,\n",
       "                      -8.9110e-03,  1.6177e-01,  1.0976e-01, -9.1992e-02, -4.8248e-03,\n",
       "                       1.9760e-03, -1.4061e-01,  3.3614e-02,  6.3684e-02,  1.7185e-01,\n",
       "                       3.0221e-01,  1.9529e-03,  1.7277e-01,  4.7763e-02,  2.4685e-02,\n",
       "                       1.2218e-01,  3.9079e-02, -3.3232e-02,  5.5526e-02,  2.0668e-01,\n",
       "                       3.8996e-02,  4.0339e-02, -8.3683e-02, -1.8666e-01,  5.1092e-02,\n",
       "                       2.4604e-01,  9.8893e-02, -3.2278e-02,  1.4846e-03,  2.0713e-01,\n",
       "                      -7.4426e-02,  3.1911e-02,  1.1951e-01,  1.1607e-01,  1.5087e-03,\n",
       "                       2.4007e-02, -1.0455e-01, -4.9115e-02,  1.4696e-02,  3.1008e-01,\n",
       "                       1.2506e-01, -4.9820e-02,  1.0140e-01, -8.8164e-02, -1.2328e-01,\n",
       "                      -7.6339e-02, -6.3785e-02, -1.6933e-01,  7.0380e-02,  2.1497e-01,\n",
       "                      -8.0665e-03,  1.3119e-01,  1.9959e-01,  3.0935e-02,  8.8485e-02,\n",
       "                       1.7266e-01,  3.7384e-02,  7.3469e-02,  1.6900e-02,  1.2657e-01,\n",
       "                       1.3990e-01,  4.8741e-02, -1.3787e-02,  1.1641e-01,  4.5858e-02,\n",
       "                       4.8695e-02,  2.1159e-02])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[-0.1285, -0.1336,  0.0843,  ..., -0.2027, -0.1367,  0.5707],\n",
       "                      [ 0.1685,  0.0560, -0.0079,  ...,  0.0462, -0.0979, -0.2228],\n",
       "                      [ 0.0562,  0.0418, -0.0808,  ..., -0.1933,  0.0354,  0.1999],\n",
       "                      ...,\n",
       "                      [-0.0628, -0.0380, -0.0432,  ...,  0.0529,  0.0090,  0.3424],\n",
       "                      [-0.0391, -0.1188, -0.0363,  ..., -0.1829,  0.0443, -0.1853],\n",
       "                      [-0.0747,  0.0249,  0.1941,  ...,  0.0372, -0.1839, -0.4395]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([ 4.4313e-01,  2.2775e-01,  3.1980e-01,  4.3016e-01,  2.9064e-06,\n",
       "                       2.2718e-01,  1.7908e-01,  1.7206e-01, -2.3585e-01,  1.7551e-01,\n",
       "                       3.9097e-02,  3.0664e-01,  2.2521e-01, -1.3365e-01,  9.5195e-02,\n",
       "                      -1.6473e-01, -1.1332e-02, -6.4975e-02, -1.2455e-01, -1.4394e-01,\n",
       "                      -2.4332e-01, -2.1285e-01,  2.4275e-01, -4.9192e-02, -7.9214e-02,\n",
       "                       6.2689e-02,  7.1197e-04, -9.6849e-02, -2.5182e-01,  7.0119e-02,\n",
       "                       2.1106e-01, -2.7854e-02, -8.5305e-02,  1.7724e-01,  7.1963e-02,\n",
       "                       1.1100e-01,  8.7667e-02,  1.1934e-02, -6.7494e-02,  3.0943e-02,\n",
       "                      -4.2911e-02, -6.9116e-03, -1.6544e-01, -1.5976e-01,  1.7572e-01,\n",
       "                      -1.7831e-01,  2.2692e-01,  1.5735e-02,  3.3122e-01,  1.0566e-01,\n",
       "                      -2.2616e-03,  1.5135e-01, -3.4722e-01,  9.1689e-02,  1.6791e-01,\n",
       "                       1.1759e-01, -5.7509e-02,  1.2653e-01, -1.6874e-01, -3.1649e-01,\n",
       "                       2.5763e-02,  7.7164e-02, -1.7304e-01, -6.0613e-02])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[-0.1454, -0.0960,  0.2250,  ...,  0.2246,  0.2003, -0.0668],\n",
       "                      [ 0.0396,  0.0991,  0.1800,  ...,  0.2727,  0.1570, -0.3400],\n",
       "                      [ 0.0716,  0.2556,  0.0185,  ..., -0.2554, -0.1783, -0.0710],\n",
       "                      ...,\n",
       "                      [-0.0161,  0.1001, -0.0873,  ...,  0.1233, -0.2815,  0.2314],\n",
       "                      [ 0.0134,  0.3345, -0.2843,  ...,  0.2831, -0.3836,  0.2310],\n",
       "                      [ 0.1559,  0.1641,  0.1962,  ..., -0.1872, -0.1645,  0.2928]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([ 0.1029, -0.2384, -0.2735, -0.0149,  0.1739,  0.0740,  0.2851, -0.0855,\n",
       "                       0.1527, -0.2016,  0.0058, -0.2181,  0.2560, -0.0258, -0.2062, -0.0747,\n",
       "                      -0.0161, -0.0352,  0.0585, -0.0415, -0.0557, -0.0414, -0.2528,  0.1878,\n",
       "                      -0.4127,  0.1719, -0.1481,  0.1314,  0.2436,  0.0452, -0.0907, -0.2411,\n",
       "                      -0.1128, -0.3081, -0.2209, -0.0437,  0.3376, -0.1465, -0.1617, -0.0410,\n",
       "                      -0.0390,  0.3032, -0.1516,  0.1032,  0.1812, -0.4415, -0.2818,  0.0797,\n",
       "                      -0.1633,  0.1836,  0.1733, -0.2545, -0.0692,  0.3049, -0.0661,  0.0905,\n",
       "                       0.1816,  0.0147,  0.0963, -0.0579, -0.1901,  0.0232,  0.0480, -0.3605])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[-0.1453,  0.0669, -0.1208,  ...,  0.0101, -0.0537,  0.3711],\n",
       "                      [ 0.2027, -0.0806,  0.3503,  ..., -0.0354,  0.0197,  0.2556],\n",
       "                      [ 0.3289,  0.1027,  0.0803,  ..., -0.0929,  0.3632,  0.3967],\n",
       "                      ...,\n",
       "                      [ 0.0373, -0.0506, -0.2053,  ..., -0.1500,  0.1907,  0.2440],\n",
       "                      [ 0.4232,  0.2415, -0.3467,  ..., -0.1418, -0.1880,  0.1492],\n",
       "                      [ 0.2278, -0.1558, -0.0157,  ..., -0.2819, -0.0122,  0.1263]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.7850, -0.0710, -0.2569,  ..., -0.2707, -0.2055, -0.0105]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
