{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=1,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 1)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 1)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5024765e-27,  4.5647297e-41, -1.5024765e-27, ...,\n",
       "         6.7041089e-39,  7.1632794e-39,  6.1530231e-39],\n",
       "       [ 6.8877645e-39,  7.0714243e-39,  6.3367613e-39, ...,\n",
       "         6.9795425e-39,  7.8061083e-39,  7.1632864e-39],\n",
       "       [ 5.9694320e-39,  6.9795425e-39,  7.1632598e-39, ...,\n",
       "         7.6224373e-39,  6.7040921e-39,  7.0713780e-39],\n",
       "       ...,\n",
       "       [ 6.0612520e-39,  6.1530693e-39,  2.9388368e-39, ...,\n",
       "         7.7142994e-39,  7.6224653e-39,  7.5305555e-39],\n",
       "       [ 7.7142728e-39,  5.9694166e-39,  5.9694194e-39, ...,\n",
       "         5.9694081e-39,  7.5305555e-39,  6.7959178e-39],\n",
       "       [ 7.0714243e-39,  7.1632598e-39,  5.9694025e-39, ...,\n",
       "         7.5306074e-39,  2.9388522e-39,  7.2551177e-39]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979f45d5dadd414fa8a74e0c30b62f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=5.8824 | F1Score=0.2725\n",
      "Batch-100: NLLLoss=5.5511 | F1Score=0.2837\n",
      "Batch-150: NLLLoss=5.0628 | F1Score=0.3165\n",
      "Batch-200: NLLLoss=4.4808 | F1Score=0.3436\n",
      "Batch-250: NLLLoss=4.4597 | F1Score=0.3691\n",
      "Batch-300: NLLLoss=4.3766 | F1Score=0.3833\n",
      "Batch-350: NLLLoss=3.2468 | F1Score=0.3963\n",
      "Batch-400: NLLLoss=3.3948 | F1Score=0.4109\n",
      "Batch-450: NLLLoss=3.2871 | F1Score=0.4265\n",
      "Batch-500: NLLLoss=3.2360 | F1Score=0.4384\n",
      "Batch-518: NLLLoss=3.2195 | F1Score=0.4424\n",
      "\n",
      "Mean NLLLoss: 4.4493 | Mean F1Score: 0.3550\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7253070df04be187e5e06ef6f22989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.4593 | F1Score=0.5644\n",
      "Batch-100: NLLLoss=1.8496 | F1Score=0.5913\n",
      "Batch-150: NLLLoss=3.3346 | F1Score=0.6044\n",
      "Batch-200: NLLLoss=2.9763 | F1Score=0.6135\n",
      "Batch-250: NLLLoss=1.4838 | F1Score=0.6230\n",
      "Batch-300: NLLLoss=2.7566 | F1Score=0.6277\n",
      "Batch-350: NLLLoss=2.5987 | F1Score=0.6316\n",
      "Batch-400: NLLLoss=2.4289 | F1Score=0.6383\n",
      "Batch-450: NLLLoss=2.8322 | F1Score=0.6423\n",
      "Batch-500: NLLLoss=2.9857 | F1Score=0.6452\n",
      "Batch-518: NLLLoss=1.4275 | F1Score=0.6459\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 2.6649 | Mean F1Score: 0.6162\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7488a2a1029b4879abc769cb8fc47b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.8222 | F1Score=0.7075\n",
      "Batch-100: NLLLoss=2.1684 | F1Score=0.7122\n",
      "Batch-150: NLLLoss=1.6876 | F1Score=0.7220\n",
      "Batch-200: NLLLoss=1.9579 | F1Score=0.7246\n",
      "Batch-250: NLLLoss=2.1274 | F1Score=0.7280\n",
      "Batch-300: NLLLoss=1.7430 | F1Score=0.7340\n",
      "Batch-350: NLLLoss=1.8182 | F1Score=0.7388\n",
      "Batch-400: NLLLoss=1.1374 | F1Score=0.7412\n",
      "Batch-450: NLLLoss=0.5167 | F1Score=0.7455\n",
      "Batch-500: NLLLoss=2.0997 | F1Score=0.7489\n",
      "Batch-518: NLLLoss=0.9352 | F1Score=0.7494\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.7644 | Mean F1Score: 0.7293\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ef7ff0200d454699458eeac95bd55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.1854 | F1Score=0.8100\n",
      "Batch-100: NLLLoss=1.1647 | F1Score=0.8103\n",
      "Batch-150: NLLLoss=1.3367 | F1Score=0.8144\n",
      "Batch-200: NLLLoss=1.6109 | F1Score=0.8134\n",
      "Batch-250: NLLLoss=1.6613 | F1Score=0.8160\n",
      "Batch-300: NLLLoss=0.8710 | F1Score=0.8152\n",
      "Batch-350: NLLLoss=1.3256 | F1Score=0.8155\n",
      "Batch-400: NLLLoss=0.7884 | F1Score=0.8171\n",
      "Batch-450: NLLLoss=1.3265 | F1Score=0.8184\n",
      "Batch-500: NLLLoss=0.9425 | F1Score=0.8190\n",
      "Batch-518: NLLLoss=1.1719 | F1Score=0.8188\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.1532 | Mean F1Score: 0.8146\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc9c820a13d4b9a98f823162f69c358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.5392 | F1Score=0.8806\n",
      "Batch-100: NLLLoss=0.5182 | F1Score=0.8766\n",
      "Batch-150: NLLLoss=0.9550 | F1Score=0.8752\n",
      "Batch-200: NLLLoss=0.8032 | F1Score=0.8733\n",
      "Batch-250: NLLLoss=1.0626 | F1Score=0.8719\n",
      "Batch-300: NLLLoss=0.6195 | F1Score=0.8712\n",
      "Batch-350: NLLLoss=1.3231 | F1Score=0.8696\n",
      "Batch-400: NLLLoss=1.3704 | F1Score=0.8686\n",
      "Batch-450: NLLLoss=0.8648 | F1Score=0.8687\n",
      "Batch-500: NLLLoss=0.6431 | F1Score=0.8688\n",
      "Batch-518: NLLLoss=0.4483 | F1Score=0.8693\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.7050 | Mean F1Score: 0.8742\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c111b4b8ba77497fa4d197ac0c9cf76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.3025 | F1Score=0.9500\n",
      "Batch-100: NLLLoss=0.6605 | F1Score=0.9461\n",
      "Batch-150: NLLLoss=0.3666 | F1Score=0.9395\n",
      "Batch-200: NLLLoss=0.2369 | F1Score=0.9373\n",
      "Batch-250: NLLLoss=0.6647 | F1Score=0.9376\n",
      "Batch-300: NLLLoss=0.2748 | F1Score=0.9348\n",
      "Batch-350: NLLLoss=0.6261 | F1Score=0.9330\n",
      "Batch-400: NLLLoss=0.3212 | F1Score=0.9312\n",
      "Batch-450: NLLLoss=0.4031 | F1Score=0.9295\n",
      "Batch-500: NLLLoss=0.1937 | F1Score=0.9288\n",
      "Batch-518: NLLLoss=0.8106 | F1Score=0.9283\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.3752 | Mean F1Score: 0.9375\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f93f312f99400ca923c995566d552d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.2668 | F1Score=0.9794\n",
      "Batch-100: NLLLoss=0.1096 | F1Score=0.9806\n",
      "Batch-150: NLLLoss=0.2218 | F1Score=0.9819\n",
      "Batch-200: NLLLoss=0.1090 | F1Score=0.9810\n",
      "Batch-250: NLLLoss=0.1786 | F1Score=0.9813\n",
      "Batch-300: NLLLoss=0.1015 | F1Score=0.9807\n",
      "Batch-350: NLLLoss=0.2802 | F1Score=0.9800\n",
      "Batch-400: NLLLoss=0.0584 | F1Score=0.9790\n",
      "Batch-450: NLLLoss=0.1446 | F1Score=0.9776\n",
      "Batch-500: NLLLoss=0.1903 | F1Score=0.9769\n",
      "Batch-518: NLLLoss=0.1844 | F1Score=0.9766\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.1588 | Mean F1Score: 0.9803\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9eed3e930e49be922c34b3c1041f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0286 | F1Score=0.9950\n",
      "Batch-100: NLLLoss=0.0728 | F1Score=0.9950\n",
      "Batch-150: NLLLoss=0.0894 | F1Score=0.9946\n",
      "Batch-200: NLLLoss=0.0543 | F1Score=0.9952\n",
      "Batch-250: NLLLoss=0.0345 | F1Score=0.9949\n",
      "Batch-300: NLLLoss=0.0238 | F1Score=0.9944\n",
      "Batch-350: NLLLoss=0.0826 | F1Score=0.9939\n",
      "Batch-400: NLLLoss=0.0790 | F1Score=0.9939\n",
      "Batch-450: NLLLoss=0.0302 | F1Score=0.9933\n",
      "Batch-500: NLLLoss=0.0811 | F1Score=0.9930\n",
      "Batch-518: NLLLoss=0.1795 | F1Score=0.9930\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0617 | Mean F1Score: 0.9938\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890eab288168453b91f431793a84dc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0123 | F1Score=0.9969\n",
      "Batch-100: NLLLoss=0.0171 | F1Score=0.9978\n",
      "Batch-150: NLLLoss=0.0035 | F1Score=0.9976\n",
      "Batch-200: NLLLoss=0.0185 | F1Score=0.9968\n",
      "Batch-250: NLLLoss=0.0187 | F1Score=0.9968\n",
      "Batch-300: NLLLoss=0.0106 | F1Score=0.9971\n",
      "Batch-350: NLLLoss=0.0215 | F1Score=0.9972\n",
      "Batch-400: NLLLoss=0.0386 | F1Score=0.9973\n",
      "Batch-450: NLLLoss=0.0253 | F1Score=0.9973\n",
      "Batch-500: NLLLoss=0.0080 | F1Score=0.9974\n",
      "Batch-518: NLLLoss=0.0394 | F1Score=0.9974\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0267 | Mean F1Score: 0.9971\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce23394372b64f038381f83a15383eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0046 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0107 | F1Score=0.9981\n",
      "Batch-150: NLLLoss=0.0111 | F1Score=0.9983\n",
      "Batch-200: NLLLoss=0.0153 | F1Score=0.9986\n",
      "Batch-250: NLLLoss=0.0110 | F1Score=0.9984\n",
      "Batch-300: NLLLoss=0.0352 | F1Score=0.9972\n",
      "Batch-350: NLLLoss=0.0421 | F1Score=0.9962\n",
      "Batch-400: NLLLoss=0.0686 | F1Score=0.9949\n",
      "Batch-450: NLLLoss=0.0361 | F1Score=0.9937\n",
      "Batch-500: NLLLoss=0.1789 | F1Score=0.9926\n",
      "Batch-518: NLLLoss=0.1188 | F1Score=0.9923\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0483 | Mean F1Score: 0.9969\n",
      "Patience = 1/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a589ed620a48d39d1d3a5026e83540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0428 | F1Score=0.9878\n",
      "Batch-100: NLLLoss=0.1145 | F1Score=0.9858\n",
      "Batch-150: NLLLoss=0.0379 | F1Score=0.9864\n",
      "Batch-200: NLLLoss=0.0760 | F1Score=0.9886\n",
      "Batch-250: NLLLoss=0.0236 | F1Score=0.9894\n",
      "Batch-300: NLLLoss=0.0136 | F1Score=0.9905\n",
      "Batch-350: NLLLoss=0.0366 | F1Score=0.9909\n",
      "Batch-400: NLLLoss=0.0647 | F1Score=0.9913\n",
      "Batch-450: NLLLoss=0.0574 | F1Score=0.9918\n",
      "Batch-500: NLLLoss=0.0747 | F1Score=0.9923\n",
      "Batch-518: NLLLoss=0.1203 | F1Score=0.9923\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0457 | Mean F1Score: 0.9894\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9668cf3496af40c88542c6a2e8719f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0050 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0137 | F1Score=0.9984\n",
      "Batch-150: NLLLoss=0.0110 | F1Score=0.9985\n",
      "Batch-200: NLLLoss=0.0109 | F1Score=0.9986\n",
      "Batch-250: NLLLoss=0.0048 | F1Score=0.9986\n",
      "Batch-300: NLLLoss=0.0046 | F1Score=0.9987\n",
      "Batch-350: NLLLoss=0.0022 | F1Score=0.9987\n",
      "Batch-400: NLLLoss=0.0082 | F1Score=0.9986\n",
      "Batch-450: NLLLoss=0.0226 | F1Score=0.9984\n",
      "Batch-500: NLLLoss=0.0126 | F1Score=0.9985\n",
      "Batch-518: NLLLoss=0.0034 | F1Score=0.9985\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0139 | Mean F1Score: 0.9985\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4dcd359b374272ba154c0e5712b38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0062 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0027 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0026 | F1Score=0.9994\n",
      "Batch-200: NLLLoss=0.0025 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0034 | F1Score=0.9994\n",
      "Batch-300: NLLLoss=0.0028 | F1Score=0.9994\n",
      "Batch-350: NLLLoss=0.0076 | F1Score=0.9993\n",
      "Batch-400: NLLLoss=0.0041 | F1Score=0.9992\n",
      "Batch-450: NLLLoss=0.0136 | F1Score=0.9985\n",
      "Batch-500: NLLLoss=0.0385 | F1Score=0.9981\n",
      "Batch-518: NLLLoss=0.0048 | F1Score=0.9980\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0126 | Mean F1Score: 0.9991\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ed37a32db74dbd80bd5d2166578611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0065 | F1Score=0.9969\n",
      "Batch-100: NLLLoss=0.0096 | F1Score=0.9969\n",
      "Batch-150: NLLLoss=0.0076 | F1Score=0.9956\n",
      "Batch-200: NLLLoss=0.0647 | F1Score=0.9941\n",
      "Batch-250: NLLLoss=0.0100 | F1Score=0.9942\n",
      "Batch-300: NLLLoss=0.0890 | F1Score=0.9931\n",
      "Batch-350: NLLLoss=0.0532 | F1Score=0.9913\n",
      "Batch-400: NLLLoss=0.0896 | F1Score=0.9894\n",
      "Batch-450: NLLLoss=0.1617 | F1Score=0.9889\n",
      "Batch-500: NLLLoss=0.0528 | F1Score=0.9884\n",
      "Batch-518: NLLLoss=0.0108 | F1Score=0.9885\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0534 | Mean F1Score: 0.9933\n",
      "Patience = 2/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae6c91c989647e685f9bafb6323c582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0180 | F1Score=0.9906\n",
      "Batch-100: NLLLoss=0.0506 | F1Score=0.9895\n",
      "Batch-150: NLLLoss=0.0297 | F1Score=0.9909\n",
      "Batch-200: NLLLoss=0.0038 | F1Score=0.9916\n",
      "Batch-250: NLLLoss=0.0137 | F1Score=0.9922\n",
      "Batch-300: NLLLoss=0.0069 | F1Score=0.9927\n",
      "Batch-350: NLLLoss=0.0268 | F1Score=0.9933\n",
      "Batch-400: NLLLoss=0.0054 | F1Score=0.9934\n",
      "Batch-450: NLLLoss=0.0087 | F1Score=0.9938\n",
      "Batch-500: NLLLoss=0.0238 | F1Score=0.9941\n",
      "Batch-518: NLLLoss=0.0136 | F1Score=0.9942\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0282 | Mean F1Score: 0.9923\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8079fdbe0fd4424b861ab1b0cc49877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0021 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0035 | F1Score=0.9984\n",
      "Batch-150: NLLLoss=0.0155 | F1Score=0.9981\n",
      "Batch-200: NLLLoss=0.0020 | F1Score=0.9981\n",
      "Batch-250: NLLLoss=0.0038 | F1Score=0.9985\n",
      "Batch-300: NLLLoss=0.0024 | F1Score=0.9984\n",
      "Batch-350: NLLLoss=0.0008 | F1Score=0.9983\n",
      "Batch-400: NLLLoss=0.0040 | F1Score=0.9984\n",
      "Batch-450: NLLLoss=0.0011 | F1Score=0.9984\n",
      "Batch-500: NLLLoss=0.0028 | F1Score=0.9984\n",
      "Batch-518: NLLLoss=0.0168 | F1Score=0.9984\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0086 | Mean F1Score: 0.9985\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787a3ae3634844d9a76d9b26bb566300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0007 | F1Score=0.9975\n",
      "Batch-100: NLLLoss=0.0042 | F1Score=0.9984\n",
      "Batch-150: NLLLoss=0.0013 | F1Score=0.9990\n",
      "Batch-200: NLLLoss=0.0015 | F1Score=0.9991\n",
      "Batch-250: NLLLoss=0.0006 | F1Score=0.9991\n",
      "Batch-300: NLLLoss=0.0007 | F1Score=0.9991\n",
      "Batch-350: NLLLoss=0.0009 | F1Score=0.9992\n",
      "Batch-400: NLLLoss=0.0012 | F1Score=0.9993\n",
      "Batch-450: NLLLoss=0.0006 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0008 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0006 | F1Score=0.9995\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0035 | Mean F1Score: 0.9988\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ebd5d0b10344db8dac1b006ee99925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0006 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0009 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0006 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0009 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0002 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0014 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0003 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0010 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4ee06eef0a4f89824bbfed515e713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0003 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0004 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0004 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0004 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0005 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f26d3553dc4ec9948ce0772ba2e2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0003 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0002 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0008 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0004 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0004\n",
      "Best F1Score      : 1.0000\n",
      "Training duration : 15.418 minutes.\n",
      "Training date     : 2022-10-11 10:29:38.207216+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2124012 ,  6.3284063 ,  2.964808  , ..., -5.671381  ,\n",
       "        -4.2607493 , -2.058475  ],\n",
       "       [-1.4116617 ,  2.5162573 , -3.7762024 , ...,  0.9673172 ,\n",
       "         3.3609328 ,  3.8394072 ],\n",
       "       [-3.9112258 , -4.049633  ,  1.1982565 , ..., -0.9925512 ,\n",
       "         8.227758  , -1.554897  ],\n",
       "       ...,\n",
       "       [-4.060899  , -0.08798943, -1.415589  , ...,  5.170133  ,\n",
       "         0.24201673,  0.99293464],\n",
       "       [ 6.7224393 , -6.941244  , -7.807855  , ...,  2.5878234 ,\n",
       "         1.915535  , -0.7668631 ],\n",
       "       [ 3.7939324 ,  3.4597893 ,  1.0748373 , ..., -4.696857  ,\n",
       "         0.9561877 ,  0.3372371 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.2058, -0.0705, -0.2561,  ..., -0.5010,  0.4840,  0.0193],\n",
       "                      [ 0.1387, -0.2229, -0.2651,  ...,  0.0125,  0.0227,  0.4425],\n",
       "                      [-0.0201,  0.0241,  0.2228,  ..., -0.1353,  0.2961,  0.1677],\n",
       "                      ...,\n",
       "                      [ 0.2259,  0.2605,  0.0422,  ..., -0.0788,  0.4095, -0.1950],\n",
       "                      [ 0.1513, -0.1694,  0.0192,  ...,  0.2640,  0.2139, -0.0019],\n",
       "                      [-0.0737, -0.4209,  0.1475,  ...,  0.0094,  0.3572, -0.2717]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.0160,  0.1850,  0.0919,  ...,  0.1170,  0.0139, -0.1057],\n",
       "                      [ 0.0130,  0.0706,  0.0237,  ..., -0.0289,  0.0668,  0.0322],\n",
       "                      [-0.3689, -0.0189,  0.1620,  ..., -0.0930,  0.1639, -0.0910],\n",
       "                      ...,\n",
       "                      [-0.0580,  0.0605, -0.0929,  ..., -0.0510,  0.0784, -0.0655],\n",
       "                      [ 0.1057, -0.1190,  0.0582,  ...,  0.0732, -0.0115,  0.1211],\n",
       "                      [ 0.0796,  0.0580, -0.1438,  ..., -0.2601,  0.1970,  0.0882]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([ 1.6134e-01,  1.2324e-01,  1.1523e-01,  9.3700e-02,  4.6188e-01,\n",
       "                       3.8235e-02,  2.9879e-01,  2.3101e-01,  5.6740e-02,  1.0166e-01,\n",
       "                       2.9734e-01,  5.5671e-02,  2.1365e-01,  4.3484e-01,  2.9748e-01,\n",
       "                       1.5444e-01,  2.2031e-01,  2.5688e-01,  3.3386e-01,  2.2751e-01,\n",
       "                       6.5549e-02,  2.0497e-01, -5.2498e-03,  1.3254e-01,  2.4552e-01,\n",
       "                       2.1744e-01,  5.5076e-02,  2.3208e-01,  2.1943e-01,  2.3130e-01,\n",
       "                       2.9790e-01,  6.5253e-02,  1.3052e-01,  2.0702e-01,  1.5579e-01,\n",
       "                       1.7453e-01,  2.1741e-01, -7.9719e-03,  4.9235e-01,  8.1142e-03,\n",
       "                       2.5208e-01,  2.4635e-01,  2.5443e-01,  3.4141e-01,  3.0302e-01,\n",
       "                       1.7303e-01, -1.7822e-02,  6.3132e-02,  8.3239e-03,  8.7450e-02,\n",
       "                       2.9870e-01,  2.3453e-01,  1.7112e-01,  2.3732e-01,  1.3714e-01,\n",
       "                      -2.1326e-02,  2.4397e-01,  4.8664e-02,  3.1720e-01,  1.4604e-01,\n",
       "                       3.1712e-01,  1.5885e-01,  2.2950e-01,  3.8956e-01,  1.8967e-01,\n",
       "                       2.6139e-01,  1.6731e-01,  1.1062e-01,  1.9402e-01,  2.6376e-01,\n",
       "                       4.3511e-02,  2.6112e-01,  1.1099e-01,  2.2366e-01,  2.9064e-01,\n",
       "                       2.5187e-01,  1.9732e-01,  2.1631e-02,  2.3805e-01,  2.5417e-01,\n",
       "                       1.6127e-01,  3.4282e-01,  2.1111e-01,  2.7718e-01,  2.0402e-01,\n",
       "                       1.3768e-01,  8.8563e-02,  9.1138e-02,  1.7257e-01,  8.9544e-02,\n",
       "                       1.1006e-01,  1.6036e-01, -4.4997e-02,  2.3068e-01,  2.0766e-01,\n",
       "                       5.3999e-02,  1.2811e-01,  5.1591e-02,  4.3075e-02,  2.2133e-01,\n",
       "                       5.7982e-02,  2.3620e-01,  2.8334e-01,  2.8092e-01,  7.4443e-02,\n",
       "                       3.4512e-01,  1.4356e-01,  1.2081e-01,  1.1243e-01,  2.4147e-01,\n",
       "                       2.4276e-01,  7.9165e-02,  4.1641e-03,  2.0658e-01,  2.1339e-01,\n",
       "                       1.4971e-01,  1.3140e-01,  1.4678e-01,  2.0792e-01,  1.8600e-01,\n",
       "                       7.9986e-02,  1.4234e-01,  2.0961e-01,  9.2752e-02,  2.1926e-01,\n",
       "                       3.7223e-01,  6.5725e-02,  2.6273e-01, -1.3694e-02,  1.7693e-02,\n",
       "                      -9.5646e-02,  6.9059e-02,  5.0833e-02, -9.8718e-02, -2.1491e-02,\n",
       "                       1.2012e-01,  1.2727e-02,  5.1306e-05, -4.3381e-02,  8.4103e-02,\n",
       "                      -3.2054e-03, -1.5929e-02,  2.2361e-02, -5.8979e-02,  6.3610e-03,\n",
       "                      -2.1072e-02, -1.6490e-02,  1.2560e-01,  1.3057e-02,  1.5866e-02,\n",
       "                       4.0491e-02, -8.9736e-02,  5.5502e-02, -5.1867e-02, -1.1634e-03,\n",
       "                      -1.5584e-02, -4.5590e-02, -3.8706e-02, -7.5770e-02, -1.7592e-02,\n",
       "                       3.2018e-02, -9.4442e-02, -4.1784e-02, -1.2675e-02,  5.6553e-02,\n",
       "                      -4.5003e-02,  2.9849e-02,  5.5433e-03,  5.5985e-02,  1.3921e-01,\n",
       "                       4.0364e-03, -1.1810e-01,  7.7128e-02, -1.4697e-01, -1.0515e-01,\n",
       "                      -5.0217e-02,  1.1370e-01, -7.3161e-02, -3.1074e-02, -4.1973e-02,\n",
       "                      -5.6062e-02,  6.9187e-02, -6.4315e-02,  9.6746e-03,  6.5169e-02,\n",
       "                      -2.5623e-02,  5.9644e-03, -8.9259e-02, -7.0487e-02,  8.2506e-03,\n",
       "                      -6.7853e-02, -1.6917e-01, -2.9452e-02,  8.9834e-02,  1.5142e-02,\n",
       "                       2.5856e-03,  1.2437e-01,  2.5299e-02,  3.8915e-02,  4.6056e-02,\n",
       "                      -1.8118e-03,  3.6416e-02, -9.1434e-02,  3.5219e-02,  5.4174e-02,\n",
       "                       1.0753e-01, -5.3929e-02,  8.6205e-02, -4.9492e-02, -9.1940e-02,\n",
       "                       7.3979e-03,  4.1191e-02, -1.5813e-02,  5.1795e-02,  4.5447e-03,\n",
       "                       6.8343e-02,  4.9196e-02,  3.3232e-02, -5.1079e-02,  6.2923e-02,\n",
       "                      -7.4721e-02, -1.0592e-01, -1.3785e-01, -4.0262e-02,  1.5630e-02,\n",
       "                      -5.9148e-02,  1.2855e-01, -1.2437e-02,  9.8118e-02, -4.2825e-02,\n",
       "                       1.9471e-02,  4.3146e-02,  4.5739e-02, -7.7712e-02,  1.1764e-01,\n",
       "                       5.6875e-02,  5.1552e-02,  1.6355e-01, -1.4376e-02,  2.6038e-02,\n",
       "                       5.2875e-02,  2.3579e-02,  5.6233e-02, -9.3329e-03, -3.2471e-02,\n",
       "                      -1.0276e-01,  2.3346e-02, -2.6689e-02, -6.8774e-02,  2.8598e-02,\n",
       "                       2.1435e-02, -7.4921e-03, -4.3772e-02,  2.4876e-02, -5.3819e-02,\n",
       "                      -8.6878e-03,  1.5200e-01,  5.6759e-03, -1.1493e-03, -6.8541e-02,\n",
       "                      -9.9127e-02, -7.6238e-02, -3.4990e-02, -3.8119e-02,  4.1880e-02,\n",
       "                       9.0538e-02, -1.1806e-01,  5.3507e-02,  4.4436e-02,  7.1213e-02,\n",
       "                       1.4499e-01,  3.3592e-02,  1.0785e-01,  1.7339e-02,  1.2254e-01,\n",
       "                      -3.0363e-02,  1.3591e-01,  2.7134e-02,  1.5745e-02,  1.0916e-01,\n",
       "                      -3.5551e-02, -3.2400e-02, -6.6737e-02,  4.7375e-02,  2.3371e-02,\n",
       "                      -8.6811e-02, -2.7110e-02, -1.6537e-03, -2.0311e-03,  1.1334e-01,\n",
       "                      -3.1765e-02,  7.1766e-03,  5.0370e-02,  4.6073e-02,  1.3524e-03,\n",
       "                      -1.4231e-02,  9.6971e-03,  2.9293e-02, -8.1309e-02, -4.9001e-02,\n",
       "                       7.0803e-02,  1.8433e-03,  1.2379e-01, -2.4153e-02, -3.7141e-02,\n",
       "                      -1.8533e-01,  3.9448e-02, -7.1852e-02, -1.2012e-01, -8.2050e-02,\n",
       "                       6.9597e-02,  1.2360e-02, -9.3724e-03, -1.8888e-02,  1.3017e-03,\n",
       "                       1.1692e-01,  1.4572e-02,  7.6848e-02, -9.0844e-02, -8.3601e-02,\n",
       "                      -1.3744e-01,  4.2852e-02, -1.5746e-02,  1.2194e-03,  8.5302e-02,\n",
       "                      -6.8087e-02,  5.6099e-02,  2.9366e-02, -1.2992e-02,  7.2556e-02,\n",
       "                      -2.2051e-02, -2.6959e-02, -7.1026e-02,  1.3403e-01, -1.0960e-02,\n",
       "                      -5.4263e-02, -3.0545e-02, -1.2430e-01,  1.2853e-02,  3.1887e-02,\n",
       "                      -1.1583e-01, -1.0671e-02, -1.1279e-01, -5.3133e-02, -2.8114e-03,\n",
       "                      -6.6902e-02,  8.7898e-03,  1.1907e-01, -4.4679e-02,  1.2596e-02,\n",
       "                      -5.3937e-02,  6.9815e-02, -8.0219e-02, -2.0590e-02, -7.7385e-02,\n",
       "                      -7.5600e-02, -5.3600e-02, -1.7115e-01,  1.2387e-01, -9.8251e-02,\n",
       "                      -6.5675e-02,  4.8010e-02, -4.9659e-02, -2.2658e-01,  6.8395e-02,\n",
       "                       1.0205e-01,  1.0567e-01,  1.8067e-03, -4.7964e-02, -1.0917e-01,\n",
       "                       6.3638e-02, -1.0090e-02,  5.1807e-02,  7.6180e-02, -6.8942e-02,\n",
       "                       6.6373e-02, -1.1251e-01,  5.7775e-02, -6.1614e-02, -9.7900e-02,\n",
       "                      -5.5802e-02, -2.9209e-02, -2.8468e-02,  8.2590e-02,  2.1197e-01,\n",
       "                      -1.1287e-01,  8.8353e-02,  5.1212e-02,  3.8213e-01,  1.3403e-01,\n",
       "                       2.0685e-01,  3.2180e-01,  5.4083e-02,  1.2971e-01,  2.6050e-01,\n",
       "                       2.0490e-01,  1.8018e-01,  3.5693e-01,  5.3914e-01,  2.3561e-01,\n",
       "                       3.1447e-01,  2.6461e-01,  2.3634e-01,  1.8381e-01,  2.4040e-02,\n",
       "                       1.5136e-01,  1.1511e-01,  1.3741e-01,  2.6488e-01,  1.5214e-01,\n",
       "                       1.1880e-01,  3.1894e-01,  1.1652e-01,  2.2570e-01,  2.3117e-01,\n",
       "                       1.4010e-01,  1.3934e-01,  3.3965e-01,  1.5274e-01,  1.9121e-01,\n",
       "                       2.4817e-01, -2.8095e-02,  3.5996e-01,  5.7984e-03,  4.2070e-01,\n",
       "                       6.5724e-02,  2.9320e-01,  3.6755e-01,  2.6234e-01,  1.3720e-01,\n",
       "                       1.0790e-01,  1.0549e-01,  2.6473e-02,  2.9567e-02,  2.6361e-01,\n",
       "                       3.8542e-02,  1.1833e-01,  1.3651e-01,  1.9307e-01,  3.9275e-02,\n",
       "                       1.6267e-01, -9.5376e-04,  3.4538e-01,  1.1153e-01,  8.9255e-02,\n",
       "                       8.7288e-02,  2.6048e-01,  2.4510e-01,  5.9865e-02,  2.2144e-01,\n",
       "                       1.0634e-01,  3.1105e-01,  2.5297e-01,  3.4524e-01,  6.8018e-02,\n",
       "                       3.2770e-01,  2.0533e-02,  2.1977e-01,  2.5448e-01,  2.7101e-01,\n",
       "                       2.0809e-01,  8.9572e-02,  3.6148e-01,  2.1931e-01,  2.3737e-01,\n",
       "                       2.7346e-01,  9.2081e-02,  2.6402e-01,  1.8683e-01, -2.0482e-02,\n",
       "                       1.7484e-01,  9.3185e-03,  2.2443e-01,  2.8174e-03,  1.3270e-01,\n",
       "                       1.7089e-01,  1.9996e-01,  2.7743e-01,  1.0502e-01,  9.2420e-02,\n",
       "                       1.5712e-01,  1.0984e-01,  2.1800e-01,  2.0808e-01,  2.6258e-01,\n",
       "                       9.5117e-02,  1.9188e-01,  3.3679e-01,  2.0217e-01,  2.0373e-01,\n",
       "                       2.3349e-01,  1.0563e-01,  1.3819e-01,  3.5970e-01,  2.5343e-01,\n",
       "                      -5.2815e-02,  2.4322e-01,  2.5892e-01,  1.8586e-01,  1.6953e-01,\n",
       "                       1.3047e-01, -5.1415e-02,  1.0713e-01,  3.2046e-01,  1.8061e-01,\n",
       "                       8.5017e-02,  2.6534e-01,  7.5041e-02,  2.6422e-01,  4.0015e-01,\n",
       "                       2.5094e-01,  2.5060e-02])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([ 2.2760e-01,  1.6873e-01,  4.5351e-04,  2.2657e-01,  4.0478e-01,\n",
       "                       2.1791e-01,  1.4043e-01,  1.8261e-01, -2.7258e-02,  8.8247e-02,\n",
       "                       2.3851e-01,  7.6360e-02,  4.9832e-02,  3.8190e-01,  3.7049e-01,\n",
       "                       2.2028e-01,  1.2723e-01,  2.1377e-01,  2.5154e-01,  2.2459e-01,\n",
       "                       1.1085e-02,  1.5979e-01,  1.1982e-01,  1.3398e-01,  1.8292e-01,\n",
       "                       2.0930e-01,  8.4988e-02,  1.9787e-01,  1.5029e-01,  3.3085e-01,\n",
       "                       2.4742e-01,  2.0188e-01,  1.8568e-01,  2.1170e-01,  1.0141e-01,\n",
       "                       1.9836e-01,  1.1468e-01, -3.8245e-02,  3.3291e-01,  1.1731e-01,\n",
       "                       2.7806e-01,  1.9004e-01,  1.6152e-01,  3.2008e-01,  1.9857e-01,\n",
       "                       5.9012e-02,  1.2440e-01,  1.1416e-01, -9.2399e-02,  7.8195e-02,\n",
       "                       1.3355e-01,  2.8440e-01,  7.2995e-02,  1.4390e-01,  1.7165e-01,\n",
       "                      -7.4502e-02,  5.3826e-02, -5.2726e-02,  1.6041e-01,  1.6776e-01,\n",
       "                       1.9221e-01,  7.9693e-02,  3.5116e-01,  3.7468e-01,  1.6174e-01,\n",
       "                       3.6507e-01,  2.8044e-01,  1.6887e-01,  2.1838e-01,  3.0655e-01,\n",
       "                       1.2803e-01,  3.2568e-01,  9.4598e-03,  2.0698e-01,  2.7968e-01,\n",
       "                       2.8299e-01,  1.4714e-01, -3.3440e-02,  2.4091e-01,  1.1482e-01,\n",
       "                       1.3341e-01,  2.0433e-01,  1.8746e-01,  1.8931e-01,  1.5831e-01,\n",
       "                      -3.4017e-03,  1.3173e-01,  9.2521e-02,  1.4113e-01,  2.2041e-01,\n",
       "                       1.2716e-01,  1.8477e-01,  3.1987e-02,  2.3845e-01,  2.5459e-01,\n",
       "                       6.4726e-02,  1.8262e-01,  1.0115e-01,  6.1674e-02,  2.0359e-01,\n",
       "                       2.0064e-01,  2.4730e-01,  1.4494e-01,  2.6380e-01,  2.3632e-01,\n",
       "                       3.4340e-01,  2.9133e-01,  1.5286e-01,  1.3627e-01,  2.5443e-01,\n",
       "                       2.7681e-01,  9.7515e-02,  1.4041e-01,  1.8461e-01,  1.8873e-01,\n",
       "                       1.3373e-01,  2.1675e-01,  1.0664e-01,  1.6667e-01,  1.9776e-01,\n",
       "                      -1.3011e-02,  3.4229e-02,  1.4584e-01,  1.7492e-01,  2.4377e-01,\n",
       "                       3.1161e-01,  1.6450e-01,  1.1285e-01,  2.6679e-02, -1.0682e-02,\n",
       "                      -7.3365e-02,  4.9677e-03,  3.0466e-02, -5.9768e-02,  3.3157e-02,\n",
       "                      -7.0872e-03,  2.3497e-03,  7.8747e-03, -5.9762e-02, -6.9412e-02,\n",
       "                       6.6026e-04,  6.2653e-02, -1.6678e-02,  1.8289e-03, -9.7805e-02,\n",
       "                      -3.7462e-02,  3.8662e-02,  2.2611e-02,  4.5445e-02,  1.0332e-02,\n",
       "                      -6.9812e-03,  5.4179e-02,  8.9143e-02,  4.1380e-02,  1.1209e-01,\n",
       "                       9.3315e-02,  7.2474e-03,  9.9283e-02, -1.6596e-02,  4.1818e-02,\n",
       "                      -6.1956e-03, -8.9264e-02,  1.1009e-02,  1.0454e-01,  7.2347e-04,\n",
       "                       1.4621e-02, -3.1098e-02, -8.5081e-02,  3.1256e-02, -2.4760e-02,\n",
       "                      -9.0137e-02, -9.8062e-02,  9.0574e-02, -8.1535e-02, -8.2958e-03,\n",
       "                      -7.7395e-05,  3.5316e-02,  1.2430e-01, -3.0671e-02, -2.7278e-02,\n",
       "                      -1.1252e-02, -3.1870e-02, -2.0983e-02,  6.6469e-02, -2.3635e-02,\n",
       "                       4.6793e-02,  5.8390e-02,  3.8155e-03, -6.7353e-02, -4.5795e-02,\n",
       "                       1.9861e-02,  8.9929e-02, -3.5127e-02,  1.1002e-01, -6.5640e-02,\n",
       "                       7.8556e-04,  1.9138e-03, -2.4851e-02, -4.2709e-02, -3.1506e-03,\n",
       "                      -2.3589e-02,  1.5973e-02,  2.0704e-02,  3.6719e-02,  2.1824e-02,\n",
       "                       1.1653e-02, -8.2143e-04,  8.4353e-02, -2.1818e-02, -8.1431e-03,\n",
       "                       1.7890e-02, -2.1113e-02, -2.7405e-02, -1.6167e-01, -9.3130e-02,\n",
       "                      -1.7769e-01,  1.5607e-02,  1.3241e-02,  9.8308e-02,  7.4174e-02,\n",
       "                       1.6809e-02, -2.9059e-02,  2.4440e-02,  5.7629e-02,  1.2818e-01,\n",
       "                       5.1288e-02,  3.7959e-02,  5.9524e-02,  1.5201e-02, -9.3465e-02,\n",
       "                       4.1067e-03,  5.8471e-02,  2.0743e-03,  7.5466e-03, -1.4540e-02,\n",
       "                       4.0637e-03, -3.0948e-02,  6.5846e-02,  1.2921e-01, -2.2127e-02,\n",
       "                       1.0668e-02,  5.8772e-02,  9.8467e-02, -7.5033e-02, -6.3730e-02,\n",
       "                      -1.4580e-02,  4.4307e-02,  1.0061e-01, -7.3457e-02,  5.3152e-02,\n",
       "                      -3.8213e-02,  3.5469e-03, -1.0798e-01, -4.1101e-02, -6.3946e-02,\n",
       "                      -3.9320e-02, -1.5752e-02, -1.6166e-01,  8.2837e-02, -1.2929e-01,\n",
       "                      -1.0167e-01,  4.4309e-02, -1.5644e-01, -7.5155e-02,  4.1371e-02,\n",
       "                      -1.4483e-02, -9.3132e-02, -6.5008e-02,  8.0411e-02,  1.1401e-01,\n",
       "                       1.3904e-01,  8.4204e-02, -8.2316e-02, -4.3598e-04,  8.5886e-02,\n",
       "                      -2.0017e-02,  1.1579e-01,  1.3930e-01,  7.7540e-02, -4.9241e-02,\n",
       "                       6.0221e-02,  2.2320e-03,  2.5896e-02,  3.1118e-03,  8.0269e-02,\n",
       "                       8.5455e-02,  8.5181e-02,  1.4744e-01, -2.4139e-02,  4.5945e-02,\n",
       "                       8.3557e-02,  4.4738e-02, -1.0417e-01, -2.5873e-02,  2.1651e-02,\n",
       "                      -1.5422e-02,  3.0736e-02,  8.2040e-02,  5.3755e-02, -1.2815e-02,\n",
       "                      -5.2979e-02,  7.4341e-02,  3.9965e-02, -1.0646e-01, -9.6890e-02,\n",
       "                      -1.6613e-02, -3.5101e-02,  5.9126e-02, -1.0180e-01, -2.5420e-02,\n",
       "                       1.0390e-01,  4.6133e-02, -1.1000e-01,  8.2269e-02, -4.5436e-03,\n",
       "                       9.3967e-02,  2.8559e-02, -9.3580e-03,  3.9275e-02,  9.8819e-02,\n",
       "                       5.8386e-02,  1.0363e-02, -3.0056e-02, -5.5986e-02,  6.5234e-02,\n",
       "                      -5.7672e-02,  1.3864e-01,  2.0346e-02,  6.6422e-02, -1.5101e-02,\n",
       "                       6.1096e-02,  1.2719e-01,  2.3828e-01,  1.4611e-03, -1.5836e-03,\n",
       "                       3.6863e-02, -3.5375e-02, -5.0381e-02, -5.3025e-02, -2.7805e-03,\n",
       "                      -8.5559e-02,  6.2062e-03,  1.8971e-03,  7.3214e-02, -6.9015e-02,\n",
       "                      -7.0036e-02,  1.7242e-02,  5.8643e-02,  3.3867e-02, -2.7844e-02,\n",
       "                      -4.3774e-02, -8.5809e-02, -1.4186e-01,  1.1617e-02, -7.4005e-02,\n",
       "                      -3.3813e-02,  1.4843e-02, -1.2689e-01,  2.9349e-02, -7.5189e-02,\n",
       "                       4.1745e-02,  2.0766e-02, -6.9526e-02, -4.3657e-02,  1.1893e-01,\n",
       "                      -9.0416e-02,  1.2889e-01, -7.1073e-02, -1.1152e-01, -1.0293e-01,\n",
       "                       5.4045e-02, -3.4714e-02,  5.2630e-03,  1.8105e-01, -5.4869e-02,\n",
       "                       1.7262e-01, -3.6826e-02, -7.4361e-03, -8.7886e-02, -6.1260e-02,\n",
       "                       4.2530e-02,  2.0190e-02, -2.5602e-03,  5.5168e-02,  1.8881e-01,\n",
       "                       4.9856e-02,  2.6397e-02,  9.5223e-02,  3.3972e-01,  1.2205e-01,\n",
       "                       2.9151e-01,  2.3033e-01,  1.0750e-01,  2.5218e-01,  2.5674e-01,\n",
       "                       1.4662e-01,  1.8742e-01,  3.7130e-01,  3.6670e-01,  2.5727e-01,\n",
       "                       1.3386e-01,  2.0665e-01,  2.5427e-01,  2.5146e-01, -3.4718e-02,\n",
       "                       2.6052e-01,  1.2668e-01,  1.5875e-01,  2.6538e-01,  1.8226e-01,\n",
       "                       8.4620e-02,  2.4780e-01,  2.3797e-01,  2.7136e-01,  2.0170e-01,\n",
       "                       1.7417e-01,  1.8069e-01,  1.8122e-01,  1.0967e-01,  1.8388e-01,\n",
       "                       1.7227e-01,  6.8286e-02,  4.0553e-01,  3.7197e-02,  2.9965e-01,\n",
       "                       1.4600e-01,  1.6822e-01,  3.6794e-01,  1.4865e-01,  1.4342e-01,\n",
       "                       5.7174e-02,  1.1977e-01,  8.9816e-03,  2.5368e-02,  2.2874e-01,\n",
       "                       4.0150e-01,  6.8545e-02,  1.1808e-01,  3.3635e-01,  1.0513e-02,\n",
       "                       2.0132e-02, -9.1700e-02,  2.5792e-01,  1.0291e-01,  1.0252e-01,\n",
       "                       1.0048e-01,  2.2036e-01,  2.4015e-01,  1.5239e-01,  3.3232e-01,\n",
       "                       1.9055e-01,  2.0258e-01,  2.1320e-01,  2.1378e-01,  1.6432e-02,\n",
       "                       4.5144e-01, -4.2911e-02,  2.1854e-01,  2.1203e-01,  3.1319e-01,\n",
       "                       1.6747e-01,  8.2184e-02,  2.6834e-01,  1.4314e-01,  1.2145e-01,\n",
       "                       3.1918e-01,  8.6285e-02,  2.9009e-01,  7.0762e-02,  1.6609e-01,\n",
       "                       1.3809e-01,  8.0881e-02,  3.2200e-01,  9.0051e-02,  5.0709e-02,\n",
       "                       7.8207e-02, -3.7761e-02,  3.1347e-01,  1.5690e-01,  4.2000e-03,\n",
       "                       3.9737e-02, -2.0535e-02,  2.1077e-01,  1.8904e-01,  2.6369e-01,\n",
       "                       2.6314e-01,  2.0614e-01,  2.4759e-01,  2.7819e-01,  2.9271e-01,\n",
       "                       2.8204e-01,  4.5321e-02,  1.3814e-01,  1.0239e-01,  1.7783e-01,\n",
       "                       2.0433e-01,  1.2267e-01,  2.4379e-01,  1.8679e-01,  1.2470e-01,\n",
       "                       1.3866e-01,  1.0367e-01,  1.1716e-01,  2.6325e-01, -1.2311e-02,\n",
       "                       1.4547e-01,  3.2862e-01,  1.0698e-01,  2.0993e-01,  3.7336e-01,\n",
       "                       1.9943e-01,  2.1176e-01])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.1265,  0.1279, -0.0588,  ..., -0.1920,  0.1326,  0.1688],\n",
       "                      [-0.6710, -0.3097,  0.4399,  ..., -0.4120, -0.0924, -0.2048],\n",
       "                      [ 0.0802, -0.0058, -0.0703,  ..., -0.0022,  0.2552, -0.1021],\n",
       "                      ...,\n",
       "                      [ 0.0778, -0.3521,  0.1327,  ..., -0.0508, -0.1336, -0.0544],\n",
       "                      [ 0.0195, -0.3800, -0.4485,  ...,  0.2456,  0.1166,  0.2723],\n",
       "                      [ 0.2720, -0.1188,  0.1735,  ..., -0.0885,  0.0106,  0.1067]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0526, -0.2675, -0.0686,  ...,  0.0984, -0.0263,  0.0861],\n",
       "                      [ 0.0091, -0.1020,  0.1930,  ..., -0.0716,  0.0403, -0.1558],\n",
       "                      [-0.1414, -0.0665,  0.1027,  ..., -0.0481,  0.0953,  0.2413],\n",
       "                      ...,\n",
       "                      [-0.0244, -0.0225,  0.1608,  ..., -0.0545,  0.0232,  0.1241],\n",
       "                      [-0.1200, -0.0109, -0.0034,  ...,  0.2317,  0.0251,  0.0128],\n",
       "                      [-0.1880,  0.0592,  0.1128,  ...,  0.0403,  0.0717,  0.1479]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0857,  0.1941,  0.0981,  0.0400,  0.2775,  0.1836,  0.1317,  0.1026,\n",
       "                       0.2356,  0.1415,  0.2273,  0.1352,  0.2032,  0.1708,  0.1957,  0.2236,\n",
       "                       0.0777,  0.0408,  0.1024,  0.0567,  0.2192,  0.0278,  0.2367, -0.0816,\n",
       "                       0.0130,  0.1114, -0.0783,  0.1518,  0.1233,  0.2777,  0.2055, -0.0436,\n",
       "                       0.1945,  0.1896,  0.1767,  0.0503,  0.0639,  0.0834,  0.3529,  0.1153,\n",
       "                       0.2313,  0.3860,  0.2471,  0.2007,  0.3340,  0.1833,  0.2181,  0.1879,\n",
       "                       0.1421,  0.1141,  0.0759,  0.1738,  0.1136,  0.1527,  0.1108,  0.1546,\n",
       "                       0.0950,  0.0548, -0.2008,  0.1903,  0.0825,  0.2865,  0.2375,  0.2169,\n",
       "                       0.2320,  0.1996,  0.0793,  0.0971,  0.2326,  0.1179,  0.0270,  0.0922,\n",
       "                       0.1139,  0.1157,  0.1109,  0.0800,  0.2811,  0.2842,  0.2686,  0.1928,\n",
       "                      -0.0609,  0.0886,  0.1107,  0.2436,  0.0048,  0.3047,  0.2394,  0.2202,\n",
       "                       0.0440, -0.0151,  0.0695,  0.1513,  0.3228,  0.2465,  0.1284,  0.2855,\n",
       "                       0.1618,  0.0711,  0.0241,  0.1172,  0.2018,  0.2205,  0.1849,  0.0844,\n",
       "                       0.1959,  0.0362,  0.1110,  0.4227,  0.1797,  0.0574,  0.0058,  0.1278,\n",
       "                       0.4398,  0.0558,  0.0813,  0.0017,  0.1043,  0.1531,  0.1718,  0.1676,\n",
       "                       0.1886,  0.0477,  0.0991,  0.2610,  0.2546,  0.3447, -0.0807,  0.1537,\n",
       "                      -0.0095, -0.0243,  0.0520, -0.1433,  0.0458,  0.0595,  0.0512, -0.0458,\n",
       "                      -0.0075, -0.0546,  0.0137,  0.1281,  0.0231,  0.1018,  0.0353, -0.0279,\n",
       "                       0.0096,  0.0540, -0.0571,  0.0895, -0.0581, -0.0706,  0.0090, -0.0171,\n",
       "                      -0.0650,  0.1098,  0.0182, -0.1241, -0.0692,  0.0406, -0.0501,  0.0046,\n",
       "                       0.1213,  0.0540,  0.0826,  0.0811,  0.0206,  0.0194, -0.0597, -0.0223,\n",
       "                      -0.0128,  0.0139,  0.0007,  0.0646, -0.0008, -0.0303, -0.1532,  0.1068,\n",
       "                      -0.0814, -0.0296, -0.1307,  0.0735, -0.0379,  0.0044, -0.0516,  0.1023,\n",
       "                      -0.0802, -0.0286,  0.0191,  0.1452, -0.0129,  0.1339, -0.0310,  0.0147,\n",
       "                       0.0780,  0.0935, -0.0257, -0.0018, -0.0373, -0.0750, -0.0215,  0.0725,\n",
       "                       0.0029, -0.0355, -0.0368, -0.0341, -0.0273, -0.0324,  0.0978, -0.1620,\n",
       "                      -0.1054,  0.0076, -0.0445,  0.0218, -0.0160,  0.0381,  0.0075,  0.0198,\n",
       "                       0.0336,  0.0457, -0.0118, -0.0311, -0.0030,  0.0626,  0.0432,  0.1457,\n",
       "                      -0.0897, -0.1244,  0.0088, -0.0596, -0.0097,  0.1180,  0.0213, -0.0601,\n",
       "                      -0.0363,  0.0305,  0.0164,  0.0466, -0.0934,  0.0163, -0.0863, -0.0038,\n",
       "                      -0.0196, -0.0564, -0.0560,  0.0759,  0.0582, -0.0013, -0.1795,  0.0730,\n",
       "                      -0.0732, -0.1855, -0.0930, -0.0517,  0.0220, -0.0719,  0.0374, -0.0129,\n",
       "                      -0.0956,  0.0392, -0.0717,  0.0360,  0.1455,  0.0674, -0.1324,  0.0050,\n",
       "                      -0.0948, -0.1037,  0.0773, -0.0608,  0.0135, -0.0189,  0.0276,  0.0898,\n",
       "                      -0.0191, -0.0656,  0.0216, -0.0465,  0.0427, -0.0842,  0.0425, -0.1205,\n",
       "                      -0.0310,  0.0159,  0.0518, -0.0547,  0.0816, -0.1387,  0.0501,  0.0663,\n",
       "                      -0.1090, -0.0197,  0.0140,  0.0154,  0.0833, -0.0306,  0.0359, -0.0931,\n",
       "                      -0.0529, -0.0731,  0.0312,  0.0348, -0.0544, -0.0362, -0.1199,  0.0367,\n",
       "                      -0.1624, -0.0424,  0.1606,  0.0634, -0.0353, -0.0805, -0.0517,  0.0300,\n",
       "                       0.0913,  0.0961,  0.0574, -0.0306, -0.0745, -0.1058, -0.0050,  0.0063,\n",
       "                      -0.0441, -0.0114, -0.1096,  0.1761,  0.0202, -0.0563, -0.0573,  0.0339,\n",
       "                      -0.0486, -0.0725, -0.0846, -0.0778,  0.0255, -0.0102, -0.0500,  0.0127,\n",
       "                      -0.1539,  0.0740, -0.0220, -0.1406, -0.0371, -0.0039, -0.0625, -0.0878,\n",
       "                      -0.0139, -0.0543,  0.0523, -0.0228,  0.1343, -0.0628, -0.0628,  0.0633,\n",
       "                      -0.0432, -0.1363, -0.0875, -0.0132, -0.1420, -0.0968, -0.0112,  0.1783,\n",
       "                       0.0240, -0.0166,  0.0218,  0.0223, -0.0123, -0.0109, -0.0670, -0.1020,\n",
       "                      -0.0658,  0.0160, -0.0438, -0.0040, -0.0420,  0.0466,  0.0327, -0.1082,\n",
       "                       0.0095,  0.1019,  0.0244,  0.0441,  0.0804, -0.0406, -0.0595, -0.0265,\n",
       "                       0.2252,  0.2613,  0.1277,  0.0426,  0.1872,  0.2759,  0.0175,  0.1954,\n",
       "                       0.3053,  0.2230,  0.1928,  0.2027,  0.1304,  0.0750,  0.1800,  0.1377,\n",
       "                       0.2395,  0.2289,  0.2046, -0.0172,  0.1682,  0.1012,  0.2173,  0.0161,\n",
       "                       0.0412,  0.0099,  0.0715,  0.2513,  0.1570,  0.2167,  0.0992,  0.0934,\n",
       "                       0.1721,  0.2271,  0.1000,  0.0488, -0.0384,  0.1065,  0.2123,  0.0659,\n",
       "                       0.3214,  0.4630,  0.0701,  0.2653,  0.2390,  0.2098,  0.1927,  0.1642,\n",
       "                      -0.0567,  0.1832, -0.0179,  0.1992,  0.0089,  0.2566, -0.0073,  0.1260,\n",
       "                       0.1371,  0.2088, -0.1561,  0.2060,  0.1959,  0.0766,  0.1148,  0.1051,\n",
       "                       0.2870,  0.2174,  0.2085,  0.1553,  0.1422,  0.0821,  0.0414, -0.0060,\n",
       "                       0.2452,  0.0415,  0.2261,  0.0365,  0.1457,  0.2819,  0.1530,  0.1941,\n",
       "                      -0.0859,  0.0685,  0.2331,  0.1370,  0.0632,  0.2079,  0.2091,  0.1769,\n",
       "                       0.1712,  0.0042,  0.0534,  0.0918,  0.3207,  0.1162,  0.2344,  0.0932,\n",
       "                       0.2532,  0.0420,  0.1001,  0.1633,  0.1362,  0.0017,  0.1275,  0.1011,\n",
       "                       0.1431,  0.0608,  0.0629,  0.3241,  0.2649,  0.2019,  0.1217,  0.0116,\n",
       "                       0.1675,  0.0841,  0.2172, -0.0485,  0.0297,  0.0940,  0.1145,  0.0024,\n",
       "                       0.0387,  0.0337,  0.3582,  0.4474,  0.1915,  0.4256, -0.1053,  0.1366])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 2.0015e-01,  3.9950e-01,  6.8177e-02,  4.6423e-03,  2.4246e-01,\n",
       "                       1.5799e-01,  1.4378e-01,  1.7909e-01,  1.1382e-01,  1.9344e-01,\n",
       "                       2.2988e-01,  2.6090e-01,  2.0981e-01,  1.3415e-01,  1.8406e-01,\n",
       "                       1.1119e-01,  1.2449e-01,  2.1162e-02,  2.7109e-01,  1.0961e-01,\n",
       "                       1.8559e-01,  1.9678e-03,  2.4043e-01,  1.1196e-01,  1.9436e-01,\n",
       "                      -1.4823e-03, -2.0447e-03,  2.2843e-01,  1.0383e-01,  1.9382e-01,\n",
       "                       2.3196e-01,  1.0311e-01,  1.7911e-01,  2.0407e-01,  1.8026e-01,\n",
       "                       8.7088e-02, -8.1710e-02,  1.0282e-01,  2.8971e-01,  7.4620e-02,\n",
       "                       1.8008e-01,  3.7627e-01,  1.4452e-01,  3.4272e-01,  3.5755e-01,\n",
       "                       2.3505e-01,  3.5191e-01,  1.6140e-01,  3.3392e-02,  2.5440e-02,\n",
       "                       7.6105e-02,  1.1108e-01,  9.5561e-02,  1.8868e-01,  1.4322e-01,\n",
       "                       7.5199e-02,  1.3453e-01,  1.9510e-01, -3.7673e-02,  2.0605e-01,\n",
       "                       1.2344e-01,  2.5382e-01,  2.4603e-01,  1.0856e-01,  1.6536e-01,\n",
       "                       2.5331e-01,  1.2559e-01, -8.0884e-04,  2.4689e-01,  1.2392e-01,\n",
       "                       4.3778e-02,  6.7551e-02,  1.8793e-01,  1.0287e-01,  1.5712e-01,\n",
       "                       3.1677e-02,  1.1693e-01,  2.7759e-01,  2.2830e-01,  3.3043e-01,\n",
       "                      -1.0304e-01,  1.6576e-01,  1.8808e-01,  2.9709e-01,  7.6356e-02,\n",
       "                       3.5139e-01,  3.0514e-01,  9.6235e-02,  1.4034e-01,  1.1726e-01,\n",
       "                       1.8143e-01,  3.9146e-02,  2.7093e-01,  1.6820e-01,  1.9483e-01,\n",
       "                       4.1510e-01,  1.7098e-01,  1.4715e-02,  1.5724e-02,  2.9624e-02,\n",
       "                       1.6603e-01,  2.8349e-01,  1.6577e-01,  8.4480e-02,  1.0679e-01,\n",
       "                       2.0155e-01,  1.6956e-01,  2.4749e-01,  1.1695e-01,  8.1610e-02,\n",
       "                       1.0052e-01,  3.3315e-02,  4.3085e-01,  1.5547e-01,  1.1293e-01,\n",
       "                       3.6622e-02,  1.5007e-01,  1.8563e-01,  1.2307e-01,  8.8697e-02,\n",
       "                       1.3579e-01,  8.0827e-02,  1.4335e-01,  3.2959e-01,  1.7956e-01,\n",
       "                       2.5565e-01, -1.6299e-01,  1.4680e-01,  1.4080e-02, -7.4387e-02,\n",
       "                      -2.3576e-02,  2.6595e-02,  8.6839e-02, -6.8419e-02, -4.6190e-02,\n",
       "                       4.7338e-02,  8.2986e-02,  4.8632e-03,  5.1946e-02, -6.2827e-02,\n",
       "                      -1.5785e-02,  1.0509e-01,  1.5741e-02,  6.6678e-03, -2.4790e-02,\n",
       "                      -3.5981e-02, -4.7553e-02, -5.7323e-02, -2.3441e-02, -2.8081e-02,\n",
       "                       5.1950e-02,  3.6410e-02, -6.9271e-02,  7.4952e-02,  1.1366e-02,\n",
       "                      -2.9419e-02,  2.6040e-02,  9.3898e-02,  5.7105e-02, -1.5342e-04,\n",
       "                       1.8988e-02,  7.1549e-02,  5.3718e-02, -6.4979e-02,  1.5341e-01,\n",
       "                      -7.3230e-02, -3.9329e-03, -8.6121e-03,  7.8592e-02,  1.0721e-01,\n",
       "                       1.0254e-01, -6.1296e-03, -5.5579e-02,  1.4582e-01, -4.0298e-02,\n",
       "                       3.0542e-03,  6.9749e-04, -1.6934e-02, -1.4544e-01, -4.4596e-02,\n",
       "                      -1.2236e-02,  7.3977e-03, -2.2917e-02,  1.0768e-01,  3.9926e-04,\n",
       "                       4.1622e-02,  2.5960e-02, -5.3772e-02, -1.7600e-02, -1.4833e-01,\n",
       "                       2.9897e-02,  2.7438e-02, -1.7400e-02,  1.1238e-01, -6.9388e-02,\n",
       "                      -4.2019e-03,  9.5877e-03, -2.0490e-02,  2.5858e-02, -5.0024e-02,\n",
       "                       2.9819e-02, -1.2477e-01,  2.4322e-02, -1.7877e-03,  2.7046e-02,\n",
       "                      -4.4720e-02, -1.0545e-03,  3.4436e-02, -1.1007e-01,  4.8744e-02,\n",
       "                      -2.7372e-04,  2.4882e-02, -7.3350e-02,  6.4498e-02, -1.3613e-01,\n",
       "                       5.4980e-02,  6.8818e-02,  8.2682e-02, -2.2820e-02,  1.2674e-01,\n",
       "                      -9.4422e-02,  3.5409e-02, -2.2886e-02, -6.0602e-02,  9.2236e-02,\n",
       "                      -6.7812e-02,  4.4859e-02, -7.0577e-02,  8.1245e-02,  8.1502e-02,\n",
       "                      -1.0162e-03, -3.0636e-02,  5.5745e-02,  1.1398e-01, -8.7163e-02,\n",
       "                       2.1287e-01,  2.5779e-02,  1.0127e-01,  5.1001e-02, -7.6436e-02,\n",
       "                      -5.2406e-03,  2.1068e-01,  7.4733e-02, -3.3874e-02,  8.3846e-03,\n",
       "                      -1.0873e-01, -6.4640e-02, -8.7497e-02, -7.4743e-02,  1.0860e-01,\n",
       "                      -6.0104e-03, -6.6005e-02, -1.6455e-02, -2.2198e-02, -7.7037e-03,\n",
       "                      -5.5282e-02, -8.1094e-02,  4.2336e-02, -1.3357e-02,  1.5484e-02,\n",
       "                       9.5645e-02, -1.5325e-02, -6.5835e-02,  1.0686e-01,  4.1596e-02,\n",
       "                      -8.3535e-02,  1.2414e-01, -3.6361e-02,  3.2325e-02, -2.9831e-02,\n",
       "                       4.9334e-02, -3.0782e-02,  2.4636e-02, -1.1900e-01, -9.9044e-02,\n",
       "                      -8.7556e-03, -2.1335e-02, -1.8749e-02, -5.5949e-02, -3.4237e-02,\n",
       "                      -5.2894e-02, -2.4928e-02,  3.3802e-02,  2.3459e-02,  8.4556e-02,\n",
       "                      -1.3124e-01, -1.1843e-02,  2.6036e-02,  5.5608e-02, -4.0633e-02,\n",
       "                       5.6722e-02, -5.8622e-02, -7.5960e-02,  7.1260e-02, -4.0566e-03,\n",
       "                       1.4775e-02, -3.9909e-02,  3.0176e-02,  7.3313e-02,  2.9500e-02,\n",
       "                       1.0325e-01,  8.6140e-02,  1.4975e-02,  1.9196e-02, -3.1146e-02,\n",
       "                      -6.6990e-02,  1.2709e-01, -3.9203e-02,  5.3549e-02,  2.5926e-02,\n",
       "                      -6.8247e-02, -4.5575e-02,  4.6418e-02, -4.8496e-02,  9.6846e-02,\n",
       "                      -2.4529e-02, -1.1496e-01, -6.1706e-03,  1.8824e-02,  1.2540e-02,\n",
       "                      -1.3653e-01,  4.4591e-02, -1.8691e-01,  1.5401e-02,  1.2891e-01,\n",
       "                      -3.2919e-02, -2.3756e-02, -9.9951e-02,  7.0987e-02, -1.8128e-01,\n",
       "                      -2.3737e-02,  9.4048e-03,  8.2556e-02, -1.1344e-01, -2.3794e-02,\n",
       "                      -5.5017e-03, -1.9576e-02,  1.7208e-01,  2.8157e-02,  2.0719e-03,\n",
       "                       8.3778e-02, -5.1165e-02, -2.1805e-02,  1.6531e-02, -1.1081e-01,\n",
       "                      -1.7114e-02,  1.2407e-01, -5.9131e-03,  1.2272e-01,  1.7790e-02,\n",
       "                      -8.8853e-02,  1.1960e-01, -8.4950e-02, -2.0367e-02,  6.4046e-02,\n",
       "                      -4.0641e-02,  3.8992e-03, -6.2682e-02,  5.5166e-02,  6.2491e-02,\n",
       "                       5.5647e-02,  7.3152e-02, -1.1581e-01,  9.5795e-02,  2.4881e-01,\n",
       "                       1.3833e-01, -1.2763e-01, -6.2746e-02,  7.5892e-02,  8.0400e-02,\n",
       "                      -7.4272e-02, -1.3531e-02,  3.9874e-02, -1.1119e-02,  7.0294e-02,\n",
       "                       6.4112e-02, -9.3105e-02,  1.2076e-01, -5.2641e-02,  1.2474e-04,\n",
       "                      -8.9359e-02, -2.8823e-02, -1.2007e-01,  3.0615e-02,  9.0844e-02,\n",
       "                       4.1783e-01,  6.8641e-02,  2.7268e-02,  3.0685e-01,  3.2337e-01,\n",
       "                       8.4969e-02,  1.9080e-01,  8.9060e-02,  2.7039e-01,  2.3338e-01,\n",
       "                       9.1438e-02,  2.5003e-01,  1.2532e-01,  1.5797e-01,  2.1477e-01,\n",
       "                       1.6963e-01,  8.1649e-02,  2.5619e-01,  1.3670e-01,  2.5903e-01,\n",
       "                       6.2256e-02,  1.4524e-01,  7.1007e-02,  1.2363e-01,  1.2318e-01,\n",
       "                       9.4030e-02,  9.9550e-02,  1.6441e-01,  1.7003e-01,  1.8168e-01,\n",
       "                       1.4354e-01,  1.2311e-01,  8.6226e-02,  2.2443e-01, -3.5936e-02,\n",
       "                       1.4199e-02,  1.8333e-02,  1.5858e-01,  3.5549e-02,  2.3812e-01,\n",
       "                       5.3187e-01,  1.8806e-01,  2.4955e-01,  2.4534e-01,  1.9834e-01,\n",
       "                       3.4562e-01,  1.3983e-01,  5.2554e-02,  1.8261e-01,  7.0877e-02,\n",
       "                       2.3191e-01,  4.7054e-02,  2.7948e-01, -7.2161e-02,  1.7277e-01,\n",
       "                       1.8430e-01,  1.6259e-01, -1.7616e-02,  1.4129e-01,  4.9283e-02,\n",
       "                       3.7676e-01,  1.4849e-01,  1.9699e-01,  2.7414e-01,  1.9124e-01,\n",
       "                       1.9293e-01,  1.8846e-02,  1.6910e-01,  1.0358e-01,  1.5312e-01,\n",
       "                       6.7827e-02,  1.9829e-01,  5.7802e-02,  1.6844e-01,  7.9501e-03,\n",
       "                       3.3291e-02,  2.4894e-01,  9.4539e-02,  1.8502e-01, -1.9177e-02,\n",
       "                       6.2537e-02,  2.0908e-01,  2.6232e-01,  1.1461e-01,  3.3100e-01,\n",
       "                       1.8521e-01,  2.2815e-01,  8.8843e-02,  2.7945e-02,  2.3559e-01,\n",
       "                       9.4422e-02,  2.5316e-01,  2.1618e-01,  2.8282e-01,  2.3472e-01,\n",
       "                       1.2271e-01,  4.9081e-02,  8.9444e-02,  1.9578e-01,  2.8659e-01,\n",
       "                       1.8006e-01,  8.4713e-02,  1.5041e-01,  1.0632e-01,  1.6972e-01,\n",
       "                       9.9854e-02,  2.5538e-01,  4.6803e-02,  1.8688e-01,  5.0374e-02,\n",
       "                      -4.2412e-02,  2.8451e-01,  1.1468e-01,  2.3356e-01,  2.6463e-02,\n",
       "                       1.9321e-02,  2.6756e-01,  2.8523e-02,  5.4627e-02,  1.9945e-01,\n",
       "                       5.9394e-02,  2.9547e-01,  3.9014e-01,  1.7890e-01,  2.9899e-01,\n",
       "                       2.2046e-02,  1.9994e-01])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.3485,  0.1776,  0.3984,  ..., -0.4762,  0.0137,  0.6250],\n",
       "                      [-0.2246,  0.0239,  0.4778,  ..., -0.6296,  0.0963,  0.2776],\n",
       "                      [-0.0566, -0.4228,  0.1936,  ..., -0.2870, -0.4584,  0.3304],\n",
       "                      ...,\n",
       "                      [ 0.0550,  0.5078,  0.4189,  ..., -0.3129, -0.0512,  0.1474],\n",
       "                      [-0.3467, -0.1909, -0.0512,  ...,  0.3436, -0.3236, -0.2872],\n",
       "                      [-0.0759,  0.2129, -0.1314,  ...,  0.1331,  0.2791, -0.1236]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.2032,  0.1354,  0.3074,  ..., -0.0621,  0.2570,  0.1764],\n",
       "                      [-0.2659, -0.0751,  0.2342,  ...,  0.1208, -0.0273,  0.0587],\n",
       "                      [-0.1520, -0.1857, -0.0395,  ..., -0.1929, -0.1729,  0.0455],\n",
       "                      ...,\n",
       "                      [-0.0161,  0.1604,  0.0087,  ..., -0.0979,  0.0088, -0.0757],\n",
       "                      [ 0.0607, -0.1403,  0.1222,  ...,  0.2021,  0.1274, -0.2098],\n",
       "                      [ 0.0580,  0.2701,  0.1563,  ...,  0.1144, -0.0460,  0.0167]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-1.7856e-02, -3.2530e-01, -1.6639e-01, -1.5048e-01, -9.1610e-02,\n",
       "                       1.4271e-02, -6.7270e-02, -1.7028e-01, -7.1806e-02, -2.2368e-01,\n",
       "                      -8.8752e-02, -2.0546e-01, -2.4457e-01, -1.0923e-01, -3.0231e-01,\n",
       "                      -1.0305e-01, -1.2128e-01, -1.0267e-01,  2.7723e-02, -7.9653e-02,\n",
       "                      -9.0918e-02, -1.0607e-01, -5.6717e-02, -6.7438e-02, -2.5350e-03,\n",
       "                      -1.0450e-01,  1.6379e-02, -8.0749e-02, -8.4661e-02,  2.8043e-02,\n",
       "                      -1.0659e-01, -1.3478e-01, -4.1649e-02, -1.2121e-01, -6.2122e-02,\n",
       "                      -1.8869e-01, -8.5739e-02,  2.1691e-02, -2.1237e-01, -1.2977e-01,\n",
       "                      -1.2243e-01, -1.2006e-01, -1.8182e-01, -1.5869e-01, -2.0147e-01,\n",
       "                      -1.0161e-01, -2.2046e-01, -9.2497e-02, -6.7179e-02, -1.3240e-01,\n",
       "                      -2.3396e-02,  2.0257e-01, -1.2554e-01, -2.8501e-02, -1.4557e-01,\n",
       "                      -4.0074e-02, -9.2917e-02, -9.0279e-02, -6.9931e-02, -4.8145e-02,\n",
       "                      -9.5985e-02, -1.8900e-01, -1.0507e-01, -8.5951e-02, -8.6382e-02,\n",
       "                      -2.1856e-01, -1.9371e-01, -3.0080e-01,  1.0817e-01, -1.5627e-01,\n",
       "                      -3.2689e-02, -1.0208e-01,  1.0024e-01, -1.1442e-01, -2.6856e-01,\n",
       "                      -2.5075e-01, -1.5955e-01, -1.2601e-02, -1.2009e-01, -6.1933e-02,\n",
       "                      -1.2998e-01, -8.5497e-02, -1.5201e-01, -6.8765e-02, -4.7949e-03,\n",
       "                      -2.0725e-01, -9.4058e-02, -9.4002e-02, -2.0036e-01, -8.9222e-02,\n",
       "                      -2.1038e-01, -1.8152e-02, -4.6497e-02,  6.2190e-03, -1.9426e-01,\n",
       "                      -1.4968e-01, -7.5123e-02, -1.6781e-02, -1.8111e-01,  4.9931e-02,\n",
       "                      -4.4309e-02, -1.2803e-01, -9.5191e-02, -3.4065e-02,  1.3258e-01,\n",
       "                      -6.3318e-02, -7.1146e-02, -1.7030e-01, -1.4707e-01, -6.9318e-02,\n",
       "                      -1.0359e-01, -1.7132e-01, -2.0982e-01, -1.2228e-01, -1.1393e-01,\n",
       "                      -4.6161e-02, -1.0601e-01, -8.6535e-02, -1.5924e-01, -2.4390e-01,\n",
       "                      -1.4143e-01, -2.5427e-01, -1.0701e-01, -7.1032e-02, -1.5330e-01,\n",
       "                       7.1535e-03, -9.4293e-02, -1.2114e-01, -1.2043e-01, -1.8082e-01,\n",
       "                       4.1062e-02, -2.1815e-01, -1.7754e-01, -2.2110e-02, -7.2907e-02,\n",
       "                      -6.8795e-02, -1.5880e-01, -9.2260e-02, -8.1662e-02, -7.4102e-02,\n",
       "                      -1.7744e-01, -6.4841e-02, -9.1534e-02, -2.5331e-01, -4.8969e-02,\n",
       "                      -4.1807e-02, -8.6305e-03, -4.7736e-02,  2.2730e-02, -2.8711e-02,\n",
       "                      -8.7542e-02, -1.3867e-01,  1.0059e-01, -1.1603e-01,  3.0522e-02,\n",
       "                      -9.6218e-02, -1.4125e-01,  1.6111e-01, -1.4926e-01, -1.4829e-01,\n",
       "                       5.8580e-03,  1.2833e-03,  2.3607e-01, -2.2828e-01, -2.2488e-01,\n",
       "                       1.7485e-01, -7.1161e-02, -5.1267e-02, -3.9990e-02, -8.4372e-02,\n",
       "                      -2.8115e-01,  5.9981e-02, -1.3496e-01,  8.9019e-02,  6.3007e-04,\n",
       "                       9.1338e-02, -1.2773e-01, -1.4582e-01,  1.7041e-01,  1.5441e-01,\n",
       "                      -7.4786e-02, -1.8132e-02, -4.2533e-02, -4.1452e-02, -1.1125e-01,\n",
       "                      -7.4386e-02, -5.6603e-02, -1.9696e-01, -1.4033e-01, -5.8138e-02,\n",
       "                       2.6103e-02, -1.0734e-02, -1.6566e-01, -1.0052e-01,  5.1708e-02,\n",
       "                      -2.7384e-02,  6.5700e-02, -1.1764e-01, -1.1200e-01, -1.0403e-01,\n",
       "                       1.0183e-01, -1.3832e-01, -1.4896e-01, -2.3861e-02, -6.2604e-02,\n",
       "                      -4.0297e-03, -8.6266e-02, -1.2749e-01, -7.3798e-03, -1.5274e-01,\n",
       "                       2.6648e-03,  1.1791e-01,  3.7886e-02,  1.3223e-03, -3.5261e-02,\n",
       "                      -6.7424e-02, -3.7397e-02, -1.9286e-01, -6.6304e-02,  9.7653e-02,\n",
       "                      -1.0204e-01,  1.8964e-02, -1.3402e-01, -5.1181e-02, -1.5424e-01,\n",
       "                      -3.1180e-02, -1.7581e-01,  1.4945e-01,  6.6520e-02, -9.2552e-02,\n",
       "                       1.5145e-01, -3.5278e-02, -5.1658e-02,  4.6013e-02, -4.9140e-02,\n",
       "                      -2.0297e-01, -1.5434e-01,  9.6540e-02, -1.1104e-01, -9.1087e-02,\n",
       "                      -8.6247e-02, -3.5677e-02, -2.2988e-01, -8.4359e-02,  2.1833e-02,\n",
       "                      -1.0392e-01, -2.7645e-02, -1.8977e-01, -1.8272e-02,  8.5849e-02,\n",
       "                      -6.5384e-02, -1.7934e-01, -1.1717e-01, -2.5186e-02,  6.8193e-02,\n",
       "                      -1.8752e-01, -1.4810e-01,  5.3271e-02,  6.5354e-02, -5.5675e-02,\n",
       "                       2.3654e-02,  6.5455e-03, -5.1206e-02, -1.3606e-02,  5.7718e-02,\n",
       "                      -4.1695e-02,  1.0917e-01,  4.1727e-04, -1.3888e-02,  3.0390e-02,\n",
       "                       1.6795e-02,  4.3726e-02, -2.7154e-02, -1.3439e-02, -6.3771e-02,\n",
       "                      -7.3871e-03, -1.2669e-02, -9.9016e-02, -7.3470e-04, -9.8804e-02,\n",
       "                       1.3723e-01, -6.7682e-02,  1.0576e-01,  2.8028e-02,  9.0992e-02,\n",
       "                      -1.4072e-02,  4.4465e-02,  1.6570e-02,  5.8830e-02,  5.9528e-02,\n",
       "                       1.0352e-01, -5.5589e-02, -1.2149e-01,  3.7956e-02,  9.0604e-02,\n",
       "                      -1.6533e-02, -5.0561e-02, -1.1520e-01, -8.2851e-02, -1.1296e-01,\n",
       "                       3.8518e-02, -1.1860e-01,  2.4833e-02, -2.9464e-02,  3.6266e-02,\n",
       "                       6.6402e-03,  1.2906e-02, -5.0389e-02, -1.1350e-01, -2.9602e-02,\n",
       "                       1.4125e-02,  2.4436e-02,  5.1960e-02,  1.7429e-02, -1.0976e-01,\n",
       "                      -3.3642e-02,  2.7550e-02,  5.9660e-02, -1.4019e-02, -6.6263e-02,\n",
       "                      -4.4198e-02, -2.0720e-02,  5.6820e-02,  2.1885e-03, -1.0273e-01,\n",
       "                      -1.3943e-01,  5.0430e-02, -8.1617e-03, -1.4192e-01,  2.9060e-02,\n",
       "                      -5.1286e-02, -2.9029e-02,  8.0891e-02,  1.0211e-01, -1.2320e-02,\n",
       "                      -3.0021e-02,  8.1286e-02,  4.0347e-02,  5.9125e-03, -6.5705e-02,\n",
       "                       1.4017e-02, -6.5799e-02, -3.3335e-03, -2.4750e-02,  3.2450e-02,\n",
       "                       1.8098e-03, -2.7790e-02,  2.7724e-04, -1.1341e-01, -7.0873e-02,\n",
       "                       7.6048e-02,  8.7456e-03, -1.2632e-01,  3.7534e-02, -1.9213e-02,\n",
       "                      -3.5716e-02,  3.3190e-02,  2.1935e-02,  4.1280e-02, -3.6451e-02,\n",
       "                      -6.4008e-02, -1.7588e-01, -1.0399e-03, -4.8634e-03, -1.1668e-02,\n",
       "                      -9.5630e-02,  7.1665e-02, -4.5865e-02, -2.2326e-02, -5.7878e-02,\n",
       "                       2.5085e-02, -5.3423e-02,  1.8555e-02, -3.1345e-02,  1.7021e-01,\n",
       "                       1.2509e-01, -8.2061e-03,  6.2451e-02, -5.2322e-02,  1.1363e-02,\n",
       "                       8.1304e-02, -4.5026e-02, -5.5297e-02, -1.4378e-02, -1.1347e-01,\n",
       "                      -1.0292e-01, -1.2697e-01,  2.8509e-02, -2.1226e-01,  3.9137e-02,\n",
       "                      -1.9466e-01, -1.9321e-01,  1.3241e-02, -1.3176e-01, -1.6779e-01,\n",
       "                      -1.6163e-01, -9.7791e-02, -2.5888e-01, -2.2278e-01, -1.8575e-01,\n",
       "                      -1.4717e-01, -7.2022e-02,  8.4838e-02,  2.2990e-02, -1.5417e-01,\n",
       "                      -7.5201e-02, -1.5298e-01, -3.1131e-02, -5.1193e-02, -1.7135e-01,\n",
       "                       3.0095e-02, -1.6726e-01, -5.2536e-02,  2.9473e-02, -2.0460e-02,\n",
       "                      -1.9461e-01, -9.1091e-02, -1.0969e-01, -1.1703e-01, -2.1718e-01,\n",
       "                      -9.0980e-02,  9.3827e-03, -3.4463e-02, -1.5906e-01, -2.1421e-01,\n",
       "                      -2.2221e-01, -1.9313e-01, -1.8099e-01, -1.7004e-01,  6.1481e-02,\n",
       "                      -1.3934e-01, -1.7203e-01, -1.4856e-01, -1.6786e-01, -6.1632e-02,\n",
       "                       1.0910e-01, -1.1158e-01, -1.1979e-01,  7.2762e-03,  6.6447e-02,\n",
       "                      -1.6443e-01, -2.5215e-01, -9.1321e-02, -9.7830e-02, -1.5725e-02,\n",
       "                       3.0959e-02,  4.3619e-02, -1.3404e-01, -1.9891e-01, -9.7814e-02,\n",
       "                      -8.7255e-02, -1.5785e-01, -1.2355e-01,  2.7299e-03, -9.6327e-02,\n",
       "                      -2.6799e-02, -2.1175e-02, -8.6632e-02, -2.1729e-01, -1.2274e-01,\n",
       "                      -1.3252e-01, -3.7000e-02, -5.7594e-02, -2.8572e-02, -1.3439e-01,\n",
       "                      -1.5809e-02, -1.4396e-01,  1.0250e-01,  1.8180e-01, -2.1992e-01,\n",
       "                      -9.1275e-02, -1.4654e-01, -1.6647e-01, -1.4647e-01, -8.4373e-02,\n",
       "                      -2.2332e-04, -9.1270e-02, -3.6785e-02, -1.4130e-01,  2.8226e-02,\n",
       "                      -1.2548e-01, -1.1600e-01, -1.7000e-01, -9.1837e-02,  4.8834e-02,\n",
       "                      -3.5072e-03,  2.6248e-03, -5.1778e-02,  1.0169e-02, -1.8384e-01,\n",
       "                      -1.6287e-01, -1.8299e-01, -5.7578e-02, -5.1318e-03, -2.1005e-01,\n",
       "                      -9.3762e-02,  7.2315e-02, -1.0018e-01, -1.7771e-01, -5.5252e-02,\n",
       "                      -1.6980e-01, -8.8841e-02, -1.0902e-01, -9.6428e-02, -1.0205e-01,\n",
       "                      -4.8143e-02,  1.3294e-02, -3.0946e-02, -1.0683e-01,  3.0110e-02,\n",
       "                      -1.5238e-01, -7.8609e-02])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-0.1248, -0.1421, -0.0548, -0.0615, -0.0225,  0.0792, -0.0167, -0.0996,\n",
       "                      -0.0553, -0.1281, -0.0397, -0.0546, -0.1191, -0.3109, -0.1797, -0.1582,\n",
       "                       0.0075, -0.0739,  0.0459, -0.0941, -0.0886, -0.0581, -0.0367, -0.0450,\n",
       "                      -0.0224, -0.1726, -0.0038, -0.2076, -0.1187, -0.0190, -0.0890, -0.1939,\n",
       "                      -0.0332, -0.0690,  0.0042, -0.1802, -0.1030, -0.1351, -0.2529, -0.1117,\n",
       "                      -0.1356, -0.1417, -0.1032, -0.0674, -0.2043, -0.0693, -0.1630, -0.1460,\n",
       "                      -0.1377, -0.1632, -0.0189,  0.1121, -0.1224, -0.1084, -0.0909,  0.0134,\n",
       "                      -0.1794,  0.0113, -0.0510, -0.0047, -0.0427, -0.1266, -0.0803, -0.0182,\n",
       "                      -0.1408, -0.1510, -0.0319, -0.2257,  0.1178, -0.0901,  0.0257,  0.0037,\n",
       "                      -0.0366, -0.0322, -0.2610, -0.0965, -0.1319, -0.1032, -0.0074, -0.1437,\n",
       "                      -0.1610, -0.1681, -0.0768, -0.1080,  0.0201, -0.1080, -0.1618, -0.1240,\n",
       "                       0.0927, -0.0900, -0.1194, -0.0685, -0.2081, -0.1050, -0.1584, -0.0787,\n",
       "                      -0.0810,  0.0115, -0.3060, -0.0327, -0.1640, -0.0679, -0.0219, -0.1550,\n",
       "                      -0.0938, -0.1802, -0.0235, -0.0983, -0.1143,  0.0013,  0.0267, -0.1571,\n",
       "                      -0.0232, -0.0324, -0.0455,  0.0306, -0.0564, -0.0419, -0.2625, -0.1751,\n",
       "                      -0.0396, -0.2227, -0.1001, -0.0782, -0.1237, -0.1128, -0.1743, -0.0947,\n",
       "                      -0.0875, -0.1862,  0.0355, -0.0224, -0.0451, -0.0804, -0.0040, -0.0348,\n",
       "                      -0.0117, -0.0065, -0.0153, -0.1191,  0.0106, -0.1909, -0.1107, -0.1540,\n",
       "                      -0.0982, -0.1034,  0.0762, -0.1607,  0.0150, -0.1713, -0.1176, -0.0737,\n",
       "                      -0.0813, -0.2017,  0.0075, -0.0524, -0.0720,  0.0269, -0.0472, -0.1364,\n",
       "                      -0.0456, -0.0044,  0.1629, -0.1569, -0.1706, -0.0062,  0.0231, -0.1038,\n",
       "                       0.1080, -0.0815, -0.2578, -0.0062, -0.1176,  0.0601, -0.0541, -0.0970,\n",
       "                      -0.0876, -0.0531,  0.1662, -0.0066, -0.0282,  0.0749, -0.0913, -0.1393,\n",
       "                      -0.0450, -0.1439, -0.0412, -0.0840, -0.0150, -0.1246,  0.0332,  0.0560,\n",
       "                       0.0251, -0.0518, -0.0941, -0.2165, -0.0385, -0.1068, -0.0523, -0.0279,\n",
       "                       0.0725, -0.0380, -0.1063, -0.0636, -0.0673, -0.0243,  0.0047, -0.1398,\n",
       "                      -0.0670, -0.0923, -0.0872,  0.0423,  0.1057, -0.1030, -0.0343, -0.0676,\n",
       "                      -0.1141, -0.0872, -0.0505, -0.0645, -0.0202, -0.0277, -0.0675, -0.0662,\n",
       "                      -0.1123, -0.1023, -0.1519,  0.0576, -0.0866, -0.0329,  0.1973, -0.0471,\n",
       "                       0.0605,  0.0281, -0.0342, -0.1192, -0.1895,  0.0213, -0.1236, -0.0385,\n",
       "                      -0.0390, -0.0528, -0.1786, -0.0580, -0.1187, -0.1514, -0.1708, -0.1472,\n",
       "                      -0.0124,  0.1296,  0.0300, -0.0202, -0.1419, -0.0475,  0.1340, -0.2328,\n",
       "                       0.0512, -0.0495,  0.0524,  0.0438,  0.0456, -0.1736,  0.0363,  0.0636,\n",
       "                      -0.0757, -0.0898,  0.0408, -0.0427,  0.1513, -0.0085,  0.0127, -0.0634,\n",
       "                      -0.0114,  0.0896, -0.0786, -0.0439, -0.0861,  0.0194,  0.0128, -0.0823,\n",
       "                      -0.0969,  0.0089, -0.0139, -0.0089,  0.0459,  0.0266,  0.0119,  0.0014,\n",
       "                      -0.0325,  0.0778,  0.2160,  0.0521,  0.0302,  0.1245,  0.0382,  0.0395,\n",
       "                       0.0995, -0.0067, -0.0031,  0.0217, -0.0073,  0.0223,  0.0615, -0.0427,\n",
       "                       0.0175, -0.0555, -0.0304,  0.0191,  0.0508, -0.0154,  0.0332,  0.0648,\n",
       "                       0.0099,  0.0474, -0.0231, -0.0721, -0.0499,  0.0166, -0.1158,  0.0500,\n",
       "                      -0.0492, -0.0357,  0.0684, -0.0667, -0.0616,  0.0044,  0.1120,  0.1512,\n",
       "                       0.0075, -0.0971,  0.0533, -0.0049, -0.0877, -0.0397,  0.0375, -0.0268,\n",
       "                      -0.0187, -0.0312, -0.0198, -0.0347,  0.0591, -0.0609, -0.0838,  0.1275,\n",
       "                       0.0169,  0.0174,  0.0029,  0.0478, -0.1424,  0.1143,  0.0437, -0.0192,\n",
       "                      -0.0227, -0.0513, -0.0031, -0.0051,  0.1119, -0.0938,  0.0845, -0.0831,\n",
       "                       0.0434, -0.0596, -0.0358,  0.0196,  0.0419,  0.0534, -0.0564, -0.0249,\n",
       "                      -0.1104, -0.0359,  0.0451, -0.0640, -0.1650,  0.0843, -0.0307, -0.0564,\n",
       "                      -0.1755,  0.0699, -0.0095, -0.0518,  0.1555, -0.0011, -0.0996,  0.0198,\n",
       "                      -0.0408, -0.1649, -0.0702,  0.0041, -0.1579, -0.0732, -0.0814, -0.1169,\n",
       "                      -0.2943, -0.1832, -0.0869, -0.0220, -0.0085, -0.1150, -0.2345, -0.1349,\n",
       "                      -0.0687,  0.0708,  0.0138, -0.0610, -0.0926, -0.0341, -0.1064, -0.1450,\n",
       "                      -0.0603, -0.0727,  0.0763, -0.1269, -0.1118,  0.0065, -0.0941, -0.2010,\n",
       "                      -0.0630, -0.1189, -0.1311, -0.2558, -0.1676, -0.0043, -0.1328, -0.0623,\n",
       "                      -0.1202, -0.1032, -0.0678, -0.1367, -0.1921,  0.0144, -0.2701, -0.0553,\n",
       "                      -0.1589, -0.0169, -0.1516,  0.0978, -0.1009, -0.0651, -0.1993, -0.0266,\n",
       "                      -0.1871, -0.2013, -0.0809, -0.1537, -0.0922, -0.0547,  0.0017, -0.0504,\n",
       "                      -0.1570, -0.2069, -0.0830, -0.0598,  0.0213, -0.1028, -0.0475, -0.0813,\n",
       "                      -0.0080,  0.0266, -0.1300, -0.2225,  0.0250,  0.0092,  0.0122, -0.1343,\n",
       "                      -0.2710, -0.1167,  0.0441,  0.0525,  0.1059, -0.2174, -0.1584, -0.0324,\n",
       "                      -0.1073, -0.0491, -0.0914, -0.1211, -0.2011, -0.0688, -0.1829, -0.0371,\n",
       "                      -0.1664, -0.0954, -0.0690, -0.1023, -0.0553, -0.0533, -0.0864, -0.0677,\n",
       "                       0.0684, -0.2924, -0.0521, -0.1669, -0.0416, -0.0273,  0.0621, -0.0921,\n",
       "                      -0.1489, -0.0977, -0.2802, -0.0924, -0.0913, -0.0901, -0.1098, -0.0985,\n",
       "                       0.0498, -0.0075, -0.0609, -0.0302, -0.1871, -0.1389, -0.0212, -0.2389])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0504, -0.0493,  0.0352,  ...,  0.7713,  0.1258, -0.6522],\n",
       "                      [-0.0674,  0.5817, -0.3460,  ..., -0.7110, -0.4212,  0.3509],\n",
       "                      [-0.1156,  0.2755, -0.3400,  ..., -0.2342,  0.2544, -0.1480],\n",
       "                      ...,\n",
       "                      [-0.1515, -0.2804, -0.0201,  ...,  0.2119,  0.3706, -0.1078],\n",
       "                      [ 0.5823, -0.4774,  0.0229,  ..., -0.5235, -0.1167,  0.2478],\n",
       "                      [ 0.2726, -0.3636,  0.1483,  ...,  0.0919,  0.5920,  0.3282]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0019,  0.3157, -0.0240,  ..., -0.3116, -0.1476, -0.3889],\n",
       "                      [-0.0716,  0.0245,  0.1821,  ..., -0.2003,  0.2944,  0.3114],\n",
       "                      [ 0.1271,  0.4870, -0.0907,  ..., -0.0832, -0.0375,  0.3810],\n",
       "                      ...,\n",
       "                      [-0.2021,  0.4523, -0.4583,  ...,  0.0251, -0.0028,  0.2623],\n",
       "                      [ 0.0100, -0.2149,  0.0597,  ..., -0.1539, -0.1725,  0.1693],\n",
       "                      [-0.0104,  0.1579, -0.0020,  ..., -0.1875,  0.1109, -0.1031]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 3.5612e-02,  4.0052e-01,  1.2775e-01,  1.7089e-01,  5.7414e-03,\n",
       "                       1.4887e-01,  2.0076e-01,  3.1461e-01,  1.6627e-01,  1.4935e-01,\n",
       "                       1.6604e-01,  1.7701e-01,  1.0780e-02,  1.5667e-01,  2.3194e-02,\n",
       "                       1.5935e-01,  2.8710e-01,  9.3219e-02,  4.0623e-02,  1.5432e-01,\n",
       "                       9.7649e-02,  2.4775e-01, -3.6275e-02,  9.9539e-02, -1.9985e-02,\n",
       "                       1.0206e-01,  2.9363e-01, -2.9186e-02,  1.1644e-01,  1.1777e-01,\n",
       "                       2.1301e-01,  1.9000e-01,  8.6157e-02,  1.4724e-03, -7.4901e-02,\n",
       "                       4.1962e-02,  1.1739e-01,  1.0699e-01,  2.1933e-01,  1.3082e-01,\n",
       "                       8.9494e-02,  2.5306e-01,  2.2749e-02,  1.5816e-01,  1.2383e-01,\n",
       "                       2.8124e-01,  1.2238e-01,  8.5754e-02,  1.6026e-01,  3.3580e-02,\n",
       "                       1.4392e-01, -2.4857e-02,  8.1560e-03,  4.6945e-02,  7.8994e-02,\n",
       "                       9.4688e-02,  7.4271e-02, -5.6544e-03,  1.8551e-01,  2.7363e-01,\n",
       "                       4.6261e-02,  1.1506e-01,  9.1126e-02,  1.9359e-01,  1.0656e-01,\n",
       "                       2.7462e-01,  5.2587e-02,  1.3115e-01,  1.4533e-01,  5.7198e-03,\n",
       "                       9.6041e-02,  3.3912e-01,  2.3242e-01,  6.0092e-02,  1.2609e-02,\n",
       "                      -9.9803e-03,  1.1179e-01,  2.1927e-01,  2.9177e-01,  2.2445e-01,\n",
       "                       6.5858e-02,  9.1107e-02,  8.5678e-02,  3.7226e-03,  2.1853e-01,\n",
       "                       6.1661e-02,  9.4539e-02,  1.7535e-01,  1.5449e-01,  2.7540e-01,\n",
       "                       1.2831e-01,  6.8071e-02,  1.0934e-01,  1.6097e-01,  2.3231e-01,\n",
       "                       8.3024e-02,  9.6125e-02,  1.0939e-01,  2.3824e-01,  2.1442e-01,\n",
       "                       2.7348e-02,  9.8209e-02,  3.1268e-01,  2.0825e-01,  6.6883e-02,\n",
       "                       5.0726e-02,  6.4988e-02,  4.0263e-02,  1.3243e-01,  2.6600e-01,\n",
       "                       2.0620e-02,  2.1284e-01,  1.4423e-01,  8.3339e-02, -3.0973e-03,\n",
       "                       1.1645e-01,  1.8541e-01,  2.0364e-01,  1.8421e-01, -1.4319e-02,\n",
       "                       1.2275e-01,  5.0512e-02,  1.1146e-01,  2.2957e-01,  2.6243e-01,\n",
       "                       1.0783e-01,  2.1202e-01,  2.9941e-02, -4.4496e-03,  6.0178e-02,\n",
       "                       4.5753e-02,  1.9636e-02,  3.7323e-02, -5.3250e-02, -1.2282e-01,\n",
       "                       6.1855e-02,  8.0223e-02, -3.3793e-02, -4.4419e-02,  9.6342e-03,\n",
       "                       1.0776e-01,  9.4785e-02,  2.4932e-01, -3.2927e-02,  6.9250e-02,\n",
       "                      -1.6803e-02, -1.3632e-01,  1.1884e-02, -3.3082e-02, -3.1927e-02,\n",
       "                       6.9670e-02,  1.2620e-01,  1.4406e-01, -9.5033e-03,  1.5097e-01,\n",
       "                       1.3065e-01, -4.3136e-02,  1.0583e-01,  5.4363e-03,  1.2383e-01,\n",
       "                       7.0065e-02, -8.1938e-02, -6.9564e-02,  3.6962e-02, -1.4356e-02,\n",
       "                       1.0505e-01, -4.6912e-03,  1.8133e-01,  2.3103e-02, -1.2791e-01,\n",
       "                       1.7830e-02,  1.7497e-01,  2.6784e-01,  1.0167e-02, -4.0919e-02,\n",
       "                      -6.0679e-03,  5.8363e-02, -8.4744e-03,  1.1415e-01,  6.2495e-02,\n",
       "                      -4.9395e-02,  1.0871e-01, -1.4257e-02,  6.4528e-02, -8.3321e-02,\n",
       "                       6.9745e-02,  9.5370e-02, -7.1062e-02,  7.2489e-02,  1.6619e-01,\n",
       "                      -8.9368e-02, -9.1856e-03,  1.3447e-01, -8.1112e-03,  2.3100e-02,\n",
       "                       3.8540e-02,  1.7448e-01,  1.1748e-01,  1.3114e-01,  3.5019e-02,\n",
       "                       3.0604e-02,  5.8488e-02,  9.3312e-02, -5.3227e-02,  2.0515e-02,\n",
       "                       1.2734e-01,  6.8690e-02, -1.0538e-01,  2.7166e-02, -5.9731e-02,\n",
       "                       4.1540e-02,  3.8769e-03,  1.2474e-01,  1.2108e-01,  8.9419e-02,\n",
       "                      -6.5031e-02,  5.1373e-02, -5.1465e-02,  8.7043e-02,  1.3948e-01,\n",
       "                      -9.9068e-02,  2.3193e-02,  1.1491e-03,  7.4994e-03, -3.5698e-02,\n",
       "                      -1.7204e-01,  1.5391e-01,  1.2500e-01,  6.2720e-02,  8.6063e-02,\n",
       "                       1.2761e-02, -4.2367e-02,  5.0295e-02, -4.8702e-02,  1.0269e-02,\n",
       "                       1.0032e-01, -6.1898e-02,  1.4068e-01, -1.4994e-02, -2.1683e-02,\n",
       "                       9.0787e-03,  2.4286e-01,  6.1103e-02,  4.3309e-02, -1.7987e-02,\n",
       "                      -1.0073e-01, -1.9149e-02, -4.7282e-02,  9.0538e-02,  9.4520e-02,\n",
       "                       1.1355e-01,  7.7363e-02, -8.1211e-02,  2.2991e-03, -4.4879e-02,\n",
       "                       1.2435e-01,  7.0626e-02, -6.2292e-02,  8.8858e-02,  2.4557e-03,\n",
       "                       4.2285e-02, -8.8960e-03,  1.3214e-01, -4.9093e-02, -1.9169e-03,\n",
       "                      -4.0449e-02, -7.2532e-02,  7.2935e-02,  3.7618e-02, -4.7631e-02,\n",
       "                       8.5905e-02,  2.3476e-02,  6.9389e-02, -6.1907e-02, -3.6814e-02,\n",
       "                      -9.4067e-02,  6.7710e-02, -3.6872e-03,  3.9458e-02, -7.4476e-02,\n",
       "                       9.8098e-02, -2.6181e-02,  3.6985e-02, -5.6874e-02,  3.7833e-02,\n",
       "                       9.2182e-02,  8.9200e-02,  3.3161e-02,  9.0231e-02, -7.4175e-02,\n",
       "                       5.8415e-02,  3.5861e-02,  3.1616e-03,  8.2044e-02, -1.3704e-02,\n",
       "                       9.5622e-02, -1.4666e-01, -9.0016e-02, -1.0750e-01,  1.2226e-02,\n",
       "                      -3.8492e-02,  3.7255e-02, -4.9686e-02,  2.7690e-02, -6.7333e-05,\n",
       "                      -9.8725e-02, -2.1664e-02, -5.0843e-02, -6.9130e-02,  3.6881e-02,\n",
       "                       9.6632e-02,  1.0377e-02, -3.3680e-02,  1.6963e-01,  7.3965e-02,\n",
       "                       3.0076e-02,  5.7529e-03, -5.9927e-02, -1.1046e-01,  4.0790e-02,\n",
       "                       3.3855e-02, -2.4356e-02, -3.6695e-02,  8.2351e-03,  1.2898e-02,\n",
       "                      -6.5678e-03,  2.7664e-02,  1.5128e-02, -1.1132e-01,  8.4627e-02,\n",
       "                      -1.1952e-02,  5.1186e-02, -3.9350e-02, -8.7834e-02,  2.2054e-02,\n",
       "                      -4.3455e-02, -1.7911e-03, -2.2821e-02, -3.0689e-02, -3.7682e-02,\n",
       "                      -9.6094e-02, -9.3407e-03, -3.4789e-02,  3.0859e-02, -5.6305e-02,\n",
       "                       4.2739e-02, -1.3575e-02,  8.0566e-02,  7.6842e-02, -4.8137e-02,\n",
       "                       3.8073e-02,  2.6627e-02, -8.2534e-02, -5.5016e-02, -6.7334e-02,\n",
       "                       4.9184e-02, -5.7641e-02,  9.9676e-02, -1.6913e-02,  6.2116e-02,\n",
       "                      -1.5977e-02,  1.4333e-02, -2.9843e-02,  1.6033e-02, -5.5511e-02,\n",
       "                       5.8549e-02,  1.8413e-02,  7.4941e-02,  1.4278e-01,  8.4103e-02,\n",
       "                       1.5015e-02,  4.6642e-02,  8.2061e-02, -6.2850e-02, -2.9637e-02,\n",
       "                      -3.2310e-02,  2.6328e-02,  4.3086e-02, -3.2537e-02,  3.0865e-02,\n",
       "                      -1.7361e-02, -4.9133e-02,  3.1973e-02,  5.5399e-02, -1.0470e-01,\n",
       "                       2.7918e-01,  2.0150e-01,  1.2255e-01,  1.0140e-01,  6.9696e-02,\n",
       "                       1.1460e-01,  1.4288e-01,  6.4191e-02,  5.0661e-02,  8.0439e-02,\n",
       "                       2.4869e-01,  2.5002e-01,  1.5664e-01,  8.2281e-02,  2.0397e-01,\n",
       "                       5.4875e-02,  1.0825e-01,  1.0444e-01,  3.0434e-01,  1.7016e-01,\n",
       "                       2.6344e-01,  7.5825e-02,  3.0644e-01,  1.0320e-01,  1.6499e-02,\n",
       "                       1.6758e-01,  1.3563e-01,  1.7959e-01,  1.1576e-01,  1.5491e-01,\n",
       "                       5.6452e-02,  1.5183e-01,  1.6890e-01,  8.7245e-02,  3.5794e-01,\n",
       "                       1.0630e-01,  3.2317e-02,  3.1895e-01,  6.5187e-02,  2.2978e-01,\n",
       "                       6.9946e-02,  1.8457e-01,  1.4675e-01,  1.8490e-01,  5.1435e-02,\n",
       "                       2.8545e-01,  2.1820e-01,  1.4540e-03,  9.5590e-02,  2.1360e-01,\n",
       "                       7.6076e-02,  2.4599e-01,  6.7992e-02,  7.2668e-02,  2.3934e-01,\n",
       "                       1.2354e-01,  2.0512e-01,  1.0699e-01,  2.4603e-01,  3.1364e-01,\n",
       "                       2.0981e-01,  3.5777e-01,  3.5087e-01,  2.5951e-01,  9.9956e-02,\n",
       "                       1.5516e-01,  1.6408e-01,  4.7891e-02,  1.3486e-01,  8.6245e-02,\n",
       "                       4.3982e-01,  1.6350e-01,  1.1284e-01,  1.4935e-01,  1.2662e-01,\n",
       "                       3.6449e-01,  1.8914e-01,  2.0904e-01,  3.6481e-01,  1.0445e-01,\n",
       "                       1.1580e-01,  2.2150e-01,  7.5541e-02,  2.4161e-01,  5.5932e-02,\n",
       "                      -4.0691e-02,  9.6362e-02,  3.1627e-01,  3.0136e-01,  3.0245e-01,\n",
       "                       2.7746e-01,  1.2804e-01,  1.0351e-01,  9.0616e-02,  1.7484e-03,\n",
       "                      -7.5208e-02,  1.8948e-01,  5.5000e-02,  1.6942e-01,  6.5163e-02,\n",
       "                       3.7992e-02,  1.4973e-01,  3.6620e-01,  1.4538e-01,  9.7997e-02,\n",
       "                       8.4658e-03,  2.1460e-01,  4.5777e-02, -1.1959e-01, -3.6774e-02,\n",
       "                       2.5996e-01,  1.8179e-01, -4.1450e-05,  1.3720e-01,  1.6605e-01,\n",
       "                       1.4943e-01,  6.3437e-02,  6.8998e-02, -1.2214e-01,  9.4353e-02,\n",
       "                       1.0803e-01,  1.8255e-01,  1.4514e-01,  8.8936e-02,  1.5760e-01,\n",
       "                       3.2344e-01,  7.3483e-02])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-5.4090e-02,  3.3120e-01,  1.7284e-01,  1.7178e-01,  2.8909e-02,\n",
       "                       1.6436e-01,  2.0807e-01,  2.8183e-01,  5.8143e-02,  1.2715e-01,\n",
       "                       1.3002e-01,  5.7687e-02,  1.2004e-01,  6.3553e-02,  6.7341e-02,\n",
       "                       5.1441e-02,  3.2716e-01,  2.4230e-02, -1.5826e-02,  1.9242e-01,\n",
       "                       5.2443e-02,  1.5524e-01,  9.2222e-03,  9.6415e-02, -4.4894e-02,\n",
       "                       1.4984e-01,  3.2144e-01, -1.1883e-01,  1.6848e-01, -6.0567e-03,\n",
       "                       1.1727e-01,  1.1260e-01,  1.0005e-02,  8.3267e-02,  1.4714e-01,\n",
       "                       1.0535e-01,  1.4421e-01,  1.0029e-01,  2.9723e-01,  1.2495e-01,\n",
       "                       1.9584e-01,  7.3632e-02,  1.6792e-01,  2.4695e-01,  1.8914e-01,\n",
       "                       2.6312e-01,  1.9219e-01,  8.6119e-02,  2.1376e-01,  1.3561e-01,\n",
       "                       2.0992e-01,  5.1281e-02,  1.3083e-02,  9.6470e-02, -4.6361e-03,\n",
       "                       9.6568e-02,  1.5562e-01,  3.4155e-02,  2.8807e-01,  1.5158e-01,\n",
       "                       3.3862e-02,  2.6093e-01,  2.2083e-01,  1.0750e-01,  1.1525e-01,\n",
       "                       2.0820e-01,  6.8409e-02,  2.1214e-01,  7.8279e-02,  5.6999e-02,\n",
       "                      -5.1531e-02,  2.4783e-01,  2.3650e-01,  1.3449e-01,  1.0885e-01,\n",
       "                       1.5974e-01,  1.2737e-01,  1.7822e-01,  2.0877e-01,  2.0494e-01,\n",
       "                      -3.6341e-02,  1.1739e-02,  2.2654e-01, -1.0999e-01,  2.2666e-01,\n",
       "                       1.1504e-01,  1.7170e-01,  2.3571e-01,  3.5165e-02,  2.3386e-01,\n",
       "                       2.2701e-01,  3.5185e-02,  2.5366e-02,  1.1350e-01,  1.2611e-02,\n",
       "                      -2.3670e-02,  1.3044e-01,  8.3783e-02,  1.1682e-02,  2.4565e-01,\n",
       "                       1.2537e-01,  2.0956e-01,  2.0960e-01,  1.2213e-01,  8.5111e-02,\n",
       "                       4.8595e-02,  2.4787e-02,  9.2675e-02,  1.8255e-01,  3.5386e-01,\n",
       "                       1.2033e-01,  2.8929e-01,  9.0346e-02,  1.9678e-01,  8.9742e-02,\n",
       "                       2.6156e-01,  9.8707e-02,  9.0202e-02,  1.2753e-01, -4.2002e-02,\n",
       "                       4.7480e-02, -5.9609e-02,  6.2240e-02,  2.0411e-01,  1.3346e-01,\n",
       "                       1.4962e-01,  2.2493e-01, -1.3075e-02,  1.4323e-01, -6.2992e-02,\n",
       "                       1.1628e-01,  1.4932e-01,  8.3884e-02, -1.4033e-02,  1.8421e-03,\n",
       "                      -2.5970e-02, -2.6116e-03,  2.3099e-02, -4.1568e-02,  7.5780e-02,\n",
       "                      -2.8220e-02,  1.4219e-01,  1.1156e-01,  1.2032e-01,  1.6560e-01,\n",
       "                      -1.3600e-02, -7.3180e-03,  3.4980e-02, -1.6099e-02,  4.6202e-02,\n",
       "                      -1.3048e-02, -4.5467e-02,  1.4138e-01,  7.6424e-02,  5.1266e-02,\n",
       "                      -5.0105e-02,  2.0083e-02, -7.8082e-02,  2.8864e-02,  1.6928e-01,\n",
       "                      -5.1404e-03, -1.5690e-01, -1.5717e-01, -5.0917e-02, -1.2465e-02,\n",
       "                       6.4756e-04, -6.2350e-02,  1.0283e-01,  1.3447e-01, -8.6116e-03,\n",
       "                      -1.6435e-03,  6.5904e-02,  2.1072e-01, -5.6481e-02,  8.5863e-02,\n",
       "                       5.6339e-02,  1.9749e-02, -8.6010e-02,  4.3076e-03,  1.9392e-01,\n",
       "                       2.7321e-03, -8.5105e-02,  1.2599e-01,  4.3328e-03, -3.7354e-02,\n",
       "                      -3.5863e-02,  3.5739e-02, -3.9408e-04,  6.7228e-02,  9.1272e-02,\n",
       "                      -1.1276e-01,  3.0995e-02,  8.9454e-02,  1.1996e-01,  2.2262e-01,\n",
       "                       8.4517e-02,  1.0670e-01,  1.4682e-01,  8.3356e-02, -2.0636e-03,\n",
       "                      -7.0668e-03,  1.7846e-01,  1.2432e-01,  5.4841e-03,  9.8137e-02,\n",
       "                       1.7477e-02, -5.7001e-02,  3.0112e-02,  5.3857e-02,  1.7611e-01,\n",
       "                       5.6962e-02,  1.1721e-02, -2.9970e-02,  7.1048e-02, -9.2134e-03,\n",
       "                      -4.6367e-02, -1.2133e-03, -9.2253e-02,  3.7650e-02,  1.4534e-01,\n",
       "                       6.5269e-02, -5.8211e-02,  1.3683e-01,  3.7866e-03,  8.6897e-02,\n",
       "                      -6.9699e-02,  2.4488e-02, -9.3132e-03, -5.1955e-02, -4.4146e-02,\n",
       "                      -5.6083e-02,  4.7302e-03,  7.6057e-02,  1.3600e-01,  7.4416e-02,\n",
       "                      -3.8408e-02,  2.2158e-02,  4.5371e-02,  5.9854e-02,  4.2331e-02,\n",
       "                       7.2077e-02,  2.0534e-01,  5.4870e-02,  7.9501e-03, -6.1893e-05,\n",
       "                      -3.0920e-02,  4.9752e-02,  1.8119e-01,  3.1766e-02, -6.4639e-03,\n",
       "                      -1.1144e-03,  3.4432e-02,  2.5984e-02,  1.3778e-01, -8.6036e-02,\n",
       "                       1.3645e-01, -3.6309e-02,  7.4040e-02,  2.7587e-03, -6.6633e-02,\n",
       "                      -1.3313e-01,  1.8131e-02, -5.0101e-04,  4.8265e-03,  1.0501e-01,\n",
       "                      -1.0567e-01,  6.0589e-02,  8.4574e-02, -5.4550e-02, -7.9692e-02,\n",
       "                      -1.1959e-02, -1.3069e-02, -4.9703e-02,  1.8747e-02, -2.9797e-02,\n",
       "                      -3.1395e-02,  6.5014e-02, -6.6558e-02,  1.6835e-01, -6.1026e-02,\n",
       "                       5.6605e-02,  2.7900e-02,  5.8005e-02, -1.0940e-01, -1.5229e-02,\n",
       "                       6.7647e-02, -1.2476e-02,  5.1843e-03, -1.2217e-02,  1.9416e-02,\n",
       "                       5.1050e-02,  1.7140e-01, -2.0807e-02,  1.1497e-01,  1.8380e-01,\n",
       "                       8.6065e-02, -1.3110e-01, -9.4639e-02, -2.8321e-02, -5.6821e-02,\n",
       "                      -1.1138e-02, -3.1490e-02,  3.3784e-02,  1.8547e-02, -1.3149e-02,\n",
       "                      -1.9864e-01, -7.9336e-03,  3.4530e-03, -1.1599e-02,  5.8182e-02,\n",
       "                      -1.4089e-01, -6.4815e-02, -6.3321e-02,  5.4900e-02,  3.9793e-02,\n",
       "                      -1.6101e-01, -8.7673e-02, -6.5550e-02, -4.0836e-02,  1.1193e-01,\n",
       "                      -1.2733e-01,  3.3920e-02,  1.8669e-02, -1.4364e-02, -6.9594e-02,\n",
       "                       1.1548e-01,  1.2602e-01,  1.0434e-01,  3.4594e-02,  4.0747e-03,\n",
       "                      -1.8113e-02, -6.3729e-03, -1.8465e-01, -1.9721e-02,  2.8395e-02,\n",
       "                      -2.1563e-02,  6.4784e-03,  9.3864e-02, -7.2601e-03,  4.9990e-02,\n",
       "                      -1.7166e-02,  1.0630e-02,  1.7278e-02,  6.8003e-02, -4.0290e-02,\n",
       "                       3.3982e-02, -7.7897e-02,  6.7031e-02, -2.2489e-03,  5.1088e-02,\n",
       "                      -1.0098e-01,  5.1374e-02, -7.3296e-02,  7.5908e-03,  8.3146e-02,\n",
       "                      -2.3254e-02, -3.9160e-02, -3.3582e-02, -4.5048e-02, -3.2957e-02,\n",
       "                       2.2823e-02, -1.0031e-01, -1.1246e-01,  3.0057e-02,  4.3757e-02,\n",
       "                       1.0484e-01,  5.6907e-02,  5.4056e-03,  6.5234e-02,  7.3324e-02,\n",
       "                      -4.9922e-02,  4.0478e-02, -9.8034e-04, -4.5575e-02, -4.2719e-02,\n",
       "                      -1.8471e-01, -2.7278e-02,  5.1660e-02, -7.8670e-02, -1.1322e-01,\n",
       "                      -1.4728e-02,  6.5472e-02,  2.1116e-02,  6.5925e-03, -4.1509e-02,\n",
       "                       2.7713e-01,  3.9995e-02,  2.2895e-01,  5.2720e-02,  1.3194e-01,\n",
       "                       2.3326e-01,  2.4978e-01,  8.4943e-02,  1.5407e-01,  1.1386e-01,\n",
       "                       2.5497e-01,  2.4654e-01,  2.7860e-01,  1.3147e-01,  1.5098e-01,\n",
       "                       1.3220e-01,  2.3885e-01,  8.3302e-02,  3.9850e-01,  1.2409e-01,\n",
       "                       3.4968e-01, -2.8746e-02,  2.1292e-01,  1.1751e-01,  1.2096e-02,\n",
       "                       4.3396e-02,  1.1128e-01,  1.9840e-01,  1.5297e-01,  1.5820e-01,\n",
       "                       1.1225e-01,  1.4154e-01,  1.0809e-01,  1.0039e-01,  3.7726e-01,\n",
       "                       4.1267e-02,  7.3539e-02,  1.8555e-01,  3.3217e-02,  3.1653e-01,\n",
       "                       1.2646e-02,  1.4484e-01,  2.1958e-01,  2.7570e-01,  9.2509e-02,\n",
       "                       1.7231e-01,  8.9336e-02,  1.3535e-02, -2.5016e-02,  2.8101e-01,\n",
       "                       1.0513e-01,  2.7817e-01,  1.7272e-01,  1.8982e-01,  1.6430e-01,\n",
       "                       1.9276e-01,  1.1851e-01,  2.4817e-01,  2.2215e-01,  2.5391e-01,\n",
       "                       1.6972e-01,  2.0884e-01,  2.7946e-01,  1.4933e-01,  1.9242e-01,\n",
       "                       7.8953e-02,  2.6693e-01, -2.9641e-02,  1.4550e-01,  1.3011e-03,\n",
       "                       4.7809e-01,  1.0507e-01,  1.3694e-01,  1.6174e-01, -2.2997e-02,\n",
       "                       3.6185e-01,  1.7171e-01,  2.0410e-01,  1.8608e-01,  9.8496e-02,\n",
       "                       1.4761e-01,  5.0449e-02,  1.0611e-01,  4.0272e-02,  2.3933e-01,\n",
       "                      -1.3795e-02,  1.7739e-01,  2.1761e-01,  2.5172e-01,  1.3258e-01,\n",
       "                       1.6505e-01,  5.9705e-02,  2.2060e-02,  1.2133e-01,  1.1633e-03,\n",
       "                       3.0973e-02,  7.8748e-02,  2.4361e-01,  4.4604e-02,  2.0218e-01,\n",
       "                       1.8267e-01,  2.2113e-01,  3.5084e-01,  5.8172e-02,  6.1657e-02,\n",
       "                      -1.7349e-03,  8.5527e-02,  1.4452e-01, -8.8387e-02,  5.0085e-02,\n",
       "                       2.5402e-01,  2.2772e-01, -2.3071e-02,  7.7171e-02,  1.8319e-01,\n",
       "                       1.1818e-01,  1.6869e-01,  5.5453e-02, -1.2120e-01, -9.2480e-03,\n",
       "                       8.0115e-02,  5.8934e-02,  1.2104e-01,  2.0836e-01,  1.5806e-01,\n",
       "                       2.9882e-01,  1.4346e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0652, -0.1263, -0.1187,  ..., -0.0920,  0.1748, -0.1033],\n",
       "                      [-0.0818,  0.0891, -0.0889,  ..., -0.1514, -0.1818, -0.1383],\n",
       "                      [-0.0154,  0.0214,  0.0117,  ..., -0.0970, -0.0587,  0.2901],\n",
       "                      ...,\n",
       "                      [ 0.1281, -0.1437, -0.1816,  ..., -0.1204, -0.0096, -0.2045],\n",
       "                      [ 0.2054, -0.3252,  0.1681,  ...,  0.2164, -0.0596, -0.1513],\n",
       "                      [ 0.0275, -0.3920, -0.1316,  ..., -0.1443, -0.0070, -0.2589]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[-1.8554e-02,  6.8414e-02, -1.1658e-02,  ...,  9.1288e-03,\n",
       "                       -2.1021e-01, -2.0986e-02],\n",
       "                      [-4.1333e-02,  1.9448e-01, -2.8689e-02,  ...,  5.3862e-02,\n",
       "                       -1.1608e-01,  1.3347e-02],\n",
       "                      [-2.3231e-01,  1.6186e-01, -2.0684e-01,  ..., -1.8361e-02,\n",
       "                        2.6997e-02, -5.8023e-02],\n",
       "                      ...,\n",
       "                      [-6.4188e-02, -1.0034e-01,  3.0699e-02,  ...,  2.9113e-02,\n",
       "                       -3.6885e-02,  1.3006e-01],\n",
       "                      [-3.7773e-02,  1.9467e-01, -1.6737e-01,  ..., -1.7365e-01,\n",
       "                       -6.7720e-03, -2.3427e-02],\n",
       "                      [-1.9944e-04, -1.3638e-01,  2.2029e-01,  ..., -1.3159e-01,\n",
       "                       -5.3452e-02,  1.5621e-01]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([ 0.0912, -0.0622,  0.1165,  0.0928,  0.2167,  0.0946,  0.1748,  0.1041,\n",
       "                      -0.0040,  0.4084,  0.2102,  0.1972,  0.2139,  0.1858,  0.0663,  0.2277,\n",
       "                       0.1688,  0.0263,  0.2319,  0.2005,  0.2067,  0.3166,  0.0861,  0.1072,\n",
       "                       0.0306,  0.1830,  0.0298,  0.1371,  0.1044,  0.1987,  0.0441,  0.1143,\n",
       "                       0.2746, -0.2174,  0.0434,  0.0598, -0.0064,  0.2199,  0.1340,  0.0346,\n",
       "                       0.1367,  0.1470,  0.0926,  0.1754,  0.2874,  0.2382,  0.2043,  0.0730,\n",
       "                       0.3480,  0.1261, -0.0149,  0.1465,  0.1137,  0.2089,  0.3019,  0.1701,\n",
       "                       0.2319,  0.0754,  0.0099,  0.0822,  0.1271,  0.1244,  0.2694,  0.2539,\n",
       "                       0.1472,  0.1458,  0.0927,  0.3359,  0.0508,  0.2884,  0.1400,  0.3456,\n",
       "                       0.2788,  0.1271,  0.2206, -0.0143,  0.2398,  0.1776,  0.1067,  0.1136,\n",
       "                       0.1553,  0.1611,  0.2052,  0.2193,  0.1001,  0.0738,  0.1698, -0.0015,\n",
       "                       0.2899, -0.0559, -0.0776,  0.1632,  0.0927,  0.1106,  0.1666,  0.1029,\n",
       "                       0.1906,  0.0044,  0.2191,  0.0980,  0.3265,  0.2698, -0.0210,  0.0439,\n",
       "                       0.1580,  0.2449,  0.0718,  0.1775,  0.1120, -0.0578,  0.0656,  0.2211,\n",
       "                       0.2668,  0.0535,  0.0556, -0.0061,  0.1009,  0.0807,  0.1555,  0.2199,\n",
       "                       0.1990,  0.2238,  0.1731, -0.0445,  0.1453,  0.1803,  0.1154,  0.2354,\n",
       "                      -0.0597, -0.0176,  0.0048, -0.1181, -0.0879, -0.0470,  0.0373, -0.0154,\n",
       "                      -0.0547,  0.0317,  0.0099, -0.0052,  0.0578, -0.0495, -0.1263,  0.0955,\n",
       "                       0.0255, -0.0552, -0.0559,  0.0319,  0.0328, -0.0310, -0.0367,  0.0359,\n",
       "                       0.0247, -0.0008, -0.0284,  0.0222, -0.0353, -0.0074,  0.0380, -0.0416,\n",
       "                       0.0842, -0.0745,  0.0443, -0.0033, -0.0860,  0.0009, -0.1021, -0.0577,\n",
       "                      -0.0195,  0.1090, -0.0804, -0.0133,  0.0776,  0.0287, -0.0514, -0.0241,\n",
       "                      -0.0771,  0.0126, -0.1453,  0.0037,  0.0135, -0.0392,  0.0384, -0.0113,\n",
       "                       0.0155,  0.0202,  0.0124,  0.0512,  0.0789, -0.0059,  0.0635,  0.0690,\n",
       "                      -0.0520,  0.0320, -0.1458, -0.0037, -0.0624, -0.0730,  0.1280, -0.0055,\n",
       "                      -0.0889, -0.0648,  0.0202,  0.0365, -0.0030, -0.0696, -0.1024, -0.0990,\n",
       "                      -0.0861,  0.0412,  0.0181, -0.0392,  0.1126, -0.0379, -0.0196, -0.0957,\n",
       "                       0.0057, -0.0967, -0.0490, -0.0282, -0.0097,  0.0555, -0.0384,  0.0290,\n",
       "                      -0.0353, -0.0447,  0.0206,  0.0750,  0.0717,  0.0370,  0.0668,  0.0080,\n",
       "                      -0.0050, -0.0075, -0.0581,  0.0494, -0.0069,  0.0445, -0.0444,  0.1022,\n",
       "                       0.0435, -0.0564, -0.0414,  0.1198, -0.1199,  0.0641, -0.0825,  0.0109,\n",
       "                       0.0055, -0.0039,  0.0175, -0.0034,  0.0112,  0.1586,  0.0268, -0.0108,\n",
       "                      -0.0528,  0.0193, -0.0752, -0.1167,  0.0197, -0.0384,  0.0696, -0.0227,\n",
       "                       0.1259, -0.0545, -0.0101, -0.0217,  0.0176,  0.0421, -0.1307, -0.0432,\n",
       "                       0.0386, -0.0186, -0.0237, -0.0588,  0.2746, -0.0092,  0.0139,  0.1196,\n",
       "                       0.0122, -0.0818, -0.0037, -0.0864,  0.0520, -0.0216, -0.0465,  0.0255,\n",
       "                       0.0870,  0.1192,  0.1168, -0.0803, -0.0551, -0.0006, -0.1105, -0.0475,\n",
       "                      -0.1681,  0.0658,  0.0317, -0.1195, -0.0671, -0.0042, -0.0506, -0.0950,\n",
       "                       0.0358,  0.0558, -0.1267, -0.0060, -0.1327,  0.0968,  0.0992,  0.1185,\n",
       "                      -0.0005,  0.0469, -0.0635, -0.0463, -0.1009,  0.0522,  0.1721,  0.0713,\n",
       "                      -0.0410, -0.1614, -0.0161,  0.0829, -0.0442, -0.0892, -0.1729,  0.0894,\n",
       "                       0.0351,  0.0251, -0.0612,  0.0552,  0.0727, -0.1203, -0.0230, -0.0201,\n",
       "                      -0.1040, -0.0493,  0.0038,  0.1143,  0.0959, -0.2012, -0.1066,  0.0656,\n",
       "                       0.0582,  0.0747, -0.0260, -0.0533,  0.0127,  0.0038,  0.0255,  0.0388,\n",
       "                      -0.1247,  0.0258, -0.0041, -0.0641, -0.0282,  0.0511,  0.0326,  0.0282,\n",
       "                       0.0366,  0.0291,  0.0346,  0.0296, -0.0279, -0.0725, -0.0241,  0.0015,\n",
       "                       0.1348, -0.0762, -0.0127,  0.0700,  0.1346, -0.1181,  0.0500, -0.0688,\n",
       "                      -0.1550,  0.0407,  0.0675, -0.0097,  0.0354, -0.0805,  0.0520,  0.1077,\n",
       "                       0.1229, -0.2399,  0.1580,  0.0918,  0.1066,  0.2689,  0.1857, -0.0369,\n",
       "                       0.0070,  0.3233,  0.2236,  0.2155,  0.1849,  0.0251,  0.1691,  0.2184,\n",
       "                       0.1630,  0.0009,  0.3278,  0.3059,  0.2304,  0.2531,  0.1049,  0.1674,\n",
       "                       0.0240,  0.2505,  0.0373,  0.0663,  0.4028,  0.2328,  0.0862, -0.0060,\n",
       "                       0.0693, -0.0654, -0.0118,  0.0895,  0.1108,  0.4062,  0.0865,  0.0850,\n",
       "                       0.1878,  0.1894,  0.0658,  0.2055,  0.1715,  0.1246,  0.1543,  0.2537,\n",
       "                       0.3946,  0.0795,  0.1271, -0.0182,  0.1413,  0.1658,  0.2036,  0.0416,\n",
       "                       0.1227,  0.0990,  0.0351,  0.1817,  0.1299,  0.0188,  0.2587,  0.2533,\n",
       "                       0.1560,  0.0956,  0.0658,  0.4907,  0.0869,  0.2968,  0.3145,  0.2871,\n",
       "                       0.0704,  0.1426,  0.2418,  0.0506,  0.2240,  0.2224,  0.0511,  0.1235,\n",
       "                       0.0561, -0.0267,  0.1712,  0.1834,  0.0135,  0.1270,  0.2775, -0.0015,\n",
       "                       0.2451,  0.0856, -0.0607,  0.1849,  0.0900,  0.2851,  0.0934,  0.0788,\n",
       "                       0.2597,  0.0088,  0.2932,  0.0626,  0.2776,  0.2265,  0.1988,  0.0056,\n",
       "                       0.2078,  0.2122,  0.1672,  0.2369,  0.2185, -0.0107,  0.0294,  0.1840,\n",
       "                       0.1204,  0.2156,  0.0581, -0.1149,  0.1678,  0.1170,  0.2016,  0.2058,\n",
       "                       0.2209,  0.0493,  0.0659,  0.0839,  0.0008,  0.0116,  0.0424,  0.2835])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([ 2.3683e-01, -1.1568e-01,  1.3754e-01,  5.7019e-02,  1.6758e-01,\n",
       "                       1.3670e-01,  2.0110e-01,  8.6864e-02,  3.5408e-02,  3.0386e-01,\n",
       "                       8.2314e-02,  1.8805e-01,  2.6451e-01,  1.4965e-01,  1.3380e-01,\n",
       "                       3.6435e-01,  1.5095e-01,  1.2646e-01,  1.8675e-01,  1.8072e-01,\n",
       "                       1.9329e-01,  3.7258e-01,  8.3479e-02,  3.6591e-02,  4.6473e-02,\n",
       "                       1.9398e-01,  1.1520e-01,  5.8836e-02,  2.7471e-01,  2.1346e-01,\n",
       "                       5.0049e-02,  2.0352e-01,  2.8644e-01,  2.8403e-02, -4.0381e-02,\n",
       "                       1.7665e-01,  1.3886e-02,  1.0677e-01,  9.4912e-02,  1.1072e-01,\n",
       "                       1.8028e-01,  1.8983e-01,  1.9338e-01,  2.1469e-01,  2.3238e-01,\n",
       "                       1.0491e-01,  2.6374e-01,  1.3315e-01,  3.4264e-01,  6.8295e-02,\n",
       "                       2.3873e-02,  1.0818e-01,  2.4742e-01,  1.6166e-01,  3.1929e-01,\n",
       "                       5.3037e-02,  9.0420e-02,  2.3698e-01,  9.1209e-02,  1.4752e-01,\n",
       "                       1.6782e-02,  2.3305e-02,  2.1114e-01,  3.2164e-01,  9.4025e-03,\n",
       "                       1.4530e-01,  1.1755e-01,  3.5302e-01,  1.0548e-01,  2.3595e-01,\n",
       "                       8.8732e-02,  3.6255e-01,  1.9651e-01,  1.3626e-01,  3.1077e-01,\n",
       "                       4.1047e-02,  1.3538e-01,  1.7230e-01,  5.6302e-02,  1.8003e-01,\n",
       "                       2.1749e-01,  7.9876e-02,  2.5722e-01,  8.2465e-02, -9.0811e-03,\n",
       "                       1.3408e-01,  9.1777e-02,  2.0906e-01,  2.3200e-01, -1.0838e-01,\n",
       "                      -2.0952e-02,  3.7220e-02,  7.3721e-02,  1.0162e-01,  2.0398e-01,\n",
       "                       1.4825e-01,  1.7580e-01,  1.1344e-01,  2.9009e-01,  1.3809e-01,\n",
       "                       1.8409e-01,  2.0286e-01,  3.9476e-02,  8.8600e-02,  1.9837e-01,\n",
       "                       3.0840e-01,  2.1925e-01,  2.4350e-01,  1.4274e-01,  8.2107e-02,\n",
       "                       1.2094e-01,  2.0708e-01,  1.9046e-01,  5.3186e-02,  9.5300e-02,\n",
       "                      -1.9299e-01,  1.0093e-01,  1.0281e-01,  8.7442e-02,  2.4006e-01,\n",
       "                       2.9225e-01,  1.8404e-01,  2.8558e-01, -6.0358e-02,  2.0775e-01,\n",
       "                       1.5013e-01,  8.2301e-02,  1.6253e-01,  4.1661e-02, -1.2683e-01,\n",
       "                      -6.0313e-02, -3.1326e-02, -2.7247e-02, -8.0612e-03, -3.8606e-02,\n",
       "                      -2.6046e-02,  1.0794e-01,  2.8451e-02, -5.5539e-02, -1.8932e-02,\n",
       "                      -5.4573e-02, -6.7633e-03,  1.1729e-01, -5.0725e-02, -6.3380e-02,\n",
       "                       4.8888e-02, -1.8300e-02,  1.1676e-02, -6.7515e-02, -4.6761e-02,\n",
       "                      -8.4346e-02, -3.1959e-02, -3.2768e-02, -2.4136e-02, -5.4532e-02,\n",
       "                       2.9897e-02, -2.2132e-02,  2.2086e-02,  5.6612e-02, -5.1237e-02,\n",
       "                      -1.5159e-02, -9.2402e-02,  5.0940e-02,  5.4567e-02, -7.9339e-02,\n",
       "                      -5.3149e-02, -9.1091e-02,  1.5083e-02,  5.2771e-02, -2.0790e-02,\n",
       "                      -1.8803e-02,  8.3521e-04, -4.5378e-02, -2.5769e-03,  4.7491e-02,\n",
       "                      -6.7059e-02, -6.2009e-02, -1.0730e-01, -4.6414e-02, -3.4048e-03,\n",
       "                      -6.9956e-02,  4.1698e-03, -4.1627e-03, -4.9056e-03,  3.6653e-02,\n",
       "                       1.1144e-02, -1.3500e-02, -6.8502e-02, -4.4216e-02,  7.3412e-02,\n",
       "                       4.5327e-02,  2.5419e-02,  5.9688e-02,  4.0783e-02,  1.5566e-02,\n",
       "                      -2.9381e-02, -2.3673e-02, -1.1272e-01,  6.1160e-02, -1.5622e-02,\n",
       "                      -2.8255e-02,  3.6722e-02, -3.0742e-02,  5.8537e-02, -2.8511e-02,\n",
       "                       2.5393e-02,  1.0537e-02,  3.8567e-02,  2.1884e-02, -5.6652e-02,\n",
       "                      -1.9373e-02,  5.9909e-02, -7.3860e-03, -5.2864e-02, -9.6963e-02,\n",
       "                      -8.7288e-02, -5.2642e-02, -1.1532e-01,  1.0889e-01, -7.9577e-02,\n",
       "                       6.9302e-03,  2.7526e-02,  8.2162e-03,  1.1369e-02, -1.0102e-01,\n",
       "                       7.4288e-02,  1.1581e-01,  2.4549e-02, -3.2093e-02, -3.7079e-02,\n",
       "                       6.2825e-03, -1.2617e-01,  3.8644e-02, -4.3555e-02, -1.2649e-01,\n",
       "                       1.3489e-02, -1.3485e-02, -2.1624e-02, -5.3328e-02, -1.4406e-02,\n",
       "                       2.4538e-02, -3.8638e-02, -4.9421e-02,  1.9396e-03, -4.6928e-02,\n",
       "                      -2.3218e-02, -2.6461e-03,  1.9441e-02, -1.8280e-02, -4.1948e-02,\n",
       "                       8.9148e-02,  1.3246e-02, -8.7273e-02, -4.7852e-02, -8.4290e-03,\n",
       "                       8.2633e-02, -5.1095e-02, -1.3477e-04,  1.1931e-02, -2.8884e-02,\n",
       "                       8.4222e-03,  1.2018e-01, -4.5261e-02,  7.8500e-03, -4.8408e-02,\n",
       "                       5.4088e-02, -4.1313e-02,  5.9163e-03, -1.5762e-01,  8.2687e-03,\n",
       "                      -1.0724e-01,  7.0785e-02,  6.0661e-02,  1.7477e-01, -4.6575e-02,\n",
       "                       1.6076e-02,  2.1898e-01,  9.7924e-02, -1.1418e-01,  1.2295e-01,\n",
       "                      -2.4279e-02, -1.1388e-01,  1.1093e-01, -1.6802e-01, -1.3402e-02,\n",
       "                      -6.6074e-02,  1.2178e-02,  3.7233e-02,  8.6677e-03, -7.6795e-02,\n",
       "                       1.7604e-02, -4.6092e-02, -2.0686e-02, -7.5238e-02, -4.0993e-02,\n",
       "                      -6.3046e-02, -9.7086e-02, -3.8108e-02,  9.2563e-03, -9.3657e-02,\n",
       "                      -2.5495e-02, -1.1472e-01, -1.7152e-02, -3.4196e-02, -4.0396e-02,\n",
       "                       5.3882e-02, -1.2054e-01,  4.9291e-02,  3.8957e-02, -8.6438e-03,\n",
       "                       2.0869e-01,  4.0642e-02,  1.9254e-01, -6.5140e-02,  1.0268e-02,\n",
       "                       1.4759e-01, -7.1887e-02, -3.1298e-03,  1.9532e-01,  1.2116e-01,\n",
       "                      -8.5415e-02, -7.7883e-03, -1.1618e-01,  5.7667e-02,  5.2610e-02,\n",
       "                      -1.6037e-01, -9.9329e-02,  5.3039e-02,  7.3823e-02, -4.2801e-02,\n",
       "                       1.0774e-01,  1.1592e-01,  6.9744e-02, -1.7432e-01,  8.0203e-02,\n",
       "                       1.3493e-01,  3.9473e-02, -5.8594e-02,  1.1221e-02,  1.9726e-01,\n",
       "                       4.8711e-02,  1.1796e-02, -7.5200e-02,  3.6053e-02,  1.0250e-02,\n",
       "                       7.0400e-02,  2.9537e-02,  1.4322e-02,  1.0245e-01, -7.3027e-02,\n",
       "                       1.2797e-02, -2.5629e-02, -4.2416e-02,  6.8446e-02,  6.2691e-03,\n",
       "                       7.1216e-02, -1.0567e-01,  6.3238e-02, -6.1821e-02, -1.3222e-02,\n",
       "                      -8.6312e-02,  2.8555e-02, -8.0015e-03,  1.0838e-01,  5.7102e-02,\n",
       "                      -3.3539e-02,  3.5455e-02, -5.2714e-02,  9.4115e-03,  2.7462e-02,\n",
       "                      -5.2299e-02, -9.6535e-02,  1.0635e-01,  2.3214e-02, -1.3936e-01,\n",
       "                      -6.6522e-02, -2.4351e-01,  5.0620e-02,  2.6513e-02,  4.6129e-02,\n",
       "                       2.4863e-02, -7.3726e-03, -5.0495e-02,  3.0636e-02,  6.0826e-02,\n",
       "                      -7.3274e-02,  1.4503e-01,  4.7551e-03,  1.0855e-01,  1.9539e-01,\n",
       "                       2.6552e-01,  7.7550e-02,  1.0727e-02,  2.5019e-01,  1.8857e-01,\n",
       "                       1.5208e-01,  2.2410e-01,  2.3577e-01,  1.0591e-01,  1.8998e-01,\n",
       "                       9.2529e-02,  2.3726e-02,  3.2341e-01, -2.8089e-02,  1.5416e-01,\n",
       "                       2.3207e-01,  1.4997e-01,  7.4115e-02,  1.6243e-02,  3.0663e-01,\n",
       "                       1.5362e-01,  1.4148e-01,  4.3077e-01,  1.7626e-01, -6.4470e-02,\n",
       "                      -5.9009e-02,  1.7986e-01, -4.0992e-02, -6.5139e-02,  1.4195e-01,\n",
       "                       7.1699e-02,  2.3233e-01,  1.1172e-01,  1.1257e-01,  1.9223e-01,\n",
       "                       2.1877e-01,  6.0551e-02,  2.7163e-01,  1.7135e-01,  3.3067e-03,\n",
       "                       2.4535e-01,  1.7591e-01,  2.6796e-01,  1.8586e-01, -6.1970e-02,\n",
       "                       1.4791e-02,  1.4699e-01,  1.8190e-01,  1.7164e-01,  1.9948e-01,\n",
       "                       6.1302e-02,  1.2295e-01,  5.6660e-02,  1.3214e-01,  1.3372e-01,\n",
       "                       7.9981e-02,  3.5896e-01,  2.4738e-01,  2.6303e-01,  1.4373e-01,\n",
       "                       1.5232e-01,  3.4291e-01,  2.1888e-01,  2.6552e-01,  1.5191e-01,\n",
       "                       1.8002e-01,  2.5002e-01,  2.0638e-01,  1.9268e-01,  2.9311e-02,\n",
       "                       2.6191e-01,  5.5223e-03,  1.4774e-01,  9.4527e-02,  3.5562e-02,\n",
       "                       6.7021e-02,  2.2084e-01,  2.3345e-01,  4.0807e-02, -4.1194e-02,\n",
       "                       3.2138e-01,  7.6888e-02,  1.3841e-01,  1.2549e-01, -2.1178e-02,\n",
       "                       1.2120e-01,  5.1891e-02,  9.5773e-02,  1.0608e-01,  1.4280e-01,\n",
       "                       2.2578e-01, -5.1614e-02,  5.3236e-02,  1.2876e-02,  2.7763e-01,\n",
       "                       2.9005e-01,  2.0476e-01,  1.2952e-01,  3.8570e-02,  3.5163e-01,\n",
       "                       2.0337e-01,  1.6011e-01,  1.4892e-01,  9.9017e-02,  1.3301e-01,\n",
       "                       2.0972e-01,  1.5486e-01,  1.0568e-01,  6.2703e-02, -3.8650e-02,\n",
       "                       1.3823e-01,  1.2992e-01,  2.7541e-01,  3.2601e-01,  1.4807e-01,\n",
       "                       8.6163e-02,  2.0614e-01, -2.0414e-02,  1.9504e-01,  5.0271e-02,\n",
       "                       8.1759e-02,  1.9426e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.0664,  0.0298, -0.1882,  ..., -0.0942,  0.7115,  0.2123],\n",
       "                      [ 0.0839,  0.0346, -0.1566,  ..., -0.0375,  0.2558,  0.2447],\n",
       "                      [-0.0473, -0.1020, -0.2337,  ..., -0.2907, -0.0941,  0.0434],\n",
       "                      ...,\n",
       "                      [ 0.1940, -0.1501, -0.3089,  ...,  0.0350,  0.0771,  0.0342],\n",
       "                      [ 0.3367,  0.3031,  0.3771,  ..., -0.3894, -0.1162, -0.2893],\n",
       "                      [-0.3969, -0.1283,  0.0291,  ...,  0.1465,  0.2588,  0.1257]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0210, -0.0012, -0.0548,  ..., -0.0015,  0.0831,  0.1648],\n",
       "                      [-0.0949, -0.0903,  0.1211,  ...,  0.0235,  0.0441, -0.0771],\n",
       "                      [ 0.1792, -0.0913, -0.1085,  ...,  0.0008, -0.0211, -0.1464],\n",
       "                      ...,\n",
       "                      [ 0.0220,  0.0292, -0.0326,  ..., -0.0238, -0.2700,  0.0559],\n",
       "                      [ 0.2027, -0.1640, -0.1328,  ...,  0.1830,  0.0446, -0.0409],\n",
       "                      [-0.2648, -0.0815,  0.4380,  ...,  0.0742, -0.0737, -0.1109]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 6.4210e-04,  4.6106e-02, -7.6997e-02,  2.5325e-01, -8.4013e-03,\n",
       "                       3.2821e-01,  2.8451e-01,  1.7684e-01, -1.3331e-02,  8.9955e-02,\n",
       "                       4.3056e-03,  2.5006e-01,  6.5853e-02,  1.4767e-01, -6.1643e-02,\n",
       "                       1.2803e-01,  7.7573e-02,  6.7970e-02,  2.0336e-01,  7.0288e-02,\n",
       "                       1.4784e-01,  1.2068e-01, -1.5433e-02,  1.2257e-01,  1.6766e-01,\n",
       "                       1.5809e-01,  5.2930e-02,  3.9410e-01,  1.0753e-01,  3.0143e-01,\n",
       "                       2.0853e-01, -1.0555e-01,  1.0060e-01,  2.8480e-01,  1.8821e-01,\n",
       "                       1.4068e-01,  2.4537e-01,  2.7571e-01,  5.2072e-02, -9.6918e-02,\n",
       "                       3.3304e-01, -5.8105e-02,  4.1869e-01,  3.7765e-01, -1.2979e-01,\n",
       "                       3.1773e-01,  9.1588e-02,  1.8876e-01, -1.4817e-02,  9.4008e-02,\n",
       "                       1.4918e-01, -8.7702e-02,  2.8845e-01,  2.0539e-01,  8.8923e-02,\n",
       "                      -3.0538e-02,  9.1436e-02,  1.5959e-01,  7.8662e-02,  2.3730e-01,\n",
       "                      -5.1933e-02,  7.7914e-02,  2.3023e-01,  1.0809e-01,  1.0897e-01,\n",
       "                       5.6978e-02,  1.5435e-01, -9.5186e-03,  4.8989e-02,  6.0417e-02,\n",
       "                       5.3776e-02,  8.7466e-02,  1.8332e-01, -4.5456e-03,  2.4840e-01,\n",
       "                      -3.3269e-02,  2.1287e-02,  1.0763e-01,  5.9610e-02,  1.2307e-01,\n",
       "                       2.9597e-01,  5.5957e-02,  1.2117e-02,  5.4625e-02,  2.9780e-02,\n",
       "                       2.2723e-01,  1.9101e-01,  9.1854e-02,  1.8355e-01,  1.2115e-01,\n",
       "                       1.6207e-01,  9.4492e-02,  1.8851e-01,  3.1826e-01,  1.0890e-01,\n",
       "                       7.5603e-02,  5.7133e-03,  2.0642e-01,  9.3815e-02,  2.4939e-01,\n",
       "                       8.7055e-02,  8.7342e-02,  1.1035e-01,  2.1544e-01, -1.5391e-01,\n",
       "                       6.1248e-02,  9.6005e-02,  1.2811e-01, -1.0579e-02,  1.5945e-01,\n",
       "                       3.0828e-01,  1.0791e-01,  7.8106e-02,  1.7387e-02, -1.3235e-01,\n",
       "                       8.0107e-02,  2.4668e-01,  4.8614e-02,  2.5375e-01,  6.7917e-02,\n",
       "                       3.2641e-01,  2.2257e-02,  4.9155e-01,  9.7968e-02,  1.3148e-01,\n",
       "                       8.1597e-02,  7.7988e-02,  1.4110e-01,  4.2124e-02,  4.9567e-02,\n",
       "                      -6.3354e-03,  8.1806e-02, -1.4925e-02, -6.6022e-03, -8.2095e-02,\n",
       "                       2.6125e-02, -5.7067e-02, -7.3142e-02, -4.1653e-02,  2.9087e-03,\n",
       "                       1.0214e-01,  1.4962e-03, -1.3979e-02,  4.5285e-02, -8.6914e-02,\n",
       "                       3.2303e-02, -4.1927e-02, -1.6314e-01,  7.6144e-02,  4.5839e-02,\n",
       "                       1.0968e-01,  5.2245e-02, -2.0626e-02, -2.8628e-02, -3.5268e-02,\n",
       "                       7.5277e-02,  5.1581e-02, -1.7355e-02, -5.0583e-02, -3.7002e-03,\n",
       "                       1.4393e-02, -3.6525e-02,  3.9893e-02, -6.0066e-02, -5.4340e-03,\n",
       "                      -4.4159e-02, -5.1211e-02, -3.9540e-02, -9.9481e-02, -9.4376e-02,\n",
       "                      -2.6510e-02, -1.1911e-02,  5.8290e-02,  5.2469e-02,  8.8906e-02,\n",
       "                      -1.9985e-02,  5.8770e-02, -9.3494e-02,  5.6306e-02,  9.6757e-02,\n",
       "                      -2.2540e-02,  2.7438e-02, -3.6832e-02, -1.2605e-02, -9.6924e-02,\n",
       "                       5.6233e-02, -3.6940e-02, -1.2138e-02,  1.2433e-01,  4.5262e-03,\n",
       "                      -8.0314e-02,  8.5295e-02,  4.2990e-03, -1.3031e-02,  6.3382e-02,\n",
       "                      -1.7060e-02, -3.8235e-02,  9.6522e-03, -9.5955e-02, -6.0631e-03,\n",
       "                       6.9826e-02, -9.7798e-02,  8.8022e-02, -1.3199e-02,  3.4863e-02,\n",
       "                      -1.2230e-01,  1.3444e-02, -1.2949e-01, -6.0926e-02, -1.0165e-01,\n",
       "                       7.8317e-03,  3.8537e-02,  2.9483e-02,  4.1002e-02, -1.3682e-01,\n",
       "                      -2.8215e-02,  1.2042e-02, -7.4545e-02, -3.2905e-02,  1.1928e-01,\n",
       "                      -3.3777e-02,  4.9435e-02, -1.5416e-02,  1.2081e-01,  3.3939e-02,\n",
       "                       4.2170e-02, -3.5695e-02,  2.1576e-02,  1.2250e-02,  4.5337e-02,\n",
       "                       2.5157e-02, -1.1679e-01, -8.4033e-02, -7.0692e-02, -9.4700e-02,\n",
       "                      -8.4019e-02, -3.9933e-05,  9.6558e-02,  5.9889e-03,  2.1429e-03,\n",
       "                      -2.7552e-03,  1.3713e-02, -1.8060e-02,  4.3748e-02,  1.4251e-01,\n",
       "                       6.5537e-02,  3.8513e-03, -2.2529e-02, -6.8185e-02, -9.0840e-04,\n",
       "                       1.0302e-01, -3.5517e-02, -5.7427e-02,  3.0265e-03,  5.0943e-02,\n",
       "                      -1.4816e-02,  5.8958e-02, -1.8018e-02,  9.6206e-02,  1.0727e-01,\n",
       "                       1.6942e-01,  1.8683e-01,  8.4639e-02, -5.7420e-03,  1.6587e-01,\n",
       "                       3.4981e-02, -5.6938e-02,  2.2697e-02, -2.9511e-02, -1.5096e-01,\n",
       "                       1.0791e-01,  7.4066e-02, -2.8419e-02, -8.6430e-03, -7.0465e-02,\n",
       "                      -1.0986e-01,  9.5837e-02,  1.0250e-01,  9.8464e-02, -3.8758e-02,\n",
       "                       4.5780e-02, -3.8182e-02, -1.6198e-01,  9.8966e-03, -2.1189e-02,\n",
       "                      -1.6309e-01, -6.1443e-02,  1.8064e-02,  2.3939e-02,  5.0882e-02,\n",
       "                       4.6384e-02, -4.1647e-02,  1.1646e-02, -3.2109e-02,  2.4047e-02,\n",
       "                       3.3791e-02, -1.9562e-01,  9.0286e-02, -2.8180e-02, -7.0607e-02,\n",
       "                       4.7796e-05, -1.9722e-01, -3.7687e-03, -4.0529e-02,  9.2184e-04,\n",
       "                       1.1177e-01,  5.6848e-02, -7.8870e-02, -7.9713e-02, -5.0051e-02,\n",
       "                       4.8811e-02, -5.1129e-02, -1.7169e-02, -5.3644e-02,  1.4981e-01,\n",
       "                       1.6476e-01,  8.0675e-02,  1.6655e-01,  2.0185e-02,  9.7919e-02,\n",
       "                      -2.4559e-02, -9.9170e-02,  2.6586e-02, -2.3420e-02, -5.6461e-02,\n",
       "                       9.6179e-02, -2.9114e-02, -1.7082e-02, -6.0683e-02, -5.6240e-02,\n",
       "                      -3.5625e-02, -6.6010e-02, -4.1881e-02,  8.4053e-02,  5.5139e-02,\n",
       "                       2.1585e-01, -1.6652e-03,  5.6421e-02,  1.0251e-02,  6.7756e-02,\n",
       "                      -9.4623e-02, -6.3766e-03,  6.9250e-03, -7.7851e-02,  2.0709e-02,\n",
       "                      -3.2843e-02,  2.2762e-02, -2.7753e-02, -8.8192e-02, -1.2536e-01,\n",
       "                       1.3663e-02, -3.6637e-02, -8.2981e-02,  1.5609e-01,  7.5173e-02,\n",
       "                      -1.4347e-01, -7.7393e-03, -2.2750e-02, -8.0578e-02, -8.5738e-02,\n",
       "                       2.1641e-01,  1.4273e-01, -4.1186e-02, -5.2877e-02,  2.7353e-04,\n",
       "                      -7.3630e-02, -1.0770e-01,  3.5974e-02,  5.8676e-02,  6.8320e-02,\n",
       "                       1.6155e-01,  1.9182e-02,  3.4303e-02, -7.7848e-03,  1.8901e-02,\n",
       "                       7.5048e-02, -2.8294e-02, -4.8105e-03, -1.0329e-01, -2.0469e-02,\n",
       "                       1.6525e-02,  9.2135e-02,  3.2689e-02,  6.4490e-02,  3.6721e-02,\n",
       "                       2.9703e-02,  3.4028e-02,  1.9899e-01, -6.0756e-02,  1.9396e-01,\n",
       "                       1.6198e-01,  2.0468e-02, -1.5538e-01, -3.5783e-02,  4.0182e-02,\n",
       "                       2.1526e-01,  1.5082e-01,  1.3986e-01, -5.2130e-02,  1.2984e-01,\n",
       "                      -1.9984e-02,  1.5015e-01,  3.3498e-01,  1.4889e-01,  2.2784e-01,\n",
       "                       3.9335e-03,  6.2908e-02,  8.2026e-02,  1.4267e-01, -6.2095e-02,\n",
       "                       7.9345e-02,  3.8440e-01,  2.8417e-01,  4.4243e-01,  7.8182e-02,\n",
       "                       4.7954e-03,  1.8023e-01,  2.4239e-01,  1.5660e-01,  1.6968e-01,\n",
       "                       2.3364e-01,  2.4047e-01, -5.1752e-02, -3.2576e-02,  1.2457e-01,\n",
       "                       1.2914e-01,  1.8015e-01,  2.7667e-01,  9.8172e-02,  2.1574e-01,\n",
       "                       2.1873e-02,  3.0549e-02,  7.3597e-02,  1.8224e-01,  5.0046e-02,\n",
       "                      -2.3818e-03,  2.1927e-01,  4.0595e-02, -6.3922e-02, -1.1800e-01,\n",
       "                       9.9418e-02,  2.6478e-01,  7.6697e-02,  9.4348e-02, -4.5588e-03,\n",
       "                       1.7256e-01,  1.9028e-01,  5.6351e-02,  9.6720e-02,  1.3728e-01,\n",
       "                       1.9649e-01,  8.4695e-02, -1.7583e-02,  1.4096e-01,  6.9772e-02,\n",
       "                      -1.0167e-02,  1.5584e-01, -3.3484e-02,  3.2322e-01, -2.9661e-01,\n",
       "                       5.3541e-02,  7.5622e-02,  2.0308e-02, -1.0904e-01,  2.7114e-01,\n",
       "                      -4.6361e-02, -2.8442e-03,  1.0674e-01, -7.2010e-02,  9.4344e-02,\n",
       "                       1.1616e-01,  7.1588e-02,  8.5859e-02, -3.2357e-02,  2.6872e-01,\n",
       "                       6.3936e-03,  1.6698e-01,  2.6626e-01,  1.8506e-02,  7.7404e-02,\n",
       "                      -4.8025e-02,  1.7293e-01,  1.2374e-01,  2.9535e-01,  1.0177e-01,\n",
       "                       1.1869e-02,  1.0044e-01,  7.7479e-02, -1.1805e-01,  2.0284e-01,\n",
       "                       4.9826e-02, -1.2865e-02, -9.6035e-02,  1.7347e-01,  2.3662e-01,\n",
       "                       1.2409e-01,  8.4991e-02,  4.2046e-02, -5.3450e-02,  1.3251e-01,\n",
       "                       2.0148e-01,  9.4416e-02,  2.1556e-01,  1.9157e-01,  1.8748e-01,\n",
       "                       1.0494e-01,  3.1380e-01,  2.2322e-01,  1.2530e-01,  8.2713e-02,\n",
       "                       8.5604e-02,  2.5229e-01])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 5.2831e-02,  1.4993e-01,  6.8879e-02,  2.5084e-01,  3.9832e-02,\n",
       "                       1.6172e-01,  2.0014e-01, -3.9612e-02, -2.1615e-01,  2.3385e-02,\n",
       "                       5.9329e-02,  3.0633e-01,  3.3661e-02,  8.2901e-02,  4.3386e-02,\n",
       "                       3.4714e-02,  9.7504e-02,  1.9850e-01,  3.0283e-01,  5.4968e-02,\n",
       "                       2.1379e-01,  1.0532e-01,  3.9373e-02,  1.0463e-01,  3.4038e-02,\n",
       "                       2.4148e-02,  5.4161e-02,  3.4555e-01,  1.0870e-01,  3.5733e-01,\n",
       "                       2.0852e-01, -4.8965e-02,  4.6934e-02,  3.2150e-01,  4.8915e-02,\n",
       "                       2.1596e-01,  6.2902e-02,  1.8568e-01,  2.7818e-02, -5.8307e-02,\n",
       "                       2.0544e-01,  1.6599e-03,  3.2485e-01,  4.4703e-01,  1.8805e-02,\n",
       "                       1.3121e-01,  9.0168e-02,  1.2929e-01,  7.6731e-02,  4.2006e-02,\n",
       "                       4.4554e-02,  3.6082e-02,  3.1531e-01,  2.2329e-01, -3.9392e-02,\n",
       "                       3.5197e-02,  1.2612e-01,  1.9346e-01,  1.7605e-01,  1.6400e-01,\n",
       "                      -1.2738e-01,  2.5358e-01,  3.0695e-01,  1.7102e-01,  1.1911e-01,\n",
       "                       3.9326e-02,  8.5973e-02,  1.0468e-01,  1.0180e-02,  2.1247e-01,\n",
       "                      -1.5726e-02,  1.2262e-01,  6.9906e-02, -3.0866e-02,  2.4737e-01,\n",
       "                      -1.3185e-01, -4.2911e-02,  1.2869e-01,  1.3690e-02,  4.2343e-02,\n",
       "                       1.9300e-01, -2.6649e-03, -6.3827e-02, -1.3546e-02,  2.1877e-02,\n",
       "                       2.7998e-01,  7.8291e-02,  7.1279e-02,  9.6744e-02,  1.7847e-01,\n",
       "                       2.0229e-01, -5.4395e-02,  1.7774e-01,  1.7878e-01,  1.0475e-01,\n",
       "                      -5.8663e-04, -3.7187e-02,  2.1933e-01,  2.2565e-01,  2.8187e-01,\n",
       "                       2.4156e-01,  8.2340e-02,  1.8745e-02,  1.9539e-01, -1.2285e-01,\n",
       "                       4.6455e-02,  6.6590e-02, -2.7542e-03,  8.3385e-03,  2.2884e-01,\n",
       "                       1.3466e-01,  1.2853e-01,  1.6518e-01,  5.1019e-02, -8.0463e-02,\n",
       "                       1.3066e-01,  5.8831e-02,  7.8737e-02,  1.9732e-01,  1.9973e-01,\n",
       "                       2.1248e-01, -3.3581e-02,  3.8677e-01,  1.0234e-01,  5.4492e-02,\n",
       "                       6.2463e-02,  9.1960e-02,  1.1652e-01, -7.6674e-02, -9.6042e-03,\n",
       "                       1.5426e-02,  1.3682e-01, -1.9016e-01,  4.5935e-02, -5.7868e-02,\n",
       "                      -5.4598e-02,  1.1312e-02, -9.2784e-03, -4.6236e-02,  3.9821e-02,\n",
       "                      -7.9880e-02,  6.0780e-03,  1.3651e-01, -3.4060e-02, -2.1957e-02,\n",
       "                      -2.7276e-03, -6.7294e-03, -2.2918e-02,  7.3848e-03,  2.5858e-02,\n",
       "                      -1.0808e-01, -8.7444e-02,  3.2406e-02, -1.0039e-02,  4.3443e-02,\n",
       "                       1.9604e-02,  4.4366e-02, -3.6850e-02,  4.4440e-02, -6.1992e-02,\n",
       "                       4.9709e-03, -9.3941e-02,  2.0387e-02, -1.9267e-02, -8.2617e-02,\n",
       "                       2.7210e-02, -1.1512e-01, -1.4736e-02, -3.0256e-02,  6.5780e-02,\n",
       "                       8.5131e-02,  9.7555e-02,  6.8066e-02, -1.0610e-01,  2.2223e-02,\n",
       "                      -1.6071e-02,  7.3710e-02, -8.2943e-02,  3.7015e-02,  3.1025e-02,\n",
       "                      -1.6445e-03, -3.4831e-02, -2.4008e-02, -2.8202e-03,  3.0839e-02,\n",
       "                      -4.1532e-02,  7.6560e-02,  3.5057e-02,  1.8796e-02, -4.6186e-02,\n",
       "                      -4.2485e-02,  3.7149e-02, -4.0107e-02,  3.7980e-02,  3.6478e-02,\n",
       "                      -1.9292e-02,  6.0573e-03,  6.0039e-04, -7.6866e-03,  6.0784e-03,\n",
       "                      -3.0036e-03, -9.9792e-03, -6.8567e-04, -7.9857e-02, -7.1400e-04,\n",
       "                      -2.5786e-03, -4.0478e-02,  9.9039e-03,  5.5989e-02,  9.6749e-03,\n",
       "                      -6.0903e-02, -2.1489e-02, -3.4175e-02,  1.2326e-01, -4.1595e-02,\n",
       "                       7.9717e-03,  2.0582e-02, -1.1834e-02, -6.5611e-02, -5.0518e-03,\n",
       "                      -9.4201e-04, -8.2139e-02,  1.4023e-01,  7.3124e-03,  4.5165e-02,\n",
       "                      -5.8292e-03, -1.8043e-02, -4.2803e-02, -1.8241e-03, -1.3450e-02,\n",
       "                      -5.3898e-02,  1.1541e-01, -5.3514e-02,  2.4884e-02,  3.5215e-02,\n",
       "                       3.6671e-03,  1.9198e-03,  6.0390e-02, -1.0598e-03,  5.7339e-02,\n",
       "                       6.8425e-02, -3.2711e-03,  1.6643e-02, -9.3335e-03, -2.6160e-02,\n",
       "                       6.4550e-02,  1.0568e-02, -6.1421e-02,  1.2131e-04,  1.6233e-01,\n",
       "                       1.4188e-02, -1.1082e-01, -7.6067e-02,  1.2346e-01, -8.0261e-02,\n",
       "                      -9.3603e-02, -1.3227e-01, -8.7592e-03, -2.5796e-03,  7.2509e-02,\n",
       "                       1.4450e-01,  1.0544e-01,  7.6683e-02,  2.1149e-02,  5.6578e-02,\n",
       "                      -7.9094e-02,  4.4222e-02,  7.2607e-02, -6.2450e-02,  2.4171e-02,\n",
       "                       1.7626e-01,  1.3931e-01, -2.8034e-02,  1.8219e-01, -1.3685e-01,\n",
       "                      -3.5574e-02,  2.3975e-02, -4.6151e-02,  5.6375e-02, -7.6712e-03,\n",
       "                       4.1362e-02, -1.0585e-02,  6.2463e-02, -2.5763e-02, -3.1111e-02,\n",
       "                      -3.8042e-02, -9.7739e-03, -6.5360e-02,  2.3774e-03,  1.1480e-01,\n",
       "                       3.3087e-02, -9.1016e-02,  1.3877e-01,  8.5094e-02, -1.4949e-02,\n",
       "                      -8.6360e-02, -1.3345e-01,  1.2576e-01,  2.4382e-02,  2.2630e-02,\n",
       "                      -2.7136e-02, -1.3121e-01,  3.4854e-02,  5.3704e-02,  7.7491e-03,\n",
       "                       1.1697e-01,  1.7100e-02,  2.3148e-02, -2.2437e-01, -1.3581e-01,\n",
       "                       6.3479e-02,  5.4791e-02,  3.6618e-02,  9.1407e-02,  3.2704e-02,\n",
       "                       2.0049e-01,  3.3402e-02, -7.1036e-02,  4.1835e-02,  3.0441e-02,\n",
       "                       1.1343e-02, -1.0346e-02, -5.7747e-02,  6.6532e-02, -1.6155e-02,\n",
       "                      -8.7560e-02, -1.5624e-02,  1.1895e-01, -1.3578e-01, -1.0233e-01,\n",
       "                       4.9364e-02,  5.1583e-02, -6.0219e-02, -2.9837e-02,  5.4840e-03,\n",
       "                       8.4353e-02,  2.5349e-02,  7.4974e-02, -5.4447e-02,  4.2163e-02,\n",
       "                       5.8317e-02,  1.0255e-02,  6.0984e-02, -4.0568e-02,  1.0729e-01,\n",
       "                      -6.8008e-02,  2.8156e-02, -2.1634e-02, -2.0564e-02, -1.1616e-01,\n",
       "                      -5.2752e-02,  1.4393e-02,  7.6709e-02, -1.3078e-02,  7.5700e-02,\n",
       "                      -4.2012e-03, -6.7996e-02, -2.9347e-02,  5.4251e-02, -5.7398e-02,\n",
       "                       3.3698e-02,  6.0694e-02,  8.4875e-02,  3.5202e-03,  8.5491e-02,\n",
       "                      -1.8203e-01, -3.0251e-02,  4.8502e-02, -8.1804e-02,  5.6404e-02,\n",
       "                      -6.2013e-02,  1.3489e-01,  4.3272e-02,  6.7393e-02,  3.5367e-02,\n",
       "                       7.6976e-02, -1.1891e-01,  5.6037e-02, -1.0380e-01, -1.6088e-02,\n",
       "                      -1.0179e-01,  8.8628e-02,  6.2296e-02, -6.7346e-02,  6.5731e-02,\n",
       "                       2.9693e-01,  6.0180e-02,  2.4286e-01,  3.0069e-02,  2.8996e-01,\n",
       "                       2.6597e-01,  1.5919e-01, -1.0806e-01,  2.9316e-02,  9.5817e-02,\n",
       "                       2.1104e-01,  2.0713e-01,  1.4030e-01, -6.2465e-02,  1.2511e-01,\n",
       "                       1.0779e-01,  2.2752e-01,  1.5986e-01,  1.3286e-01,  2.0608e-02,\n",
       "                       1.8180e-01, -8.2653e-03,  2.3762e-01,  7.6602e-02, -2.1430e-02,\n",
       "                      -2.1770e-02,  2.8589e-01,  1.5782e-01,  3.3043e-01,  1.5762e-01,\n",
       "                      -2.8315e-03,  2.0667e-02,  2.3339e-01,  1.0434e-01,  8.0592e-02,\n",
       "                       1.2558e-01,  2.5644e-01,  6.8459e-02, -4.7595e-02,  1.7315e-01,\n",
       "                      -9.8721e-03,  1.4402e-01,  3.3210e-01,  2.5723e-02,  1.5059e-01,\n",
       "                       7.9718e-02,  3.8279e-02,  7.3597e-02,  1.2392e-01,  7.2834e-02,\n",
       "                       9.4380e-02,  1.8961e-01,  1.3419e-01, -8.6946e-02, -1.9797e-02,\n",
       "                      -1.8375e-02,  1.4959e-01,  2.1516e-01,  1.8314e-01, -1.3183e-01,\n",
       "                       1.6681e-01,  3.0258e-01,  1.4975e-01,  2.7913e-01,  4.1961e-02,\n",
       "                       1.8158e-02,  6.5220e-02,  1.5982e-01,  1.5417e-01, -9.4296e-02,\n",
       "                       9.5909e-02,  2.2561e-01, -4.0773e-02,  1.9083e-01, -1.6248e-01,\n",
       "                       1.5211e-01,  1.0599e-02, -6.7469e-02, -7.4948e-03,  1.6095e-01,\n",
       "                      -1.3893e-01,  4.5590e-03,  1.0171e-01,  2.0563e-02,  2.0725e-01,\n",
       "                       1.7520e-01,  1.6978e-02,  8.0825e-02,  1.0313e-01,  2.2848e-01,\n",
       "                      -3.3775e-02,  2.1482e-01,  2.5123e-01,  6.5520e-02,  9.9989e-02,\n",
       "                      -5.1025e-03, -1.5088e-02,  1.8474e-01,  2.1185e-01,  2.0038e-01,\n",
       "                       6.0249e-03,  1.5458e-01,  6.6925e-02, -1.3918e-01,  1.3136e-01,\n",
       "                       4.3360e-02,  1.5212e-01, -1.2006e-01,  2.4756e-01,  1.5447e-01,\n",
       "                       7.4903e-02,  7.0371e-02, -1.0695e-02, -1.5325e-01,  2.9830e-02,\n",
       "                       2.9700e-01,  2.2685e-01,  3.1245e-01,  5.5723e-02,  2.2726e-01,\n",
       "                      -6.5769e-02,  4.7223e-01,  1.6624e-01,  1.1222e-01,  1.9763e-02,\n",
       "                      -1.6662e-02,  1.8313e-01])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[ 9.7907e-02,  4.8695e-02,  5.2946e-02,  ..., -3.1163e-01,\n",
       "                       -3.3652e-02,  3.5968e-01],\n",
       "                      [-4.0847e-02,  6.2442e-02, -1.9060e-01,  ..., -8.7308e-03,\n",
       "                       -1.1989e-01, -1.5632e-01],\n",
       "                      [-5.0619e-05,  4.9415e-02, -7.4286e-02,  ...,  3.6902e-01,\n",
       "                       -3.0718e-01, -3.8870e-01],\n",
       "                      ...,\n",
       "                      [-4.4707e-02, -1.6942e-01,  1.1016e-01,  ..., -5.0139e-02,\n",
       "                        1.3776e-02, -3.2926e-01],\n",
       "                      [-3.9153e-02, -5.4296e-02,  6.6387e-02,  ..., -8.2740e-02,\n",
       "                       -9.0322e-02, -4.1891e-01],\n",
       "                      [ 1.1662e-02, -2.5841e-02,  9.6840e-02,  ...,  1.7380e-01,\n",
       "                        1.4561e-01,  8.5915e-02]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-1.5635e-01,  1.2891e-02,  1.9869e-01, -4.3789e-02, -2.4459e-01,\n",
       "                       2.6846e-02, -4.0472e-02,  2.7183e-01,  3.4453e-02, -3.0298e-01,\n",
       "                      -9.7069e-02,  1.3882e-03,  1.1372e-01, -3.2647e-04, -4.1517e-02,\n",
       "                       2.2960e-01, -2.3059e-01, -6.1122e-02, -1.3404e-01, -2.9723e-01,\n",
       "                      -2.4910e-01,  1.2452e-01,  1.4825e-01, -7.3093e-02,  1.1986e-02,\n",
       "                      -1.3106e-01,  1.3484e-01,  6.8099e-02, -4.3469e-02,  1.3949e-01,\n",
       "                      -1.7898e-02, -1.8758e-01, -5.9588e-02,  8.6405e-04, -1.7952e-01,\n",
       "                      -2.4363e-01, -4.4615e-02,  4.4039e-02,  1.8331e-01,  1.1055e-01,\n",
       "                      -1.2433e-01,  1.9150e-03, -6.2030e-02, -3.9623e-02,  2.0522e-01,\n",
       "                       2.2274e-01, -1.6479e-02, -1.5791e-02,  3.9010e-03, -4.7883e-01,\n",
       "                      -6.4823e-02, -3.2240e-02,  7.4970e-02,  9.9607e-02, -4.0217e-02,\n",
       "                       1.5427e-01, -2.1343e-01, -9.2350e-02, -4.7850e-02, -6.5171e-02,\n",
       "                      -1.3807e-01, -2.4485e-01,  1.4414e-01,  9.1403e-02])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[ 0.1401, -0.0770,  0.0078,  ..., -0.1489, -0.1482,  0.1364],\n",
       "                      [ 0.2249, -0.0319,  0.0041,  ...,  0.1156,  0.0113, -0.1171],\n",
       "                      [-0.0008, -0.1200,  0.0970,  ..., -0.1429,  0.1395,  0.0336],\n",
       "                      ...,\n",
       "                      [ 0.1695,  0.1897, -0.2713,  ..., -0.2063, -0.1637,  0.1404],\n",
       "                      [-0.0738,  0.1343, -0.0177,  ..., -0.2436, -0.1695, -0.2449],\n",
       "                      [ 0.2354,  0.0549,  0.2448,  ..., -0.0158, -0.2133, -0.0408]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([-0.0862,  0.1863,  0.0860,  0.2074, -0.0499,  0.2107,  0.1794, -0.0694,\n",
       "                      -0.0721,  0.2598,  0.1319, -0.1283, -0.0071,  0.1498,  0.0807, -0.1436,\n",
       "                      -0.1025,  0.1380,  0.2298, -0.3007, -0.1487, -0.1636,  0.0282, -0.0371,\n",
       "                      -0.0985, -0.0411,  0.0768, -0.1534,  0.1055, -0.1483, -0.0979, -0.3739,\n",
       "                      -0.0228, -0.1104,  0.0024,  0.0178, -0.2245,  0.1355, -0.2479,  0.1341,\n",
       "                      -0.0751,  0.1467, -0.2329,  0.1870,  0.0152,  0.1266,  0.0119, -0.0175,\n",
       "                      -0.1532,  0.1034,  0.0634, -0.0184,  0.2770, -0.2584,  0.0722,  0.0370,\n",
       "                      -0.0924, -0.1294, -0.4404,  0.0536, -0.1343, -0.0901,  0.1597, -0.0150])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[-0.2331, -0.1181, -0.1793,  ...,  0.1457, -0.0302,  0.1372],\n",
       "                      [ 0.0008,  0.0486, -0.4462,  ...,  0.1866,  0.2442,  0.0526],\n",
       "                      [-0.3827, -0.5120,  0.2497,  ..., -0.2212,  0.2049, -0.1580],\n",
       "                      ...,\n",
       "                      [-0.1119, -0.2823, -0.0545,  ..., -0.0163,  0.0588,  0.0943],\n",
       "                      [-0.0156, -0.3852, -0.1178,  ..., -0.1268,  0.2146,  0.0394],\n",
       "                      [ 0.2296,  0.0271,  0.1427,  ..., -0.4877,  0.1321, -0.3702]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.6715, -0.1188, -0.2868,  ...,  0.0132, -0.0731, -0.1220]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
