{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=30,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 30)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 30)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.9385925e-12,  4.5556213e-41,  3.9385925e-12, ...,\n",
       "         4.5554812e-41, -1.3380729e-21,  4.5554812e-41],\n",
       "       [ 1.0283149e-40,  0.0000000e+00, -8.0235954e-22, ...,\n",
       "         0.0000000e+00, -8.0237893e-22,  4.5554812e-41],\n",
       "       [-1.3381375e-21,  4.5554812e-41,  1.0284690e-40, ...,\n",
       "         4.5554812e-41,  1.0286091e-40,  0.0000000e+00],\n",
       "       ...,\n",
       "       [-1.3463899e-21,  4.5554812e-41,  1.0457890e-40, ...,\n",
       "         4.5554812e-41,  1.0459292e-40,  0.0000000e+00],\n",
       "       [-9.5246031e-23,  4.5554812e-41, -1.3464610e-21, ...,\n",
       "         4.5554812e-41, -1.3465256e-21,  4.5554812e-41],\n",
       "       [ 1.0460833e-40,  0.0000000e+00, -9.5248252e-23, ...,\n",
       "         0.0000000e+00, -9.5250272e-23,  4.5554812e-41]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53182f7404bd490a844b261b3d5eff8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=6.4993 | F1Score=0.2794\n",
      "Batch-100: NLLLoss=4.2904 | F1Score=0.2994\n",
      "Batch-150: NLLLoss=5.6347 | F1Score=0.3196\n",
      "Batch-200: NLLLoss=3.6625 | F1Score=0.3447\n",
      "Batch-250: NLLLoss=3.7020 | F1Score=0.3580\n",
      "Batch-300: NLLLoss=5.1127 | F1Score=0.3751\n",
      "Batch-350: NLLLoss=5.1064 | F1Score=0.3893\n",
      "Batch-400: NLLLoss=2.5857 | F1Score=0.4041\n",
      "Batch-450: NLLLoss=4.0027 | F1Score=0.4170\n",
      "Batch-500: NLLLoss=2.9926 | F1Score=0.4305\n",
      "Batch-518: NLLLoss=2.6311 | F1Score=0.4350\n",
      "\n",
      "Mean NLLLoss: 4.5035 | Mean F1Score: 0.3534\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724ef99859264295a014e37f8651a5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=3.3247 | F1Score=0.5850\n",
      "Batch-100: NLLLoss=2.0386 | F1Score=0.5938\n",
      "Batch-150: NLLLoss=2.0551 | F1Score=0.6007\n",
      "Batch-200: NLLLoss=2.2565 | F1Score=0.6074\n",
      "Batch-250: NLLLoss=2.8872 | F1Score=0.6147\n",
      "Batch-300: NLLLoss=3.6169 | F1Score=0.6182\n",
      "Batch-350: NLLLoss=3.3008 | F1Score=0.6230\n",
      "Batch-400: NLLLoss=1.8985 | F1Score=0.6287\n",
      "Batch-450: NLLLoss=2.7400 | F1Score=0.6307\n",
      "Batch-500: NLLLoss=3.2289 | F1Score=0.6339\n",
      "Batch-518: NLLLoss=3.8658 | F1Score=0.6366\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 2.7144 | Mean F1Score: 0.6102\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf1e2ee04d24c01a9592080eed90cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.5124 | F1Score=0.7362\n",
      "Batch-100: NLLLoss=1.5699 | F1Score=0.7320\n",
      "Batch-150: NLLLoss=1.9397 | F1Score=0.7311\n",
      "Batch-200: NLLLoss=2.2181 | F1Score=0.7355\n",
      "Batch-250: NLLLoss=1.9842 | F1Score=0.7375\n",
      "Batch-300: NLLLoss=1.9012 | F1Score=0.7385\n",
      "Batch-350: NLLLoss=2.1425 | F1Score=0.7374\n",
      "Batch-400: NLLLoss=0.9773 | F1Score=0.7396\n",
      "Batch-450: NLLLoss=0.4878 | F1Score=0.7403\n",
      "Batch-500: NLLLoss=1.5232 | F1Score=0.7423\n",
      "Batch-518: NLLLoss=1.0234 | F1Score=0.7426\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.7973 | Mean F1Score: 0.7370\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35733f203eec4e8b902458a366c19b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.0812 | F1Score=0.8138\n",
      "Batch-100: NLLLoss=0.6741 | F1Score=0.8198\n",
      "Batch-150: NLLLoss=1.0817 | F1Score=0.8132\n",
      "Batch-200: NLLLoss=1.1882 | F1Score=0.8136\n",
      "Batch-250: NLLLoss=1.5005 | F1Score=0.8117\n",
      "Batch-300: NLLLoss=1.2739 | F1Score=0.8147\n",
      "Batch-350: NLLLoss=1.1387 | F1Score=0.8137\n",
      "Batch-400: NLLLoss=1.0974 | F1Score=0.8133\n",
      "Batch-450: NLLLoss=1.2232 | F1Score=0.8130\n",
      "Batch-500: NLLLoss=1.4754 | F1Score=0.8137\n",
      "Batch-518: NLLLoss=0.5081 | F1Score=0.8142\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.1735 | Mean F1Score: 0.8149\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbb5a488bc34989abb5b828da52b87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.6109 | F1Score=0.8706\n",
      "Batch-100: NLLLoss=0.5010 | F1Score=0.8719\n",
      "Batch-150: NLLLoss=0.5983 | F1Score=0.8713\n",
      "Batch-200: NLLLoss=0.6440 | F1Score=0.8694\n",
      "Batch-250: NLLLoss=0.8977 | F1Score=0.8694\n",
      "Batch-300: NLLLoss=0.5379 | F1Score=0.8669\n",
      "Batch-350: NLLLoss=0.8821 | F1Score=0.8683\n",
      "Batch-400: NLLLoss=1.2265 | F1Score=0.8674\n",
      "Batch-450: NLLLoss=0.6276 | F1Score=0.8680\n",
      "Batch-500: NLLLoss=0.8933 | F1Score=0.8680\n",
      "Batch-518: NLLLoss=0.6304 | F1Score=0.8685\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.7063 | Mean F1Score: 0.8688\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebb75b23d474b6bad5e18e04d085350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1626 | F1Score=0.9447\n",
      "Batch-100: NLLLoss=0.3869 | F1Score=0.9498\n",
      "Batch-150: NLLLoss=0.2720 | F1Score=0.9478\n",
      "Batch-200: NLLLoss=0.4986 | F1Score=0.9475\n",
      "Batch-250: NLLLoss=0.2748 | F1Score=0.9455\n",
      "Batch-300: NLLLoss=0.2331 | F1Score=0.9445\n",
      "Batch-350: NLLLoss=0.3621 | F1Score=0.9432\n",
      "Batch-400: NLLLoss=0.4012 | F1Score=0.9396\n",
      "Batch-450: NLLLoss=0.3252 | F1Score=0.9379\n",
      "Batch-500: NLLLoss=0.0990 | F1Score=0.9362\n",
      "Batch-518: NLLLoss=0.3218 | F1Score=0.9353\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.3526 | Mean F1Score: 0.9444\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4809c019a364dcfb71ce3f91fc0f510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1135 | F1Score=0.9912\n",
      "Batch-100: NLLLoss=0.1469 | F1Score=0.9916\n",
      "Batch-150: NLLLoss=0.1704 | F1Score=0.9912\n",
      "Batch-200: NLLLoss=0.0240 | F1Score=0.9891\n",
      "Batch-250: NLLLoss=0.1063 | F1Score=0.9892\n",
      "Batch-300: NLLLoss=0.0932 | F1Score=0.9874\n",
      "Batch-350: NLLLoss=0.1683 | F1Score=0.9877\n",
      "Batch-400: NLLLoss=0.1251 | F1Score=0.9871\n",
      "Batch-450: NLLLoss=0.1501 | F1Score=0.9860\n",
      "Batch-500: NLLLoss=0.1647 | F1Score=0.9848\n",
      "Batch-518: NLLLoss=0.2829 | F1Score=0.9847\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.1265 | Mean F1Score: 0.9888\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2155f5e502454c91dc52b0c5a57fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0387 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0501 | F1Score=0.9984\n",
      "Batch-150: NLLLoss=0.0180 | F1Score=0.9987\n",
      "Batch-200: NLLLoss=0.0109 | F1Score=0.9984\n",
      "Batch-250: NLLLoss=0.0150 | F1Score=0.9983\n",
      "Batch-300: NLLLoss=0.0113 | F1Score=0.9985\n",
      "Batch-350: NLLLoss=0.0168 | F1Score=0.9984\n",
      "Batch-400: NLLLoss=0.0477 | F1Score=0.9981\n",
      "Batch-450: NLLLoss=0.0290 | F1Score=0.9981\n",
      "Batch-500: NLLLoss=0.0343 | F1Score=0.9981\n",
      "Batch-518: NLLLoss=0.0143 | F1Score=0.9981\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0357 | Mean F1Score: 0.9986\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb6d52783e54f4594e26c24cd433800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0113 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0092 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0168 | F1Score=0.9992\n",
      "Batch-200: NLLLoss=0.0111 | F1Score=0.9992\n",
      "Batch-250: NLLLoss=0.0113 | F1Score=0.9989\n",
      "Batch-300: NLLLoss=0.0227 | F1Score=0.9990\n",
      "Batch-350: NLLLoss=0.0083 | F1Score=0.9992\n",
      "Batch-400: NLLLoss=0.0053 | F1Score=0.9992\n",
      "Batch-450: NLLLoss=0.0041 | F1Score=0.9993\n",
      "Batch-500: NLLLoss=0.0108 | F1Score=0.9993\n",
      "Batch-518: NLLLoss=0.0043 | F1Score=0.9993\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0125 | Mean F1Score: 0.9992\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872db82d02154fd5bcd2b33db2ffc7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0030 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0057 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0065 | F1Score=0.9994\n",
      "Batch-200: NLLLoss=0.0062 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0031 | F1Score=0.9995\n",
      "Batch-300: NLLLoss=0.0064 | F1Score=0.9995\n",
      "Batch-350: NLLLoss=0.0039 | F1Score=0.9995\n",
      "Batch-400: NLLLoss=0.0081 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0095 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0046 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0052 | F1Score=0.9995\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0073 | Mean F1Score: 0.9995\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068506f44bce4bea8affdeeb523a3ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0054 | F1Score=0.9984\n",
      "Batch-100: NLLLoss=0.0039 | F1Score=0.9989\n",
      "Batch-150: NLLLoss=0.0064 | F1Score=0.9991\n",
      "Batch-200: NLLLoss=0.0053 | F1Score=0.9991\n",
      "Batch-250: NLLLoss=0.0074 | F1Score=0.9992\n",
      "Batch-300: NLLLoss=0.0064 | F1Score=0.9992\n",
      "Batch-350: NLLLoss=0.0085 | F1Score=0.9993\n",
      "Batch-400: NLLLoss=0.0033 | F1Score=0.9994\n",
      "Batch-450: NLLLoss=0.0053 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0135 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0042 | F1Score=0.9995\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0058 | Mean F1Score: 0.9988\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cafd5b188e8490d8011a67d74e0fdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0041 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0979 | F1Score=0.9981\n",
      "Batch-150: NLLLoss=0.1935 | F1Score=0.9781\n",
      "Batch-200: NLLLoss=0.3583 | F1Score=0.9589\n",
      "Batch-250: NLLLoss=0.3174 | F1Score=0.9536\n",
      "Batch-300: NLLLoss=0.1723 | F1Score=0.9515\n",
      "Batch-350: NLLLoss=0.1999 | F1Score=0.9508\n",
      "Batch-400: NLLLoss=0.1861 | F1Score=0.9519\n",
      "Batch-450: NLLLoss=0.1066 | F1Score=0.9531\n",
      "Batch-500: NLLLoss=0.2666 | F1Score=0.9537\n",
      "Batch-518: NLLLoss=0.0836 | F1Score=0.9539\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.1936 | Mean F1Score: 0.9667\n",
      "Patience = 1/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2223b0024a45818e4e329ad25a0d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1007 | F1Score=0.9894\n",
      "Batch-100: NLLLoss=0.0480 | F1Score=0.9912\n",
      "Batch-150: NLLLoss=0.0850 | F1Score=0.9911\n",
      "Batch-200: NLLLoss=0.0328 | F1Score=0.9920\n",
      "Batch-250: NLLLoss=0.0469 | F1Score=0.9924\n",
      "Batch-300: NLLLoss=0.0478 | F1Score=0.9924\n",
      "Batch-350: NLLLoss=0.0027 | F1Score=0.9931\n",
      "Batch-400: NLLLoss=0.0199 | F1Score=0.9937\n",
      "Batch-450: NLLLoss=0.0188 | F1Score=0.9934\n",
      "Batch-500: NLLLoss=0.0201 | F1Score=0.9934\n",
      "Batch-518: NLLLoss=0.0128 | F1Score=0.9934\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0411 | Mean F1Score: 0.9921\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3cb6778f66406c84944bf6ac6e410d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0147 | F1Score=0.9981\n",
      "Batch-100: NLLLoss=0.0056 | F1Score=0.9987\n",
      "Batch-150: NLLLoss=0.0045 | F1Score=0.9990\n",
      "Batch-200: NLLLoss=0.0016 | F1Score=0.9992\n",
      "Batch-250: NLLLoss=0.0031 | F1Score=0.9994\n",
      "Batch-300: NLLLoss=0.0043 | F1Score=0.9993\n",
      "Batch-350: NLLLoss=0.0056 | F1Score=0.9992\n",
      "Batch-400: NLLLoss=0.0038 | F1Score=0.9993\n",
      "Batch-450: NLLLoss=0.0072 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0012 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0020 | F1Score=0.9993\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0064 | Mean F1Score: 0.9990\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95071b01c8764830be19cc7623989659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0010 | F1Score=0.9991\n",
      "Batch-100: NLLLoss=0.0015 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0014 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0012 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0006 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0033 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0006 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0017 | F1Score=0.9996\n",
      "Batch-450: NLLLoss=0.0021 | F1Score=0.9997\n",
      "Batch-500: NLLLoss=0.0010 | F1Score=0.9997\n",
      "Batch-518: NLLLoss=0.0019 | F1Score=0.9997\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0021 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b91313ef6d4c5ab0d3c346e1fec03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0017 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0011 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0012 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0006 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0013 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0010 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0012 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0009 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0014 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f51a63a982418e84068b6f9f042ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0007 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0006 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0026 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0011 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0076 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0015 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0007 | F1Score=0.9998\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0023 | Mean F1Score: 0.9999\n",
      "Patience = 2/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddf8fc8e7fd47a6ac16790b111f9085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0009 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0007 | F1Score=0.9996\n",
      "Batch-250: NLLLoss=0.0003 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0009 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0019 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0007 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0007 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0004 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0007 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0014 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6a62b882b546978e73f6ddea73db31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0022 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0010 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0022 | F1Score=0.9992\n",
      "Batch-200: NLLLoss=0.0006 | F1Score=0.9989\n",
      "Batch-250: NLLLoss=0.0028 | F1Score=0.9989\n",
      "Batch-300: NLLLoss=0.1101 | F1Score=0.9989\n",
      "Batch-350: NLLLoss=0.0030 | F1Score=0.9989\n",
      "Batch-400: NLLLoss=0.0031 | F1Score=0.9986\n",
      "Batch-450: NLLLoss=0.0066 | F1Score=0.9984\n",
      "Batch-500: NLLLoss=0.5148 | F1Score=0.9951\n",
      "Batch-518: NLLLoss=0.7817 | F1Score=0.9926\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0322 | Mean F1Score: 0.9986\n",
      "Patience = 3/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bcefa7bf19467ba74a59575df52957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.6232 | F1Score=0.8981\n",
      "Batch-100: NLLLoss=0.4198 | F1Score=0.9144\n",
      "Batch-150: NLLLoss=0.2608 | F1Score=0.9216\n",
      "Batch-200: NLLLoss=0.0911 | F1Score=0.9293\n",
      "Batch-250: NLLLoss=0.0261 | F1Score=0.9366\n",
      "Batch-300: NLLLoss=0.1130 | F1Score=0.9422\n",
      "Batch-350: NLLLoss=0.2011 | F1Score=0.9464\n",
      "Batch-400: NLLLoss=0.0412 | F1Score=0.9503\n",
      "Batch-450: NLLLoss=0.0221 | F1Score=0.9527\n",
      "Batch-500: NLLLoss=0.0376 | F1Score=0.9557\n",
      "Batch-518: NLLLoss=0.1776 | F1Score=0.9565\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.1622 | Mean F1Score: 0.9329\n",
      "Patience = 4/20â—\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0014\n",
      "Best F1Score      : 0.9998\n",
      "Training duration : 19.927 minutes.\n",
      "Training date     : 2022-10-11 12:44:19.991638+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5684142 ,  4.4026966 , -5.161681  , ..., -0.9332031 ,\n",
       "        -2.1682408 , -1.5861797 ],\n",
       "       [ 7.900182  , -4.2804966 ,  1.2274776 , ...,  3.7002096 ,\n",
       "         0.23248395, -7.4988823 ],\n",
       "       [ 1.939774  , -2.6527407 , -0.20368597, ...,  0.7792417 ,\n",
       "        -1.2239039 , -5.68875   ],\n",
       "       ...,\n",
       "       [-3.5974808 ,  4.587639  ,  4.6234045 , ...,  3.6064558 ,\n",
       "         0.6107724 , -0.75483525],\n",
       "       [ 3.059668  , -1.5155329 ,  2.8406253 , ..., -8.311807  ,\n",
       "         3.555589  , -0.84691447],\n",
       "       [ 2.7419899 , -4.17962   , -2.3793387 , ...,  2.6236758 ,\n",
       "        -3.092748  , -4.9991994 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.2876, -0.3304, -0.1217,  ..., -0.2179,  0.3653, -0.2291],\n",
       "                      [ 0.2022, -0.2851, -0.2804,  ..., -0.1242,  0.0334, -0.2856],\n",
       "                      [-0.1469, -0.3241,  0.3307,  ...,  0.0863, -0.0749, -0.0933],\n",
       "                      ...,\n",
       "                      [-0.2489, -0.0345,  0.1685,  ...,  0.0520, -0.2098, -0.1616],\n",
       "                      [ 0.1578, -0.2681,  0.5545,  ..., -0.0715,  0.3075, -0.1358],\n",
       "                      [-0.1957, -0.1269,  0.3334,  ..., -0.2132, -0.1281,  0.0496]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.0479, -0.0818,  0.1068,  ..., -0.2664,  0.1703, -0.0761],\n",
       "                      [-0.0289, -0.3116, -0.0923,  ..., -0.0306,  0.0715, -0.1455],\n",
       "                      [ 0.3413, -0.1629, -0.1162,  ..., -0.0185,  0.0419, -0.1844],\n",
       "                      ...,\n",
       "                      [ 0.0671, -0.2361,  0.1300,  ..., -0.0807,  0.0656, -0.1910],\n",
       "                      [-0.0240, -0.0911,  0.0566,  ..., -0.1189,  0.2360, -0.1000],\n",
       "                      [ 0.1071,  0.1285,  0.1914,  ..., -0.1297,  0.0091, -0.0313]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-0.1931, -0.0447, -0.1496, -0.2621, -0.0918, -0.3113, -0.0494, -0.0456,\n",
       "                      -0.3220, -0.0388, -0.1148, -0.2432, -0.1159, -0.0178, -0.2157, -0.0435,\n",
       "                      -0.1658, -0.2975, -0.1591, -0.1764, -0.0182, -0.1876, -0.3027, -0.3295,\n",
       "                      -0.1396, -0.1799, -0.1710, -0.0235, -0.1188, -0.0355, -0.1445, -0.1966,\n",
       "                      -0.1566, -0.1236, -0.2515, -0.1416, -0.0104, -0.2538, -0.0122, -0.2135,\n",
       "                      -0.1843, -0.1421, -0.3030,  0.0249, -0.3457, -0.0845, -0.2007, -0.0639,\n",
       "                      -0.1809, -0.2928, -0.0799, -0.1453, -0.0180,  0.0281, -0.0031, -0.2218,\n",
       "                      -0.2009, -0.3430, -0.1085, -0.2245, -0.0888, -0.1954, -0.3754, -0.0833,\n",
       "                      -0.2978, -0.2303, -0.2169, -0.3445, -0.2395, -0.2507, -0.1397, -0.1690,\n",
       "                      -0.1798,  0.0005, -0.0335, -0.2096, -0.1221, -0.2610, -0.1519, -0.1116,\n",
       "                      -0.1028, -0.1072, -0.2456, -0.1355, -0.2364, -0.0809, -0.1480, -0.0604,\n",
       "                      -0.0140, -0.0438, -0.1911, -0.1803,  0.0378, -0.1077, -0.1587, -0.2090,\n",
       "                      -0.2700, -0.1086, -0.1675, -0.1394, -0.1379, -0.1325, -0.0980, -0.1767,\n",
       "                      -0.1032, -0.1227, -0.0478,  0.0624, -0.1435, -0.1424, -0.0799, -0.0770,\n",
       "                      -0.2000, -0.0644, -0.2056, -0.1004, -0.1723, -0.0424, -0.2517, -0.1491,\n",
       "                      -0.2635, -0.1974, -0.2256, -0.1361, -0.1218, -0.0380, -0.2389, -0.0809,\n",
       "                      -0.0496,  0.0011, -0.0801, -0.0676, -0.0203, -0.1580, -0.2383, -0.1974,\n",
       "                      -0.4183, -0.1774, -0.0819, -0.1053, -0.1149, -0.0606, -0.0650, -0.0178,\n",
       "                      -0.0303, -0.2595, -0.1991, -0.0380, -0.2519, -0.0728, -0.1624, -0.0720,\n",
       "                      -0.0876, -0.0621, -0.0800, -0.0749, -0.0776, -0.1837, -0.0696, -0.2760,\n",
       "                      -0.0498, -0.0148, -0.1684, -0.0292, -0.1660, -0.1172, -0.0879, -0.0505,\n",
       "                      -0.1579, -0.1395, -0.1671, -0.0794, -0.1814, -0.1399, -0.0538, -0.1730,\n",
       "                       0.0164, -0.0232, -0.2393,  0.0957, -0.0520, -0.2175,  0.0539, -0.0454,\n",
       "                      -0.1786,  0.0322, -0.1691, -0.1222, -0.1413, -0.0992, -0.2179, -0.2601,\n",
       "                      -0.1689, -0.1445, -0.2690, -0.0845, -0.1823, -0.0661,  0.0027, -0.2046,\n",
       "                      -0.2107, -0.0298, -0.0533, -0.0772, -0.0298, -0.0657, -0.1338, -0.1286,\n",
       "                      -0.1546,  0.0152, -0.1295,  0.0247, -0.1773, -0.0248, -0.1206, -0.1158,\n",
       "                      -0.0267, -0.0513,  0.0546, -0.2057, -0.0228, -0.0977, -0.0594, -0.0917,\n",
       "                      -0.1492, -0.0709, -0.0750,  0.0397, -0.1645, -0.1201, -0.1822, -0.0774,\n",
       "                      -0.2487, -0.1096, -0.0869, -0.2201, -0.1550, -0.1790, -0.1613, -0.0926,\n",
       "                      -0.0583, -0.0901, -0.0824, -0.1230, -0.0578,  0.2006, -0.1180, -0.1788,\n",
       "                      -0.1400, -0.0423, -0.1687, -0.2137, -0.0984,  0.0175, -0.2023, -0.1354,\n",
       "                      -0.0394,  0.0455,  0.0771,  0.1231,  0.0169, -0.1379, -0.0061,  0.0376,\n",
       "                       0.1004, -0.0056, -0.0203,  0.0076, -0.0006,  0.0852,  0.0845, -0.1019,\n",
       "                       0.0449,  0.0129, -0.0639, -0.0032, -0.0098, -0.0209, -0.0447, -0.0122,\n",
       "                       0.0151,  0.0188,  0.0641,  0.0796, -0.0016,  0.0585,  0.0144, -0.0764,\n",
       "                       0.0269, -0.0449,  0.0932,  0.0467,  0.0237, -0.0222,  0.0582, -0.0766,\n",
       "                      -0.1213, -0.0340, -0.0416,  0.0067,  0.0114,  0.0791,  0.0249, -0.0045,\n",
       "                      -0.1130,  0.0221,  0.0507, -0.0592, -0.0720, -0.1161, -0.1586, -0.0817,\n",
       "                       0.0863,  0.0772,  0.0352,  0.0295, -0.0321,  0.1003, -0.0396,  0.1126,\n",
       "                       0.0606, -0.0638, -0.0256, -0.0056, -0.0575, -0.0702,  0.0472, -0.0797,\n",
       "                       0.1119, -0.0065, -0.0129, -0.0313,  0.0198, -0.0932,  0.0314, -0.0336,\n",
       "                      -0.0428, -0.0739, -0.0502,  0.0766,  0.0312, -0.1041,  0.0115, -0.0482,\n",
       "                      -0.0886, -0.1164, -0.0476, -0.0590, -0.0233,  0.0675, -0.0821, -0.0357,\n",
       "                      -0.0065, -0.0440, -0.0394,  0.0652, -0.0426, -0.0563, -0.0089,  0.0142,\n",
       "                       0.0281,  0.0403,  0.0705,  0.0276, -0.0400,  0.0089, -0.0656, -0.0189,\n",
       "                       0.0467, -0.0107, -0.0694, -0.0791, -0.0131, -0.0929, -0.0600,  0.0173,\n",
       "                       0.0135,  0.0142, -0.1181, -0.0369, -0.1990,  0.0623, -0.0082,  0.1075,\n",
       "                      -0.1378, -0.0186, -0.2611, -0.2083, -0.0768, -0.3190, -0.0398, -0.1553,\n",
       "                      -0.1908, -0.0428, -0.1206, -0.1200, -0.0196, -0.0302, -0.1917,  0.0556,\n",
       "                      -0.2220, -0.1565, -0.1593, -0.2041, -0.1270, -0.1003, -0.1694, -0.1332,\n",
       "                      -0.0563, -0.1633, -0.0458, -0.0731,  0.0156, -0.0084, -0.0117, -0.1745,\n",
       "                      -0.2250,  0.0010, -0.2258, -0.2461, -0.1463, -0.1655, -0.0187, -0.1342,\n",
       "                      -0.1695, -0.1401, -0.1890, -0.0445, -0.1917,  0.0520, -0.0943, -0.1441,\n",
       "                      -0.2427, -0.1380, -0.1210,  0.0805, -0.0602, -0.1649, -0.2821, -0.1261,\n",
       "                       0.0460, -0.1457, -0.1547, -0.2992, -0.0788, -0.1473, -0.1469, -0.2511,\n",
       "                      -0.3812, -0.1250, -0.2075, -0.1558, -0.0614, -0.1711, -0.1363, -0.1860,\n",
       "                      -0.0764, -0.0793, -0.0829, -0.0777, -0.1556, -0.2663,  0.0314, -0.2293,\n",
       "                      -0.1523,  0.0243, -0.2060, -0.0139, -0.0742, -0.0670, -0.0709, -0.1273,\n",
       "                      -0.2091, -0.0959, -0.0182, -0.1573, -0.0718,  0.0006, -0.0545, -0.2875,\n",
       "                      -0.3535, -0.0691, -0.0979, -0.0681, -0.0467, -0.2097, -0.1863, -0.2337,\n",
       "                      -0.1123, -0.1712, -0.1188, -0.1121, -0.1944, -0.3227, -0.1632,  0.0414,\n",
       "                      -0.1561, -0.1684, -0.0057, -0.0275, -0.0320, -0.0376, -0.1678, -0.2242,\n",
       "                      -0.0506, -0.1340, -0.2412,  0.0093, -0.0284,  0.0050, -0.1101, -0.1346])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([-1.9226e-01, -3.2165e-02, -2.0406e-01, -2.3329e-01, -7.4518e-02,\n",
       "                      -1.5536e-01, -6.6590e-02, -9.3208e-02, -3.1904e-01, -1.9201e-02,\n",
       "                      -2.8075e-01, -2.4393e-01, -1.0396e-01,  8.7370e-02, -1.4531e-01,\n",
       "                      -1.6718e-01, -2.4384e-01, -2.7105e-01, -2.2813e-01, -1.9708e-01,\n",
       "                      -3.0042e-02, -6.8661e-02, -7.3144e-02, -1.9879e-01, -9.0350e-02,\n",
       "                      -7.1054e-02, -1.4677e-01, -2.8892e-01, -6.8938e-02, -1.5450e-01,\n",
       "                      -2.1243e-01, -1.2514e-01, -1.3256e-01, -2.4672e-01, -1.9992e-01,\n",
       "                      -2.0382e-01, -1.2875e-01, -9.5703e-02, -1.2940e-02, -5.9033e-02,\n",
       "                      -2.5124e-01, -2.0154e-01, -3.2413e-01, -1.6568e-01, -2.2538e-01,\n",
       "                      -4.5633e-02, -2.3278e-01, -2.5949e-01, -1.0622e-01, -1.6775e-01,\n",
       "                      -8.6695e-02, -1.9611e-01, -2.1593e-01, -1.0087e-01, -1.1810e-01,\n",
       "                      -1.9246e-02, -1.6951e-01, -2.5789e-01, -5.1953e-02, -2.4485e-01,\n",
       "                      -1.3063e-01,  3.3232e-02, -3.2843e-01, -2.1667e-01, -2.8234e-01,\n",
       "                      -1.8431e-01, -2.2795e-01, -1.8937e-01, -3.3623e-01, -1.9755e-01,\n",
       "                      -1.9881e-01, -2.8921e-01, -1.1591e-01, -1.0359e-01, -2.0270e-02,\n",
       "                      -5.1762e-02, -2.1775e-01, -1.8494e-01,  5.2367e-02, -1.5620e-01,\n",
       "                      -1.5185e-01, -1.1026e-01, -1.2993e-01, -4.2779e-02, -1.1709e-01,\n",
       "                      -1.0625e-01, -9.4076e-02, -1.3469e-01, -1.1116e-01,  4.0718e-03,\n",
       "                      -1.8245e-01, -1.5234e-01, -9.9483e-02, -2.2952e-02, -1.4150e-01,\n",
       "                      -1.1926e-01, -1.5675e-01, -7.1440e-02, -2.1499e-01, -1.1889e-01,\n",
       "                      -1.3020e-01, -3.9965e-02, -2.1020e-01, -2.6072e-01, -9.4473e-02,\n",
       "                      -9.6577e-03, -1.8796e-01, -9.1007e-02, -1.6718e-01, -8.8258e-02,\n",
       "                      -1.8657e-01, -1.0106e-01, -3.0393e-01, -1.9470e-01, -2.5020e-01,\n",
       "                      -1.8513e-01, -1.1451e-01, -6.2217e-02, -1.2733e-01, -1.1152e-01,\n",
       "                      -1.3841e-01, -2.2224e-01, -3.0522e-01, -7.6680e-02,  3.3018e-03,\n",
       "                      -1.1025e-01, -9.5933e-02, -1.8365e-01, -1.7089e-01, -1.2659e-01,\n",
       "                      -2.0384e-01, -2.2371e-01,  2.7261e-02, -2.0917e-01, -1.9545e-01,\n",
       "                      -8.6211e-02, -2.9182e-01,  4.6784e-02,  1.7692e-02,  7.0694e-02,\n",
       "                      -1.9355e-01, -3.3629e-02,  4.5179e-03, -9.0404e-02,  5.0161e-02,\n",
       "                      -3.1212e-01,  3.7657e-03, -2.2633e-01, -1.6616e-01, -5.4551e-02,\n",
       "                      -9.5149e-02, -1.7711e-01, -8.8100e-02, -1.1950e-01, -1.0635e-01,\n",
       "                      -3.5437e-02, -3.1129e-02, -1.1080e-01, -1.7202e-01, -2.9677e-01,\n",
       "                      -6.2915e-02, -8.9210e-02, -1.1684e-01, -7.4663e-02, -1.8754e-01,\n",
       "                      -2.0134e-01, -2.0931e-01, -1.4314e-01, -1.9826e-01, -1.5227e-01,\n",
       "                      -2.2732e-01, -4.4966e-02, -2.2087e-01, -1.7967e-01, -1.3273e-01,\n",
       "                      -1.1480e-01,  3.5807e-02, -8.0977e-02, -2.7428e-01, -1.1462e-02,\n",
       "                      -3.0957e-02, -1.6067e-01,  4.6532e-02, -1.7998e-01, -1.7855e-01,\n",
       "                      -2.0134e-03, -1.4961e-01, -3.7699e-02, -1.7053e-01, -1.6968e-01,\n",
       "                      -1.7902e-01, -2.1001e-01, -1.8291e-01, -1.5774e-01, -1.8295e-01,\n",
       "                      -8.6157e-02, -2.3423e-01, -1.4062e-01, -2.1993e-01, -1.6086e-01,\n",
       "                      -2.7159e-01, -1.5457e-01, -9.8623e-02,  9.2955e-02, -2.7048e-02,\n",
       "                      -1.1912e-01, -1.3344e-01, -1.7714e-01, -1.8600e-01, -8.7876e-02,\n",
       "                       3.2875e-02, -1.4570e-01, -2.1317e-01, -1.2213e-01, -2.0322e-01,\n",
       "                      -2.0267e-01,  6.2439e-02, -6.6557e-02, -2.9036e-02, -1.5408e-01,\n",
       "                      -1.3951e-01, -1.0826e-01, -8.6131e-02, -2.3773e-01, -1.1041e-01,\n",
       "                       8.5042e-03,  4.2715e-03, -3.4511e-02, -1.8557e-01, -1.9469e-01,\n",
       "                      -1.3618e-01, -1.8189e-01, -1.2861e-01, -1.4019e-01, -5.0057e-02,\n",
       "                      -1.1792e-01, -2.0676e-01, -3.3469e-02, -1.6562e-01, -3.4004e-02,\n",
       "                       5.4924e-03, -5.3330e-03, -1.0669e-01, -1.5912e-01, -7.2661e-02,\n",
       "                       5.3220e-02, -1.6817e-01, -4.1812e-02, -2.5046e-03, -1.6837e-02,\n",
       "                      -1.5616e-01, -1.4024e-01, -5.3972e-02, -3.6669e-02, -1.7670e-01,\n",
       "                      -9.2059e-02, -5.7927e-02,  1.2010e-02,  7.9836e-02, -9.3939e-03,\n",
       "                      -2.7966e-02, -1.1209e-01, -2.2032e-02, -2.8108e-02,  4.6139e-02,\n",
       "                       4.7660e-02,  2.5974e-02,  1.3095e-02, -4.4268e-02,  1.7890e-02,\n",
       "                      -4.4680e-02, -1.1601e-01, -1.0264e-02, -5.6012e-02,  5.7444e-03,\n",
       "                      -9.9237e-02,  1.1650e-01, -1.5368e-01,  7.4592e-02,  1.4445e-02,\n",
       "                      -1.4170e-02, -5.8128e-03,  4.3344e-03,  1.1354e-01,  5.5979e-02,\n",
       "                       3.3752e-02,  6.0947e-02, -4.4435e-02,  4.3893e-03, -2.2609e-02,\n",
       "                      -2.1326e-02, -4.4562e-02, -8.7397e-03,  4.8705e-02, -1.1418e-01,\n",
       "                       8.1004e-02,  9.5690e-02, -6.1343e-02, -3.6571e-02,  1.2204e-02,\n",
       "                       2.0559e-02, -6.0307e-02, -3.1717e-02,  1.1991e-02,  4.3065e-02,\n",
       "                       6.3162e-02,  2.2959e-02, -5.2004e-02, -7.8349e-02, -6.7213e-02,\n",
       "                      -4.1653e-02,  1.4970e-01, -1.5651e-01,  4.4907e-02,  4.2092e-02,\n",
       "                      -3.4930e-02,  6.9053e-02, -2.5669e-02,  1.0284e-02, -6.0445e-02,\n",
       "                       1.3039e-02,  7.9837e-02, -1.2585e-01,  6.4149e-02, -8.3220e-02,\n",
       "                      -1.2194e-02, -1.6025e-01,  9.3863e-03, -8.7577e-02, -8.0521e-02,\n",
       "                      -3.8652e-02, -1.1305e-02,  1.2237e-03, -8.5866e-02, -2.8039e-04,\n",
       "                      -8.7925e-02, -1.3713e-02, -6.4956e-03,  5.8222e-02,  5.8572e-02,\n",
       "                       1.6204e-02, -1.3193e-01,  4.5272e-02, -5.6458e-02, -1.3828e-02,\n",
       "                      -2.9380e-02,  6.9098e-02,  3.1128e-02,  7.6891e-03, -9.0977e-02,\n",
       "                       3.2125e-02,  1.2970e-01,  5.7907e-02, -6.5355e-02,  7.9013e-02,\n",
       "                      -3.3112e-02, -1.1679e-01, -1.7900e-02, -2.8706e-02,  2.5431e-03,\n",
       "                      -4.0304e-02, -1.2152e-02, -1.5863e-02, -8.5244e-02, -2.2784e-02,\n",
       "                      -1.0956e-01, -1.8877e-02,  1.2405e-01,  1.6955e-02, -9.6697e-02,\n",
       "                      -3.2653e-02, -8.8509e-02, -4.8215e-02,  3.4915e-02, -1.2747e-01,\n",
       "                      -4.1318e-02,  1.2690e-01, -3.7131e-02, -3.0081e-02,  6.0358e-02,\n",
       "                      -1.5654e-01,  3.1301e-02, -2.3423e-02, -5.9172e-02, -1.7429e-01,\n",
       "                      -6.0378e-02, -1.6092e-01, -2.3678e-01, -5.5467e-02, -2.5737e-01,\n",
       "                      -1.2411e-01, -7.6653e-02, -3.0200e-01,  9.7316e-02, -8.3156e-02,\n",
       "                      -1.7372e-01, -1.0545e-01, -1.1070e-01, -1.2346e-01, -3.1240e-02,\n",
       "                      -2.4535e-01, -1.9638e-01,  3.8651e-02, -7.5752e-02, -1.5276e-02,\n",
       "                      -1.3105e-01, -3.4533e-01, -4.6539e-02,  1.7388e-02, -1.6394e-01,\n",
       "                      -1.4303e-01, -9.6119e-02, -1.4757e-02, -1.4806e-01, -1.6268e-01,\n",
       "                      -1.4525e-01, -1.4310e-01, -9.4977e-02, -1.4045e-01, -8.1181e-02,\n",
       "                      -1.2231e-01, -3.4213e-02,  9.6473e-03, -6.3260e-02, -6.4143e-02,\n",
       "                      -1.1140e-01, -2.7475e-01, -4.3877e-02, -1.2646e-01, -1.0592e-01,\n",
       "                      -1.7996e-01, -8.5940e-02, -1.9010e-01, -9.3110e-02, -7.1366e-02,\n",
       "                       8.0493e-02, -9.5733e-02, -2.4246e-01, -1.5457e-01, -1.0671e-01,\n",
       "                      -7.0562e-02, -1.3621e-01, -5.5755e-02, -2.8824e-01, -1.4507e-01,\n",
       "                      -1.3736e-01, -1.7548e-01, -1.3749e-01, -2.6638e-01, -1.3046e-01,\n",
       "                      -6.9701e-02, -2.0956e-01, -1.6154e-01, -1.5469e-01, -9.0347e-02,\n",
       "                      -2.2800e-01, -1.4178e-01, -1.8020e-01,  3.4107e-02, -3.8573e-02,\n",
       "                      -1.4110e-01, -1.4646e-01, -7.9963e-02, -1.6101e-01, -1.0723e-01,\n",
       "                      -1.9900e-01, -1.6427e-01, -8.8753e-02, -1.5168e-01,  2.4022e-02,\n",
       "                      -1.6573e-01, -1.9250e-01, -1.4852e-01, -8.9055e-02,  3.7752e-02,\n",
       "                      -3.4566e-02, -1.7872e-01, -1.4357e-01, -1.6877e-01, -2.6196e-01,\n",
       "                      -2.1601e-01, -1.4150e-01, -1.1579e-01, -8.2845e-02, -1.1475e-01,\n",
       "                      -1.9386e-01, -8.0707e-02, -8.2268e-02, -4.7320e-02, -2.3564e-01,\n",
       "                      -1.1542e-01, -9.8175e-02, -2.7797e-01, -1.8229e-01, -2.3813e-01,\n",
       "                      -1.9885e-02, -2.6429e-02, -1.9155e-01, -1.0176e-01,  3.2597e-02,\n",
       "                      -9.9072e-02, -1.9119e-02, -1.4830e-01, -1.7646e-01, -1.9170e-01,\n",
       "                      -4.0921e-02, -3.2791e-01, -1.4180e-01, -9.3087e-02, -2.2628e-02,\n",
       "                      -1.7796e-01, -1.3896e-01])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0285,  0.2281, -0.0277,  ...,  0.2867, -0.3488,  0.2195],\n",
       "                      [ 0.2369,  0.2895,  0.0378,  ...,  0.1079, -0.3785, -0.0162],\n",
       "                      [-0.0104,  0.0945,  0.0476,  ..., -0.3126,  0.3563,  0.0098],\n",
       "                      ...,\n",
       "                      [-0.0662, -0.3996,  0.1469,  ..., -0.4032,  0.4863,  0.3196],\n",
       "                      [-0.3093, -0.1284,  0.0138,  ..., -0.3898, -0.0794, -0.2274],\n",
       "                      [-0.0191, -0.2018, -0.0475,  ..., -0.0263, -0.4812, -0.2273]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0977, -0.0893,  0.0289,  ...,  0.1152, -0.3779, -0.0388],\n",
       "                      [-0.0761,  0.0885,  0.0696,  ...,  0.2819,  0.1266, -0.4932],\n",
       "                      [-0.1870, -0.3023,  0.0509,  ..., -0.1557,  0.0030,  0.0969],\n",
       "                      ...,\n",
       "                      [ 0.1588,  0.0528, -0.1663,  ...,  0.0398, -0.0221,  0.1155],\n",
       "                      [ 0.0615,  0.0970, -0.2416,  ..., -0.0795, -0.3222,  0.0789],\n",
       "                      [ 0.1534, -0.1970, -0.0822,  ...,  0.1181,  0.1516,  0.1872]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 1.1462e-02,  3.0068e-02,  1.1469e-01, -3.8489e-02,  1.5296e-01,\n",
       "                       8.3756e-02,  1.7968e-01, -1.2200e-01, -1.6704e-02, -5.9527e-02,\n",
       "                      -1.9709e-03, -2.0414e-02,  1.7996e-01,  2.2576e-02,  2.4060e-02,\n",
       "                      -1.9881e-02,  1.2409e-04, -7.6941e-02,  9.4825e-02,  2.0480e-01,\n",
       "                       1.6830e-01,  7.4915e-02,  2.3420e-01, -5.1657e-02,  2.4212e-01,\n",
       "                       2.5480e-02, -1.1099e-01,  6.5950e-02,  7.9934e-02,  1.5074e-01,\n",
       "                       1.8158e-01,  1.8850e-02,  9.5233e-02, -8.8525e-02, -9.9531e-02,\n",
       "                       5.8046e-02, -6.0889e-02,  2.7636e-02, -1.1675e-01,  2.0329e-02,\n",
       "                       6.1033e-03,  1.5740e-01, -7.7333e-02, -1.0675e-02,  1.0983e-01,\n",
       "                      -6.8080e-02,  1.0258e-01, -1.8411e-01, -2.5204e-02,  1.1895e-01,\n",
       "                       2.1580e-02, -6.6696e-03, -1.5719e-02,  1.0878e-01, -5.7776e-02,\n",
       "                       1.9907e-02,  1.2258e-01, -1.5651e-02,  6.2966e-02, -5.7350e-02,\n",
       "                      -4.4286e-02,  6.6036e-03,  2.6129e-02,  6.1805e-02,  3.8360e-02,\n",
       "                       1.8315e-02,  1.8615e-01, -1.0709e-01,  8.0991e-02, -5.9998e-02,\n",
       "                      -1.4101e-01,  9.3870e-03,  1.6559e-02,  1.1609e-02,  3.3738e-03,\n",
       "                      -5.3655e-02,  3.8232e-03,  4.3773e-02,  9.6742e-02, -2.1161e-01,\n",
       "                       8.0224e-02,  1.2718e-01,  1.2169e-01,  1.5246e-02,  9.9877e-03,\n",
       "                      -3.5842e-03, -7.9140e-02, -4.2880e-02,  3.3815e-02, -3.8997e-02,\n",
       "                       7.0830e-03, -1.0668e-01,  7.2662e-03,  1.8270e-02,  1.7152e-02,\n",
       "                       2.2890e-02, -1.9053e-02,  3.7203e-02,  1.2215e-01,  8.2685e-02,\n",
       "                       1.3584e-01,  9.6277e-02,  1.1035e-01,  1.0366e-01,  4.1698e-02,\n",
       "                      -6.9245e-02, -7.2377e-02, -1.6902e-02,  2.0719e-02, -8.1025e-03,\n",
       "                       1.5825e-01,  2.2009e-02,  1.1017e-01, -1.1627e-01,  1.3413e-01,\n",
       "                       1.4859e-01,  1.5912e-01,  1.8980e-01,  1.1576e-02,  5.7537e-02,\n",
       "                      -5.4320e-02,  3.9077e-02, -7.4271e-02,  2.3542e-02,  1.9853e-01,\n",
       "                       1.3395e-01,  4.9489e-02,  4.7219e-02, -5.2428e-02, -5.6794e-02,\n",
       "                      -2.2519e-02, -4.7026e-03, -1.9761e-01, -1.7373e-01, -9.2623e-02,\n",
       "                       3.0663e-02, -2.4104e-01, -2.0394e-02, -5.4325e-02,  1.3034e-01,\n",
       "                      -1.6525e-01, -1.2537e-01, -1.3656e-01, -4.8176e-02, -7.7118e-02,\n",
       "                      -3.8989e-02, -6.4682e-02,  3.7768e-02, -1.4973e-01, -8.8744e-02,\n",
       "                      -5.9205e-02, -4.5630e-02, -1.5598e-01, -1.3854e-01, -1.8271e-01,\n",
       "                      -1.8582e-01, -2.5193e-02, -4.7866e-03, -3.2653e-02, -2.6164e-01,\n",
       "                      -1.1363e-01, -6.9096e-02, -1.9739e-01,  1.1525e-01, -1.7152e-01,\n",
       "                      -3.1089e-02, -2.8485e-02, -1.2646e-01, -2.6709e-02, -2.3765e-01,\n",
       "                      -9.4339e-02, -7.8688e-02, -1.0851e-01, -1.9157e-01, -9.0539e-02,\n",
       "                      -1.6433e-01,  1.5189e-02, -1.4622e-01, -1.6439e-02,  1.2171e-01,\n",
       "                      -1.7347e-01,  1.3003e-01, -1.7768e-01,  7.9200e-02, -9.5115e-02,\n",
       "                      -1.1788e-02, -2.1362e-01, -1.5774e-01,  2.6276e-01, -7.4957e-02,\n",
       "                      -1.0325e-01, -1.0250e-01,  3.3367e-02,  5.0757e-02, -2.7054e-02,\n",
       "                       1.2066e-01, -7.1079e-02,  7.1928e-02, -9.5734e-02,  1.5563e-02,\n",
       "                      -8.4789e-02, -1.1414e-01,  5.9740e-02, -1.5449e-01, -7.5944e-03,\n",
       "                      -1.8586e-01, -1.1096e-01, -4.7850e-02,  1.7568e-02, -1.3353e-01,\n",
       "                      -2.6432e-01, -8.6429e-02, -2.7096e-02, -9.1322e-02, -9.2951e-02,\n",
       "                      -1.0286e-02, -9.3471e-02, -6.5133e-02, -9.8384e-02, -2.1654e-02,\n",
       "                       9.0187e-02, -8.7694e-02, -4.8745e-02, -7.3328e-03, -1.8874e-01,\n",
       "                       1.8935e-02, -2.4030e-01,  8.7888e-02, -3.8442e-02, -6.7543e-02,\n",
       "                       4.9233e-02, -1.0093e-01,  1.3942e-02, -2.5984e-02, -9.8112e-04,\n",
       "                       4.0146e-02, -3.9243e-02,  3.3923e-03, -2.4235e-01, -7.2100e-03,\n",
       "                      -2.2053e-01, -5.2463e-02, -1.7365e-01, -2.5211e-01,  1.7307e-02,\n",
       "                      -1.0255e-01, -2.2605e-01, -3.1090e-02, -4.7922e-02, -4.1123e-02,\n",
       "                       1.0180e-02, -5.9647e-02,  4.9281e-02, -5.9469e-02,  8.7604e-02,\n",
       "                      -1.2646e-02, -1.3980e-02,  2.9783e-02,  5.9128e-02, -7.9267e-02,\n",
       "                      -1.1799e-02, -6.0420e-02,  8.4812e-02, -3.2506e-02,  7.0681e-02,\n",
       "                      -3.0763e-03,  2.2767e-02,  9.9458e-02,  1.3390e-02, -4.1531e-02,\n",
       "                       1.0467e-02, -2.0539e-02,  1.3109e-01, -2.4200e-02,  1.1628e-01,\n",
       "                      -4.8216e-02,  1.5412e-02, -1.3689e-01,  7.6603e-02, -1.5043e-01,\n",
       "                      -3.4646e-02, -5.8767e-02,  6.0272e-02, -2.8762e-02, -6.2690e-02,\n",
       "                      -4.0259e-02, -1.1650e-01, -3.9814e-03,  1.8046e-02,  8.4576e-02,\n",
       "                       6.1143e-02,  8.3398e-03,  2.8672e-02, -4.4880e-02, -1.7521e-02,\n",
       "                      -7.0291e-02, -6.6596e-02,  6.1405e-03, -4.3060e-02,  2.5040e-02,\n",
       "                       7.7240e-03,  3.9242e-02,  1.9650e-02,  1.9417e-02,  7.2677e-02,\n",
       "                       1.8296e-02, -3.2059e-02, -6.6176e-02, -5.1348e-02,  1.3267e-01,\n",
       "                       3.7985e-02,  1.1884e-01,  4.7975e-02, -1.6500e-02,  5.3035e-02,\n",
       "                      -1.6064e-01, -3.0847e-03,  2.4382e-02,  5.6372e-02, -7.5918e-02,\n",
       "                      -6.0639e-02,  1.1156e-01, -1.2789e-01,  1.2682e-01,  3.1228e-02,\n",
       "                      -1.3549e-03,  7.4091e-02, -5.4010e-02, -2.5489e-02,  1.8900e-02,\n",
       "                       1.6737e-02,  8.7170e-02,  6.6399e-02,  6.7356e-02,  7.9883e-02,\n",
       "                      -2.4784e-02, -1.1701e-01, -8.1482e-02,  8.7853e-03, -2.1657e-02,\n",
       "                       5.0802e-02,  4.3854e-02, -7.3052e-02, -1.1785e-01,  7.9423e-02,\n",
       "                      -8.3340e-02, -1.6093e-02,  8.0731e-02, -5.6462e-02, -1.6431e-02,\n",
       "                      -1.1920e-01,  5.8707e-02,  2.1129e-01, -1.5327e-01, -1.0844e-01,\n",
       "                      -2.7057e-03, -1.3923e-01, -3.6441e-02,  1.1651e-01, -1.0162e-01,\n",
       "                       3.4770e-02,  1.0027e-01, -1.1612e-01,  8.7434e-02, -3.0622e-02,\n",
       "                      -7.0387e-02, -4.2513e-02,  1.4652e-01, -4.9815e-02,  5.8870e-03,\n",
       "                       5.4340e-03,  4.6344e-02,  5.5890e-02,  9.7270e-02,  3.4690e-02,\n",
       "                       1.1474e-02,  8.8955e-02, -1.6469e-01,  2.4395e-02, -6.4598e-02,\n",
       "                       7.4335e-02,  2.2800e-02,  9.0336e-02, -1.7526e-02,  1.2139e-01,\n",
       "                      -4.4840e-02,  4.3919e-03, -5.9405e-02,  1.5747e-01,  1.4051e-01,\n",
       "                       3.9796e-02,  8.3912e-02, -2.7064e-01, -3.4773e-02, -3.7352e-02,\n",
       "                       8.2315e-02,  7.7217e-02,  3.5125e-02,  1.4964e-01,  2.2247e-02,\n",
       "                       6.6457e-03, -8.7271e-03,  1.7119e-01,  9.8130e-02,  2.5621e-01,\n",
       "                      -4.4790e-02,  2.0747e-01, -3.5971e-02,  1.4525e-01, -1.3646e-01,\n",
       "                       4.0499e-02, -6.5242e-02,  4.8581e-02,  1.8147e-01, -1.6512e-02,\n",
       "                      -3.6179e-02, -3.2136e-02,  2.8072e-02,  7.2386e-02,  8.1197e-02,\n",
       "                      -1.5341e-03,  8.7488e-02,  4.8900e-02,  2.9162e-02, -5.8679e-03,\n",
       "                       1.2885e-01, -6.9160e-02, -7.3828e-02,  8.3617e-02,  1.1702e-01,\n",
       "                       1.4514e-01, -1.2625e-01,  1.3965e-01,  6.6833e-02, -2.0537e-01,\n",
       "                       7.5870e-02,  6.4725e-02, -7.4631e-03,  6.7905e-02,  4.4516e-02,\n",
       "                       1.5959e-01, -2.2361e-02, -9.3430e-02, -5.7514e-03,  1.0073e-02,\n",
       "                       6.2438e-03,  1.2446e-02,  1.3101e-01,  1.6511e-01,  3.7648e-02,\n",
       "                       3.5140e-02, -8.0739e-02,  5.6139e-02,  6.2648e-02,  4.2945e-02,\n",
       "                       2.6632e-02, -1.3525e-01,  1.1368e-01, -4.3962e-02,  1.4927e-01,\n",
       "                       5.9380e-03,  1.3542e-01,  1.3971e-01,  2.9368e-02, -5.8014e-02,\n",
       "                       9.5091e-02,  2.3839e-01,  1.1174e-03, -4.1528e-02,  1.3876e-01,\n",
       "                       7.0792e-02, -4.8216e-02, -1.2615e-01, -1.2900e-02, -2.5067e-02,\n",
       "                      -1.3553e-01,  8.8516e-02,  1.1828e-01,  8.7482e-02,  1.3195e-01,\n",
       "                       1.2230e-01, -3.4185e-02,  8.7610e-02, -2.6666e-04, -2.6434e-03,\n",
       "                       6.9297e-02,  7.6211e-02,  1.4615e-02,  1.1010e-01,  5.6789e-02,\n",
       "                      -9.8184e-02,  7.4501e-02, -1.3359e-01,  2.8972e-02,  1.0309e-01,\n",
       "                       5.6004e-03,  1.2487e-01, -7.4110e-02,  1.5631e-01,  1.5562e-01,\n",
       "                       1.8295e-01,  2.5870e-01,  9.0461e-02,  1.9171e-01,  8.0343e-02,\n",
       "                       7.0099e-02, -6.9257e-02,  8.6097e-03,  2.0033e-02,  4.6294e-02,\n",
       "                       7.3002e-02,  9.7024e-02])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-0.0569,  0.1331,  0.0448, -0.0593,  0.0617,  0.0396,  0.0895,  0.0651,\n",
       "                      -0.0832,  0.1035,  0.0874,  0.0296,  0.0919,  0.0565,  0.1079, -0.0664,\n",
       "                      -0.1332,  0.0381,  0.1312,  0.0838,  0.2135,  0.0452,  0.2079, -0.0326,\n",
       "                       0.1466,  0.0554,  0.0081,  0.0652, -0.0282,  0.2448,  0.0807, -0.0982,\n",
       "                      -0.0505,  0.0719, -0.0447,  0.1634, -0.0695,  0.1477,  0.0056, -0.0672,\n",
       "                       0.0978,  0.0743, -0.0046,  0.0162,  0.1761, -0.0545,  0.0188,  0.0066,\n",
       "                      -0.0504,  0.1776, -0.0258, -0.0930,  0.1899,  0.0848, -0.0405, -0.0516,\n",
       "                       0.2362, -0.1244,  0.0168, -0.0108,  0.1492,  0.0857, -0.0290,  0.0788,\n",
       "                       0.0206,  0.0221,  0.0625, -0.1708,  0.0810, -0.0465, -0.0365,  0.0926,\n",
       "                       0.0362, -0.0686,  0.0275,  0.0457, -0.0762,  0.2624, -0.0507, -0.0890,\n",
       "                      -0.0894,  0.0735,  0.0828,  0.0137,  0.0124,  0.0509,  0.0731, -0.0618,\n",
       "                       0.0337, -0.1378,  0.0123,  0.0193, -0.1399,  0.0202,  0.0832,  0.0454,\n",
       "                      -0.0971,  0.0877,  0.0751,  0.0952,  0.0244,  0.0053,  0.0447,  0.1311,\n",
       "                       0.0091, -0.0989, -0.1458,  0.0708, -0.0472,  0.1270,  0.1191, -0.0955,\n",
       "                       0.0941, -0.1208,  0.1432, -0.1182,  0.1381,  0.2545,  0.0723,  0.1209,\n",
       "                       0.0106,  0.1482, -0.0276, -0.1459,  0.2094,  0.0910, -0.0343, -0.0557,\n",
       "                       0.0025, -0.0554, -0.1613, -0.0858, -0.1021, -0.1278, -0.1273, -0.0258,\n",
       "                      -0.1434, -0.1690,  0.0206,  0.0341, -0.1842, -0.0795, -0.0277, -0.0752,\n",
       "                      -0.0332,  0.0018, -0.1842, -0.0309, -0.1131,  0.0272, -0.0068, -0.1388,\n",
       "                      -0.2202, -0.1323, -0.2644, -0.0466,  0.0812,  0.0514, -0.0357, -0.1885,\n",
       "                      -0.1225,  0.0216, -0.0576, -0.0334, -0.1082, -0.0424,  0.0142, -0.2940,\n",
       "                      -0.1898, -0.2606, -0.1260, -0.1011, -0.0798, -0.1511,  0.0085, -0.1167,\n",
       "                      -0.0345, -0.0646,  0.0039, -0.0678, -0.2030,  0.0299, -0.2002, -0.1105,\n",
       "                      -0.0691,  0.0207,  0.0144, -0.1440,  0.1362, -0.0483, -0.1610, -0.0621,\n",
       "                      -0.0520, -0.0373, -0.0254,  0.0587, -0.0313,  0.0527, -0.1343,  0.0451,\n",
       "                      -0.1821, -0.0045, -0.0531, -0.2717, -0.0529, -0.0537, -0.0023, -0.0577,\n",
       "                       0.0729, -0.1264, -0.2055, -0.0898,  0.0014, -0.0626, -0.0688,  0.0626,\n",
       "                      -0.2251, -0.1209, -0.1063,  0.0621,  0.0339, -0.0826, -0.0932, -0.1324,\n",
       "                      -0.1144, -0.0603, -0.1689,  0.0331, -0.0376, -0.0497, -0.1022, -0.0790,\n",
       "                       0.0580, -0.0580,  0.0389, -0.0705, -0.1575, -0.0114, -0.1877,  0.0211,\n",
       "                      -0.2213, -0.0373, -0.1638, -0.0517,  0.0685, -0.2605, -0.0955,  0.0161,\n",
       "                      -0.1455, -0.0088, -0.0371, -0.0308,  0.0898, -0.0474, -0.0704,  0.1412,\n",
       "                      -0.0427,  0.0774,  0.0942,  0.0152,  0.0021,  0.0876,  0.0234,  0.0267,\n",
       "                      -0.0689,  0.1132,  0.0167, -0.0436,  0.0526, -0.0020, -0.1091,  0.0310,\n",
       "                       0.0615, -0.0362, -0.0055,  0.0487,  0.0899,  0.0699, -0.0774,  0.0028,\n",
       "                       0.0069, -0.0940,  0.0481, -0.1464, -0.0624, -0.0108, -0.0284,  0.1243,\n",
       "                       0.1091, -0.0276, -0.0074,  0.0319, -0.0447, -0.0491,  0.0415, -0.0322,\n",
       "                      -0.0727,  0.0749, -0.0049,  0.0878,  0.0097, -0.1045,  0.0909,  0.0905,\n",
       "                      -0.0299, -0.0294,  0.0498,  0.0031,  0.1363,  0.0549,  0.0215, -0.0242,\n",
       "                       0.0441, -0.1165,  0.0607,  0.0478,  0.0220,  0.0902, -0.0558, -0.0683,\n",
       "                      -0.0190,  0.0617,  0.0074,  0.0179,  0.0151, -0.0543,  0.0851, -0.0399,\n",
       "                       0.0512,  0.1011, -0.0099,  0.0052,  0.0110, -0.0479,  0.0189, -0.0510,\n",
       "                      -0.0884, -0.0763, -0.0295, -0.0256, -0.0018, -0.1341,  0.0544,  0.0585,\n",
       "                       0.0248, -0.1009,  0.0009,  0.1185, -0.0070,  0.0537,  0.0485,  0.0991,\n",
       "                       0.0379, -0.0913, -0.0200, -0.0557,  0.0720,  0.1574, -0.0307, -0.0116,\n",
       "                       0.0697, -0.0293, -0.0418, -0.0874,  0.0721, -0.0637,  0.0757,  0.1297,\n",
       "                       0.0363, -0.0413, -0.0436, -0.0705, -0.0006, -0.0626,  0.0068,  0.0722,\n",
       "                      -0.1221,  0.0131, -0.0313, -0.0277,  0.0253,  0.0484, -0.0267, -0.0351,\n",
       "                      -0.0116,  0.0317,  0.0570, -0.0818,  0.1361,  0.1765,  0.2901, -0.0049,\n",
       "                      -0.1111,  0.0271,  0.0658,  0.1399,  0.0735,  0.1248,  0.1507,  0.0418,\n",
       "                      -0.1129,  0.0297,  0.1982,  0.0881,  0.0981, -0.0477,  0.1836, -0.0566,\n",
       "                       0.3009, -0.1263,  0.0393,  0.0368, -0.1146,  0.1948,  0.0580, -0.0138,\n",
       "                      -0.0491,  0.0133, -0.0478,  0.1029,  0.0699,  0.1906,  0.1041,  0.0111,\n",
       "                       0.0769,  0.1283,  0.1795,  0.0645,  0.2085,  0.0863,  0.0139, -0.0035,\n",
       "                       0.0604,  0.1860, -0.1640,  0.0711,  0.1197,  0.1395,  0.1305, -0.0640,\n",
       "                       0.0400,  0.0078, -0.0391, -0.0093,  0.0808,  0.0078,  0.0253,  0.0631,\n",
       "                       0.0356, -0.1304,  0.1230,  0.0188, -0.0436,  0.1325, -0.0625, -0.0771,\n",
       "                       0.0389,  0.0443, -0.0220, -0.0032, -0.0283,  0.1246,  0.0243,  0.0328,\n",
       "                       0.1050, -0.0378,  0.0649,  0.0921,  0.1759,  0.0556,  0.0368, -0.0605,\n",
       "                       0.0086, -0.0150,  0.0283, -0.1141, -0.0634,  0.1831,  0.0094,  0.0838,\n",
       "                      -0.0236,  0.0544,  0.1004, -0.0130,  0.0642,  0.0583,  0.0670, -0.0860,\n",
       "                       0.1030,  0.0536, -0.0443,  0.0788, -0.2590,  0.0847,  0.0587, -0.0582,\n",
       "                       0.1347, -0.0302,  0.2007,  0.0456,  0.1902,  0.1309,  0.0874,  0.1544,\n",
       "                       0.0759,  0.1769,  0.0066, -0.0440,  0.0832, -0.0046, -0.0478,  0.0414])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.2765,  0.6493, -0.0387,  ..., -0.3290,  0.2683, -0.3414],\n",
       "                      [ 0.3056,  0.4601, -0.0755,  ..., -0.2497,  0.2574,  0.6285],\n",
       "                      [-0.8067, -0.2267,  0.0123,  ..., -0.6245, -0.6328, -0.3187],\n",
       "                      ...,\n",
       "                      [ 0.3844, -0.1451,  0.3017,  ..., -0.5716,  0.0788,  0.4558],\n",
       "                      [-0.1573, -0.3294,  0.3143,  ..., -0.2731, -0.0167,  0.3842],\n",
       "                      [ 0.0512,  0.1017, -0.0038,  ..., -0.2413, -0.0374,  0.7365]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0530, -0.2371,  0.0079,  ..., -0.0224, -0.3298, -0.2103],\n",
       "                      [ 0.1710,  0.0967, -0.0998,  ..., -0.1255,  0.3316, -0.2387],\n",
       "                      [ 0.0344, -0.2008,  0.1109,  ...,  0.0075,  0.1499,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.2508, -0.0414, -0.1609,  ..., -0.0633, -0.0163, -0.2464],\n",
       "                      [-0.0145,  0.1145,  0.0882,  ..., -0.0427,  0.2294, -0.2181],\n",
       "                      [-0.0947,  0.0351, -0.0537,  ..., -0.1013,  0.0315, -0.2681]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-1.8529e-01,  1.5704e-02, -2.3438e-01, -1.9745e-01,  1.5124e-02,\n",
       "                      -1.6910e-01, -1.3205e-01, -5.4874e-02, -1.0462e-02, -1.2152e-01,\n",
       "                      -3.0734e-01, -8.0839e-02,  2.3161e-02, -1.1863e-01, -1.2127e-01,\n",
       "                      -5.7981e-02, -4.7176e-02, -2.0224e-01, -3.2841e-01, -1.9169e-01,\n",
       "                       1.6926e-03,  3.6454e-02, -1.8009e-01, -1.7862e-01, -1.5657e-01,\n",
       "                      -2.5848e-01, -6.9809e-02, -1.5669e-01, -1.8601e-02, -1.2722e-01,\n",
       "                      -8.3793e-02, -1.9983e-01,  6.4755e-03, -1.1318e-01, -1.4626e-01,\n",
       "                      -1.0085e-01, -1.9033e-01, -3.4792e-02, -2.4865e-02, -1.6095e-01,\n",
       "                      -8.1316e-02, -1.0507e-02, -1.8402e-01, -7.3061e-02, -1.1158e-01,\n",
       "                      -1.9857e-01, -5.4794e-02, -5.8986e-02, -1.1019e-01, -3.1351e-01,\n",
       "                      -2.7728e-01, -1.3197e-01, -1.3004e-01, -5.3909e-02, -1.0483e-01,\n",
       "                      -1.0975e-01, -1.0663e-01, -1.7223e-01, -3.0532e-02, -1.0962e-01,\n",
       "                      -2.0995e-01, -2.0904e-01, -3.7837e-02, -2.1672e-01, -1.0894e-01,\n",
       "                      -1.7435e-01, -1.8910e-01, -2.2748e-01, -6.0074e-02, -1.6913e-01,\n",
       "                      -1.0478e-01, -1.4582e-01, -1.5048e-01, -1.5716e-01, -1.9246e-01,\n",
       "                      -1.5824e-01, -8.8658e-02, -8.0554e-02, -2.1703e-01, -1.9773e-02,\n",
       "                      -2.1686e-01,  1.1244e-01, -1.8522e-01, -1.3889e-01, -3.1275e-02,\n",
       "                      -1.2408e-01, -6.1867e-02,  2.2039e-02, -1.4443e-01, -1.9868e-01,\n",
       "                      -9.6906e-02, -1.2397e-01,  4.2519e-02, -1.7713e-01, -1.4425e-01,\n",
       "                      -9.3906e-02, -1.7304e-01, -1.8136e-03, -7.4095e-02, -9.9395e-02,\n",
       "                       7.8078e-02, -1.7154e-01, -2.5499e-01, -2.3799e-02,  2.0368e-04,\n",
       "                      -6.4530e-02, -1.5911e-01, -1.1882e-01,  2.9328e-02, -1.5754e-01,\n",
       "                      -1.1374e-01, -1.5773e-01, -1.8640e-02,  5.6431e-02,  1.3490e-01,\n",
       "                      -2.1094e-01, -1.6944e-01, -1.0512e-02, -4.8829e-02,  6.5977e-02,\n",
       "                      -1.6694e-01,  1.1912e-02, -1.1820e-01, -1.0707e-01, -1.3933e-01,\n",
       "                      -9.1362e-02, -5.4525e-02, -5.3521e-02, -7.6146e-02, -1.0299e-03,\n",
       "                      -4.2139e-02, -5.5925e-02,  5.7142e-02,  6.6317e-03, -1.9849e-01,\n",
       "                      -3.9562e-02,  5.9156e-02,  2.2306e-02, -1.3428e-01, -1.6479e-02,\n",
       "                       2.6875e-02, -6.1831e-02, -1.0875e-01,  1.8684e-01, -1.1491e-01,\n",
       "                      -2.2347e-01, -1.4881e-01, -6.5248e-03, -1.5239e-01,  1.3649e-01,\n",
       "                      -1.8883e-01, -1.3325e-01, -1.3560e-01,  1.4997e-02,  1.0409e-02,\n",
       "                      -2.5943e-01, -1.2967e-01, -3.0889e-02, -5.2596e-02, -5.7216e-02,\n",
       "                       1.2490e-01, -1.1295e-01,  1.3622e-02, -1.1990e-01, -2.0116e-01,\n",
       "                      -3.0203e-02,  3.0468e-02, -1.3425e-01,  3.5797e-02, -1.5545e-01,\n",
       "                      -2.9717e-01, -3.3236e-03, -1.4567e-01, -1.0250e-01, -5.9461e-02,\n",
       "                       4.9339e-02, -8.0081e-02, -2.0153e-01, -3.0002e-02, -2.8680e-02,\n",
       "                       2.9162e-02, -2.2368e-01, -1.1658e-01, -1.2276e-01, -2.5176e-01,\n",
       "                      -5.1855e-02,  1.1147e-02, -5.7155e-02, -8.7374e-02, -2.1346e-01,\n",
       "                      -2.0751e-01, -1.3363e-01,  6.4097e-02, -1.0654e-01, -1.2737e-01,\n",
       "                      -5.6479e-02,  7.7583e-02, -3.9791e-02,  5.2478e-02, -1.8043e-01,\n",
       "                       4.4404e-02,  3.4400e-03, -2.7209e-01, -4.5566e-02, -3.9454e-02,\n",
       "                      -3.5546e-02, -9.3991e-02,  9.0849e-02,  3.7148e-02,  3.7000e-02,\n",
       "                      -1.1154e-03, -9.8591e-02, -1.1541e-01, -1.5351e-01, -1.5699e-01,\n",
       "                       3.2605e-02,  1.7127e-01, -2.0997e-01, -1.8344e-02, -7.0561e-03,\n",
       "                       4.0472e-02, -1.3806e-01,  2.7920e-02, -1.2831e-01, -2.6872e-01,\n",
       "                       1.2316e-01,  1.5254e-01, -1.5663e-01,  1.3052e-01, -1.0991e-01,\n",
       "                      -1.3559e-01,  3.4995e-02, -2.9200e-02,  9.7682e-03, -4.4952e-02,\n",
       "                      -1.2229e-01, -5.5696e-03, -2.6334e-01, -9.6695e-02, -4.8029e-02,\n",
       "                       6.8705e-02,  3.1654e-02,  1.2762e-01,  5.0597e-02, -1.6984e-01,\n",
       "                      -4.9288e-02,  4.5806e-02, -2.0019e-02,  1.5980e-02,  8.0662e-02,\n",
       "                       1.4271e-01, -1.5051e-01, -1.4626e-02, -4.1938e-02, -1.2260e-01,\n",
       "                      -3.7424e-02,  2.0952e-02,  2.8270e-02,  4.7386e-02, -1.0928e-01,\n",
       "                       5.7713e-02, -3.3186e-02, -2.8570e-02, -7.5776e-02, -4.3975e-03,\n",
       "                      -1.2113e-01, -3.8767e-02,  5.3110e-02,  4.9233e-02,  1.0134e-01,\n",
       "                       1.7941e-02,  6.0718e-02, -1.0668e-01, -1.1926e-01, -4.7604e-02,\n",
       "                       2.1779e-02,  2.5991e-02,  1.8482e-02, -4.9842e-02, -9.4603e-02,\n",
       "                       2.8115e-02,  8.6268e-03,  6.3638e-02,  5.1173e-02,  8.7333e-02,\n",
       "                      -1.3961e-02, -2.3536e-02,  7.5335e-02,  1.0965e-01,  3.5182e-03,\n",
       "                       7.8005e-03, -2.8031e-02, -3.7135e-02,  2.0924e-02, -9.7627e-02,\n",
       "                       1.8611e-02,  4.5916e-02, -5.6176e-02,  8.1730e-02,  2.9080e-02,\n",
       "                      -2.3403e-02, -5.8177e-02, -1.3882e-01, -1.1119e-02, -6.1649e-02,\n",
       "                       3.0049e-04, -1.9099e-02,  7.7921e-03,  6.1862e-02,  2.1868e-03,\n",
       "                       9.4611e-02, -1.5111e-02, -3.6853e-02, -1.2969e-02,  3.3504e-02,\n",
       "                      -1.6937e-01,  2.8327e-02, -1.6077e-01,  5.5869e-02,  7.2093e-02,\n",
       "                       1.2108e-01, -1.4996e-03,  4.0964e-02,  2.2113e-02,  1.2475e-01,\n",
       "                       1.5675e-03,  7.6046e-03, -3.5934e-02,  3.1901e-02, -7.0805e-02,\n",
       "                      -8.0941e-03, -1.2806e-01,  8.8657e-02,  7.9004e-02, -8.2881e-02,\n",
       "                      -1.4521e-01, -1.7744e-02,  5.2511e-02, -3.4131e-03,  8.3656e-02,\n",
       "                      -5.7435e-02, -4.2010e-02,  3.2909e-02, -3.2333e-02, -1.1891e-02,\n",
       "                      -7.1242e-02, -7.7327e-02, -4.4140e-02,  1.6113e-02, -4.8932e-02,\n",
       "                       4.6161e-03,  1.6403e-02,  7.7843e-02, -5.9476e-02,  4.7279e-02,\n",
       "                       3.3562e-03, -2.2186e-03, -1.1423e-01,  7.0398e-02, -5.1676e-02,\n",
       "                      -8.9019e-02,  4.1178e-03,  3.1412e-02,  1.1083e-03, -5.3343e-02,\n",
       "                      -4.9443e-02,  8.9236e-02, -6.6688e-03, -2.6661e-02,  8.0206e-03,\n",
       "                      -1.0862e-01, -4.6466e-02,  2.7523e-02, -1.0503e-01, -2.6454e-02,\n",
       "                      -2.0353e-03,  7.6270e-02, -8.3269e-02, -1.8834e-01,  3.4782e-02,\n",
       "                      -8.7816e-02, -1.5148e-02, -1.8545e-02,  6.9985e-03, -1.2604e-01,\n",
       "                      -6.5402e-02, -5.2426e-02, -9.0981e-02, -1.3914e-01, -1.5680e-01,\n",
       "                      -1.8740e-01, -1.7130e-02, -1.2757e-01, -1.6599e-01, -2.7445e-01,\n",
       "                      -1.6155e-01, -3.8812e-02, -1.4617e-01, -9.7928e-02, -7.5715e-02,\n",
       "                      -3.8853e-02, -1.6630e-01, -2.5346e-01, -5.7739e-02, -1.0296e-01,\n",
       "                       4.3284e-02, -9.1208e-02, -9.2297e-02, -1.0288e-01, -2.1886e-01,\n",
       "                      -1.7609e-01, -1.3236e-01, -1.0144e-01,  3.6556e-02, -9.9870e-02,\n",
       "                      -2.4781e-01, -5.3514e-03, -1.7114e-01, -2.2079e-01, -1.4006e-01,\n",
       "                      -2.0560e-01, -1.6948e-01, -1.0924e-01, -1.3780e-01, -2.3499e-02,\n",
       "                      -4.3585e-02, -1.7560e-01, -2.9484e-02, -2.2485e-01, -1.1793e-01,\n",
       "                      -1.7453e-01, -6.5566e-02, -4.0174e-02, -2.4076e-01,  5.6475e-02,\n",
       "                      -1.3125e-01, -1.4132e-01, -2.1450e-01, -1.8528e-01, -1.3886e-01,\n",
       "                       3.6328e-02, -2.2889e-01, -1.1524e-01, -1.8143e-01, -2.1581e-01,\n",
       "                      -2.4136e-01, -2.3815e-01, -1.0281e-01, -4.6717e-02, -2.2563e-01,\n",
       "                      -1.9262e-01, -1.9216e-01, -2.8277e-01,  1.1758e-01,  2.4469e-02,\n",
       "                      -2.6934e-01, -4.9184e-02, -2.1153e-01, -1.2362e-01, -6.8487e-02,\n",
       "                      -1.7777e-01,  4.5265e-02, -1.2812e-01, -1.0310e-01, -8.3373e-02,\n",
       "                      -1.1265e-02, -1.2691e-01, -1.9430e-01, -9.5793e-02, -1.5696e-01,\n",
       "                       3.3600e-02, -1.9847e-02, -1.2710e-01, -2.2208e-01, -2.2825e-01,\n",
       "                      -1.1878e-01, -7.0323e-02, -1.5757e-01, -4.5693e-02, -6.6348e-02,\n",
       "                      -3.6526e-01, -2.6571e-03, -3.5364e-02, -7.5248e-02, -2.8723e-02,\n",
       "                      -1.6010e-01, -1.4388e-01, -3.4544e-03, -2.0881e-01, -1.7127e-01,\n",
       "                      -1.1432e-01, -2.5085e-02, -3.6080e-02, -1.8289e-01, -1.8294e-01,\n",
       "                      -1.1951e-01, -7.5456e-02, -4.5862e-02,  1.3691e-01, -1.8959e-01,\n",
       "                      -2.2283e-01, -1.5181e-01, -5.0259e-02, -4.4622e-02, -1.5283e-01,\n",
       "                       9.8151e-02,  1.2302e-01,  1.3731e-02, -1.7376e-01, -2.1319e-01,\n",
       "                      -2.2030e-01, -1.2315e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-1.1532e-01, -1.3276e-02, -1.4828e-01, -7.6324e-02, -1.5351e-01,\n",
       "                      -1.0490e-01, -1.4920e-01, -2.1547e-01, -8.4393e-03, -1.2102e-01,\n",
       "                      -2.1499e-01, -2.5593e-01, -8.7318e-02, -1.5876e-02, -6.9267e-02,\n",
       "                       5.8582e-02, -2.9178e-01, -2.1340e-01, -2.0360e-01, -3.2576e-02,\n",
       "                      -9.5733e-02,  3.1463e-02, -1.8058e-01, -4.1784e-02, -5.3832e-03,\n",
       "                      -1.7140e-01, -1.6476e-01, -1.9332e-01, -1.6322e-01, -6.1780e-02,\n",
       "                      -1.4503e-01, -1.9697e-01, -1.5299e-02, -1.0018e-01, -2.3312e-01,\n",
       "                      -8.8208e-02, -1.8072e-01, -5.0862e-02,  5.0146e-02, -6.2495e-02,\n",
       "                      -3.6845e-02, -3.6865e-02, -8.5051e-02, -1.3673e-01, -1.7027e-01,\n",
       "                      -7.5801e-02, -1.5180e-01, -1.0460e-01,  7.7728e-02, -3.1161e-01,\n",
       "                      -4.8859e-02, -1.5373e-01, -1.3032e-01, -1.6163e-01, -1.8599e-01,\n",
       "                      -1.5763e-01, -5.9591e-02, -1.1911e-01, -1.2994e-01, -1.5224e-01,\n",
       "                      -2.0579e-01, -1.9405e-01, -2.2184e-01, -3.7091e-02, -3.3113e-02,\n",
       "                      -1.4338e-01, -1.3628e-01, -1.6528e-01, -6.9975e-02, -7.6928e-02,\n",
       "                      -1.9133e-02, -2.1919e-01, -1.5328e-01, -4.7889e-02, -1.8212e-01,\n",
       "                      -1.7723e-01, -1.6972e-01, -5.2085e-02, -1.7082e-01, -4.5492e-02,\n",
       "                      -1.2713e-01, -2.3951e-02, -2.1678e-01, -7.4017e-02, -1.4707e-01,\n",
       "                      -1.6384e-01,  7.8920e-02, -3.2911e-02, -1.6810e-01, -2.0819e-01,\n",
       "                      -2.5289e-01, -2.2631e-01, -1.2336e-01, -1.3546e-01, -1.0863e-01,\n",
       "                       1.9008e-02, -3.0128e-01, -1.4775e-02, -1.4793e-01, -1.7492e-01,\n",
       "                      -5.9499e-02, -2.1628e-01, -2.6046e-01, -4.9001e-03, -8.1470e-02,\n",
       "                      -8.2097e-02, -9.9988e-02, -4.6412e-03, -6.9365e-02, -9.8503e-02,\n",
       "                      -1.1652e-01, -1.0123e-01,  5.4887e-02,  1.0733e-01,  6.1615e-02,\n",
       "                      -2.6264e-01, -1.1257e-01, -1.4213e-01, -8.7634e-02, -5.2183e-02,\n",
       "                      -1.7898e-01,  1.2761e-02, -9.8784e-02, -5.6688e-02, -2.2520e-01,\n",
       "                      -1.7966e-01, -1.2702e-01, -8.4676e-02,  3.5782e-02, -8.9898e-02,\n",
       "                      -9.6731e-02, -6.0801e-03, -2.2207e-02, -5.7290e-02, -6.5542e-02,\n",
       "                       9.9394e-03,  1.7811e-02, -9.9784e-02, -2.0048e-01, -4.4807e-02,\n",
       "                       2.2016e-02, -1.0378e-01, -1.0819e-01,  2.0097e-01, -4.2478e-02,\n",
       "                      -1.7557e-01, -1.7433e-01,  3.2733e-03, -2.4064e-02,  9.5417e-03,\n",
       "                      -2.0405e-01, -6.3858e-02, -1.5333e-01,  7.5447e-02,  2.5232e-02,\n",
       "                      -1.6627e-01, -9.2620e-02, -1.2034e-01, -1.6771e-01, -1.0734e-01,\n",
       "                      -6.2292e-03, -3.1833e-02,  1.7389e-02, -6.0900e-02, -7.0421e-03,\n",
       "                      -1.1427e-01,  8.3536e-02, -7.3291e-03,  4.2660e-02, -1.0842e-03,\n",
       "                      -4.1004e-01, -1.1968e-01, -1.7170e-01, -2.2038e-01, -1.1673e-01,\n",
       "                       2.7977e-02,  1.9656e-02, -1.9323e-01,  1.0103e-03, -5.8551e-02,\n",
       "                      -4.5835e-03, -1.6486e-01, -1.7482e-01, -4.9194e-03, -5.0587e-02,\n",
       "                      -9.9516e-03,  8.0564e-02, -5.6261e-02, -1.2344e-01, -2.1927e-01,\n",
       "                      -2.4232e-01, -2.6016e-02,  4.1844e-02, -4.4976e-02, -8.7100e-02,\n",
       "                      -1.7781e-01,  1.0103e-02, -5.4898e-02,  4.8605e-02, -1.5167e-01,\n",
       "                       9.9233e-02,  3.8416e-02, -1.0150e-01, -6.6989e-02,  2.9355e-02,\n",
       "                       1.3315e-01, -3.0767e-01, -2.2019e-02,  2.7526e-02,  1.6056e-01,\n",
       "                       2.5985e-02, -1.7172e-01, -7.9453e-02, -3.5357e-01, -1.5130e-02,\n",
       "                      -1.7734e-02,  3.3988e-02, -3.0763e-01, -4.2727e-02, -1.1576e-01,\n",
       "                       5.0701e-02,  1.5176e-02,  8.9950e-02, -1.5057e-01, -3.1501e-01,\n",
       "                       1.9025e-01,  7.0693e-03,  4.7092e-02, -3.8844e-02, -8.9692e-02,\n",
       "                      -1.5976e-01,  1.7880e-01, -4.0231e-02, -3.3643e-02, -4.0009e-03,\n",
       "                      -7.9891e-03, -6.4251e-02, -2.2476e-01, -2.1757e-01, -8.1446e-02,\n",
       "                       3.5632e-02,  4.5641e-02,  1.2782e-01, -8.3838e-02, -1.3245e-02,\n",
       "                      -8.6321e-02, -9.9969e-02,  2.2513e-02,  1.2177e-01,  2.4275e-02,\n",
       "                      -7.2701e-03,  6.0947e-02, -1.0419e-01,  7.6640e-03, -6.2286e-02,\n",
       "                      -6.7142e-02,  5.9329e-02, -1.0198e-01, -1.7562e-02,  2.0554e-02,\n",
       "                      -2.4739e-02, -9.4891e-02,  8.7000e-03, -3.0452e-02, -4.8217e-02,\n",
       "                       7.5314e-02, -1.2730e-02, -8.1336e-02,  2.1581e-03,  2.3032e-02,\n",
       "                      -4.0219e-02,  5.4603e-02,  3.1596e-02,  1.4415e-02,  8.1525e-02,\n",
       "                      -6.3491e-02,  5.8545e-02,  1.0816e-01,  2.9963e-02, -1.9709e-02,\n",
       "                      -2.9504e-02, -4.6107e-02, -9.4374e-02, -1.0338e-01,  1.2512e-01,\n",
       "                      -6.6741e-02, -5.7285e-02,  4.1579e-02, -4.1328e-02,  2.8650e-02,\n",
       "                      -1.1223e-01, -6.4737e-03,  1.0123e-02, -5.0124e-03,  6.2697e-02,\n",
       "                       6.4312e-02,  2.5518e-02,  9.9332e-02,  3.5088e-03, -2.6209e-02,\n",
       "                      -5.4375e-03,  3.7435e-02,  9.5500e-02, -7.5649e-02, -6.9629e-02,\n",
       "                      -1.4198e-01,  4.2480e-02, -8.4519e-02,  8.6190e-04, -6.3800e-02,\n",
       "                      -1.7008e-01, -1.3866e-01,  1.9031e-02,  5.8444e-02, -1.7659e-02,\n",
       "                      -4.1975e-02, -2.5668e-02, -1.7605e-01,  7.3975e-02,  8.1466e-02,\n",
       "                       3.1953e-02, -3.5960e-02, -6.3242e-02, -2.9513e-04,  1.4591e-02,\n",
       "                       8.1161e-02,  6.9795e-02, -4.3880e-02, -8.9930e-02, -6.3327e-02,\n",
       "                      -5.2552e-02, -6.0274e-02, -3.0070e-02, -1.7688e-02,  6.9498e-03,\n",
       "                       6.5936e-02,  7.9176e-03,  1.4916e-01,  3.4990e-03,  9.9911e-02,\n",
       "                      -1.1148e-01, -6.2937e-02,  1.1407e-03,  7.5328e-02, -1.1244e-01,\n",
       "                       7.2706e-02, -1.3234e-01, -4.1514e-02, -5.8849e-02,  7.1388e-04,\n",
       "                      -9.7669e-02,  3.2933e-02,  1.0311e-01,  5.3130e-02, -9.6675e-02,\n",
       "                       5.0901e-02, -2.6179e-02, -5.6703e-02,  1.2096e-02, -5.5065e-02,\n",
       "                      -4.1946e-02, -5.6740e-03, -1.1269e-02,  1.2646e-01, -4.7325e-02,\n",
       "                       5.3712e-02, -2.7198e-03, -7.3511e-02, -1.7403e-02,  3.0609e-02,\n",
       "                      -6.7441e-02,  5.1753e-03, -4.5978e-02, -4.4738e-02, -1.1519e-01,\n",
       "                      -8.1106e-03,  4.9482e-02, -6.4740e-03,  1.5729e-02,  4.9125e-02,\n",
       "                       5.7792e-02, -4.8282e-02, -1.5471e-01, -1.9128e-02, -1.0211e-01,\n",
       "                      -4.3222e-02, -3.7378e-02, -8.1720e-02, -9.2141e-02, -2.5900e-02,\n",
       "                      -1.9172e-01, -6.0442e-02,  2.9225e-02, -6.0187e-02, -2.8174e-01,\n",
       "                      -2.4309e-02, -2.4088e-02, -1.7907e-01, -1.7918e-01, -1.0509e-01,\n",
       "                      -1.6051e-01, -2.1138e-01, -1.9730e-01,  3.2308e-03, -6.8806e-02,\n",
       "                       2.3423e-02, -1.4043e-01, -8.8826e-02, -1.9612e-02, -2.2639e-01,\n",
       "                      -1.1546e-01, -1.2428e-01, -9.4080e-02, -5.2503e-02, -1.2133e-01,\n",
       "                      -2.1598e-01, -7.1035e-02,  5.6930e-02, -2.0516e-01, -1.4914e-01,\n",
       "                      -1.6387e-01,  2.6833e-02, -1.4861e-01, -7.3640e-02, -5.9852e-02,\n",
       "                      -1.7459e-01, -1.5890e-01, -9.0156e-02, -4.0025e-01, -2.0510e-01,\n",
       "                      -2.8230e-01, -1.4204e-01, -2.1130e-01, -2.2509e-01,  1.2395e-01,\n",
       "                      -1.6837e-01, -1.9250e-01, -1.4855e-01, -1.1415e-01, -1.8980e-01,\n",
       "                      -1.6776e-01, -2.2797e-01, -5.5168e-02, -2.8495e-01, -1.7267e-01,\n",
       "                      -2.3566e-01, -2.5145e-01, -1.5204e-01, -9.9967e-02, -1.8232e-01,\n",
       "                      -2.6108e-01, -1.1429e-01,  6.0331e-03, -7.2964e-02,  1.4267e-01,\n",
       "                      -2.4257e-01, -5.3674e-02, -8.7768e-02, -4.0646e-01, -1.4212e-01,\n",
       "                      -1.4984e-01, -9.5180e-02, -6.6069e-02, -4.7603e-02, -1.9912e-01,\n",
       "                      -8.4294e-02, -8.3841e-02,  1.4188e-03, -6.7778e-02, -1.2948e-01,\n",
       "                      -1.1523e-01, -9.5047e-02, -1.5225e-01, -2.3248e-01, -2.2673e-01,\n",
       "                      -1.2049e-01, -6.3366e-02, -1.6426e-01,  6.9589e-02, -6.7627e-02,\n",
       "                      -3.4961e-01, -5.4023e-02, -1.5316e-01, -2.6380e-01, -1.3055e-01,\n",
       "                      -4.3397e-02, -1.8090e-01,  2.8460e-02, -2.5301e-01, -2.2271e-01,\n",
       "                      -2.7372e-02, -1.3204e-01,  8.1281e-02, -2.0427e-01, -1.5826e-01,\n",
       "                      -1.5039e-01, -4.5804e-02, -4.2089e-03,  5.0962e-02, -6.5311e-02,\n",
       "                      -1.8105e-01, -1.9800e-01, -1.2399e-01, -1.2624e-01, -2.0637e-01,\n",
       "                       4.5158e-02,  1.8421e-01, -7.1150e-02, -5.5643e-02, -2.0718e-01,\n",
       "                      -9.0911e-02, -1.0335e-01])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-7.0524e-02, -2.7438e-01, -1.3006e-01,  ...,  3.5164e-01,\n",
       "                        1.7674e-01,  1.6141e-01],\n",
       "                      [-3.3884e-01, -7.8363e-02, -1.2256e-01,  ..., -4.5289e-01,\n",
       "                       -2.5439e-02, -3.4213e-01],\n",
       "                      [-2.2465e-01,  1.2510e-01,  4.0610e-01,  ...,  1.9450e-01,\n",
       "                        4.2069e-01,  3.7588e-01],\n",
       "                      ...,\n",
       "                      [-5.3277e-02, -4.0628e-01, -3.9968e-02,  ..., -1.1212e-01,\n",
       "                       -3.9837e-01, -4.2716e-01],\n",
       "                      [ 1.9607e-01, -2.7895e-01,  5.4630e-01,  ..., -4.1252e-01,\n",
       "                       -3.1495e-01,  2.3926e-01],\n",
       "                      [ 3.1819e-01,  2.3036e-02, -8.1584e-01,  ..., -3.3167e-04,\n",
       "                       -5.7742e-03, -1.5687e-01]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.1476, -0.0128, -0.0667,  ..., -0.3626,  0.0680, -0.0474],\n",
       "                      [ 0.0019, -0.1717, -0.2213,  ..., -0.2051, -0.0967, -0.0194],\n",
       "                      [-0.2526,  0.0917, -0.0956,  ...,  0.2393,  0.3057,  0.1059],\n",
       "                      ...,\n",
       "                      [ 0.2779, -0.1435, -0.0525,  ..., -0.0988, -0.4541, -0.2183],\n",
       "                      [-0.0981, -0.1650, -0.1119,  ...,  0.0796, -0.1000, -0.0418],\n",
       "                      [ 0.1114, -0.0190, -0.0962,  ...,  0.4321, -0.0672, -0.3134]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([-1.2386e-01,  9.7094e-02,  2.3618e-02,  3.3258e-01, -4.9448e-02,\n",
       "                       2.9263e-02,  1.9454e-01,  2.5250e-01,  1.8859e-01,  6.0097e-02,\n",
       "                      -2.7498e-02,  1.3013e-01,  1.9707e-01,  6.8246e-02, -8.5730e-02,\n",
       "                       7.4236e-02,  9.9157e-02,  2.6375e-01,  9.2984e-02,  1.3092e-01,\n",
       "                       2.8114e-03,  2.6537e-01,  2.6653e-01,  7.4082e-02,  1.2213e-01,\n",
       "                       1.8032e-01,  1.7229e-01,  1.3104e-01,  2.0170e-01,  7.4264e-02,\n",
       "                       9.6943e-02,  1.6293e-01,  2.6568e-01,  1.9884e-01,  8.2400e-02,\n",
       "                      -4.9906e-02,  1.6067e-03,  5.5511e-02,  4.9539e-02,  3.6255e-01,\n",
       "                       2.8454e-01,  1.8274e-02,  3.9209e-01,  8.8871e-02,  2.1977e-01,\n",
       "                       2.4248e-01,  1.4793e-01,  1.6690e-01,  3.4757e-01,  9.4514e-02,\n",
       "                      -8.9635e-02,  2.3811e-01,  1.4770e-01,  1.5787e-01,  2.1228e-01,\n",
       "                       1.5544e-01,  1.4850e-01,  2.1955e-01,  3.0697e-02, -2.4030e-02,\n",
       "                       1.1013e-01,  1.4159e-01,  2.9613e-01,  5.8083e-02,  2.4324e-01,\n",
       "                       7.5634e-02, -4.4381e-02,  1.0503e-01,  1.4007e-01,  2.0597e-01,\n",
       "                       1.7232e-01, -2.6338e-03,  1.7111e-01,  2.3677e-01,  5.9977e-02,\n",
       "                       2.4004e-01,  7.5317e-02, -3.2899e-05,  1.0821e-02,  1.9629e-01,\n",
       "                       1.1254e-01,  4.7767e-02,  3.3543e-01,  2.6961e-01,  6.2058e-02,\n",
       "                       1.2566e-01,  4.0730e-02,  1.8901e-01,  1.7030e-01,  1.0293e-01,\n",
       "                       1.0860e-01,  3.0017e-01,  9.8318e-02,  1.9282e-01,  1.4295e-01,\n",
       "                       1.9861e-01,  1.8627e-01,  7.8427e-02,  1.6208e-02,  1.2093e-01,\n",
       "                       2.7986e-01,  6.8005e-02,  7.3584e-02,  1.4210e-02,  3.9040e-01,\n",
       "                       1.6983e-01,  1.6864e-01,  6.5305e-03,  2.3363e-01, -1.8913e-02,\n",
       "                       9.2108e-02,  4.7141e-02,  8.1791e-02,  9.7943e-02, -1.1801e-01,\n",
       "                      -1.2102e-02,  3.0556e-02,  2.2381e-01,  1.9823e-02,  1.4737e-01,\n",
       "                       2.0384e-01,  1.2283e-01,  1.1894e-01,  8.2069e-02,  4.7042e-02,\n",
       "                       1.4578e-01,  1.8203e-01,  1.4142e-01,  5.5465e-02,  5.3195e-02,\n",
       "                       1.5100e-01,  1.7775e-01,  1.1334e-01,  3.6884e-02, -2.7935e-02,\n",
       "                       1.1040e-01,  1.6099e-01,  1.1930e-01,  3.4743e-02,  1.5274e-02,\n",
       "                       3.7307e-03,  6.4494e-02, -3.1367e-02,  1.0296e-01,  5.3418e-02,\n",
       "                       6.4862e-02,  9.0120e-02,  1.4601e-02,  2.9154e-02,  6.8020e-04,\n",
       "                       7.8584e-03, -7.2660e-02,  1.2937e-01,  9.1958e-02,  2.6295e-02,\n",
       "                       4.0420e-02, -5.1656e-02,  2.3770e-02,  1.2262e-01,  6.7599e-02,\n",
       "                      -5.5860e-02,  1.4677e-01, -1.1577e-01,  5.1661e-02,  5.9823e-02,\n",
       "                       9.4783e-02,  1.0503e-01, -7.5403e-02, -6.8160e-02,  1.7137e-02,\n",
       "                      -4.9041e-02,  3.8975e-02,  7.0354e-02, -3.9879e-02,  1.8827e-01,\n",
       "                       6.2064e-02,  2.7729e-02,  1.2951e-02,  1.6212e-02,  1.1458e-01,\n",
       "                       1.1183e-03,  1.0781e-01, -6.7362e-02, -6.4575e-02,  3.8396e-02,\n",
       "                       9.7830e-02,  1.1033e-01,  1.0166e-01,  1.6108e-01,  1.6636e-01,\n",
       "                       5.4378e-02,  1.3939e-02, -5.6220e-02, -2.0178e-03,  1.6178e-02,\n",
       "                       1.3976e-01,  1.3616e-01,  2.1420e-02,  6.8108e-02, -4.0358e-02,\n",
       "                      -2.0381e-02,  1.2247e-01,  5.7574e-02,  6.7507e-02,  1.6184e-01,\n",
       "                      -1.4076e-02,  6.3362e-03,  1.1523e-01, -4.4270e-02,  5.0879e-02,\n",
       "                       9.4106e-02,  1.6710e-02, -4.9981e-02,  9.1199e-02, -2.4704e-02,\n",
       "                       1.1828e-01,  7.8052e-02,  1.1478e-01,  5.9893e-02,  1.4802e-01,\n",
       "                       1.2310e-01, -3.0573e-02,  5.1353e-02,  1.2600e-01,  1.1620e-01,\n",
       "                       9.0245e-02,  4.5526e-02,  2.5632e-01,  4.2069e-02,  1.9228e-01,\n",
       "                       2.0933e-02,  5.7136e-02, -4.1705e-02,  1.6269e-02,  1.7040e-01,\n",
       "                      -5.5912e-03,  7.1720e-02, -1.1855e-01,  9.8764e-02,  1.9905e-01,\n",
       "                      -4.4358e-02,  2.8578e-01,  1.2137e-01,  8.9767e-02, -8.0455e-03,\n",
       "                      -1.0559e-01,  1.6990e-02,  1.4138e-01,  1.1832e-01,  1.0982e-01,\n",
       "                      -3.3917e-02,  1.1531e-02,  5.1950e-02,  6.9713e-02,  5.9020e-02,\n",
       "                       4.0944e-02, -1.3585e-02, -8.7113e-02,  1.9824e-02, -4.0822e-02,\n",
       "                      -1.8216e-02,  7.3493e-02,  3.3671e-02,  1.0081e-02,  2.9193e-02,\n",
       "                      -4.0274e-02, -6.9190e-02,  5.1412e-02, -5.9987e-02, -6.4251e-02,\n",
       "                      -2.6209e-03, -8.1984e-02,  3.9410e-02,  8.2153e-02,  5.2753e-02,\n",
       "                       8.9133e-02, -8.2001e-02,  1.1880e-01, -2.1300e-02, -1.0262e-01,\n",
       "                       8.8287e-02, -5.2254e-02,  9.2817e-02, -7.2867e-02, -6.4807e-02,\n",
       "                       3.7234e-02,  4.1775e-02, -5.5059e-02,  5.0825e-02, -7.2995e-03,\n",
       "                       5.4593e-02,  9.3539e-02,  9.4626e-02,  8.4773e-02,  9.2887e-02,\n",
       "                       5.5976e-02,  1.6239e-01,  2.6969e-02, -1.4080e-01, -1.2979e-01,\n",
       "                       2.0960e-02,  1.7109e-03,  6.4114e-02, -3.8746e-03, -9.8081e-02,\n",
       "                       4.7677e-02,  1.2995e-01, -4.1764e-02,  1.4412e-01,  6.4750e-02,\n",
       "                       7.0893e-02,  1.1007e-02,  1.8400e-01,  1.3443e-01, -1.4435e-01,\n",
       "                       4.9000e-02,  3.4749e-02,  1.8081e-01,  2.8328e-02,  1.3358e-02,\n",
       "                      -1.2908e-01,  1.5773e-01,  3.0999e-02, -6.2683e-02,  4.5048e-02,\n",
       "                      -4.3657e-02,  5.8943e-02,  1.1752e-02, -2.5519e-02,  5.2040e-02,\n",
       "                       1.8915e-01, -9.3399e-02, -2.4328e-01, -2.5156e-01, -2.5708e-02,\n",
       "                       4.7714e-02, -7.2789e-02,  1.6864e-02, -1.0498e-01,  1.5454e-02,\n",
       "                       1.2345e-01, -9.1297e-02, -1.2075e-01, -2.2803e-02,  3.8567e-02,\n",
       "                      -5.8929e-02,  6.3917e-02,  4.0066e-02, -4.8658e-02,  3.9290e-03,\n",
       "                       8.9912e-03, -4.9949e-02, -1.3910e-02, -4.2075e-02, -3.0203e-02,\n",
       "                      -1.0458e-02, -7.2533e-02,  5.9948e-02,  7.9523e-02, -3.8609e-02,\n",
       "                       8.3787e-02,  5.3586e-02,  5.9054e-02, -1.0861e-02,  3.3793e-02,\n",
       "                       2.2060e-02, -8.3107e-02,  4.3332e-02, -8.2315e-02, -2.7824e-02,\n",
       "                       2.4624e-02,  6.5385e-02, -4.1568e-02,  1.7368e-02, -8.4745e-02,\n",
       "                       4.0191e-02, -5.2171e-02, -4.5664e-02, -6.1361e-02,  1.1836e-01,\n",
       "                       4.8963e-02,  9.0113e-02,  1.2578e-01, -1.3028e-01,  3.3447e-02,\n",
       "                       2.4289e-02,  2.1289e-02,  1.5179e-01,  8.8129e-02,  1.4914e-01,\n",
       "                       8.9320e-02,  3.2821e-01,  1.6426e-01,  1.9688e-01,  5.7397e-02,\n",
       "                       7.3992e-02,  6.6922e-02,  1.3228e-01,  2.0841e-01,  1.7564e-02,\n",
       "                       1.4648e-01, -1.4065e-01,  3.2681e-02,  2.5452e-01, -6.3347e-02,\n",
       "                       1.4445e-01,  1.1634e-01,  6.5831e-03,  2.1997e-02,  1.1068e-01,\n",
       "                       3.1629e-01,  2.7034e-03,  3.8289e-01,  1.9295e-01,  1.5383e-01,\n",
       "                       2.2927e-01,  2.2513e-01,  1.9634e-02,  5.3375e-02,  1.3842e-01,\n",
       "                       1.5849e-01, -1.5205e-02,  5.0779e-02,  1.8705e-01,  2.0512e-01,\n",
       "                       2.3062e-01,  3.5082e-01,  3.1536e-01,  6.9525e-02,  1.7368e-01,\n",
       "                       1.6328e-01,  1.3412e-01,  4.3336e-02,  1.9973e-01,  8.0615e-02,\n",
       "                      -1.7743e-02,  8.4463e-02,  3.1037e-01,  7.7599e-02,  2.3037e-02,\n",
       "                       1.7426e-01,  4.1291e-02,  1.9467e-01,  6.3520e-02,  1.9925e-01,\n",
       "                       3.4067e-01,  7.9687e-03,  1.1217e-01,  2.3013e-01,  1.5857e-01,\n",
       "                       2.0362e-01,  1.7692e-01,  1.5494e-01,  2.1034e-01,  1.2926e-01,\n",
       "                       2.6818e-01,  1.3436e-01,  2.5735e-01,  1.7662e-01,  1.2983e-01,\n",
       "                       1.3290e-02,  8.7211e-02,  1.4350e-01,  4.6311e-03,  5.8381e-02,\n",
       "                       5.1624e-02, -3.8370e-02,  2.4511e-01,  1.6641e-01,  1.4000e-01,\n",
       "                       1.6692e-02,  1.7625e-01, -1.6295e-02,  7.3753e-02,  7.3810e-02,\n",
       "                      -1.5234e-02,  2.0527e-01,  2.9739e-01,  1.2136e-01,  2.4364e-01,\n",
       "                       1.4934e-01, -7.9655e-02,  1.0951e-01, -3.0639e-02,  2.6094e-01,\n",
       "                       6.3168e-02,  1.4908e-02,  1.1710e-02,  4.4943e-03,  3.1609e-01,\n",
       "                       6.7109e-02,  2.1052e-01,  4.0520e-02,  5.6967e-02, -6.8332e-02,\n",
       "                       1.1619e-01,  1.2841e-01,  1.8585e-01, -6.7676e-02, -4.7249e-02,\n",
       "                       1.7758e-02,  9.9140e-02,  1.0413e-01,  3.0813e-01,  2.5592e-01,\n",
       "                       2.1833e-01,  6.8368e-02,  2.5248e-01,  1.5669e-01,  1.6978e-01,\n",
       "                       2.4701e-01,  3.9247e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 8.1976e-02,  7.9026e-03, -7.5190e-03,  4.0402e-01, -1.9639e-02,\n",
       "                       3.7043e-02,  2.3399e-01,  2.9320e-01,  1.8554e-01,  9.6089e-02,\n",
       "                      -5.7464e-02,  7.6856e-02,  7.0785e-02, -1.2735e-02,  1.8037e-01,\n",
       "                       1.1360e-01,  1.0266e-02,  2.5631e-01,  1.3070e-02,  8.4967e-02,\n",
       "                      -5.1671e-02,  1.7173e-01,  4.0547e-01,  3.0916e-01,  6.9205e-02,\n",
       "                       1.0695e-01,  1.7073e-01,  2.8636e-01,  3.0760e-01,  1.9224e-01,\n",
       "                       1.9567e-01,  2.4935e-01,  4.0572e-01,  1.3681e-01,  7.7971e-02,\n",
       "                       9.2817e-02,  1.1421e-01, -2.0164e-02,  4.4043e-03,  3.1637e-01,\n",
       "                       2.0296e-01,  1.3327e-01,  3.0379e-01,  8.9652e-02,  1.8264e-01,\n",
       "                       2.3400e-01,  1.2595e-01,  1.9448e-01,  3.3981e-01,  6.8763e-02,\n",
       "                       8.2467e-02,  8.2619e-02,  1.0414e-01,  3.2131e-01,  1.1107e-01,\n",
       "                       1.3519e-01,  1.6560e-01,  1.3394e-01,  1.2930e-01,  4.3355e-02,\n",
       "                       1.8314e-01,  1.1571e-01,  2.8235e-01,  1.3322e-01,  3.5882e-01,\n",
       "                       1.0373e-01,  1.7013e-01,  1.4166e-01,  1.9535e-01,  1.0677e-01,\n",
       "                       9.9790e-02,  2.2423e-01,  1.8289e-01,  2.1504e-01,  1.6317e-01,\n",
       "                       1.3387e-01,  6.6562e-02,  2.1496e-01,  5.0047e-02,  2.5006e-01,\n",
       "                       1.0307e-01, -7.5549e-02,  2.8910e-01,  1.0809e-01,  5.6337e-02,\n",
       "                       8.2588e-02,  2.7433e-02,  1.8469e-01,  2.6401e-01,  7.8946e-03,\n",
       "                       9.9078e-02,  3.3150e-01, -4.7339e-02,  2.0312e-01,  1.9096e-01,\n",
       "                       2.6179e-01,  1.4487e-01, -1.4116e-02,  7.6854e-02,  3.1867e-01,\n",
       "                       1.6222e-01,  7.9919e-02,  1.4394e-01, -1.6169e-02,  3.3636e-01,\n",
       "                       1.6615e-01,  1.3168e-01,  9.8222e-02,  2.8235e-01, -1.6927e-02,\n",
       "                       4.0433e-02,  1.0425e-01,  7.7074e-02,  1.5356e-01,  3.3687e-03,\n",
       "                       1.0280e-01,  1.5116e-01,  1.7339e-01,  9.0010e-02,  2.0427e-01,\n",
       "                       2.0528e-01,  1.9182e-01,  1.6981e-01,  1.6255e-01, -6.7629e-02,\n",
       "                       9.2965e-02,  2.0128e-01,  2.5962e-01,  5.9176e-02,  6.3850e-02,\n",
       "                       1.6484e-01,  2.0984e-02,  1.1349e-01,  3.4473e-03,  1.2843e-01,\n",
       "                       7.6574e-02, -3.8044e-02,  1.3209e-02,  5.3142e-02, -2.6439e-02,\n",
       "                       1.2818e-01,  4.7169e-02,  8.6969e-02,  1.5381e-01,  1.6515e-01,\n",
       "                       7.6858e-02,  1.0105e-01,  5.5296e-02, -3.0843e-02, -1.1923e-01,\n",
       "                      -3.2407e-02, -7.8787e-02,  1.2454e-01,  5.8105e-03,  2.5810e-02,\n",
       "                       2.9275e-03,  5.8473e-02, -5.1660e-02,  2.0711e-02,  7.6157e-02,\n",
       "                       6.4063e-02,  1.7682e-01,  1.6110e-01, -1.6230e-02,  1.3400e-02,\n",
       "                      -5.2735e-02,  1.4325e-01, -5.3244e-02,  3.9004e-02, -1.0710e-01,\n",
       "                      -8.8018e-02, -1.1481e-01,  9.2124e-02, -3.7388e-02,  1.0066e-01,\n",
       "                       1.0530e-01,  4.7018e-02, -4.8186e-02,  5.0982e-02, -5.3033e-02,\n",
       "                       6.3640e-02,  1.5022e-02,  1.5836e-01, -1.4427e-01,  1.3779e-01,\n",
       "                       5.2744e-02,  9.5569e-02,  1.0774e-01, -8.8301e-02,  2.0833e-02,\n",
       "                       5.8548e-02, -5.0687e-02,  9.9702e-03, -5.9564e-02,  4.4163e-02,\n",
       "                       1.3232e-01,  4.4478e-02,  1.1504e-01,  1.3862e-02,  3.7611e-02,\n",
       "                       2.5202e-02,  6.3038e-02,  5.9692e-02,  6.6696e-02,  9.6271e-02,\n",
       "                      -7.8257e-02, -6.1414e-03,  1.3045e-01,  6.9171e-02,  7.3130e-02,\n",
       "                       1.1639e-01, -1.2493e-01, -7.8492e-02,  3.6026e-02,  1.1509e-01,\n",
       "                       3.6212e-02,  1.2222e-01,  1.4053e-01,  3.5190e-02,  1.6704e-01,\n",
       "                       1.1113e-01, -2.3251e-02,  9.5937e-02,  3.1416e-02,  3.8672e-02,\n",
       "                       6.1059e-02,  1.5048e-01,  1.0947e-01,  2.9359e-02,  2.1657e-01,\n",
       "                      -8.4198e-02,  6.1310e-02, -1.8843e-02, -8.2691e-02,  5.4226e-02,\n",
       "                      -3.0731e-03, -2.4666e-02, -1.4852e-02,  2.0482e-01,  1.4798e-01,\n",
       "                       3.1428e-02,  2.0688e-01,  9.2185e-02,  4.9425e-02,  3.5786e-02,\n",
       "                       2.0724e-02,  1.2074e-01,  1.1382e-01, -3.2589e-02,  8.6798e-02,\n",
       "                       4.9619e-02,  1.4182e-02,  3.7654e-02,  7.4930e-02,  4.7179e-02,\n",
       "                      -1.1572e-01, -9.6830e-02, -1.6310e-02, -1.3024e-03,  6.6738e-02,\n",
       "                      -7.4582e-02, -2.2608e-02, -3.6661e-02, -4.3512e-02,  1.4036e-03,\n",
       "                       2.0792e-02, -2.6353e-03,  1.4778e-01, -1.5028e-01,  5.4990e-02,\n",
       "                      -8.2689e-02,  5.7931e-02,  3.9455e-02, -3.5753e-02,  1.0865e-02,\n",
       "                       2.3143e-02, -7.2881e-02,  3.7778e-02, -4.4248e-03, -1.1901e-01,\n",
       "                      -6.7089e-02, -5.7644e-02,  1.2470e-01, -7.1781e-02,  1.2818e-02,\n",
       "                      -4.8457e-03,  1.1219e-01, -5.6429e-02,  1.6116e-01,  3.0700e-03,\n",
       "                       1.5705e-02,  8.5872e-03,  4.0229e-02,  9.0859e-03,  1.0559e-01,\n",
       "                      -1.4124e-01,  8.4084e-02, -2.6235e-02, -1.6463e-01, -8.3113e-02,\n",
       "                       8.6918e-02,  1.1409e-01,  5.6680e-02,  9.6341e-03,  1.4592e-01,\n",
       "                       2.2058e-03,  1.6518e-01, -1.2464e-01, -1.2133e-03,  1.0736e-01,\n",
       "                       1.0603e-02,  5.3352e-02,  3.1077e-02,  7.1993e-02, -5.6667e-02,\n",
       "                      -2.9087e-02,  2.8197e-02, -4.9957e-02,  8.0339e-02,  3.5028e-03,\n",
       "                       1.6960e-02,  2.1989e-02,  1.2611e-01, -1.0494e-02,  1.9315e-03,\n",
       "                       1.1823e-01, -1.0195e-01, -4.5236e-02, -2.6461e-03,  5.3489e-02,\n",
       "                       1.3643e-01, -5.6529e-02, -1.7739e-01, -7.5869e-02,  1.5885e-02,\n",
       "                      -3.2401e-02, -1.0291e-01, -2.5566e-02, -1.5611e-02, -1.9448e-02,\n",
       "                       3.8895e-02, -7.8654e-02, -9.9862e-02,  4.5492e-02,  2.9465e-02,\n",
       "                       3.0970e-02, -5.9557e-02,  4.1051e-02,  2.4120e-02, -4.9107e-02,\n",
       "                       4.2675e-02, -2.2772e-02,  1.5677e-01, -1.2273e-01, -2.7810e-02,\n",
       "                       1.1492e-01, -8.1576e-02, -1.1062e-02,  4.5363e-03,  1.6959e-02,\n",
       "                       3.1017e-02,  1.6502e-02,  1.1358e-01, -1.1096e-01, -1.9651e-03,\n",
       "                      -4.9031e-02, -1.1912e-01,  5.8709e-02, -5.9105e-03, -1.5628e-03,\n",
       "                      -1.0212e-02,  8.5169e-02,  3.9988e-04,  8.5804e-02,  3.3450e-02,\n",
       "                       8.6374e-02,  3.1554e-02, -5.5934e-02, -1.2044e-01, -2.5887e-02,\n",
       "                       1.0925e-01,  9.9571e-02, -1.5856e-02, -7.5164e-03, -4.3748e-02,\n",
       "                       3.9521e-02,  1.8831e-01,  8.3369e-02,  3.3409e-02,  1.5825e-01,\n",
       "                       1.1622e-01,  3.4706e-01,  6.5014e-02,  2.1093e-01,  1.8145e-02,\n",
       "                       5.5886e-02, -7.5031e-03,  8.4628e-02, -1.6692e-02, -2.1616e-02,\n",
       "                       2.0064e-01, -1.3942e-02, -2.1221e-02,  2.5442e-01, -5.4550e-02,\n",
       "                       1.6505e-01,  1.4707e-01, -6.4731e-02, -4.7734e-02, -5.0867e-02,\n",
       "                       2.8608e-01, -1.6349e-03,  1.6038e-01,  1.5518e-01, -3.4834e-03,\n",
       "                       1.7327e-01,  1.7021e-01, -6.2978e-02,  2.0248e-02,  1.5534e-01,\n",
       "                       1.8038e-01,  2.2683e-02, -1.7527e-01,  2.9335e-01,  1.0455e-01,\n",
       "                       2.0370e-01,  4.1638e-01,  2.3103e-01,  1.4453e-01,  2.7703e-01,\n",
       "                       1.3346e-01,  1.5957e-01,  7.9799e-02,  2.1874e-01,  5.8159e-02,\n",
       "                      -1.6332e-01, -1.1496e-01,  4.2890e-01,  1.7506e-02,  1.7518e-02,\n",
       "                       2.5955e-01,  3.9520e-02,  2.2020e-01,  6.1341e-02,  1.3697e-01,\n",
       "                       3.1890e-01,  2.0602e-02,  2.8835e-01,  3.8626e-01,  1.2978e-01,\n",
       "                       1.9104e-01,  1.3973e-01,  1.2034e-01,  2.1518e-01,  7.5548e-02,\n",
       "                       2.9583e-01,  9.9228e-03,  6.4860e-02,  2.6838e-01,  3.1674e-01,\n",
       "                      -1.8052e-03,  1.2489e-01,  1.2863e-01,  1.3644e-01, -3.9206e-02,\n",
       "                       5.8505e-02, -4.0780e-02,  2.0858e-01,  1.6062e-01,  1.3342e-01,\n",
       "                      -2.3443e-02,  2.0819e-01,  5.3009e-02,  1.5991e-01,  4.0755e-02,\n",
       "                      -8.2802e-03,  8.2494e-02,  2.0024e-01,  2.4134e-01,  2.0932e-01,\n",
       "                       1.2522e-01,  9.0529e-02,  2.2448e-01,  2.6790e-02,  1.3503e-01,\n",
       "                       9.3898e-02,  9.8228e-02, -6.4288e-02,  1.8111e-01,  2.3706e-01,\n",
       "                       1.9114e-01,  1.5608e-01,  1.0477e-01,  8.1189e-03,  3.4174e-02,\n",
       "                       5.7519e-02,  1.4514e-01,  1.3806e-01,  2.7471e-02, -1.0217e-01,\n",
       "                      -9.6254e-02,  2.5595e-01,  1.0258e-01,  2.3887e-01,  3.2414e-01,\n",
       "                       1.8256e-02,  6.9040e-02,  1.8877e-01,  1.3984e-01,  1.5750e-01,\n",
       "                       1.8533e-01,  3.4957e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0190, -0.0731, -0.3543,  ..., -0.2359,  0.1259, -0.3828],\n",
       "                      [-0.0782,  0.0179, -0.1463,  ...,  0.0441,  0.2981,  0.0501],\n",
       "                      [ 0.1433, -0.1383,  0.3829,  ..., -0.1096,  0.0559, -0.0802],\n",
       "                      ...,\n",
       "                      [ 0.3575,  0.4566, -0.1722,  ..., -0.3086,  0.4138, -0.1107],\n",
       "                      [ 0.3322, -0.0969, -0.3012,  ..., -0.2504,  0.0817, -0.0930],\n",
       "                      [-0.0320, -0.2330,  0.1448,  ..., -0.1871,  0.1354,  0.2800]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.3208, -0.1856,  0.2889,  ..., -0.0820,  0.2516,  0.0310],\n",
       "                      [ 0.1139, -0.2664, -0.1702,  ...,  0.1186, -0.3358, -0.0610],\n",
       "                      [-0.0214, -0.0135, -0.1824,  ...,  0.1505,  0.2949, -0.0606],\n",
       "                      ...,\n",
       "                      [ 0.2721, -0.1069,  0.0122,  ..., -0.0554, -0.1168,  0.1971],\n",
       "                      [ 0.3237, -0.2170, -0.2280,  ..., -0.0296, -0.3106, -0.0565],\n",
       "                      [ 0.0061,  0.0888,  0.0080,  ...,  0.1797,  0.0413,  0.1516]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([-0.2556, -0.1509, -0.1076, -0.3808, -0.1351, -0.2289, -0.0759, -0.0327,\n",
       "                      -0.3401, -0.2123, -0.2202, -0.1275, -0.2755, -0.1189, -0.2285, -0.0173,\n",
       "                      -0.2178, -0.1931, -0.1391, -0.1734, -0.1007, -0.2066, -0.0069, -0.2219,\n",
       "                      -0.2498, -0.1918, -0.2022, -0.0654, -0.2643,  0.0108, -0.1831, -0.1912,\n",
       "                      -0.0538, -0.1900, -0.3226, -0.1117, -0.1986, -0.1646, -0.1468, -0.0579,\n",
       "                      -0.0733, -0.1242, -0.0127, -0.1407, -0.1033, -0.2658,  0.0232, -0.2387,\n",
       "                      -0.1270, -0.2188, -0.2437, -0.3409, -0.1176, -0.1413, -0.2863, -0.0922,\n",
       "                      -0.1070, -0.1673, -0.1978, -0.2058, -0.1327, -0.3316, -0.1168, -0.0036,\n",
       "                      -0.1297, -0.1759, -0.0827, -0.1143, -0.3790, -0.3005, -0.2564, -0.2479,\n",
       "                      -0.0934, -0.2185, -0.1871, -0.1141, -0.2005, -0.0842, -0.0651, -0.1065,\n",
       "                      -0.1242, -0.1030, -0.1598, -0.3149, -0.1203, -0.2459, -0.1410, -0.2215,\n",
       "                      -0.2248, -0.0991, -0.0614, -0.2287, -0.1332, -0.2479, -0.0417, -0.0910,\n",
       "                      -0.1942, -0.3337, -0.1074, -0.0308, -0.1744, -0.1830, -0.0441, -0.3003,\n",
       "                      -0.1032, -0.2259, -0.2855, -0.2461, -0.1010, -0.0956, -0.1468, -0.1801,\n",
       "                      -0.0666, -0.2301, -0.1380, -0.0379, -0.1363, -0.1664, -0.1692, -0.2941,\n",
       "                      -0.1900, -0.0369, -0.2221, -0.0909, -0.1093, -0.0594, -0.2448, -0.0632,\n",
       "                      -0.0567, -0.0408, -0.0574, -0.1189,  0.0042, -0.2583, -0.2611,  0.0433,\n",
       "                      -0.1503, -0.0681,  0.2050, -0.0667, -0.1245, -0.0584, -0.2169, -0.0997,\n",
       "                      -0.2147, -0.0809, -0.1304, -0.1308, -0.2657, -0.1000, -0.1718, -0.2098,\n",
       "                      -0.1675, -0.2823, -0.1138,  0.0237, -0.1268, -0.0786, -0.0620, -0.0464,\n",
       "                      -0.1646, -0.1710, -0.1585, -0.0101, -0.1088, -0.2273, -0.1109,  0.0507,\n",
       "                      -0.0377, -0.1735, -0.2724,  0.0013,  0.1825, -0.2553, -0.0035, -0.2399,\n",
       "                      -0.1410, -0.1864, -0.0583, -0.1597, -0.0906, -0.2040, -0.1376, -0.1034,\n",
       "                      -0.0841, -0.2223, -0.2118, -0.0385, -0.1264, -0.0947, -0.1175, -0.0464,\n",
       "                      -0.0761, -0.1545, -0.1196, -0.0757, -0.0201, -0.1509, -0.0594, -0.0585,\n",
       "                      -0.1194, -0.1562, -0.0939, -0.0043, -0.3226, -0.0143, -0.2210,  0.0278,\n",
       "                      -0.2206, -0.0214, -0.2523, -0.1200, -0.1058, -0.0117,  0.0264, -0.1284,\n",
       "                      -0.1531, -0.2270, -0.1797, -0.2165, -0.2050, -0.1200, -0.1469, -0.2003,\n",
       "                      -0.1342, -0.2493, -0.2520,  0.0340,  0.0108, -0.0746, -0.0171, -0.1463,\n",
       "                      -0.2090, -0.2187, -0.0548, -0.1009, -0.0965, -0.1454, -0.0388, -0.0766,\n",
       "                      -0.1113, -0.1587, -0.0977, -0.1324, -0.0837, -0.2689, -0.1267, -0.1263,\n",
       "                       0.0067, -0.0591, -0.0765, -0.0461, -0.1721, -0.0897, -0.0891,  0.1645,\n",
       "                      -0.0261,  0.0201,  0.0427, -0.0920, -0.1413,  0.0666, -0.0807,  0.0959,\n",
       "                       0.0636,  0.0420,  0.0169,  0.0202,  0.0286,  0.0531,  0.0567, -0.0179,\n",
       "                      -0.0745,  0.0203, -0.0332, -0.0895, -0.0091,  0.0999,  0.1417,  0.1324,\n",
       "                      -0.0210, -0.0887, -0.0214,  0.0278, -0.0875, -0.0165,  0.0890, -0.0082,\n",
       "                      -0.0561, -0.0733,  0.1105, -0.1237,  0.0382,  0.0622,  0.0044, -0.0622,\n",
       "                       0.1134,  0.0761,  0.0977,  0.0074,  0.0116, -0.0503,  0.0188,  0.0148,\n",
       "                      -0.0915, -0.0820, -0.0747, -0.0370,  0.0651,  0.0239, -0.0095,  0.0686,\n",
       "                       0.0349,  0.0252, -0.0149,  0.1304, -0.1300,  0.0472, -0.0812, -0.0363,\n",
       "                      -0.0006, -0.0254,  0.0664, -0.0311, -0.0332,  0.0480,  0.0195,  0.0165,\n",
       "                      -0.0359,  0.0140,  0.1053,  0.0037, -0.0262,  0.1035, -0.1262,  0.1166,\n",
       "                      -0.0015,  0.1310, -0.0162, -0.0264,  0.0028, -0.0223, -0.0658,  0.0171,\n",
       "                       0.0777, -0.1193, -0.0321, -0.0027,  0.0840, -0.0662, -0.0589,  0.0342,\n",
       "                      -0.0882,  0.0699,  0.0143, -0.0182,  0.0425,  0.0315,  0.0326,  0.0699,\n",
       "                       0.0445, -0.0121, -0.1107,  0.0583, -0.1259, -0.0225,  0.0124, -0.0278,\n",
       "                       0.0097, -0.0352,  0.0302,  0.0459,  0.0848, -0.0603,  0.1812, -0.0759,\n",
       "                      -0.0187, -0.0157,  0.0652,  0.0436,  0.1162, -0.0533,  0.0094,  0.0111,\n",
       "                      -0.0392, -0.1439, -0.1411, -0.2338,  0.0290, -0.2501, -0.2834, -0.1459,\n",
       "                      -0.3040, -0.2200,  0.0954, -0.1237, -0.2784, -0.0288, -0.2268, -0.1848,\n",
       "                      -0.0824, -0.1541, -0.1888, -0.1379, -0.2919, -0.0788,  0.0479, -0.1095,\n",
       "                      -0.2444, -0.1257, -0.1357, -0.1061, -0.2622, -0.0620, -0.1831, -0.2721,\n",
       "                      -0.0382, -0.2156, -0.0762, -0.1298, -0.1727, -0.1843, -0.2822, -0.0699,\n",
       "                      -0.0929, -0.1535, -0.0477, -0.0655, -0.0162, -0.2525, -0.1638, -0.1402,\n",
       "                      -0.1611, -0.1750, -0.0786, -0.2593, -0.0181, -0.0772, -0.2323, -0.0582,\n",
       "                      -0.1462, -0.1003, -0.1522, -0.2230, -0.1952, -0.2427, -0.1597, -0.1277,\n",
       "                      -0.1969, -0.0794, -0.1274, -0.0669, -0.2162, -0.2178,  0.0315, -0.1039,\n",
       "                      -0.1449, -0.0998, -0.0203, -0.1328, -0.2590, -0.1670,  0.0473, -0.1095,\n",
       "                      -0.1015,  0.0654, -0.2903, -0.1262, -0.1295, -0.0536, -0.1289, -0.1794,\n",
       "                      -0.0151, -0.1521, -0.0421, -0.2153, -0.1321, -0.1769, -0.1447, -0.1720,\n",
       "                      -0.0853, -0.2537, -0.2429, -0.1896, -0.1566, -0.0786, -0.0201, -0.1479,\n",
       "                      -0.2336, -0.2051, -0.1388,  0.0454, -0.1006, -0.1598, -0.1122, -0.2249,\n",
       "                      -0.0479, -0.1444, -0.0904, -0.0948, -0.1391, -0.1397, -0.1610, -0.3108,\n",
       "                      -0.1091, -0.0717, -0.1317,  0.1230, -0.1147, -0.0400, -0.0819,  0.0032])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-0.1298, -0.1883, -0.1446, -0.2123, -0.0964, -0.1916, -0.1965, -0.1525,\n",
       "                      -0.2520, -0.1353, -0.1290, -0.1731, -0.2641, -0.0651, -0.3017, -0.1776,\n",
       "                      -0.0799, -0.1006, -0.0877, -0.1163, -0.3243, -0.1507, -0.0358, -0.0451,\n",
       "                      -0.2142, -0.2256, -0.1182, -0.0989, -0.0832,  0.0276, -0.0640, -0.2908,\n",
       "                      -0.0960, -0.1776, -0.2229, -0.0647, -0.0572, -0.2071, -0.1545, -0.1269,\n",
       "                      -0.1242, -0.0170, -0.0459, -0.1879, -0.1735, -0.1657, -0.1313, -0.1667,\n",
       "                      -0.1879, -0.0457, -0.2770, -0.2319, -0.1499, -0.0914, -0.2103, -0.0465,\n",
       "                      -0.0708, -0.1282, -0.0713, -0.1623, -0.1303, -0.3767, -0.2317, -0.1325,\n",
       "                      -0.1483, -0.1617, -0.0411, -0.2540, -0.3877, -0.2568, -0.1242, -0.1834,\n",
       "                      -0.1059, -0.1476, -0.1243, -0.0963, -0.1672, -0.1035, -0.1662,  0.0129,\n",
       "                      -0.1189, -0.0830, -0.3596, -0.1573, -0.1295, -0.1401, -0.1607, -0.1333,\n",
       "                      -0.1967, -0.2173, -0.0665, -0.2871, -0.1451, -0.2299,  0.0363, -0.1804,\n",
       "                      -0.1315, -0.2850, -0.1531, -0.0906, -0.1356, -0.1773, -0.1739, -0.1442,\n",
       "                      -0.1396, -0.2398, -0.1446, -0.1694, -0.1367, -0.1735, -0.2403, -0.2557,\n",
       "                      -0.1450, -0.4383, -0.1415, -0.0987, -0.1066, -0.1458, -0.1230, -0.1868,\n",
       "                      -0.1619, -0.0918, -0.1513, -0.0657, -0.1623, -0.2928, -0.0601, -0.0247,\n",
       "                      -0.0992,  0.0205, -0.2047, -0.1956, -0.0722, -0.1686, -0.2680, -0.0768,\n",
       "                      -0.3220, -0.0904,  0.1660, -0.0210, -0.2138, -0.1294, -0.1737,  0.0068,\n",
       "                      -0.2834, -0.1526, -0.1168, -0.1113, -0.3272, -0.0128, -0.1725, -0.2191,\n",
       "                      -0.1491, -0.1092, -0.0835,  0.0108, -0.1762, -0.0801, -0.2169, -0.0944,\n",
       "                      -0.1092, -0.2038, -0.1584, -0.1953, -0.1355, -0.1587, -0.0654,  0.0561,\n",
       "                      -0.0087, -0.1131, -0.1465,  0.0080,  0.1286, -0.0947,  0.0480, -0.2523,\n",
       "                      -0.1238, -0.1725, -0.0225, -0.1506, -0.1232, -0.1945, -0.2179, -0.0664,\n",
       "                      -0.1339, -0.2468, -0.1851, -0.1930, -0.0088, -0.1591, -0.0598,  0.0095,\n",
       "                      -0.1527, -0.1922, -0.1080, -0.0598, -0.2348, -0.0143, -0.0639, -0.0222,\n",
       "                      -0.0885, -0.1990, -0.0121,  0.0168, -0.2209, -0.0445, -0.0517, -0.0591,\n",
       "                      -0.1230, -0.0365, -0.2363, -0.0490, -0.0669, -0.1361,  0.0015, -0.1471,\n",
       "                      -0.1306, -0.2307, -0.0439, -0.2836, -0.1470, -0.0841, -0.0958, -0.1141,\n",
       "                      -0.0624, -0.2403, -0.2349, -0.0102, -0.1163, -0.1474,  0.0190, -0.0649,\n",
       "                      -0.1468, -0.2151, -0.1425, -0.0610, -0.0655, -0.2173,  0.0069, -0.1123,\n",
       "                      -0.1493, -0.1489, -0.2655, -0.1197, -0.1797, -0.1269, -0.1389, -0.1723,\n",
       "                      -0.0853, -0.1354,  0.0291,  0.0474, -0.0761, -0.0803, -0.1446,  0.0404,\n",
       "                      -0.0074, -0.1055, -0.0326, -0.0146,  0.0282, -0.0393,  0.0486, -0.0575,\n",
       "                      -0.0271, -0.0317, -0.0399,  0.0242, -0.0835,  0.0695,  0.0049,  0.0347,\n",
       "                      -0.0059,  0.0006,  0.0016, -0.0392,  0.0348,  0.0490, -0.0717, -0.0094,\n",
       "                      -0.0239, -0.0399, -0.0159, -0.0438, -0.0257,  0.0227, -0.0056,  0.0025,\n",
       "                       0.0137, -0.1050, -0.0425, -0.1092,  0.0102, -0.1142,  0.0798, -0.0016,\n",
       "                       0.0402,  0.0817, -0.0126, -0.1051,  0.0202, -0.0624,  0.0467, -0.1181,\n",
       "                       0.0537, -0.0190, -0.0104,  0.0764,  0.0773, -0.0255, -0.0165,  0.0010,\n",
       "                      -0.0265,  0.0305, -0.0732,  0.0568,  0.0606, -0.0209,  0.0589,  0.1183,\n",
       "                      -0.0194,  0.0513, -0.0153, -0.0857,  0.1095,  0.0242,  0.0737, -0.0231,\n",
       "                      -0.0059, -0.0300, -0.0276, -0.1206,  0.0276,  0.0480,  0.0986, -0.0152,\n",
       "                       0.1070,  0.0422, -0.0021, -0.0529,  0.0761,  0.0386,  0.0618, -0.0182,\n",
       "                      -0.0479,  0.1371, -0.0100,  0.0489,  0.0020, -0.0225, -0.0278, -0.0475,\n",
       "                       0.0312, -0.0639, -0.0450,  0.0058,  0.0071,  0.1832, -0.0773,  0.0654,\n",
       "                       0.0005, -0.0156, -0.0586, -0.0600,  0.0225,  0.0411, -0.0299,  0.0445,\n",
       "                       0.1061, -0.0237, -0.0194, -0.0874, -0.0612, -0.0330,  0.0217, -0.0364,\n",
       "                       0.1229,  0.0869, -0.0276, -0.0480, -0.1451,  0.1129, -0.0257,  0.0572,\n",
       "                      -0.1973, -0.1984, -0.2526, -0.2474,  0.0063, -0.2139, -0.1287, -0.0019,\n",
       "                      -0.2464, -0.2711,  0.0397, -0.2329, -0.3258, -0.1450, -0.2096, -0.1516,\n",
       "                       0.0120, -0.0698, -0.1906, -0.0331, -0.2232, -0.2894, -0.1162, -0.2102,\n",
       "                      -0.1059, -0.2698, -0.1391,  0.0315, -0.2512, -0.0302, -0.1469, -0.1717,\n",
       "                      -0.0704, -0.1055, -0.0852, -0.1956, -0.0618, -0.2485, -0.1411, -0.0625,\n",
       "                      -0.1408, -0.0753, -0.1473, -0.1737, -0.0802, -0.2682, -0.0419, -0.1827,\n",
       "                      -0.1162, -0.1351, -0.0957, -0.3212, -0.2197, -0.1071, -0.2888, -0.1115,\n",
       "                      -0.1788, -0.1347, -0.2189, -0.0518, -0.2364, -0.2684, -0.2856, -0.2014,\n",
       "                      -0.2163, -0.1338, -0.0701,  0.0180, -0.3406, -0.2727,  0.0506, -0.1684,\n",
       "                      -0.2595, -0.0657, -0.0439, -0.1583, -0.2498, -0.1831, -0.0363,  0.0323,\n",
       "                      -0.1013, -0.0689, -0.3162, -0.1929, -0.0660, -0.2109, -0.1113, -0.1025,\n",
       "                      -0.1271, -0.1320,  0.0186, -0.0923, -0.0459, -0.2292, -0.0582, -0.1744,\n",
       "                      -0.0888, -0.2079, -0.2100, -0.0469, -0.0857, -0.2440, -0.1089, -0.0888,\n",
       "                      -0.0850, -0.3019, -0.2467,  0.0366, -0.0901, -0.1200, -0.1412, -0.2606,\n",
       "                      -0.0441, -0.2549, -0.0644, -0.0631, -0.0948, -0.0968, -0.1483, -0.2588,\n",
       "                      -0.1322, -0.1658, -0.1371, -0.0655, -0.0099, -0.0261, -0.0617, -0.0608])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.2769, -0.2406, -0.3505,  ...,  0.2901,  0.3174,  0.2289],\n",
       "                      [ 0.2634, -0.2334, -0.1510,  ..., -0.4700, -0.1241,  0.1277],\n",
       "                      [ 0.3534,  0.2001, -0.0441,  ..., -0.0779,  0.2772, -0.4698],\n",
       "                      ...,\n",
       "                      [ 0.2503,  0.1395,  0.1155,  ..., -0.1663, -0.1965,  0.1129],\n",
       "                      [-0.5094,  0.2780, -0.1011,  ..., -0.0501,  0.6128, -0.0431],\n",
       "                      [ 0.2429,  0.0530,  0.0906,  ..., -0.0898, -0.0760, -0.2109]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0573,  0.0266, -0.1297,  ...,  0.1346,  0.1702,  0.0526],\n",
       "                      [ 0.1543, -0.2464, -0.1698,  ..., -0.1065,  0.1349, -0.0317],\n",
       "                      [ 0.1969, -0.1888,  0.0327,  ...,  0.5182,  0.0672, -0.0218],\n",
       "                      ...,\n",
       "                      [-0.0555,  0.2630, -0.0363,  ...,  0.1378, -0.1337,  0.1236],\n",
       "                      [ 0.2067,  0.2483, -0.0040,  ...,  0.0612, -0.3785,  0.1962],\n",
       "                      [ 0.1637, -0.3142,  0.0526,  ...,  0.1789,  0.2630,  0.2517]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 3.2552e-03,  9.9057e-02, -1.1444e-02, -1.2342e-02,  1.0764e-01,\n",
       "                       2.6159e-02, -5.3202e-02,  5.6155e-02,  2.8659e-02, -6.4163e-02,\n",
       "                       1.2566e-02,  4.1413e-02, -5.8503e-02, -1.1369e-02, -4.6309e-02,\n",
       "                       2.0228e-01,  1.7415e-01, -9.0829e-02,  2.5590e-01,  9.9191e-02,\n",
       "                      -1.7496e-02, -3.0844e-02, -1.0597e-03,  1.3241e-01,  4.7813e-03,\n",
       "                       1.1961e-02,  1.3171e-02,  1.6282e-01, -1.4258e-01,  1.4578e-01,\n",
       "                       1.8921e-01, -9.9251e-02,  1.1746e-01,  8.7360e-02,  8.8368e-02,\n",
       "                      -4.5999e-02,  1.3323e-01,  8.8408e-02, -2.8279e-02, -1.0577e-02,\n",
       "                       1.0220e-01,  2.0655e-02, -2.2827e-02,  9.0485e-02,  4.1604e-02,\n",
       "                       4.4075e-02,  1.1911e-03,  5.5750e-02,  4.9273e-03,  3.1768e-02,\n",
       "                      -3.1842e-02, -1.1082e-01,  1.4729e-01,  6.4029e-02,  6.2203e-02,\n",
       "                       9.2593e-02,  1.7720e-01,  3.1308e-01, -1.4542e-01, -1.4231e-01,\n",
       "                      -6.9119e-02,  3.8017e-02, -8.9061e-02, -2.4371e-02, -3.3714e-02,\n",
       "                       2.7532e-02,  7.2428e-02,  2.8972e-03, -3.6710e-02, -5.3544e-02,\n",
       "                      -8.7911e-02,  1.1156e-01, -1.7844e-02,  1.7135e-01,  7.8613e-02,\n",
       "                      -1.0436e-01, -3.5788e-02,  2.5910e-02,  1.2506e-01,  6.5503e-02,\n",
       "                       9.3064e-02,  1.5361e-01,  1.2567e-01, -1.3333e-02,  7.8532e-02,\n",
       "                       8.1430e-02,  5.0180e-02,  9.9003e-02,  5.5243e-02, -4.0000e-03,\n",
       "                      -1.5455e-02,  9.9059e-02,  8.6723e-02,  3.5997e-02,  6.9288e-02,\n",
       "                       1.1833e-01,  7.8084e-02, -1.0388e-01,  5.7933e-02,  1.2261e-01,\n",
       "                       8.4969e-02,  1.7908e-01, -2.8088e-02,  7.0096e-04,  4.3356e-02,\n",
       "                       7.7569e-02,  3.2384e-03, -9.6044e-03, -1.3472e-02,  3.5029e-02,\n",
       "                       2.2898e-02,  1.9313e-01, -9.9853e-02,  1.4548e-01, -2.1486e-02,\n",
       "                       5.3990e-02, -6.5897e-03, -2.6109e-02,  1.9805e-03,  4.8784e-02,\n",
       "                       1.2486e-01,  3.9867e-02,  3.2701e-02,  1.6407e-01,  1.9179e-01,\n",
       "                       1.4964e-01, -1.2868e-01,  8.1033e-02, -3.3496e-02, -4.2798e-02,\n",
       "                       3.8950e-02,  2.1773e-02, -2.4198e-01,  1.3355e-01,  1.2242e-03,\n",
       "                      -8.7375e-02, -1.2833e-01, -1.4921e-01, -6.1044e-02,  4.4387e-02,\n",
       "                      -8.5908e-02, -1.3046e-01, -1.6721e-01,  3.4846e-02, -3.5526e-02,\n",
       "                       1.8214e-01, -1.1624e-01, -8.4734e-02,  5.5585e-02, -8.9253e-02,\n",
       "                      -2.2218e-01, -8.7241e-02,  1.3854e-03, -4.8670e-02, -2.7840e-02,\n",
       "                      -3.9838e-02, -1.1599e-03, -2.1987e-01, -3.2524e-02, -5.3404e-02,\n",
       "                      -3.1706e-02, -1.5368e-02, -8.0523e-02, -2.8999e-01, -1.1957e-01,\n",
       "                      -1.5430e-01, -1.6399e-01, -3.8280e-02, -5.7341e-02,  3.5657e-02,\n",
       "                      -3.6191e-02, -1.5757e-01,  2.9235e-02, -8.5595e-03, -4.1672e-02,\n",
       "                      -1.2946e-01,  3.9347e-03,  4.1389e-02,  1.7374e-01, -1.7364e-01,\n",
       "                       1.0379e-02, -2.0520e-01, -2.3290e-01, -1.4633e-01,  4.3319e-02,\n",
       "                      -5.4797e-02,  1.4255e-01, -2.8935e-02, -9.7794e-02, -1.2802e-01,\n",
       "                      -7.1476e-05, -1.3011e-01,  1.0907e-02,  5.7751e-02, -1.6630e-01,\n",
       "                       4.8769e-02, -7.2923e-02, -9.9195e-02,  5.8450e-02, -1.4538e-01,\n",
       "                      -1.9314e-02, -1.5914e-02,  5.8196e-03, -1.5708e-01, -4.4373e-02,\n",
       "                      -9.2085e-02,  6.3095e-02, -2.0626e-01, -8.2086e-02,  3.4710e-02,\n",
       "                      -1.8820e-01, -1.0877e-01, -2.4849e-02, -9.2184e-02, -4.6020e-02,\n",
       "                      -1.2862e-01, -5.7966e-02, -1.4782e-01,  3.3381e-02, -4.5469e-02,\n",
       "                       4.9851e-02,  3.0088e-02,  4.1407e-02, -1.0181e-01, -1.4063e-01,\n",
       "                      -6.0642e-02, -1.1454e-01, -1.6530e-01,  5.6854e-02, -2.6154e-01,\n",
       "                       1.5268e-02, -7.3028e-02, -2.4787e-01,  9.3784e-03, -2.2203e-01,\n",
       "                      -2.0467e-01,  1.1615e-01, -1.6531e-01, -1.6514e-01,  2.0643e-02,\n",
       "                      -1.3498e-01, -1.6485e-01, -4.8278e-02, -2.8370e-02, -1.8018e-01,\n",
       "                      -6.0218e-02, -7.5149e-02, -1.3753e-01, -7.2809e-02, -7.5063e-02,\n",
       "                       2.2237e-02,  6.5599e-02,  7.7810e-02, -2.3190e-01, -1.1555e-01,\n",
       "                      -3.4560e-02,  3.7556e-02,  3.2993e-02, -7.4402e-02, -9.0296e-02,\n",
       "                      -1.3199e-01, -6.4185e-02,  8.3946e-02, -3.4245e-02, -3.7142e-02,\n",
       "                      -8.0484e-02, -1.3926e-01, -1.8685e-02,  5.5277e-02, -4.3728e-02,\n",
       "                      -3.6366e-02,  1.7043e-02, -6.8678e-03,  1.6867e-02, -1.6825e-01,\n",
       "                      -1.3596e-02,  2.9109e-02,  1.4285e-01,  2.3070e-02,  1.3534e-01,\n",
       "                      -2.4001e-02,  8.6813e-02, -6.3001e-02,  3.1027e-02, -1.1365e-03,\n",
       "                      -5.0828e-02, -5.2062e-02, -9.1398e-02,  3.3072e-02, -1.0066e-01,\n",
       "                       2.7487e-02,  5.6115e-02, -9.2448e-03, -5.7820e-04,  1.7373e-01,\n",
       "                      -3.2577e-02,  1.3087e-02,  1.1325e-01,  4.5206e-02, -1.0270e-01,\n",
       "                      -7.8421e-02, -7.8195e-02, -8.7663e-03,  7.8183e-02, -1.3788e-01,\n",
       "                      -1.0119e-01, -3.5508e-02,  1.3244e-01, -6.3066e-02, -1.8549e-02,\n",
       "                      -7.6729e-02, -1.5665e-01,  9.2605e-02, -1.8715e-02, -5.3759e-02,\n",
       "                      -1.4997e-02, -1.1201e-01, -6.5837e-02,  3.2354e-02, -4.4304e-02,\n",
       "                       1.6683e-02,  5.2863e-02, -7.4749e-02, -7.7009e-02,  9.4551e-02,\n",
       "                      -1.3089e-01, -1.9662e-02, -1.8859e-02,  1.0225e-02, -3.2215e-02,\n",
       "                       1.0178e-01, -8.7098e-02,  3.6515e-02, -1.0079e-01, -5.2373e-02,\n",
       "                       5.4941e-03, -7.9467e-02, -4.8557e-02,  2.9068e-03,  5.5651e-02,\n",
       "                       1.0205e-01, -3.9341e-02, -2.1071e-02,  7.7820e-02,  9.3632e-02,\n",
       "                      -8.6680e-02,  1.2417e-03, -1.8449e-03,  8.7906e-02, -8.7560e-02,\n",
       "                      -1.0661e-01, -1.4121e-01, -1.2341e-02,  3.2236e-04,  3.8134e-02,\n",
       "                       9.8997e-02, -4.6627e-02, -3.9381e-02, -9.0276e-03,  3.2522e-04,\n",
       "                      -6.3099e-03,  9.7523e-02, -1.7826e-02,  6.4926e-02, -3.3158e-02,\n",
       "                       7.2952e-02,  1.0413e-01,  7.1300e-02,  8.2729e-02, -1.0601e-02,\n",
       "                      -8.0193e-02,  9.7551e-02,  1.3374e-01,  4.5383e-02,  1.1552e-01,\n",
       "                      -4.5427e-02, -6.5938e-02,  4.1199e-02, -7.8414e-02, -1.2202e-01,\n",
       "                      -7.7928e-03, -1.0023e-01,  3.9144e-02, -1.2111e-01,  3.0799e-02,\n",
       "                       6.7938e-02,  9.5349e-02, -9.5829e-02,  1.3139e-01,  1.9132e-02,\n",
       "                       8.4672e-02, -1.1743e-01,  2.4094e-01, -1.1795e-02,  6.7638e-02,\n",
       "                       8.0130e-02, -4.1861e-02, -3.1141e-02,  1.7041e-02,  7.4672e-02,\n",
       "                       8.4148e-02, -3.6097e-03,  2.3936e-01,  9.4805e-02,  1.3952e-03,\n",
       "                      -3.9552e-02,  1.2644e-01,  1.6826e-01,  6.6626e-02, -1.0647e-03,\n",
       "                      -8.7852e-03,  1.8515e-02, -7.6058e-02,  5.2967e-02,  1.0249e-01,\n",
       "                       3.7316e-03,  2.3722e-01,  1.2281e-01, -3.4039e-02, -5.0166e-02,\n",
       "                      -3.0113e-05,  6.4501e-03,  1.1182e-02,  7.6763e-02,  5.3325e-02,\n",
       "                       2.3310e-02,  6.7021e-02, -1.2537e-02,  2.3923e-01,  1.0116e-02,\n",
       "                      -6.4476e-02, -5.8220e-02,  1.1628e-01, -1.0180e-02, -5.2183e-02,\n",
       "                      -1.1613e-01,  2.1025e-01,  4.7359e-02,  2.4225e-01,  2.5000e-01,\n",
       "                       1.8382e-01,  1.8860e-01,  4.2342e-03, -4.2950e-02, -2.4654e-02,\n",
       "                      -7.1004e-02, -1.2174e-01,  1.3144e-01,  1.4521e-01, -1.2586e-02,\n",
       "                      -3.2885e-03,  1.3943e-01,  9.1718e-02,  7.0018e-02,  1.2761e-01,\n",
       "                       2.3978e-01,  3.0439e-02,  4.8462e-02,  7.6385e-02,  1.1581e-02,\n",
       "                       9.6150e-02,  2.9525e-02,  1.3282e-02,  1.3781e-01,  1.1459e-01,\n",
       "                       1.0937e-01,  2.2389e-01,  4.0696e-02, -1.3209e-02,  1.8716e-03,\n",
       "                       2.2794e-01,  4.6756e-02,  8.7838e-02, -8.7591e-02, -8.4281e-02,\n",
       "                       3.8157e-02,  7.4546e-02,  8.0080e-02,  2.1410e-02,  1.7533e-01,\n",
       "                      -4.1338e-02, -9.9097e-02,  1.1467e-01,  1.6952e-01, -5.2010e-02,\n",
       "                       2.9462e-01,  5.0722e-02,  8.1668e-02,  7.2553e-02,  2.6322e-02,\n",
       "                       1.2336e-01, -2.5850e-03, -8.5161e-03,  6.9493e-02,  1.5620e-01,\n",
       "                       2.0353e-02,  7.4167e-02,  5.0212e-02,  2.0176e-01,  4.2903e-02,\n",
       "                      -1.2535e-01, -6.0143e-02,  2.3171e-02, -4.9358e-02,  8.7033e-02,\n",
       "                       4.3170e-02, -4.6171e-02,  6.5389e-02,  1.7322e-01,  7.7617e-02,\n",
       "                       3.0747e-02,  2.0870e-01])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 9.0901e-02,  3.1723e-02,  1.0666e-01, -1.5703e-02,  1.2642e-01,\n",
       "                       1.0022e-01,  8.9652e-02,  4.5882e-02,  5.7528e-02,  2.2690e-02,\n",
       "                      -3.3358e-02,  8.8972e-02,  3.8011e-02, -1.3569e-01, -2.3320e-02,\n",
       "                       2.4576e-01,  1.0036e-01,  4.6493e-02,  2.2607e-01,  4.7094e-02,\n",
       "                      -9.3856e-02,  7.9799e-02,  1.3659e-01,  1.5858e-01,  8.5506e-02,\n",
       "                       3.8408e-02, -1.0261e-02,  1.0259e-01, -1.1242e-01,  1.3100e-01,\n",
       "                       1.6964e-01,  5.4179e-02, -2.8448e-02,  7.3275e-02,  1.0227e-01,\n",
       "                       5.7022e-02,  3.1361e-02,  5.6392e-02,  2.3023e-02, -1.7214e-02,\n",
       "                       1.2651e-01,  6.0314e-02,  1.3260e-01,  9.6467e-02,  1.2465e-01,\n",
       "                       1.4175e-01, -3.8449e-02,  5.4032e-02,  2.5803e-02,  8.4241e-02,\n",
       "                      -1.4179e-02, -1.3115e-01,  1.5435e-01,  1.3143e-01,  1.3045e-01,\n",
       "                       6.3541e-02,  1.5911e-01,  2.1949e-01, -4.1045e-02,  1.5435e-02,\n",
       "                       6.0840e-02,  5.5556e-02, -2.3712e-01,  2.5589e-02, -3.5578e-02,\n",
       "                      -2.1914e-02,  7.3906e-02, -5.9399e-02,  7.2577e-02, -1.3345e-01,\n",
       "                       8.2397e-02,  3.0264e-01,  1.6439e-01,  1.7651e-01,  3.0633e-02,\n",
       "                       1.7371e-01,  2.5328e-02,  1.6571e-02,  1.9339e-01,  1.4269e-01,\n",
       "                       3.3992e-02,  1.1171e-01,  1.0820e-01, -4.3308e-02,  2.5048e-02,\n",
       "                       5.8856e-02,  9.1489e-02,  2.2775e-01, -2.6162e-02, -1.9430e-02,\n",
       "                      -2.5446e-02,  1.4928e-01, -8.4120e-02,  1.0925e-01,  8.0508e-02,\n",
       "                       9.2973e-02,  1.8904e-02, -4.3551e-02,  1.9026e-01,  9.6066e-02,\n",
       "                      -2.0413e-02,  1.3674e-01,  1.1169e-01,  3.1868e-02,  1.3616e-01,\n",
       "                      -1.0133e-02,  1.6083e-01,  6.4527e-02,  2.9585e-02,  6.1085e-02,\n",
       "                      -8.2549e-02,  9.9153e-02,  3.0078e-02,  5.0144e-02,  3.9961e-02,\n",
       "                       3.5980e-02, -8.5054e-02, -1.8270e-01,  5.8407e-02, -3.5574e-02,\n",
       "                       1.1350e-01, -1.3849e-02, -3.7208e-02,  1.7684e-01, -2.6319e-02,\n",
       "                       3.0778e-01, -1.7406e-02, -5.2993e-02,  1.7101e-01, -2.3087e-01,\n",
       "                       1.3778e-02, -1.0469e-01, -3.2719e-01,  2.9679e-02, -8.5642e-02,\n",
       "                      -2.6920e-02, -7.4968e-02, -1.5143e-01, -6.8444e-02, -3.8934e-02,\n",
       "                       5.6907e-02, -2.1034e-01, -2.0533e-01, -3.9342e-02, -9.4801e-03,\n",
       "                       2.1775e-01, -2.5008e-02, -1.4628e-01,  6.3404e-02, -5.5390e-02,\n",
       "                      -1.2296e-01, -2.3384e-02,  2.8517e-02, -1.0917e-01, -1.8817e-01,\n",
       "                      -1.3006e-01,  3.1073e-02, -2.2924e-01, -1.4901e-01, -7.9981e-02,\n",
       "                      -6.3803e-02, -1.1939e-01, -1.8803e-01, -2.4660e-01, -2.5920e-01,\n",
       "                      -7.6762e-02, -5.7998e-02, -9.1393e-02, -1.3133e-03, -5.8289e-02,\n",
       "                      -5.2231e-03, -2.4784e-01,  1.8377e-01,  6.1788e-02, -7.3862e-03,\n",
       "                      -1.7570e-03,  1.6963e-01, -7.8698e-03, -2.0658e-02, -2.1541e-01,\n",
       "                      -1.8703e-04, -1.1153e-01, -1.3950e-01, -2.1380e-01, -8.6135e-02,\n",
       "                      -4.6148e-02,  1.4316e-01, -1.1702e-01, -2.0543e-03, -8.6202e-02,\n",
       "                      -8.1955e-02, -2.0465e-01, -8.2324e-03, -3.3161e-02, -4.1723e-02,\n",
       "                      -4.7183e-02, -1.8724e-01, -1.8497e-02, -1.3293e-01,  4.2429e-02,\n",
       "                       7.6935e-02,  9.6293e-02,  7.0293e-02, -1.6310e-01,  9.6925e-03,\n",
       "                      -5.2077e-02, -7.9411e-02, -1.0585e-01, -3.9181e-02, -9.4918e-02,\n",
       "                      -1.4970e-01, -9.7969e-02, -1.8504e-01, -5.4970e-02, -1.7247e-01,\n",
       "                      -1.4120e-01, -4.3757e-02, -1.7472e-01,  7.0172e-02, -1.4058e-01,\n",
       "                      -1.4121e-01, -8.6841e-02, -2.6756e-02, -1.1100e-01, -7.0509e-02,\n",
       "                      -4.2801e-02, -2.0017e-01, -2.6167e-01, -1.2466e-01, -1.7919e-01,\n",
       "                      -6.9849e-02, -4.3767e-03, -1.1687e-01,  8.0204e-03, -3.3859e-02,\n",
       "                      -2.6451e-02, -8.5963e-02, -1.9445e-01, -6.2498e-02,  7.1144e-02,\n",
       "                      -2.7896e-02, -4.8812e-02, -7.9517e-02, -1.1281e-01, -1.9134e-01,\n",
       "                      -1.6107e-01,  1.6677e-02, -6.1218e-03, -7.1424e-02,  1.0501e-02,\n",
       "                       1.8356e-02,  3.3177e-02, -7.8414e-02, -9.5582e-02, -1.3096e-01,\n",
       "                      -6.6360e-03,  1.4095e-02, -6.0654e-02,  9.5782e-02, -7.9433e-02,\n",
       "                      -5.3644e-02, -6.2697e-02, -2.6136e-02,  8.2172e-02,  4.9601e-04,\n",
       "                      -9.7731e-02, -2.3594e-02, -1.8979e-01,  2.4997e-02,  7.4080e-02,\n",
       "                      -1.4105e-01,  3.1206e-03, -8.1159e-02,  2.8451e-03, -1.2263e-01,\n",
       "                       2.4204e-02,  1.4128e-01,  8.4418e-03, -2.1864e-02,  2.2885e-02,\n",
       "                      -3.9741e-03,  6.4945e-02,  1.2806e-02,  7.1909e-02,  5.7630e-02,\n",
       "                       1.1479e-01, -2.9991e-02, -1.1286e-02, -6.7311e-02, -1.7150e-01,\n",
       "                       9.7140e-02, -1.0316e-02,  5.7388e-02,  3.0011e-02,  4.0248e-02,\n",
       "                       9.0593e-02,  2.1339e-02,  3.6015e-02, -2.3107e-02, -5.1133e-02,\n",
       "                      -1.3642e-02,  2.2004e-02,  6.1301e-03,  9.2436e-02, -8.4147e-02,\n",
       "                      -7.1327e-02,  6.6711e-02, -4.8539e-02, -6.6125e-02, -1.3022e-01,\n",
       "                      -9.9978e-02, -9.5114e-02,  6.4112e-02,  3.3353e-02, -7.8067e-02,\n",
       "                      -1.3040e-01, -1.1742e-02,  1.6689e-02, -2.3727e-03,  4.5471e-02,\n",
       "                      -1.9676e-01, -5.0357e-03, -3.3177e-02, -1.1474e-04,  8.8560e-02,\n",
       "                       7.0816e-02, -1.2104e-02,  3.1360e-02,  7.6249e-02,  6.8458e-02,\n",
       "                       5.2784e-02, -1.0343e-01, -5.0131e-03, -2.5778e-02,  1.2958e-01,\n",
       "                      -8.8556e-02, -7.4047e-02, -7.2256e-02, -5.8643e-02, -4.2716e-02,\n",
       "                      -4.5111e-02,  5.8194e-03, -5.7949e-02, -8.6629e-02,  1.4626e-01,\n",
       "                      -3.6452e-02, -5.0828e-02,  1.2839e-02,  1.3308e-01, -2.7011e-02,\n",
       "                       2.6633e-02, -5.2034e-02,  7.6472e-02, -1.1037e-01,  4.3423e-02,\n",
       "                       9.9908e-02,  4.8079e-02, -1.5085e-01, -5.7244e-03, -3.0481e-03,\n",
       "                      -3.9512e-02, -1.1053e-02, -1.0807e-03,  9.1186e-02, -1.0867e-01,\n",
       "                       1.2912e-01, -3.1928e-02,  7.9865e-02,  2.8060e-02, -7.0121e-02,\n",
       "                      -3.7562e-02,  4.9693e-03,  5.5345e-02,  1.8419e-02,  7.8364e-02,\n",
       "                       5.7374e-02, -3.4736e-02,  1.2574e-01, -4.4356e-02,  7.2570e-02,\n",
       "                       6.9660e-02, -1.5242e-02, -2.6038e-02,  4.1603e-03,  1.4654e-01,\n",
       "                      -7.6374e-04,  1.8900e-01, -1.0495e-01,  7.0175e-02,  3.9472e-02,\n",
       "                       5.6808e-02, -1.2982e-02,  1.1996e-01,  2.8515e-02,  5.4174e-02,\n",
       "                       4.1808e-02, -3.5665e-02, -6.9624e-02,  1.1789e-01,  1.5415e-01,\n",
       "                       1.9192e-01,  2.5798e-02,  2.6049e-01, -6.5207e-02,  1.0808e-01,\n",
       "                       2.0854e-03,  6.9752e-02,  1.0615e-01, -6.3688e-02,  4.2829e-02,\n",
       "                      -2.4235e-02,  9.5641e-02,  3.4198e-03,  1.4293e-01,  3.9859e-02,\n",
       "                      -8.1394e-02,  2.6283e-01,  6.9115e-04,  6.7705e-03, -5.1937e-02,\n",
       "                      -6.5537e-02,  8.1837e-02,  6.6284e-02,  9.7825e-02,  1.3664e-02,\n",
       "                       5.9987e-02,  9.4418e-02,  6.4223e-02,  1.6878e-01, -4.8200e-02,\n",
       "                      -4.3965e-02, -2.3822e-02,  1.2980e-02,  1.0310e-02, -4.8666e-02,\n",
       "                      -1.3745e-01,  1.2002e-01, -1.6185e-02,  2.5487e-01, -1.7987e-03,\n",
       "                       1.3074e-01,  2.6921e-01,  9.3466e-02,  4.0962e-02, -1.0125e-01,\n",
       "                      -1.1820e-01, -9.9207e-02,  3.0198e-02,  1.0604e-01,  1.2296e-01,\n",
       "                       1.1071e-01,  3.2304e-02,  8.9222e-02,  5.6536e-02,  7.4848e-02,\n",
       "                       2.0870e-01,  3.3292e-02,  7.1923e-03,  1.2161e-01,  9.9940e-02,\n",
       "                      -3.9730e-02,  2.3094e-02,  2.1495e-02,  1.0733e-01,  6.0953e-02,\n",
       "                       2.8109e-01,  1.6689e-01, -3.6263e-02, -1.0430e-02,  8.5831e-02,\n",
       "                       9.2003e-02,  1.3034e-01,  5.3226e-02,  4.8540e-02, -3.4864e-02,\n",
       "                       1.9298e-01,  1.2054e-01,  2.1235e-03, -7.0407e-02,  2.7041e-01,\n",
       "                       8.4068e-02, -9.1392e-02,  1.0114e-01,  1.1028e-01, -4.7717e-02,\n",
       "                       1.3427e-01,  1.5832e-01,  2.2080e-01,  1.8603e-02,  7.6003e-02,\n",
       "                       4.9813e-02, -2.7577e-02, -1.5790e-02, -4.8857e-02,  1.8961e-02,\n",
       "                       6.6726e-02,  8.1480e-02, -4.4183e-02,  1.0521e-01, -4.8025e-03,\n",
       "                      -1.1934e-02, -1.0079e-01,  3.0803e-02, -6.7065e-03,  9.1518e-02,\n",
       "                       8.9738e-02, -1.7459e-01,  1.4222e-01,  1.1917e-01,  5.9517e-02,\n",
       "                       3.0699e-02, -5.1283e-02])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[-0.1137, -0.0663,  0.0463,  ..., -0.0879, -0.3308, -0.2051],\n",
       "                      [ 0.0073,  0.0070,  0.2583,  ...,  0.1960,  0.0761, -0.0293],\n",
       "                      [-0.0366,  0.0636,  0.1398,  ...,  0.1969,  0.0156,  0.1670],\n",
       "                      ...,\n",
       "                      [-0.0253, -0.0683, -0.0132,  ..., -0.0903,  0.5334, -0.1304],\n",
       "                      [ 0.0148,  0.0226, -0.0503,  ...,  0.0894, -0.1843, -0.1592],\n",
       "                      [ 0.0629, -0.1634, -0.1140,  ...,  0.0744,  0.1242,  0.1929]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-0.1411, -0.0623,  0.1316,  0.0111, -0.2774, -0.0319, -0.0392,  0.2144,\n",
       "                       0.0183, -0.1112, -0.0502, -0.1326,  0.1532, -0.1152,  0.1001,  0.0866,\n",
       "                      -0.1377,  0.1993,  0.0292,  0.1900, -0.1668, -0.1140, -0.0428,  0.1256,\n",
       "                       0.0707, -0.0875, -0.2058, -0.1718, -0.0651, -0.1719,  0.3128, -0.0783,\n",
       "                       0.0789, -0.0051,  0.0356, -0.1844, -0.2086, -0.1460, -0.0560,  0.0052,\n",
       "                       0.0415,  0.1004,  0.1357, -0.0628,  0.0185, -0.0067,  0.1078, -0.1725,\n",
       "                      -0.0788, -0.2653, -0.1499,  0.0156,  0.1996, -0.1486,  0.1597, -0.2404,\n",
       "                      -0.0917, -0.2489, -0.0569, -0.0895, -0.0576,  0.1282,  0.2920,  0.0219])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[ 0.1093,  0.3097, -0.1241,  ..., -0.0390,  0.0963,  0.2377],\n",
       "                      [ 0.1223, -0.0192, -0.0639,  ...,  0.0578, -0.0958,  0.3316],\n",
       "                      [ 0.0606,  0.2252, -0.1173,  ...,  0.1203,  0.0445, -0.0780],\n",
       "                      ...,\n",
       "                      [-0.0565, -0.3652, -0.0026,  ..., -0.0377, -0.1923,  0.1644],\n",
       "                      [ 0.0316, -0.2471,  0.4204,  ...,  0.0123,  0.1228, -0.1706],\n",
       "                      [ 0.3248, -0.0262, -0.3355,  ..., -0.1670, -0.2843,  0.0101]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([-0.1310,  0.1070,  0.0033,  0.0460, -0.0473, -0.2322, -0.1481,  0.0763,\n",
       "                       0.0146,  0.0977, -0.0160, -0.0696, -0.1544, -0.0879,  0.0694,  0.0184,\n",
       "                      -0.1092,  0.0852, -0.0827, -0.2942, -0.1396,  0.0593,  0.1442,  0.1221,\n",
       "                       0.2361,  0.0077, -0.1418,  0.1209, -0.0621,  0.3600, -0.0686,  0.0483,\n",
       "                       0.0277, -0.1638, -0.2336,  0.1730, -0.1448,  0.1314,  0.3273, -0.1629,\n",
       "                      -0.0460,  0.0779, -0.2323,  0.1786,  0.1811,  0.0216, -0.0023,  0.0308,\n",
       "                      -0.0145, -0.3566,  0.1679,  0.1440, -0.2743, -0.1772,  0.0643,  0.1084,\n",
       "                       0.1854,  0.1132, -0.0575, -0.0248, -0.1559, -0.1462, -0.0605, -0.2237])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[ 0.0448, -0.0041,  0.1412,  ...,  0.0559, -0.0627,  0.0714],\n",
       "                      [ 0.3309, -0.1815,  0.0397,  ...,  0.1677,  0.0525, -0.2993],\n",
       "                      [-0.1200, -0.0568, -0.1698,  ..., -0.0657,  0.0296, -0.2340],\n",
       "                      ...,\n",
       "                      [-0.2262,  0.0981, -0.0378,  ..., -0.0386, -0.0511, -0.0172],\n",
       "                      [ 0.2016, -0.0021, -0.1533,  ..., -0.4441,  0.0217, -0.0802],\n",
       "                      [-0.0294,  0.0590,  0.0023,  ...,  0.0752, -0.5302, -0.2866]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.6384, -0.0841, -0.1963,  ..., -0.4595, -0.1837, -0.1373]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
