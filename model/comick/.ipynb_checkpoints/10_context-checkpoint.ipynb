{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=10,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 10)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 10)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0689257e-15,  4.5809848e-41, -3.0689257e-15, ...,\n",
       "         0.0000000e+00,  1.8273016e-24,  4.5809848e-41],\n",
       "       [ 3.0474359e-24,  4.5809848e-41,  6.6415942e-41, ...,\n",
       "         4.5809848e-41,  6.6429955e-41,  0.0000000e+00],\n",
       "       [ 1.8273710e-24,  4.5809848e-41,  3.0475748e-24, ...,\n",
       "         4.5809848e-41,  3.0477010e-24,  4.5809848e-41],\n",
       "       ...,\n",
       "       [ 1.6603723e-24,  4.5809848e-41,  2.5645513e-25, ...,\n",
       "         4.5809848e-41,  2.5647091e-25,  4.5809848e-41],\n",
       "       [ 1.2034491e-40,  0.0000000e+00,  1.6604418e-24, ...,\n",
       "         0.0000000e+00,  1.6605049e-24,  4.5809848e-41],\n",
       "       [ 2.5648826e-25,  4.5809848e-41,  1.2036033e-40, ...,\n",
       "         4.5809848e-41,  1.2037434e-40,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46150ddaf1147f1b3e35ee787d95ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=6.6161 | F1Score=0.2575\n",
      "Batch-100: NLLLoss=5.4021 | F1Score=0.2857\n",
      "Batch-150: NLLLoss=5.1594 | F1Score=0.3107\n",
      "Batch-200: NLLLoss=4.4588 | F1Score=0.3335\n",
      "Batch-250: NLLLoss=4.3697 | F1Score=0.3478\n",
      "Batch-300: NLLLoss=4.9656 | F1Score=0.3607\n",
      "Batch-350: NLLLoss=2.9714 | F1Score=0.3789\n",
      "Batch-400: NLLLoss=3.5231 | F1Score=0.3987\n",
      "Batch-450: NLLLoss=3.6786 | F1Score=0.4100\n",
      "Batch-500: NLLLoss=4.1256 | F1Score=0.4222\n",
      "Batch-518: NLLLoss=1.8689 | F1Score=0.4271\n",
      "\n",
      "Mean NLLLoss: 4.5707 | Mean F1Score: 0.3418\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d0cbd672d44389bf43bf73a3529cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.1860 | F1Score=0.5689\n",
      "Batch-100: NLLLoss=3.7099 | F1Score=0.5810\n",
      "Batch-150: NLLLoss=2.6385 | F1Score=0.5928\n",
      "Batch-200: NLLLoss=3.1819 | F1Score=0.5970\n",
      "Batch-250: NLLLoss=2.6389 | F1Score=0.6057\n",
      "Batch-300: NLLLoss=3.4193 | F1Score=0.6099\n",
      "Batch-350: NLLLoss=1.6312 | F1Score=0.6154\n",
      "Batch-400: NLLLoss=2.3567 | F1Score=0.6193\n",
      "Batch-450: NLLLoss=2.7415 | F1Score=0.6258\n",
      "Batch-500: NLLLoss=2.2347 | F1Score=0.6302\n",
      "Batch-518: NLLLoss=2.2178 | F1Score=0.6322\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 2.7557 | Mean F1Score: 0.6019\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81810062b2441349fe34514b41eaecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.1480 | F1Score=0.7312\n",
      "Batch-100: NLLLoss=1.7670 | F1Score=0.7297\n",
      "Batch-150: NLLLoss=1.8446 | F1Score=0.7315\n",
      "Batch-200: NLLLoss=2.0178 | F1Score=0.7314\n",
      "Batch-250: NLLLoss=1.6914 | F1Score=0.7336\n",
      "Batch-300: NLLLoss=1.5755 | F1Score=0.7337\n",
      "Batch-350: NLLLoss=1.7900 | F1Score=0.7372\n",
      "Batch-400: NLLLoss=1.4998 | F1Score=0.7402\n",
      "Batch-450: NLLLoss=1.9725 | F1Score=0.7444\n",
      "Batch-500: NLLLoss=2.7514 | F1Score=0.7455\n",
      "Batch-518: NLLLoss=2.2237 | F1Score=0.7453\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.8014 | Mean F1Score: 0.7349\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a342d2185346a08a6df8d6145409bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.0025 | F1Score=0.8084\n",
      "Batch-100: NLLLoss=1.1215 | F1Score=0.8133\n",
      "Batch-150: NLLLoss=2.2680 | F1Score=0.8124\n",
      "Batch-200: NLLLoss=1.1430 | F1Score=0.8117\n",
      "Batch-250: NLLLoss=0.9065 | F1Score=0.8152\n",
      "Batch-300: NLLLoss=1.3140 | F1Score=0.8159\n",
      "Batch-350: NLLLoss=1.0431 | F1Score=0.8150\n",
      "Batch-400: NLLLoss=1.1578 | F1Score=0.8150\n",
      "Batch-450: NLLLoss=0.7152 | F1Score=0.8183\n",
      "Batch-500: NLLLoss=0.9237 | F1Score=0.8181\n",
      "Batch-518: NLLLoss=1.1630 | F1Score=0.8185\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.1405 | Mean F1Score: 0.8130\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88a7908b742457e9fb5acbf4401e303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.0554 | F1Score=0.9025\n",
      "Batch-100: NLLLoss=0.5171 | F1Score=0.9009\n",
      "Batch-150: NLLLoss=0.2884 | F1Score=0.8971\n",
      "Batch-200: NLLLoss=0.8655 | F1Score=0.8950\n",
      "Batch-250: NLLLoss=1.0602 | F1Score=0.8935\n",
      "Batch-300: NLLLoss=0.8755 | F1Score=0.8910\n",
      "Batch-350: NLLLoss=0.7103 | F1Score=0.8868\n",
      "Batch-400: NLLLoss=1.1527 | F1Score=0.8834\n",
      "Batch-450: NLLLoss=0.6605 | F1Score=0.8816\n",
      "Batch-500: NLLLoss=0.4004 | F1Score=0.8798\n",
      "Batch-518: NLLLoss=0.8269 | F1Score=0.8786\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.6520 | Mean F1Score: 0.8924\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a327d2e2dde54c95bc11ab38477c725c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.3814 | F1Score=0.9625\n",
      "Batch-100: NLLLoss=0.4178 | F1Score=0.9661\n",
      "Batch-150: NLLLoss=0.1388 | F1Score=0.9643\n",
      "Batch-200: NLLLoss=0.2237 | F1Score=0.9620\n",
      "Batch-250: NLLLoss=0.3474 | F1Score=0.9606\n",
      "Batch-300: NLLLoss=0.2987 | F1Score=0.9585\n",
      "Batch-350: NLLLoss=0.4713 | F1Score=0.9573\n",
      "Batch-400: NLLLoss=0.2357 | F1Score=0.9558\n",
      "Batch-450: NLLLoss=0.3109 | F1Score=0.9547\n",
      "Batch-500: NLLLoss=0.5216 | F1Score=0.9533\n",
      "Batch-518: NLLLoss=0.3936 | F1Score=0.9524\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.2834 | Mean F1Score: 0.9600\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7901e44df74fe4bc9a24456420c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1037 | F1Score=0.9950\n",
      "Batch-100: NLLLoss=0.0655 | F1Score=0.9953\n",
      "Batch-150: NLLLoss=0.0820 | F1Score=0.9954\n",
      "Batch-200: NLLLoss=0.0857 | F1Score=0.9955\n",
      "Batch-250: NLLLoss=0.2057 | F1Score=0.9951\n",
      "Batch-300: NLLLoss=0.1150 | F1Score=0.9940\n",
      "Batch-350: NLLLoss=0.0314 | F1Score=0.9937\n",
      "Batch-400: NLLLoss=0.0938 | F1Score=0.9939\n",
      "Batch-450: NLLLoss=0.0904 | F1Score=0.9938\n",
      "Batch-500: NLLLoss=0.1364 | F1Score=0.9932\n",
      "Batch-518: NLLLoss=0.0059 | F1Score=0.9930\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0819 | Mean F1Score: 0.9946\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900e8637bf5f47eda27e45a75228d4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0812 | F1Score=0.9969\n",
      "Batch-100: NLLLoss=0.0153 | F1Score=0.9972\n",
      "Batch-150: NLLLoss=0.0117 | F1Score=0.9979\n",
      "Batch-200: NLLLoss=0.0097 | F1Score=0.9983\n",
      "Batch-250: NLLLoss=0.0147 | F1Score=0.9984\n",
      "Batch-300: NLLLoss=0.0121 | F1Score=0.9984\n",
      "Batch-350: NLLLoss=0.0069 | F1Score=0.9982\n",
      "Batch-400: NLLLoss=0.0419 | F1Score=0.9982\n",
      "Batch-450: NLLLoss=0.0201 | F1Score=0.9983\n",
      "Batch-500: NLLLoss=0.1130 | F1Score=0.9983\n",
      "Batch-518: NLLLoss=0.0035 | F1Score=0.9983\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0232 | Mean F1Score: 0.9980\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e497712af485401990b3695845f283df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0113 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0051 | F1Score=0.9983\n",
      "Batch-150: NLLLoss=0.0061 | F1Score=0.9989\n",
      "Batch-200: NLLLoss=0.0091 | F1Score=0.9991\n",
      "Batch-250: NLLLoss=0.0109 | F1Score=0.9992\n",
      "Batch-300: NLLLoss=0.0045 | F1Score=0.9993\n",
      "Batch-350: NLLLoss=0.0073 | F1Score=0.9993\n",
      "Batch-400: NLLLoss=0.0043 | F1Score=0.9994\n",
      "Batch-450: NLLLoss=0.0166 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0135 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0122 | F1Score=0.9994\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0095 | Mean F1Score: 0.9991\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e368168a429f41a787b10f96becc14b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0041 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0025 | F1Score=0.9992\n",
      "Batch-150: NLLLoss=0.0070 | F1Score=0.9991\n",
      "Batch-200: NLLLoss=0.0052 | F1Score=0.9993\n",
      "Batch-250: NLLLoss=0.0038 | F1Score=0.9993\n",
      "Batch-300: NLLLoss=0.0042 | F1Score=0.9993\n",
      "Batch-350: NLLLoss=0.0085 | F1Score=0.9992\n",
      "Batch-400: NLLLoss=0.0025 | F1Score=0.9993\n",
      "Batch-450: NLLLoss=0.0062 | F1Score=0.9993\n",
      "Batch-500: NLLLoss=0.0040 | F1Score=0.9993\n",
      "Batch-518: NLLLoss=0.0028 | F1Score=0.9993\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0074 | Mean F1Score: 0.9993\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d7f2e64494ac8b241ed34860a1894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0025 | F1Score=0.9991\n",
      "Batch-100: NLLLoss=0.0017 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0025 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0051 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0051 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0039 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0043 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0036 | F1Score=0.9997\n",
      "Batch-450: NLLLoss=0.4269 | F1Score=0.9959\n",
      "Batch-500: NLLLoss=0.4019 | F1Score=0.9850\n",
      "Batch-518: NLLLoss=0.1380 | F1Score=0.9831\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0737 | Mean F1Score: 0.9980\n",
      "Patience = 1/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0be35f702f4af9ab8fca4f16e51ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1796 | F1Score=0.9413\n",
      "Batch-100: NLLLoss=0.0415 | F1Score=0.9469\n",
      "Batch-150: NLLLoss=0.1376 | F1Score=0.9525\n",
      "Batch-200: NLLLoss=0.1093 | F1Score=0.9581\n",
      "Batch-250: NLLLoss=0.1948 | F1Score=0.9599\n",
      "Batch-300: NLLLoss=0.1582 | F1Score=0.9629\n",
      "Batch-350: NLLLoss=0.0285 | F1Score=0.9650\n",
      "Batch-400: NLLLoss=0.0407 | F1Score=0.9676\n",
      "Batch-450: NLLLoss=0.0700 | F1Score=0.9689\n",
      "Batch-500: NLLLoss=0.0722 | F1Score=0.9701\n",
      "Batch-518: NLLLoss=0.0237 | F1Score=0.9706\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.1371 | Mean F1Score: 0.9575\n",
      "Patience = 2/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528613f64cf5462fb2863fecf923f95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0032 | F1Score=0.9962\n",
      "Batch-100: NLLLoss=0.0239 | F1Score=0.9962\n",
      "Batch-150: NLLLoss=0.0078 | F1Score=0.9969\n",
      "Batch-200: NLLLoss=0.0160 | F1Score=0.9977\n",
      "Batch-250: NLLLoss=0.0064 | F1Score=0.9977\n",
      "Batch-300: NLLLoss=0.0068 | F1Score=0.9977\n",
      "Batch-350: NLLLoss=0.0051 | F1Score=0.9977\n",
      "Batch-400: NLLLoss=0.0138 | F1Score=0.9976\n",
      "Batch-450: NLLLoss=0.0026 | F1Score=0.9978\n",
      "Batch-500: NLLLoss=0.0020 | F1Score=0.9981\n",
      "Batch-518: NLLLoss=0.0034 | F1Score=0.9981\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0137 | Mean F1Score: 0.9974\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabfc49d5e1345b8b80404201f141cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0013 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0025 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0019 | F1Score=0.9997\n",
      "Batch-200: NLLLoss=0.0041 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0028 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0024 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0020 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0023 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0008 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0008 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0028 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f75f004c48547798c3d3208c4492402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0012 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0022 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0004 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0011 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0010 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0009 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0012 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0010 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0015 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0008 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0015 | Mean F1Score: 0.9997\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5805e9fcd7d246f780915674fc4d07f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0014 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0011 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0014 | F1Score=1.0000\n",
      "Batch-400: NLLLoss=0.0011 | F1Score=1.0000\n",
      "Batch-450: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0009 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d6fc3171e6437a8bf85ee51ad13e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0009 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0007 | F1Score=1.0000\n",
      "Batch-400: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-450: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0010 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0007 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0007 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a8cdef155d44c5a582cbddab1451e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0007 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-300: NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-350: NLLLoss=0.0003 | F1Score=1.0000\n",
      "Batch-400: NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-450: NLLLoss=0.0003 | F1Score=1.0000\n",
      "Batch-500: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-518: NLLLoss=0.0004 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0005 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dafb4aba75e4aad92441081b62649c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0003 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0003 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0017 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0004 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983d1a8649a94ac5932e50ef0ad3d9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0003 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0006 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0003 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0003 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0004 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0004\n",
      "Best F1Score      : 0.9998\n",
      "Training duration : 25.805 minutes.\n",
      "Training date     : 2022-10-11 11:02:37.520688+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4715198 , -2.9584742 , -1.0972756 , ..., -0.4662549 ,\n",
       "         2.6249635 , -2.5874336 ],\n",
       "       [ 0.85127926,  1.7937291 ,  4.750274  , ..., -3.7468984 ,\n",
       "        -1.171509  , -7.177794  ],\n",
       "       [ 0.01456787,  3.329747  ,  1.1640563 , ..., -0.02282718,\n",
       "        -1.2869803 , -7.4951777 ],\n",
       "       ...,\n",
       "       [ 2.600227  , -3.9608204 ,  7.38987   , ..., -1.7271055 ,\n",
       "         0.03943641, -0.7385575 ],\n",
       "       [ 1.9560866 ,  4.3672667 , -3.2608178 , ...,  0.4114646 ,\n",
       "         2.850926  ,  4.6069546 ],\n",
       "       [ 0.34141612,  2.7573419 ,  2.678247  , ..., -6.154591  ,\n",
       "         0.03715273, -5.356519  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.1006, -0.0576, -0.3218,  ...,  0.2054, -0.1937,  0.2880],\n",
       "                      [ 0.0631, -0.0566, -0.0370,  ..., -0.0865,  0.4971,  0.0767],\n",
       "                      [ 0.1092,  0.0548, -0.2442,  ..., -0.2460,  0.1316, -0.2307],\n",
       "                      ...,\n",
       "                      [-0.4107,  0.1587,  0.0656,  ..., -0.1546, -0.1822,  0.1971],\n",
       "                      [ 0.1522,  0.1379,  0.1523,  ..., -0.1279,  0.1987, -0.4938],\n",
       "                      [ 0.1795,  0.3778,  0.2182,  ...,  0.1095,  0.2764, -0.0416]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0071, -0.1847, -0.1341,  ...,  0.1078, -0.2394, -0.2015],\n",
       "                      [-0.0075, -0.1401,  0.0221,  ..., -0.1555,  0.0041,  0.1648],\n",
       "                      [-0.0872,  0.0102,  0.1674,  ..., -0.2750,  0.0664,  0.3029],\n",
       "                      ...,\n",
       "                      [-0.0301,  0.0770, -0.0984,  ...,  0.2642,  0.0699, -0.1529],\n",
       "                      [ 0.2279,  0.0298, -0.0287,  ..., -0.1719,  0.1562,  0.1142],\n",
       "                      [-0.1008, -0.2030, -0.2058,  ...,  0.0829, -0.1004,  0.0256]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-1.3592e-01, -8.1817e-02, -8.0482e-04, -3.0455e-02, -1.0205e-01,\n",
       "                      -1.5296e-01, -4.5418e-02,  1.0423e-01, -2.2659e-02,  1.7820e-01,\n",
       "                       7.7568e-02,  4.2735e-02,  3.4167e-02, -1.4417e-01, -6.6611e-03,\n",
       "                       7.4563e-02,  4.5856e-02,  7.4573e-03, -8.1220e-02, -2.0489e-03,\n",
       "                      -1.2926e-01,  4.0646e-02,  1.4049e-01,  1.2172e-01, -2.6975e-03,\n",
       "                      -7.1414e-02,  1.2225e-03, -6.4365e-02, -3.0012e-03,  2.9329e-02,\n",
       "                      -3.3962e-02, -7.6650e-02, -3.7687e-03, -9.2305e-02, -6.3192e-02,\n",
       "                       5.7424e-03, -1.0147e-02, -1.9436e-01,  9.6871e-02, -3.9945e-02,\n",
       "                       8.1530e-02,  1.1765e-01, -5.6470e-02, -3.5866e-02,  1.6033e-01,\n",
       "                      -2.5998e-03, -1.1191e-01,  4.5781e-02, -2.0323e-02,  2.3397e-02,\n",
       "                      -3.7903e-02,  3.1341e-02, -1.9626e-02, -2.1523e-02,  1.3937e-02,\n",
       "                       1.0427e-01,  2.0567e-02,  2.2580e-01,  4.8769e-03,  3.1530e-02,\n",
       "                      -4.5904e-02,  4.4619e-03,  3.6986e-02,  5.3807e-02,  2.4051e-02,\n",
       "                      -5.3377e-02,  7.7145e-02,  1.3737e-01,  4.8606e-02, -2.1843e-02,\n",
       "                      -1.3394e-01,  3.2375e-02,  6.0513e-02,  4.2125e-03, -2.5754e-01,\n",
       "                      -4.3132e-02, -7.7550e-02,  9.8902e-02,  1.0003e-01, -9.3017e-02,\n",
       "                       7.7183e-02,  1.1106e-01,  1.0024e-01,  3.0989e-02, -4.9110e-02,\n",
       "                      -1.0031e-01,  1.4635e-01,  4.2004e-02, -8.7578e-02, -4.7528e-02,\n",
       "                      -1.5649e-01, -5.9576e-02, -4.0052e-02,  4.9510e-02, -1.5853e-02,\n",
       "                       1.3747e-01,  3.6218e-02, -1.5761e-01,  6.8434e-02,  2.4882e-01,\n",
       "                       1.0977e-01,  1.8841e-02,  2.3775e-02,  4.1049e-02,  2.8789e-02,\n",
       "                       2.0233e-02, -6.0648e-04, -8.3204e-02,  5.3717e-02, -1.5857e-02,\n",
       "                       3.6699e-02,  4.7800e-04,  3.1738e-02, -1.9539e-02,  1.0031e-01,\n",
       "                       1.3764e-01, -6.4419e-02,  9.9648e-03, -9.1710e-03,  9.1524e-02,\n",
       "                       2.8453e-02,  4.8931e-02,  2.0322e-02,  1.5675e-01, -1.2780e-02,\n",
       "                      -1.6787e-01,  1.9934e-02, -1.6677e-01,  4.6597e-02,  2.8072e-02,\n",
       "                      -4.5693e-02, -3.9964e-02,  3.4942e-02, -5.5547e-02,  1.5111e-02,\n",
       "                       1.9196e-01,  1.6602e-02, -1.7849e-01, -6.2007e-03,  7.7386e-02,\n",
       "                      -4.7281e-02,  8.4507e-03, -7.7279e-02,  2.8950e-02, -3.1703e-02,\n",
       "                      -7.7657e-02, -8.2456e-02, -3.4511e-02, -1.7504e-02, -5.8323e-02,\n",
       "                      -8.2464e-02, -1.1275e-01,  8.6440e-02,  3.7290e-02,  8.5801e-02,\n",
       "                       8.4901e-02, -1.0021e-01,  8.4273e-02,  1.4811e-02, -9.4245e-02,\n",
       "                      -5.9109e-02, -2.6181e-02, -1.7650e-02,  6.7728e-02, -7.0100e-02,\n",
       "                      -1.5622e-02, -1.0606e-01, -8.3823e-02,  6.4935e-02, -2.2001e-02,\n",
       "                       1.3461e-01, -4.9819e-02, -2.1349e-02, -6.9834e-02, -4.6682e-02,\n",
       "                       3.5538e-02, -2.3727e-02,  1.8953e-02, -1.0418e-01, -1.8944e-01,\n",
       "                      -9.7179e-02,  3.2694e-02,  9.9266e-02,  3.9852e-02,  2.8116e-02,\n",
       "                       1.1579e-01,  7.1745e-03,  6.7671e-02,  1.5146e-01,  8.6801e-05,\n",
       "                       5.5227e-02, -3.7306e-03, -1.0654e-01, -4.9340e-02,  1.2767e-02,\n",
       "                       1.1954e-01, -1.5778e-01,  1.2928e-01,  8.7138e-03,  1.5439e-01,\n",
       "                       9.4369e-02, -2.0182e-01, -2.0824e-01, -7.3465e-03, -6.8881e-02,\n",
       "                       1.0185e-01,  2.0007e-01, -3.7446e-02, -6.6029e-02,  3.7064e-02,\n",
       "                       6.5168e-02, -4.9917e-03, -2.8299e-02, -1.0189e-01, -1.6228e-01,\n",
       "                      -1.0779e-02,  2.5188e-02, -6.4026e-02, -1.0190e-01,  8.3994e-03,\n",
       "                       2.6978e-02, -1.0340e-01,  3.9902e-02, -9.5308e-02,  7.7223e-03,\n",
       "                      -4.0743e-02,  5.0610e-03, -3.3085e-02,  7.8263e-02, -1.1415e-01,\n",
       "                       6.8833e-03, -9.9717e-02, -2.3042e-02, -2.2414e-03,  3.8796e-02,\n",
       "                       1.0557e-01,  1.7709e-03, -5.0628e-02,  1.8711e-01,  2.9662e-02,\n",
       "                       1.2940e-02,  1.5610e-02,  6.0606e-02,  5.2839e-02, -1.9655e-02,\n",
       "                       1.2782e-01, -1.3095e-02, -1.1476e-01, -5.9540e-02, -5.2587e-02,\n",
       "                       3.6955e-02,  9.8854e-02, -5.9217e-03, -8.2977e-02, -7.1469e-02,\n",
       "                      -1.2443e-01,  1.6055e-01, -9.8218e-03, -3.3835e-02, -2.8910e-02,\n",
       "                       2.6637e-02, -6.7582e-02, -1.0580e-02, -3.7185e-02, -3.5580e-03,\n",
       "                       6.1387e-02, -7.0654e-02, -2.7310e-02,  2.5563e-03,  7.1194e-02,\n",
       "                      -8.5842e-03, -1.5580e-02, -4.6102e-02, -5.6785e-02,  8.5612e-02,\n",
       "                      -7.6063e-02, -1.5083e-02,  5.5169e-03,  3.8140e-02,  1.6207e-02,\n",
       "                       6.4329e-02,  3.8781e-02, -1.9424e-02,  5.1206e-02,  1.3459e-01,\n",
       "                      -1.4336e-02,  2.9824e-02,  4.0627e-02, -7.0135e-02, -9.8691e-02,\n",
       "                       6.3197e-02,  7.2598e-03, -3.2455e-02, -5.7335e-02, -2.4107e-02,\n",
       "                       1.1242e-01, -4.1086e-02,  5.0237e-02,  9.1885e-02, -3.0616e-02,\n",
       "                      -7.0798e-02, -3.7770e-02,  7.9806e-03,  3.1616e-02,  9.5383e-02,\n",
       "                      -3.0678e-02,  1.9395e-02, -4.4577e-02, -2.0941e-02,  6.3206e-03,\n",
       "                       1.0753e-01, -5.8175e-02, -8.5367e-02, -9.7975e-02, -8.3444e-02,\n",
       "                      -6.4496e-02,  1.1859e-02,  5.6758e-02,  2.4773e-02,  2.7903e-02,\n",
       "                      -1.0053e-01, -2.6731e-02,  4.5553e-02,  1.1651e-02, -1.7278e-02,\n",
       "                      -9.4751e-02, -1.5698e-02,  5.4991e-02,  6.0211e-02,  2.4124e-02,\n",
       "                       2.9250e-02,  4.8086e-02,  7.6310e-02, -1.7161e-02, -4.2533e-02,\n",
       "                       8.0377e-02,  8.7157e-03,  4.1825e-02, -2.6807e-02, -7.5859e-02,\n",
       "                       5.1650e-02,  5.6793e-02, -6.4462e-02, -3.1403e-02,  2.9288e-02,\n",
       "                       1.7145e-02, -1.1892e-01,  8.3599e-03, -2.4167e-02,  1.0238e-02,\n",
       "                       9.8505e-03, -2.3967e-02,  1.3256e-02, -6.8648e-02, -8.3217e-03,\n",
       "                       7.9469e-02, -3.6355e-03,  2.0350e-02, -6.1983e-02,  1.6996e-02,\n",
       "                      -1.7902e-02,  1.6237e-02, -3.8690e-02,  5.0571e-02,  6.3352e-02,\n",
       "                       3.7803e-02, -4.2791e-02, -6.4812e-03,  6.6037e-02, -3.9295e-02,\n",
       "                      -8.4630e-03, -1.0932e-02, -5.0356e-02,  1.2628e-01,  1.7650e-02,\n",
       "                       1.9078e-02, -3.9150e-02, -7.6522e-02,  1.2234e-01,  7.1604e-02,\n",
       "                      -6.8955e-02,  1.7257e-01, -2.6609e-02, -2.9687e-02, -7.0780e-02,\n",
       "                       5.2264e-02,  5.2372e-02,  3.8584e-02, -1.6217e-02, -7.3878e-02,\n",
       "                       1.2947e-02,  7.4530e-02,  5.6366e-02,  5.4993e-02,  1.8832e-02,\n",
       "                       1.3651e-01,  1.1728e-01, -3.4713e-02,  1.0438e-01,  2.1935e-01,\n",
       "                       6.8924e-02,  4.5011e-02,  1.0002e-01,  2.4058e-02, -6.1186e-03,\n",
       "                      -2.1726e-01,  7.3591e-03,  1.9358e-01,  2.1383e-02, -6.0031e-02,\n",
       "                       2.9099e-02,  3.7130e-02,  3.3072e-02,  1.0449e-01,  3.5752e-02,\n",
       "                      -2.3440e-02, -4.8394e-02, -5.8264e-02, -1.2427e-01,  6.5653e-02,\n",
       "                       1.5760e-04, -6.3690e-02,  9.5714e-02,  7.9870e-02,  3.0067e-02,\n",
       "                       1.0619e-01, -1.4338e-01,  6.0761e-02,  2.1026e-01, -8.3272e-02,\n",
       "                       9.8576e-03,  2.2730e-01, -7.7852e-02,  1.1951e-01, -3.8779e-02,\n",
       "                       1.1361e-02, -8.6792e-03, -9.0922e-02, -1.4328e-01,  5.0874e-02,\n",
       "                      -3.3752e-02,  8.6612e-02, -8.0019e-03, -5.1169e-03,  3.6444e-02,\n",
       "                       8.2850e-02,  5.7763e-02,  2.4117e-03,  1.0723e-01,  1.1391e-01,\n",
       "                       1.3391e-01,  4.5542e-02,  5.4778e-02,  7.5428e-02,  1.2836e-01,\n",
       "                       6.2746e-02, -9.3263e-02,  1.3684e-03, -1.2351e-01, -2.9794e-02,\n",
       "                       1.0486e-02, -2.5287e-02,  2.4861e-02, -8.4598e-04,  1.6267e-01,\n",
       "                       1.7153e-01, -2.0978e-02,  2.6191e-02,  3.2313e-02, -7.5006e-02,\n",
       "                       7.9667e-02,  5.9262e-02, -3.8289e-02, -1.6840e-01,  3.3852e-02,\n",
       "                      -5.2016e-02,  1.9476e-01,  3.7712e-02,  1.1781e-01,  2.4256e-01,\n",
       "                      -1.3856e-03, -4.5351e-02,  4.3959e-02,  1.1214e-01,  4.7318e-02,\n",
       "                      -4.6005e-02,  1.2087e-01,  1.1327e-01, -4.1856e-02,  4.7073e-02,\n",
       "                       1.6582e-01, -1.0912e-01,  1.8916e-01,  1.9002e-02, -6.0216e-02,\n",
       "                      -4.1081e-02,  1.6292e-01,  3.7018e-02,  4.1176e-02,  5.2678e-02,\n",
       "                       6.7192e-02,  1.5305e-01,  2.1108e-02,  4.8191e-02, -1.1756e-01,\n",
       "                       7.8543e-03,  1.0524e-01,  1.5463e-01,  1.2684e-02, -2.9602e-03,\n",
       "                      -8.3434e-03, -1.9139e-02])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([ 3.7492e-02, -1.2219e-01, -1.8361e-02, -7.3086e-03,  1.6712e-01,\n",
       "                      -1.7144e-01,  1.3483e-02,  8.1073e-02,  1.6013e-02,  2.0260e-02,\n",
       "                       7.7555e-02, -7.7692e-02,  6.9346e-02,  4.7849e-02, -1.5195e-01,\n",
       "                      -1.1634e-02,  7.4940e-02, -3.6349e-02, -1.0653e-01, -8.8742e-02,\n",
       "                      -1.6793e-01, -1.3106e-01, -1.2603e-01,  6.1193e-04, -3.8415e-05,\n",
       "                      -1.9466e-03, -3.0898e-02, -7.4591e-02, -3.0364e-02,  8.5263e-02,\n",
       "                       1.1956e-01, -1.8349e-01,  3.1582e-02, -1.0623e-01, -1.0882e-01,\n",
       "                       1.9642e-02,  4.2220e-02, -2.0643e-02,  1.5515e-01,  3.7371e-02,\n",
       "                       1.1510e-01, -4.6511e-02, -1.2644e-01,  2.4569e-02,  6.8495e-02,\n",
       "                      -7.1358e-02,  2.9045e-02,  1.0050e-01, -2.0844e-01,  3.8817e-02,\n",
       "                      -6.9158e-03, -1.9436e-02, -9.1936e-02,  1.8650e-02, -2.8321e-02,\n",
       "                       4.6275e-02, -3.6671e-02,  7.0509e-02, -7.5351e-03, -3.0387e-02,\n",
       "                       1.5235e-01,  1.3885e-01,  3.2627e-02,  7.2157e-02,  6.2365e-02,\n",
       "                       3.1241e-02,  1.1812e-01,  5.3504e-02, -1.2525e-01,  9.8804e-03,\n",
       "                      -6.3737e-02, -1.0196e-01,  3.6655e-02,  1.4766e-01, -7.3548e-02,\n",
       "                       3.3446e-02,  8.1382e-02, -7.3095e-02,  1.5070e-02,  8.0877e-02,\n",
       "                       1.5065e-01,  7.3663e-03, -5.3449e-02, -2.1503e-02, -4.8105e-02,\n",
       "                       1.1933e-01,  6.4306e-02,  8.2988e-02, -3.2959e-02, -2.4102e-02,\n",
       "                      -7.3928e-02, -9.2096e-03,  9.9007e-02, -4.1032e-02, -8.6762e-03,\n",
       "                       1.6544e-01,  3.5864e-05, -7.3653e-02, -9.1678e-03,  1.2398e-01,\n",
       "                       9.1400e-02, -5.5887e-02, -1.4964e-02,  5.4653e-02,  2.7311e-02,\n",
       "                      -2.5203e-02,  7.1888e-02,  6.3560e-02,  6.9573e-02,  1.9980e-02,\n",
       "                      -6.2203e-03,  2.7950e-02, -3.3409e-02,  3.2843e-02,  2.5467e-02,\n",
       "                       1.0869e-02, -9.7342e-02, -1.2461e-01, -1.5513e-01, -2.8064e-02,\n",
       "                      -1.8796e-02,  3.9534e-02, -2.5086e-03,  7.7940e-02, -1.2181e-01,\n",
       "                      -5.6040e-02, -7.3845e-02, -1.3278e-01,  3.2493e-02, -3.6557e-02,\n",
       "                       2.0589e-02,  3.6977e-02, -1.3986e-01, -1.4117e-01, -7.6897e-02,\n",
       "                       5.4258e-02,  5.8169e-02, -1.4501e-01, -1.5811e-02,  7.8979e-03,\n",
       "                      -7.6394e-03,  1.9599e-01, -7.4464e-02, -4.4142e-02, -1.6209e-02,\n",
       "                      -1.3613e-01, -4.1142e-02, -6.2998e-02, -1.7620e-01, -8.4846e-02,\n",
       "                      -4.9214e-02, -2.9413e-02,  4.2022e-02,  1.9955e-02,  4.5505e-02,\n",
       "                      -2.0614e-02, -5.3925e-02,  1.4144e-01,  6.3843e-02, -1.0156e-02,\n",
       "                      -8.9329e-02, -1.4100e-01, -5.7526e-02,  7.2529e-02, -5.5171e-03,\n",
       "                      -1.1217e-01, -8.3154e-02, -4.7790e-02,  8.2104e-02,  4.9770e-02,\n",
       "                       3.2846e-02, -1.1217e-01, -1.3266e-01, -8.5654e-02,  4.5977e-02,\n",
       "                      -1.7857e-02,  1.8517e-02,  2.9758e-02, -1.6431e-01, -7.2516e-02,\n",
       "                       3.6934e-04,  3.5367e-02, -1.9491e-02,  8.6736e-03, -1.7361e-01,\n",
       "                      -2.4471e-02,  1.4459e-02,  6.4347e-02,  1.4180e-01, -7.1520e-02,\n",
       "                      -4.3898e-02, -8.6596e-02, -1.0753e-01,  6.7204e-02,  7.7977e-02,\n",
       "                       4.6270e-02, -3.3395e-02,  1.7941e-01, -3.2766e-02,  1.0652e-01,\n",
       "                       1.2320e-02, -1.2264e-01,  9.5963e-04,  1.6937e-01, -2.4309e-02,\n",
       "                       4.3224e-03,  2.1075e-01,  4.3380e-02, -9.3688e-03,  1.8290e-01,\n",
       "                       6.3334e-02, -2.0333e-02, -7.0047e-02,  8.9193e-03, -1.9411e-01,\n",
       "                      -1.4631e-01, -5.1172e-02,  8.4213e-02, -1.0872e-01, -2.9875e-02,\n",
       "                      -3.9363e-02, -1.6909e-02,  7.3941e-02, -1.1236e-01,  7.8528e-02,\n",
       "                       6.9682e-02,  3.3316e-02,  1.4633e-02, -1.7382e-02, -5.2904e-02,\n",
       "                      -5.4500e-02, -7.5221e-02,  2.5674e-02, -2.9413e-05, -4.3473e-02,\n",
       "                       6.3492e-02,  5.2262e-02, -8.9539e-02,  3.4732e-02,  1.0827e-01,\n",
       "                       4.2180e-02,  1.3519e-01, -4.3166e-02,  1.6916e-01, -4.8624e-02,\n",
       "                       9.3390e-02, -1.3767e-02,  7.2964e-02, -2.5916e-02, -1.4761e-01,\n",
       "                       3.9582e-02,  1.0904e-01,  4.6028e-02, -1.2256e-02,  3.0877e-02,\n",
       "                      -1.0297e-01, -7.0971e-03, -1.7323e-03,  4.0423e-02,  6.5574e-02,\n",
       "                       1.6703e-02,  3.2691e-02,  2.2810e-03,  5.2228e-02,  1.0576e-02,\n",
       "                      -7.1960e-02,  5.4708e-02,  5.8050e-02,  4.5739e-02,  3.7251e-02,\n",
       "                      -2.2330e-04,  9.5592e-02,  4.4736e-02, -1.1139e-02, -7.3665e-04,\n",
       "                       1.2137e-01, -8.5396e-02, -3.7673e-02, -5.2218e-02, -2.2113e-02,\n",
       "                      -2.4429e-02, -3.9813e-02, -1.9891e-02,  1.7781e-02, -1.6293e-01,\n",
       "                       3.9751e-02,  7.8022e-02, -8.7386e-02, -4.1873e-02,  7.7061e-03,\n",
       "                      -2.7917e-02,  3.4187e-02,  1.0950e-01,  9.3175e-02, -1.0750e-01,\n",
       "                       2.8648e-02, -7.4720e-02,  6.3026e-02, -9.5744e-02,  4.3371e-02,\n",
       "                      -3.9527e-02,  3.7128e-02,  7.5575e-02, -2.8358e-02,  1.4580e-02,\n",
       "                      -2.7811e-02, -7.3795e-02, -3.6387e-02, -4.1812e-02,  3.2929e-02,\n",
       "                       3.9774e-02,  1.7686e-02,  1.3247e-02, -5.1429e-02,  8.6386e-03,\n",
       "                       7.7589e-02,  5.8731e-03, -1.7255e-01, -5.9170e-02,  4.5168e-02,\n",
       "                       6.7084e-02,  5.6832e-02, -3.2408e-03,  7.4392e-02,  4.3138e-02,\n",
       "                      -4.9322e-02, -9.0362e-02,  4.0559e-03, -7.4884e-02, -5.8721e-02,\n",
       "                       1.8087e-02,  3.2350e-02, -3.5444e-04, -1.3466e-02,  9.3229e-02,\n",
       "                      -3.6010e-02, -4.8604e-05,  2.5319e-02,  6.7888e-02,  3.4919e-02,\n",
       "                       7.3291e-02,  1.0339e-02, -2.1296e-02,  5.7472e-02,  2.7758e-02,\n",
       "                      -5.3427e-02,  4.1014e-02,  3.0619e-03,  1.0921e-01,  7.9360e-02,\n",
       "                      -9.6414e-02, -1.1555e-01,  2.9185e-02,  1.0441e-01,  1.1723e-01,\n",
       "                      -2.4405e-02, -4.7414e-02,  1.8412e-02,  5.0017e-02, -1.7467e-02,\n",
       "                       3.5948e-02,  5.1700e-02, -1.1615e-02,  4.5801e-02, -1.3210e-03,\n",
       "                      -5.0613e-02, -2.3129e-03, -1.5920e-01, -6.0548e-02,  7.8077e-02,\n",
       "                      -7.5441e-02, -3.8170e-02, -1.0738e-02,  1.8040e-02,  6.6025e-02,\n",
       "                       8.7208e-02, -6.0374e-02,  6.0614e-02, -2.4165e-02, -2.9424e-02,\n",
       "                      -1.2215e-01, -5.2982e-02, -8.8812e-02, -3.8082e-02, -5.5300e-02,\n",
       "                      -2.4439e-01,  3.0300e-02,  1.7011e-01,  5.3886e-02, -6.6794e-02,\n",
       "                       1.2752e-01,  2.1016e-01,  9.3247e-02,  8.0720e-02,  1.0019e-01,\n",
       "                       1.9038e-01, -6.3694e-03, -2.7237e-02, -9.4127e-02,  2.2627e-01,\n",
       "                       1.0739e-01,  9.1800e-02,  7.3996e-02,  1.1195e-02,  2.3011e-02,\n",
       "                      -3.3188e-02,  2.5843e-03,  1.8195e-01,  7.8705e-03,  5.8677e-02,\n",
       "                       4.9414e-02,  3.0446e-02, -2.4895e-02, -5.2872e-02, -1.6191e-02,\n",
       "                       7.1900e-02, -2.6658e-02, -3.0176e-02, -6.3768e-02, -5.4855e-02,\n",
       "                      -1.8799e-02,  1.3276e-01,  1.7715e-01,  1.5539e-02,  9.9642e-02,\n",
       "                       6.1163e-02, -9.6189e-02, -4.7758e-03,  4.4520e-02, -2.6511e-02,\n",
       "                       1.5167e-02,  8.9943e-02, -1.8126e-01,  6.5874e-02,  3.1938e-02,\n",
       "                      -1.1501e-01,  1.9288e-01,  5.7555e-02,  5.6770e-02,  1.7341e-01,\n",
       "                      -4.7409e-04,  7.8778e-02,  2.8879e-02, -5.5391e-02,  3.8494e-02,\n",
       "                       1.4634e-01, -2.5121e-03, -1.1533e-02,  8.5405e-02,  3.0821e-01,\n",
       "                       3.4092e-02,  1.1605e-01,  6.2197e-03,  1.7595e-01, -1.0044e-02,\n",
       "                       5.6983e-02, -9.4926e-02, -9.5469e-02, -8.9009e-02,  3.5462e-02,\n",
       "                       3.7644e-02,  8.8563e-02,  1.1355e-01,  7.3808e-02,  1.6036e-01,\n",
       "                       5.3131e-02,  1.0942e-01,  3.6456e-02, -3.9163e-02, -1.1781e-01,\n",
       "                       9.8542e-02,  6.0321e-02, -6.5787e-02, -5.1617e-02, -1.6430e-02,\n",
       "                       6.3655e-02,  4.8303e-02,  1.0110e-01,  1.5158e-01,  1.1805e-02,\n",
       "                       2.9856e-02, -6.3265e-03,  7.2056e-02,  1.3357e-02,  4.1915e-02,\n",
       "                       1.1706e-02,  2.5746e-02,  1.6391e-03, -2.0919e-02, -9.6637e-02,\n",
       "                      -5.7927e-02, -2.7317e-02,  1.6660e-01, -1.1441e-02, -1.0660e-01,\n",
       "                       1.5852e-01,  2.1881e-01, -8.0225e-02,  8.4469e-03,  1.5564e-01,\n",
       "                      -1.9489e-02,  3.1479e-02, -3.0466e-02,  5.9819e-02, -1.0179e-01,\n",
       "                       3.4443e-02,  9.1693e-03,  6.5802e-02,  4.6162e-02, -1.1159e-01,\n",
       "                      -5.4891e-02,  7.4832e-03])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.3852, -0.1142,  0.0966,  ...,  0.1551,  0.4597, -0.3868],\n",
       "                      [ 0.2727,  0.3136, -0.1165,  ..., -0.2451,  0.1058, -0.0735],\n",
       "                      [-0.0552, -0.3744, -0.2751,  ...,  0.2615,  0.4729, -0.3240],\n",
       "                      ...,\n",
       "                      [ 0.2538, -0.0637, -0.4466,  ..., -0.4676,  0.4484, -0.2611],\n",
       "                      [-0.2137,  0.2220, -0.0381,  ..., -0.6029,  0.3180, -0.3980],\n",
       "                      [-0.2143,  0.0239, -0.0360,  ...,  0.0180,  0.1781, -0.0451]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.2481, -0.1453, -0.0477,  ..., -0.0530,  0.0991, -0.1170],\n",
       "                      [ 0.0228,  0.3891,  0.1762,  ...,  0.0744, -0.1721,  0.0637],\n",
       "                      [ 0.1668,  0.2194,  0.1448,  ..., -0.0602,  0.3652, -0.1921],\n",
       "                      ...,\n",
       "                      [-0.3221,  0.1990, -0.0545,  ...,  0.1362, -0.0436, -0.0717],\n",
       "                      [ 0.1688,  0.1050,  0.1301,  ...,  0.0295,  0.1931,  0.0713],\n",
       "                      [-0.1720, -0.1047, -0.0239,  ..., -0.0172, -0.0641,  0.1758]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([-1.4937e-01,  1.3930e-01,  1.3730e-02, -1.0780e-02, -1.0725e-01,\n",
       "                       1.5208e-01,  9.2787e-02,  1.0131e-01,  3.5473e-02,  3.0602e-02,\n",
       "                       9.4644e-02,  6.2357e-02,  1.4688e-01,  1.1212e-01, -3.3638e-02,\n",
       "                       3.0227e-02,  1.2518e-02,  6.7151e-02, -9.0357e-02,  1.2150e-01,\n",
       "                      -1.1326e-01,  1.3458e-01, -2.8846e-02,  2.2655e-01,  1.7550e-02,\n",
       "                       1.1129e-01,  2.1224e-02,  4.0956e-02, -1.0276e-01, -9.2421e-02,\n",
       "                       1.0399e-01,  8.0016e-02,  6.2463e-02, -7.5454e-02,  2.6643e-02,\n",
       "                      -3.0404e-02,  2.4517e-02,  1.2033e-01,  1.5274e-01,  7.0549e-02,\n",
       "                      -1.7421e-02, -2.1722e-02, -3.7796e-02,  7.1341e-02,  4.8205e-02,\n",
       "                       1.2340e-01,  8.7871e-03,  6.6209e-02,  5.6092e-02, -3.5351e-02,\n",
       "                       8.8066e-02,  1.0069e-01, -1.1529e-02, -6.3156e-02,  4.3753e-02,\n",
       "                       1.9632e-01,  3.1449e-02, -8.7494e-02,  2.8836e-01,  1.4454e-01,\n",
       "                      -1.1540e-01, -1.2887e-03, -9.6280e-03, -1.7972e-02,  1.7739e-01,\n",
       "                       2.3454e-02, -4.5760e-02,  1.9085e-01,  2.5856e-02,  5.5055e-02,\n",
       "                      -1.5974e-01,  6.6799e-02,  2.0841e-02,  2.0865e-01,  1.0263e-01,\n",
       "                      -1.6736e-02,  3.0185e-02,  6.4970e-02,  2.6087e-02,  5.0992e-02,\n",
       "                      -1.3337e-02,  6.4448e-02,  1.1702e-01,  4.2487e-02,  5.2728e-02,\n",
       "                       5.5899e-03, -8.5283e-02, -1.9691e-02, -5.7556e-03,  1.1833e-01,\n",
       "                       5.1141e-02,  6.0264e-02,  7.5483e-02, -4.1770e-02,  1.4940e-01,\n",
       "                      -2.5182e-02,  9.0070e-02,  1.9632e-01, -4.7440e-03,  5.5539e-02,\n",
       "                       8.1261e-02,  1.8642e-02, -6.5102e-02,  1.0355e-01, -6.0412e-02,\n",
       "                       1.0869e-01, -2.8180e-02,  7.7875e-02,  8.5387e-03,  7.1974e-02,\n",
       "                      -5.8415e-02,  1.9898e-02, -1.0529e-02,  3.7146e-02, -8.4099e-02,\n",
       "                       1.0586e-01,  9.7425e-02,  1.1312e-02, -6.1593e-02, -3.6728e-02,\n",
       "                       2.4327e-01,  5.1071e-02,  2.0599e-02,  8.7938e-02,  1.5909e-01,\n",
       "                       7.3305e-02,  5.7544e-02, -1.8464e-02,  6.7277e-02,  3.9336e-02,\n",
       "                      -3.0016e-02, -6.7855e-02,  1.9146e-02, -3.0974e-02, -6.7778e-02,\n",
       "                      -1.2278e-01,  6.7902e-02,  4.3515e-02,  1.3092e-02, -1.3831e-01,\n",
       "                       3.9511e-02, -7.7704e-02,  4.5023e-02, -7.3879e-02, -2.1105e-02,\n",
       "                      -1.3031e-01, -3.8193e-02, -1.6557e-01,  2.1921e-03, -7.3594e-02,\n",
       "                      -3.6731e-02, -9.8030e-02, -9.4979e-02, -1.1021e-02,  7.3880e-02,\n",
       "                       8.7724e-03, -2.6869e-02, -2.5453e-02,  3.4635e-02, -1.5830e-01,\n",
       "                       9.1513e-02, -1.1450e-01, -3.6245e-02, -9.0155e-02, -2.5747e-02,\n",
       "                      -2.3596e-02, -7.2217e-02, -4.6071e-02, -6.1210e-02, -1.2208e-01,\n",
       "                      -1.1656e-02,  5.2587e-02,  6.2919e-02,  6.8232e-02, -1.2718e-01,\n",
       "                      -2.3501e-02,  3.7866e-02, -1.7273e-02, -1.3053e-01, -1.4262e-02,\n",
       "                       2.7571e-02, -4.2133e-02, -1.6466e-02, -6.6064e-03, -2.5444e-01,\n",
       "                       3.5216e-02,  3.8197e-02,  6.2854e-04, -1.1025e-01, -1.2080e-01,\n",
       "                       3.3948e-03, -4.5524e-02,  1.1169e-01,  3.5986e-02, -1.3347e-01,\n",
       "                       1.5090e-02,  1.6307e-01, -2.6515e-02, -6.9923e-02, -4.5617e-02,\n",
       "                      -8.1709e-03, -3.3055e-02, -1.1831e-01,  2.0065e-01,  2.2243e-02,\n",
       "                      -1.3733e-01, -7.1536e-02, -1.0765e-01,  2.9978e-02,  1.5694e-04,\n",
       "                       4.5794e-02, -6.5262e-02, -1.1688e-01, -4.3574e-03, -2.2061e-02,\n",
       "                      -5.8216e-03, -1.3239e-01,  7.1249e-02,  9.2186e-02,  3.7167e-02,\n",
       "                      -7.3363e-02,  4.7873e-02, -2.2907e-01,  8.2525e-02, -6.3048e-02,\n",
       "                      -1.1956e-01, -7.8638e-03, -6.0401e-02,  8.1250e-02,  2.8434e-03,\n",
       "                       1.1158e-01, -7.2567e-02,  1.9338e-02,  6.3101e-02, -8.6515e-02,\n",
       "                      -1.1275e-01, -5.4329e-02,  1.1016e-02,  6.1033e-02, -1.4680e-01,\n",
       "                       3.0954e-02, -1.0452e-02, -1.5503e-01, -5.1713e-02, -7.7405e-02,\n",
       "                       7.9487e-02, -1.2705e-02, -9.5078e-02, -2.3335e-02,  7.5960e-02,\n",
       "                       5.3136e-02, -1.3894e-02, -3.0181e-02,  9.4138e-02,  1.8776e-02,\n",
       "                      -1.5794e-01, -3.4323e-02,  3.9274e-02, -3.7522e-03, -3.3851e-02,\n",
       "                      -4.9326e-03, -3.1815e-02, -6.3420e-02,  6.9086e-02, -6.1856e-02,\n",
       "                      -8.8016e-02, -9.7712e-03,  1.9659e-02,  6.1600e-02,  1.1250e-02,\n",
       "                      -1.4941e-02, -4.6048e-02, -1.2775e-01, -2.9508e-02, -6.7132e-02,\n",
       "                      -3.0603e-02, -2.0226e-02,  9.4057e-02, -3.1333e-02, -3.3003e-02,\n",
       "                       6.5242e-02, -8.7755e-02, -1.6202e-02, -5.9948e-02, -2.4295e-02,\n",
       "                      -1.0750e-01, -5.7462e-02,  7.7604e-02, -2.4713e-02,  9.3596e-03,\n",
       "                      -8.7095e-02, -7.5695e-02,  3.8881e-02, -1.2161e-02,  5.4498e-02,\n",
       "                      -7.5698e-02,  7.0939e-02, -2.1403e-02,  1.0540e-01,  4.2022e-02,\n",
       "                      -1.3761e-03, -4.8765e-02, -6.0827e-02,  6.8478e-03, -5.5118e-02,\n",
       "                       1.6675e-02, -1.2110e-01,  3.8270e-02,  6.3885e-02, -1.4080e-03,\n",
       "                       4.1611e-02, -1.1742e-02, -5.1128e-02, -5.2887e-02, -5.7720e-02,\n",
       "                       3.9956e-02,  1.4100e-01, -2.6130e-03,  7.7263e-02, -7.5793e-02,\n",
       "                       1.9987e-02, -1.5078e-02, -3.4081e-02, -2.5398e-02, -8.3766e-02,\n",
       "                       3.4521e-02, -1.1799e-01, -7.0460e-04,  4.5947e-02,  1.1622e-02,\n",
       "                       5.4451e-02,  2.5542e-02, -3.0556e-03,  6.1884e-02, -5.1048e-02,\n",
       "                      -9.9135e-03,  6.5453e-02,  5.3395e-02,  2.6336e-02, -9.1335e-02,\n",
       "                      -1.9591e-02, -3.1239e-02,  1.8654e-02, -1.7414e-03,  1.1302e-01,\n",
       "                      -2.1021e-03, -1.0460e-01,  5.1645e-02,  8.4503e-02, -5.9248e-02,\n",
       "                       8.0367e-03, -1.1078e-01,  2.3544e-02,  2.1837e-02, -2.1298e-02,\n",
       "                      -4.9659e-03,  2.5978e-02,  4.9160e-02, -3.0371e-02,  1.8565e-02,\n",
       "                       3.1170e-02,  1.9676e-02,  2.1443e-01, -3.6692e-02,  2.2159e-02,\n",
       "                      -4.7883e-02, -1.5553e-02, -6.6436e-02, -3.7620e-02, -1.9983e-02,\n",
       "                      -1.8991e-02, -9.0927e-02, -5.9501e-03,  6.0635e-02,  5.9006e-02,\n",
       "                      -8.6663e-02, -4.8225e-02,  4.7825e-02,  7.5859e-04,  1.1321e-02,\n",
       "                      -9.9107e-02,  1.2261e-02, -1.4093e-01, -3.2660e-02, -1.7714e-02,\n",
       "                       1.3389e-01, -2.9833e-02,  8.0875e-02,  2.3812e-02,  4.9298e-02,\n",
       "                      -8.1353e-02,  7.8780e-02, -2.9157e-02,  6.4796e-02,  5.2644e-02,\n",
       "                      -6.3887e-02,  9.4892e-02, -2.1838e-02, -9.0207e-03, -1.5596e-01,\n",
       "                       5.2938e-03,  6.1121e-03,  5.1481e-02, -1.8560e-02,  9.2248e-02,\n",
       "                      -2.4352e-02, -1.4886e-02,  1.0700e-01, -2.9055e-02,  4.8010e-02,\n",
       "                       1.5081e-01,  1.9858e-02, -5.9044e-02, -1.8767e-02, -2.3744e-02,\n",
       "                       5.6098e-02,  6.1600e-02,  7.9390e-02,  1.4361e-02, -8.6188e-04,\n",
       "                       1.1950e-01,  9.7005e-02, -1.1386e-02,  2.0034e-01,  1.2253e-02,\n",
       "                       5.0247e-02,  1.4426e-01,  3.8457e-02,  1.6078e-01, -2.7381e-02,\n",
       "                       1.5261e-01,  1.3968e-01,  4.3067e-02, -7.1760e-02,  1.1432e-01,\n",
       "                       1.0317e-01,  1.1569e-01, -4.3215e-02, -4.8721e-02,  1.0671e-01,\n",
       "                      -7.8300e-02, -5.6990e-02,  1.1770e-01, -5.3449e-02, -1.8010e-01,\n",
       "                      -1.0615e-01, -6.9823e-02, -1.8742e-02, -3.4134e-02,  1.6941e-01,\n",
       "                       7.6220e-03,  1.4467e-01, -8.1582e-03,  7.4499e-02, -1.1278e-01,\n",
       "                      -2.5120e-04, -4.4903e-02,  1.1295e-01,  9.5626e-03,  1.9384e-02,\n",
       "                      -1.2410e-02,  1.5658e-01,  3.0539e-03,  8.5817e-02,  6.0777e-03,\n",
       "                       8.9827e-02, -9.5018e-03, -7.6426e-02,  4.6308e-02, -3.2219e-02,\n",
       "                       2.8205e-02,  1.1754e-02,  4.3164e-02,  1.0675e-01,  1.8629e-01,\n",
       "                       7.4598e-02,  2.0242e-02, -8.3691e-03,  7.1336e-02, -1.5909e-01,\n",
       "                       1.2384e-01,  2.6039e-01, -1.7211e-01, -1.2010e-02,  1.5220e-02,\n",
       "                      -8.5361e-02, -5.7340e-02,  5.9978e-02,  8.1552e-02,  1.1840e-01,\n",
       "                       8.9326e-02,  1.3037e-01, -6.7833e-02,  3.8173e-02,  2.2336e-01,\n",
       "                       1.1700e-02,  1.6876e-01,  7.8265e-02, -5.8970e-03,  1.5049e-01,\n",
       "                      -4.0812e-02,  1.6858e-01,  8.8974e-02,  1.8534e-02,  3.2522e-01,\n",
       "                       5.6846e-02,  2.2561e-02,  3.8041e-02,  2.0173e-01,  1.5493e-01,\n",
       "                      -4.5438e-02,  1.6695e-02])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-1.0680e-01,  1.0593e-01, -3.6674e-02,  8.2107e-03,  9.9130e-02,\n",
       "                       9.5060e-02,  3.2068e-02, -2.7025e-02, -9.0055e-02, -1.1360e-01,\n",
       "                       5.7172e-02, -8.9133e-02,  1.1192e-01,  7.3284e-02, -5.0927e-03,\n",
       "                      -2.4301e-02, -2.7195e-02, -2.0044e-02,  9.2695e-02,  4.0809e-02,\n",
       "                      -1.9862e-02,  9.6172e-02, -1.6045e-02,  1.2270e-02, -6.9819e-03,\n",
       "                      -8.5199e-03, -4.8517e-02, -7.1342e-02, -1.0978e-02, -1.1859e-01,\n",
       "                       1.4934e-01,  2.5588e-01,  1.4917e-01, -7.2475e-02, -9.5070e-02,\n",
       "                      -2.0804e-02, -5.3265e-02,  1.7768e-01,  1.6425e-01,  8.2003e-02,\n",
       "                       6.7380e-02,  3.8016e-02,  4.4537e-02, -3.4439e-02,  5.4529e-02,\n",
       "                       1.3003e-01,  4.2647e-02,  1.2196e-01,  8.7660e-02, -5.8573e-02,\n",
       "                       8.5001e-02,  4.0066e-03,  4.1757e-02, -1.9214e-02, -8.6747e-02,\n",
       "                       1.9750e-01, -1.7232e-01, -1.1718e-01,  1.7477e-01,  1.5627e-01,\n",
       "                      -1.5850e-01,  6.0413e-02, -3.1479e-02, -3.7107e-02,  1.4604e-01,\n",
       "                       6.6796e-02, -4.2806e-02,  5.9885e-02,  1.3054e-02,  8.5808e-04,\n",
       "                       2.4961e-03, -1.8831e-02, -8.5848e-02,  8.1870e-02, -4.0698e-02,\n",
       "                       7.1711e-02, -2.2095e-02,  8.8711e-02,  2.8026e-02,  8.8141e-02,\n",
       "                      -3.3685e-02, -6.0019e-02,  1.1806e-02,  2.5351e-02,  1.4925e-01,\n",
       "                       2.9300e-02, -4.0711e-02,  7.3507e-02, -2.9554e-02,  1.6223e-01,\n",
       "                       1.5152e-01,  1.2783e-01,  7.8733e-02, -6.3358e-02, -3.8905e-02,\n",
       "                      -6.7364e-02, -9.9450e-03,  1.4875e-01, -4.5124e-02,  1.3025e-01,\n",
       "                       6.6075e-02,  1.0751e-01, -7.7789e-03,  1.8376e-01,  1.2383e-02,\n",
       "                      -2.5157e-02,  6.6946e-02,  1.3141e-01,  1.8069e-02,  2.0871e-03,\n",
       "                      -7.9372e-03, -1.9446e-02,  4.4679e-02,  7.1173e-02, -8.0386e-02,\n",
       "                       1.3041e-01,  4.7983e-02,  3.8339e-02,  5.4510e-02,  6.5571e-02,\n",
       "                       2.0481e-01, -2.2252e-03,  1.0991e-01,  1.4348e-01,  2.3768e-01,\n",
       "                       4.7090e-02,  1.6623e-02,  2.7529e-02, -4.5049e-03, -5.8963e-02,\n",
       "                      -1.4082e-02, -2.4237e-02,  2.3615e-03, -8.0277e-02, -6.1519e-02,\n",
       "                       1.7013e-02, -5.5448e-02,  5.1391e-02,  3.8569e-02, -6.2418e-02,\n",
       "                      -1.4349e-01, -1.6974e-01, -1.8218e-01, -1.8758e-01,  8.6952e-03,\n",
       "                      -1.4837e-01,  6.2775e-02, -2.0641e-02, -6.5409e-02, -3.4752e-02,\n",
       "                       1.5371e-02,  1.8707e-02, -2.0765e-01,  4.2226e-02, -2.9977e-02,\n",
       "                      -1.9691e-01, -1.5772e-01, -4.2262e-02,  1.1674e-02, -1.2033e-01,\n",
       "                      -9.1465e-02,  1.1083e-01, -2.0770e-01, -1.2909e-01,  2.8393e-03,\n",
       "                      -3.4075e-02, -1.0901e-01,  1.2546e-01, -1.3272e-02, -6.9572e-02,\n",
       "                       2.5333e-02, -5.2252e-02,  6.9855e-02, -7.1362e-02,  1.8845e-02,\n",
       "                      -9.8604e-02, -6.5161e-02, -1.0767e-01, -1.4090e-01, -4.0435e-02,\n",
       "                       1.0784e-01, -8.4159e-02, -1.6401e-01, -1.5603e-02, -6.8218e-02,\n",
       "                       9.6016e-02, -7.0380e-02,  9.6023e-02, -9.2707e-02, -1.1450e-01,\n",
       "                      -7.9237e-02, -1.8049e-04,  5.4879e-02, -4.3577e-02, -3.0594e-02,\n",
       "                      -7.4092e-02,  7.3818e-02, -8.7840e-03, -1.0947e-01, -5.4406e-02,\n",
       "                      -1.0887e-01, -2.0679e-02, -1.2700e-01,  6.3431e-03, -1.3780e-02,\n",
       "                      -8.0903e-02,  6.6086e-02,  6.0296e-02, -5.2175e-02, -7.3455e-02,\n",
       "                      -8.0813e-02, -1.0259e-01, -1.6010e-01, -1.4785e-01, -2.0484e-02,\n",
       "                       1.6713e-02,  3.2553e-02,  1.0914e-01, -1.0745e-01, -1.2279e-02,\n",
       "                      -3.2660e-02, -2.0735e-02, -3.9673e-02, -7.3189e-02, -5.9216e-02,\n",
       "                      -8.0899e-02, -6.0537e-02, -1.2281e-01,  8.2209e-02, -1.1706e-01,\n",
       "                      -5.8375e-02, -7.3835e-02, -3.0871e-02,  1.1571e-01,  5.2536e-02,\n",
       "                      -8.6910e-02,  3.9874e-02, -9.6593e-02, -7.4733e-02, -1.6406e-01,\n",
       "                      -7.5525e-02, -4.4662e-02, -3.0091e-03,  1.0841e-03, -1.2203e-01,\n",
       "                       5.7062e-02,  9.8129e-02, -6.8587e-02, -8.2202e-02,  2.0834e-02,\n",
       "                      -1.0287e-01, -9.8426e-02, -1.5113e-01,  5.0931e-02,  1.3884e-01,\n",
       "                      -1.4924e-01, -1.7353e-02, -1.4772e-02,  6.7495e-02,  3.6676e-02,\n",
       "                      -2.2016e-02,  1.2954e-01,  1.5355e-01, -5.1983e-02,  1.1017e-01,\n",
       "                       2.7002e-02,  5.7178e-02, -9.6005e-02, -5.0427e-02,  4.6443e-02,\n",
       "                       5.0953e-02, -6.5178e-02, -2.0976e-02,  6.4553e-02,  5.3638e-02,\n",
       "                      -1.0286e-01, -5.5817e-02, -1.4209e-02, -2.3176e-02,  1.7826e-02,\n",
       "                       4.8978e-02,  5.6233e-02,  3.5477e-02, -3.0383e-02, -3.2442e-02,\n",
       "                      -1.3826e-02,  8.9770e-02, -5.5231e-02,  1.9590e-02,  1.4334e-01,\n",
       "                      -8.8573e-02, -2.0358e-02, -7.8205e-02,  7.5570e-02, -6.5824e-02,\n",
       "                       3.7679e-02, -4.2077e-02, -1.2167e-02,  3.9757e-02,  5.3586e-02,\n",
       "                       1.1477e-01, -2.8172e-02,  2.3752e-02, -7.7306e-02, -2.4689e-02,\n",
       "                       6.2859e-02,  8.8491e-02,  1.1972e-01,  9.1159e-02, -1.6202e-02,\n",
       "                      -3.7164e-02,  1.6001e-02,  9.0927e-02, -1.0571e-03,  7.8658e-02,\n",
       "                      -1.2188e-02, -1.0126e-01, -4.6605e-02,  1.6887e-02, -2.0950e-03,\n",
       "                       3.9294e-02,  3.9819e-02, -4.2736e-02,  1.4537e-02, -4.4769e-02,\n",
       "                      -9.9974e-02,  4.6265e-02,  1.0090e-02,  4.2160e-02,  4.4186e-02,\n",
       "                      -1.6507e-02, -5.6889e-02,  4.3867e-02,  2.0025e-02,  3.5217e-02,\n",
       "                       8.2316e-02,  2.5667e-02, -2.3892e-02,  6.3742e-02, -4.7306e-03,\n",
       "                      -7.0566e-02, -8.9654e-02,  2.4431e-02, -5.5139e-02,  6.7165e-02,\n",
       "                       1.0662e-01, -4.6116e-02,  1.1365e-02, -2.4800e-02,  2.4644e-02,\n",
       "                      -8.6724e-04,  1.8032e-02,  1.4168e-02,  5.1206e-02,  3.4283e-04,\n",
       "                      -2.1327e-02, -4.6973e-02, -9.0798e-03, -7.4902e-02,  3.2161e-02,\n",
       "                      -1.1410e-01, -5.4018e-02,  1.9350e-02,  6.1566e-03,  7.5313e-03,\n",
       "                       6.2735e-02,  4.4995e-02,  8.1074e-03,  9.1445e-02,  2.8860e-03,\n",
       "                      -2.8846e-02,  3.4327e-02, -1.2229e-02,  2.2915e-02, -1.2161e-01,\n",
       "                       7.9832e-03,  9.7322e-03, -1.7928e-02, -5.8356e-02, -1.2074e-01,\n",
       "                      -9.8417e-02,  7.4146e-02, -1.1834e-01, -3.3571e-02,  1.6379e-03,\n",
       "                       1.6150e-01,  2.1962e-01,  1.3088e-01,  1.3247e-01,  8.3118e-02,\n",
       "                      -3.8579e-03,  1.8065e-01,  1.3847e-01,  1.0623e-01,  5.7067e-02,\n",
       "                      -6.4973e-02,  2.0739e-01, -1.4187e-02,  1.4583e-02,  1.6274e-02,\n",
       "                       1.0912e-01,  1.0456e-01,  9.6414e-04, -4.7329e-02,  1.1736e-01,\n",
       "                       4.0754e-02, -9.2091e-02, -1.4763e-02, -9.7136e-02,  3.2030e-02,\n",
       "                       1.2493e-01,  8.9932e-02, -2.6064e-02, -9.5335e-02, -6.8366e-02,\n",
       "                       2.2593e-01, -8.1773e-02,  1.7367e-01, -1.5301e-02, -9.3238e-02,\n",
       "                       4.1425e-02,  5.6524e-02, -6.7847e-03,  1.8546e-01,  9.7299e-02,\n",
       "                       8.8251e-02,  4.9329e-02,  1.0120e-01,  1.9510e-01,  2.1404e-02,\n",
       "                       1.6908e-01,  7.8980e-02,  5.4559e-02, -5.0781e-02,  7.6279e-02,\n",
       "                       1.3386e-01,  1.4617e-01, -4.9316e-02,  1.2041e-01,  2.1508e-01,\n",
       "                      -6.2914e-02,  3.0143e-02,  4.7748e-02,  6.4780e-02, -9.3829e-02,\n",
       "                       2.7798e-03, -3.2925e-02,  6.7996e-02,  1.4423e-01,  7.2478e-02,\n",
       "                       8.8777e-02,  1.5218e-01,  1.1599e-01,  9.8911e-02, -1.9582e-02,\n",
       "                       4.6251e-02,  1.3736e-03,  5.1261e-02,  6.7187e-02,  4.0046e-03,\n",
       "                      -6.3049e-03,  5.5393e-02,  7.6931e-02,  6.5145e-03, -3.7809e-02,\n",
       "                      -2.3891e-02, -6.8355e-02, -1.0678e-01,  7.7136e-02,  3.0689e-02,\n",
       "                      -8.1552e-02, -2.3815e-03, -5.4033e-02,  1.3707e-01,  6.6814e-02,\n",
       "                       1.5436e-01,  4.8463e-02,  6.6779e-02,  1.3914e-01, -7.9705e-02,\n",
       "                       1.4130e-01,  3.5379e-02,  4.7526e-02,  1.1134e-01,  1.2490e-01,\n",
       "                       7.8741e-02, -1.1812e-01,  2.5055e-01, -3.0661e-02,  1.1383e-01,\n",
       "                       6.8890e-02, -1.2259e-02, -1.0741e-02, -1.4282e-02,  1.0238e-01,\n",
       "                      -6.9445e-02, -1.5459e-02,  5.1663e-02,  2.8757e-02,  1.1078e-01,\n",
       "                       1.7495e-02,  1.3041e-01,  1.6412e-02,  8.8380e-02,  3.6540e-01,\n",
       "                      -4.7816e-02,  1.5318e-01,  1.3116e-01,  1.8793e-01,  8.4855e-02,\n",
       "                       2.1211e-01,  1.9820e-02])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.3165,  0.0544,  0.2054,  ..., -0.3866,  0.3734,  0.1335],\n",
       "                      [-0.0195,  0.5149, -0.0985,  ...,  0.1523, -0.2629, -0.5118],\n",
       "                      [-0.0927,  0.2841,  0.1537,  ...,  0.9156,  0.1275,  0.3386],\n",
       "                      ...,\n",
       "                      [-0.1348,  0.0813,  0.1340,  ..., -0.2663, -0.3241,  0.1136],\n",
       "                      [ 0.1075,  0.0343, -0.0965,  ..., -0.2740,  0.1428, -0.1190],\n",
       "                      [-0.4199, -0.3599, -0.2312,  ..., -0.1476, -0.3695, -0.3922]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.1845,  0.2244, -0.3396,  ...,  0.1072, -0.2763,  0.0869],\n",
       "                      [-0.1708, -0.0570,  0.1829,  ..., -0.1521, -0.0997,  0.1430],\n",
       "                      [-0.0544,  0.1308, -0.1127,  ..., -0.0152, -0.4124,  0.2686],\n",
       "                      ...,\n",
       "                      [-0.2404,  0.1747, -0.3072,  ..., -0.0768,  0.0587, -0.0205],\n",
       "                      [-0.0813, -0.1101, -0.1936,  ..., -0.0604, -0.1334, -0.0091],\n",
       "                      [ 0.0251, -0.4268,  0.0710,  ..., -0.1041,  0.0949,  0.2591]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-1.8076e-02, -4.3854e-02, -9.0162e-02, -1.4372e-01, -2.1097e-01,\n",
       "                      -1.8761e-01, -1.5230e-01,  8.1704e-02, -3.5404e-02, -1.0697e-01,\n",
       "                       1.6370e-02, -7.0762e-02, -6.5904e-02, -1.0538e-01,  5.3356e-02,\n",
       "                      -7.7903e-02, -1.0384e-01, -4.7418e-02, -2.6183e-02, -1.4293e-01,\n",
       "                      -6.4782e-03, -1.4606e-01,  6.2688e-03, -5.8157e-02, -3.1576e-02,\n",
       "                      -1.3976e-01, -1.2810e-01, -1.3079e-01, -1.2551e-01, -1.3760e-01,\n",
       "                      -1.7963e-01, -1.5812e-01, -2.1050e-01, -1.6730e-01, -4.3867e-02,\n",
       "                      -2.9945e-02, -4.3126e-02,  1.5656e-02, -4.8734e-02, -4.4844e-02,\n",
       "                      -5.1657e-02, -1.3870e-01, -2.9929e-02, -1.9075e-01, -7.4311e-02,\n",
       "                      -1.3472e-02, -1.0096e-01, -1.3622e-01, -2.4651e-01, -3.1183e-02,\n",
       "                       7.8209e-02, -8.8440e-02, -7.6029e-02, -1.1279e-01, -7.8965e-02,\n",
       "                      -9.1262e-02, -1.4306e-01, -3.3703e-02, -1.2549e-01, -4.6791e-02,\n",
       "                      -1.5014e-01, -1.6424e-01, -9.5400e-02, -1.1683e-01, -7.2046e-02,\n",
       "                      -5.5200e-02, -9.4639e-02, -2.4472e-02, -7.6969e-02, -1.7174e-01,\n",
       "                      -9.0718e-02, -1.3815e-01, -7.8463e-02, -7.4523e-02,  4.5848e-02,\n",
       "                      -1.3848e-01, -8.3706e-02, -1.5871e-01,  3.0156e-02, -9.7164e-02,\n",
       "                      -1.0353e-01,  1.3120e-03, -1.0553e-01,  9.0772e-03, -5.9702e-02,\n",
       "                      -1.0385e-01, -1.3179e-01, -1.8351e-01, -1.3662e-01, -5.2395e-02,\n",
       "                      -1.4857e-02,  2.2467e-02,  1.3415e-02, -7.3169e-03,  1.0174e-02,\n",
       "                      -1.0940e-01, -3.6120e-02, -8.7117e-02, -5.8501e-02,  8.6438e-02,\n",
       "                      -1.0410e-01, -2.7737e-01, -5.9550e-02, -8.4314e-02, -8.5680e-02,\n",
       "                      -1.6275e-01, -2.0509e-01, -5.5371e-02, -2.5819e-01, -2.9355e-02,\n",
       "                      -5.5485e-02, -5.0306e-02, -1.0973e-01, -7.0185e-02, -8.8257e-02,\n",
       "                      -7.3669e-02, -1.5088e-02,  6.9974e-03, -5.6221e-02, -8.0530e-02,\n",
       "                      -1.9953e-01, -1.1052e-01, -1.7635e-01, -2.8146e-01, -1.2306e-01,\n",
       "                      -2.9003e-02, -1.0198e-01, -1.3895e-01, -7.4868e-02, -1.4508e-01,\n",
       "                      -2.1811e-02,  3.2679e-02, -1.4006e-01, -1.0385e-01, -6.3486e-02,\n",
       "                       7.9304e-02,  1.4118e-01,  8.7108e-02, -1.0625e-01,  1.1600e-02,\n",
       "                      -1.6233e-01, -7.1210e-02, -5.5375e-02, -1.1344e-01, -1.2665e-02,\n",
       "                      -1.0209e-01, -2.3901e-01, -2.1620e-01,  7.0933e-02,  1.2414e-02,\n",
       "                      -1.2167e-01, -6.4261e-02, -9.4681e-02, -9.0056e-02, -4.5620e-02,\n",
       "                      -6.2970e-02, -1.5573e-01, -2.5088e-03, -1.8181e-01, -9.5318e-02,\n",
       "                      -9.3445e-02, -1.6662e-01, -1.4956e-01, -8.6562e-02, -5.1900e-02,\n",
       "                       3.7734e-02, -4.7886e-02, -5.0802e-02, -4.2492e-02, -9.3620e-02,\n",
       "                      -5.9730e-03,  5.3121e-03,  1.1711e-01, -1.1868e-01, -4.3393e-02,\n",
       "                       3.0685e-02, -3.6657e-02,  3.2275e-02,  2.2295e-02, -7.6153e-04,\n",
       "                      -1.2794e-01, -1.2531e-01, -9.9070e-02, -1.0990e-02, -2.7369e-01,\n",
       "                      -2.0431e-02, -1.6145e-01, -6.5348e-02,  7.8538e-02, -5.5619e-02,\n",
       "                      -9.1956e-02, -1.0699e-01,  3.4477e-03, -3.9671e-02, -1.3279e-01,\n",
       "                       7.0563e-02, -2.3056e-03,  2.0180e-03,  5.4083e-02, -9.6754e-02,\n",
       "                       2.2482e-02, -1.5010e-01, -1.6139e-03, -1.9329e-01,  1.1331e-01,\n",
       "                      -4.4686e-02, -7.8549e-02, -1.1687e-01, -1.0737e-02, -3.1671e-02,\n",
       "                       8.0416e-02,  1.3475e-01,  1.2417e-01, -4.8573e-02, -1.1132e-01,\n",
       "                       8.1177e-03,  1.3604e-01, -7.8118e-02, -2.4184e-01, -1.6228e-01,\n",
       "                      -1.8163e-01, -1.5132e-01, -4.1875e-02,  2.3591e-02,  1.7610e-01,\n",
       "                      -1.4380e-02, -2.0107e-02,  1.0366e-01, -1.0987e-01,  7.1898e-02,\n",
       "                      -6.2251e-02, -9.9394e-02, -1.1297e-01,  8.0043e-02, -4.0613e-02,\n",
       "                       1.2294e-02, -1.0761e-01,  1.6793e-03,  1.9957e-02, -3.6061e-02,\n",
       "                       8.4868e-03, -9.9252e-02, -9.0340e-02, -1.2838e-01,  1.0114e-01,\n",
       "                       5.3174e-03, -1.5375e-01, -1.0734e-01, -1.2082e-01, -1.0263e-01,\n",
       "                       2.5415e-02, -8.6687e-02, -1.2922e-01, -4.5204e-02, -3.3891e-02,\n",
       "                      -1.0439e-01, -7.4840e-02, -1.2439e-01,  8.9630e-02,  6.3047e-02,\n",
       "                       1.6851e-02,  3.2017e-02,  1.1716e-02, -1.0806e-01, -4.9602e-02,\n",
       "                       3.2970e-02, -8.3790e-02,  4.7077e-02, -2.2912e-02,  7.0064e-02,\n",
       "                      -1.1641e-01,  3.5956e-02,  1.5323e-01, -4.2299e-02,  8.3302e-02,\n",
       "                       1.0858e-01, -1.0547e-02, -1.5234e-02, -4.0764e-02, -3.0591e-02,\n",
       "                       9.8677e-04,  7.8736e-02, -3.6246e-02, -5.5311e-02,  9.2713e-02,\n",
       "                      -4.7627e-02,  4.5140e-02,  1.0069e-02,  2.8036e-02,  1.4856e-02,\n",
       "                       2.4403e-03,  3.7294e-02,  6.4784e-02,  5.0639e-02, -3.1280e-02,\n",
       "                      -9.7585e-03, -9.3431e-02,  1.2917e-01, -1.4186e-01, -3.7985e-03,\n",
       "                      -7.0480e-03, -8.7177e-02,  1.9284e-02, -4.3829e-02,  3.3813e-02,\n",
       "                      -2.0159e-02, -1.3735e-01, -2.2138e-02, -4.9514e-02, -4.1273e-02,\n",
       "                       1.2251e-01,  9.8924e-02, -3.6025e-02, -3.9868e-02,  3.7690e-02,\n",
       "                      -8.0969e-03, -1.2508e-02,  3.7974e-02,  9.2455e-02, -3.9797e-02,\n",
       "                      -2.5871e-02,  7.5078e-03, -2.0942e-02,  1.2977e-01,  9.0596e-02,\n",
       "                      -1.1890e-01,  3.8874e-02,  4.5643e-02,  1.7720e-02, -3.2330e-02,\n",
       "                       2.0473e-02,  6.4068e-03,  4.0041e-02, -5.5368e-02, -3.5352e-02,\n",
       "                       2.2889e-02, -3.1498e-02, -8.0427e-02,  7.5941e-02,  2.3313e-02,\n",
       "                       2.9635e-02,  7.0963e-02, -1.1930e-01, -6.9795e-02, -4.1614e-02,\n",
       "                      -2.1090e-02,  3.2018e-02,  3.2508e-02, -1.3400e-02, -6.0118e-02,\n",
       "                      -1.5678e-01, -2.8003e-02, -6.8159e-02, -6.7634e-02, -9.6848e-03,\n",
       "                      -1.3681e-01,  6.4743e-02,  2.8054e-03,  8.6444e-02,  7.2156e-02,\n",
       "                      -6.4191e-02,  1.4807e-01, -1.0812e-04,  6.1626e-02,  1.4701e-02,\n",
       "                       2.7204e-02,  1.7437e-03,  8.5149e-02,  2.5306e-02, -2.8187e-02,\n",
       "                      -5.1411e-02, -5.9831e-02, -1.7010e-01,  1.3417e-02, -9.7044e-02,\n",
       "                      -1.2198e-01,  5.1426e-02, -4.5882e-03, -5.2964e-02, -2.7293e-02,\n",
       "                      -3.7302e-02, -1.8288e-01,  2.9751e-02, -3.1812e-03,  9.0300e-02,\n",
       "                      -9.0139e-02, -1.2920e-01, -1.2371e-01,  6.2470e-02, -6.1070e-02,\n",
       "                      -1.9964e-01,  7.0172e-03, -4.4024e-02, -1.4855e-01, -1.0276e-01,\n",
       "                      -1.4227e-01, -1.0180e-01, -8.6246e-02, -1.8104e-02, -1.5848e-01,\n",
       "                      -9.1799e-02, -9.4736e-02, -6.2084e-02, -1.5235e-01, -5.3254e-02,\n",
       "                      -1.5546e-01, -1.1929e-01, -9.4776e-03, -1.4922e-01,  3.4013e-02,\n",
       "                      -7.7656e-02, -6.0011e-02, -1.7622e-01, -1.5188e-01,  6.4157e-04,\n",
       "                      -5.0832e-02, -9.8262e-02, -9.9654e-02, -1.1668e-01, -7.2636e-02,\n",
       "                       1.4397e-02, -1.8727e-01, -1.8603e-01,  7.8794e-02,  3.9503e-03,\n",
       "                      -1.1453e-01, -4.3687e-02, -1.5931e-01, -7.7188e-02, -1.0142e-01,\n",
       "                       1.8865e-02, -7.5981e-04, -4.1399e-02, -2.0348e-01,  8.8622e-02,\n",
       "                       6.4280e-02, -5.0518e-02, -1.9004e-01, -5.5674e-02, -8.0179e-02,\n",
       "                      -3.6669e-02, -1.2524e-01, -1.0146e-01, -1.2329e-01, -8.4466e-02,\n",
       "                      -1.4196e-01, -1.0733e-02, -3.9388e-02, -9.1988e-02, -1.0111e-01,\n",
       "                      -4.9689e-02, -6.0990e-02, -1.5482e-01, -1.9059e-01, -2.1121e-01,\n",
       "                      -1.1882e-01, -7.6538e-02, -8.1994e-02,  7.9167e-02, -1.2586e-01,\n",
       "                       1.0550e-01, -1.2125e-01, -2.0611e-01, -6.4052e-02, -5.0889e-02,\n",
       "                       1.7300e-02, -1.0773e-02, -8.2194e-02, -4.3832e-02, -5.4717e-02,\n",
       "                      -7.6729e-02, -3.4247e-02, -8.2232e-02, -1.1709e-01, -1.5549e-01,\n",
       "                      -2.4033e-02, -1.0230e-01, -1.1638e-01, -1.0817e-01, -4.0495e-02,\n",
       "                       1.3492e-01, -1.2751e-01, -6.0113e-02, -4.1406e-02, -2.0799e-01,\n",
       "                      -7.0212e-03, -1.0068e-02, -1.7737e-01, -8.0654e-02, -1.1283e-01,\n",
       "                      -8.8842e-02, -9.9255e-02, -2.0323e-01, -1.4214e-01, -3.7489e-02,\n",
       "                      -6.9573e-02, -1.6393e-01, -2.7133e-01, -9.0103e-02, -1.5821e-01,\n",
       "                      -7.8881e-03, -4.9779e-02, -3.7369e-02, -1.6623e-02, -2.6321e-02,\n",
       "                      -7.4123e-02,  2.1531e-02, -1.0302e-01,  7.2603e-03, -1.2453e-01,\n",
       "                      -3.1501e-02, -1.1901e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-1.4749e-01, -1.4437e-01, -1.0330e-01, -1.7745e-01, -5.8161e-02,\n",
       "                      -3.1970e-02, -1.9281e-01,  1.1346e-01,  9.5471e-03, -5.8944e-02,\n",
       "                      -1.2659e-01, -1.9468e-01, -1.5881e-02, -1.2402e-01, -1.0092e-01,\n",
       "                      -1.1116e-01, -4.6620e-02, -1.3647e-01, -5.0355e-02, -2.6310e-01,\n",
       "                       5.9420e-02, -1.2898e-01, -3.2763e-02, -1.2089e-02, -6.7838e-02,\n",
       "                      -1.8510e-01, -1.9305e-01, -3.4297e-02, -1.5610e-02,  1.4366e-02,\n",
       "                      -1.6007e-01, -1.4819e-01, -1.1998e-01, -1.0471e-01, -6.7332e-02,\n",
       "                      -1.3054e-01, -6.7967e-02, -1.0169e-01, -1.5800e-01,  1.0069e-02,\n",
       "                      -1.3442e-01, -9.7699e-02, -1.0273e-01, -7.3320e-02, -3.0304e-02,\n",
       "                      -6.7408e-02, -1.2496e-01, -1.4043e-01, -1.5099e-01, -4.5950e-02,\n",
       "                       4.7502e-02, -8.4048e-02, -6.9704e-02, -9.9770e-02, -1.5946e-01,\n",
       "                      -1.3224e-01, -1.0632e-01, -1.0523e-01, -1.8017e-01,  1.6241e-02,\n",
       "                       3.6993e-03, -1.1532e-01, -1.1025e-01, -1.3314e-01, -1.0942e-01,\n",
       "                      -6.2784e-02, -1.3182e-02,  1.7659e-02, -1.6344e-03, -9.4761e-02,\n",
       "                      -4.4029e-02, -3.3698e-02, -1.0691e-01, -2.5815e-02, -1.7311e-02,\n",
       "                      -1.5996e-01, -7.9754e-05, -1.5450e-01, -8.1866e-02, -4.0344e-02,\n",
       "                      -5.6814e-02, -9.2463e-02, -3.2936e-02,  7.8638e-03, -6.6942e-02,\n",
       "                      -1.3831e-02,  7.2679e-03,  5.6091e-02, -3.4580e-02,  2.6174e-02,\n",
       "                      -2.1893e-02,  2.7210e-03,  1.6784e-02, -6.7165e-02, -1.3921e-01,\n",
       "                      -1.2624e-01, -5.7346e-02, -2.6719e-02, -6.9279e-03, -1.2648e-01,\n",
       "                      -2.1568e-01, -9.9140e-02, -1.1103e-01, -2.3117e-02, -9.9552e-02,\n",
       "                      -6.9857e-04, -1.2840e-01, -1.1162e-01, -4.1647e-02, -1.0099e-01,\n",
       "                      -7.3975e-02, -1.2736e-01, -1.6416e-01, -1.5865e-01, -1.5881e-01,\n",
       "                      -1.0837e-01, -1.2313e-02,  6.9857e-02,  3.3836e-02, -9.6025e-02,\n",
       "                      -1.3162e-01, -8.2375e-02, -1.1651e-01, -2.4192e-01, -2.0934e-01,\n",
       "                      -3.3301e-02, -1.5387e-01, -7.9631e-02, -2.0648e-01, -6.9248e-03,\n",
       "                      -8.8255e-02, -1.1671e-01, -1.1726e-01, -1.4559e-01, -1.4700e-01,\n",
       "                      -9.4378e-03,  1.9665e-02, -1.6422e-02, -5.7700e-03,  3.6092e-03,\n",
       "                      -5.3992e-02, -1.8096e-01, -5.0214e-02, -4.0115e-02,  3.2243e-02,\n",
       "                       3.3573e-03,  3.5312e-03, -1.0279e-01, -4.0139e-02, -4.9241e-02,\n",
       "                      -1.1396e-01,  1.0950e-01, -2.0163e-01, -1.0838e-01, -2.9620e-02,\n",
       "                       1.2987e-02, -8.3937e-02, -5.8404e-02, -1.1786e-01, -1.1231e-01,\n",
       "                      -2.3300e-01, -1.9969e-01, -1.2914e-01, -7.3814e-02, -2.4488e-02,\n",
       "                       1.2396e-01, -6.1317e-02, -1.3723e-01, -1.1359e-01,  5.4459e-02,\n",
       "                      -5.8630e-02, -3.7645e-02, -5.1619e-02, -6.1099e-02, -8.8044e-02,\n",
       "                       1.0423e-01, -8.9309e-02, -1.0692e-01, -8.4092e-02, -8.0820e-03,\n",
       "                       1.9460e-02, -1.4049e-01, -1.7099e-02,  2.8258e-02, -1.2467e-01,\n",
       "                       7.6444e-02, -1.7843e-01, -9.7822e-02,  7.2124e-02, -1.8499e-01,\n",
       "                      -1.9104e-01,  2.5997e-02, -1.1332e-01, -7.0127e-02, -2.3622e-02,\n",
       "                      -1.3364e-01, -1.3244e-01,  4.3479e-02, -1.5554e-01, -7.5602e-02,\n",
       "                      -5.1964e-02, -5.3420e-02,  7.5643e-02, -1.6861e-01,  1.2402e-01,\n",
       "                      -8.0733e-03,  4.7966e-02, -8.7928e-02, -7.2958e-03, -1.6629e-01,\n",
       "                      -8.6977e-02, -6.9608e-02,  1.6770e-01, -1.7813e-02, -3.5372e-02,\n",
       "                      -5.0444e-02,  1.3199e-02,  4.7342e-02, -2.0593e-01, -3.8676e-02,\n",
       "                       1.7167e-02, -2.1396e-01,  1.1579e-01, -8.0384e-02,  2.4108e-01,\n",
       "                       7.5226e-02,  6.0790e-03,  1.4241e-01, -1.1339e-01, -7.5325e-02,\n",
       "                       1.3471e-01, -6.2729e-02, -1.1897e-02,  6.2101e-02, -8.0951e-02,\n",
       "                      -3.2475e-02, -4.5558e-02,  3.1303e-03, -5.8330e-03, -1.4299e-01,\n",
       "                      -7.8614e-02,  8.6260e-03, -8.7751e-02, -7.3849e-02,  2.3443e-01,\n",
       "                      -2.3925e-02, -9.1783e-02, -1.5348e-01, -3.9723e-02, -1.4246e-01,\n",
       "                      -7.5004e-02, -1.4124e-01, -6.5097e-03,  3.5284e-02, -3.7808e-02,\n",
       "                      -1.5720e-01,  1.1355e-01, -1.1465e-02, -6.5820e-02,  8.7585e-02,\n",
       "                      -2.2912e-02,  1.3176e-02,  9.4311e-02, -4.6067e-02, -7.4601e-02,\n",
       "                      -2.8443e-02, -5.6055e-02, -1.8902e-02, -1.0453e-01,  5.6657e-02,\n",
       "                      -2.8102e-02,  2.3179e-01,  1.2612e-01, -1.3746e-02, -3.1827e-02,\n",
       "                      -4.1657e-02, -5.9880e-02, -1.1627e-01, -4.4603e-02,  3.5233e-02,\n",
       "                      -1.1162e-01,  2.7774e-02, -5.0953e-02,  5.0631e-02, -7.6618e-02,\n",
       "                       6.9103e-03,  2.5868e-02,  1.2131e-01, -3.3218e-02, -6.8807e-03,\n",
       "                       3.5007e-02, -4.0498e-02,  5.0778e-02,  7.8165e-02,  1.8708e-02,\n",
       "                      -6.7142e-02, -2.8028e-02, -1.1699e-02, -1.0552e-02,  2.3652e-03,\n",
       "                      -5.6481e-03,  2.8902e-02, -1.9713e-02,  1.5062e-02, -7.3491e-02,\n",
       "                      -6.2196e-02,  5.0529e-02,  7.3598e-02, -4.6813e-02,  3.9240e-03,\n",
       "                      -8.2937e-02, -5.9928e-02, -5.3273e-02, -5.0144e-02,  2.3122e-01,\n",
       "                       4.7519e-02, -2.6343e-02, -6.4470e-02,  7.2457e-03, -3.7728e-02,\n",
       "                      -1.1415e-02,  7.9245e-02, -6.5529e-03,  1.9103e-01, -1.0957e-01,\n",
       "                      -7.1994e-02, -9.8593e-02,  1.8805e-02,  5.9242e-02,  8.5475e-02,\n",
       "                      -9.3186e-03,  3.7370e-02, -1.0038e-01, -5.7505e-02, -2.6970e-02,\n",
       "                       4.2655e-02,  7.5868e-02,  8.5534e-03,  5.7076e-03, -6.9178e-02,\n",
       "                       4.3143e-03,  3.4797e-02, -9.0324e-02, -2.3995e-02,  9.0840e-02,\n",
       "                      -4.4851e-02, -4.4698e-02,  9.2993e-03, -7.6896e-02, -1.5381e-02,\n",
       "                       6.1058e-02, -2.7838e-02, -3.4995e-02, -2.1290e-02, -7.9921e-02,\n",
       "                      -1.2436e-02, -4.7497e-02,  8.2982e-02, -1.7174e-02,  4.2345e-02,\n",
       "                      -7.4878e-02,  2.5656e-02, -1.0008e-01,  6.0584e-02, -4.7447e-02,\n",
       "                      -8.9466e-02, -1.7404e-01,  1.0158e-02,  1.1166e-01,  4.4158e-02,\n",
       "                      -1.2422e-01, -1.0905e-01,  1.9572e-02,  2.2646e-02,  2.9760e-02,\n",
       "                       1.0495e-01,  2.0476e-01,  1.1367e-01,  1.9827e-02,  5.5455e-02,\n",
       "                       4.0692e-02,  3.6739e-02, -4.2056e-02,  1.4588e-01,  3.2030e-03,\n",
       "                      -6.1199e-02, -1.8862e-01, -1.3978e-01, -4.7617e-02, -3.5604e-02,\n",
       "                      -1.5419e-01, -6.2586e-02, -1.5579e-01,  7.0161e-02,  3.3698e-02,\n",
       "                      -1.2042e-01, -4.8467e-02, -1.4622e-01,  4.6287e-02,  3.6742e-02,\n",
       "                       8.5219e-02, -4.9532e-02, -3.3035e-02, -2.0402e-01, -3.9688e-02,\n",
       "                      -1.1912e-01,  4.7075e-02, -4.4860e-02, -1.1033e-01, -1.4823e-01,\n",
       "                      -1.5207e-01, -7.1459e-02, -3.4134e-02, -1.0236e-02,  9.1784e-03,\n",
       "                      -1.6631e-01, -2.0966e-01, -1.5954e-02,  1.2222e-02, -5.0057e-02,\n",
       "                      -1.0019e-01, -1.2522e-01, -7.8240e-02, -6.2896e-02, -3.1207e-02,\n",
       "                      -2.2032e-02, -1.2994e-01, -6.8408e-02, -1.0394e-01, -2.4062e-01,\n",
       "                      -5.7042e-02, -2.7190e-04, -4.4389e-02, -1.3618e-01,  9.4449e-03,\n",
       "                      -3.5613e-02, -7.4190e-02, -2.4572e-01,  1.8254e-03, -1.8427e-01,\n",
       "                      -1.5303e-01, -1.1743e-01, -7.9930e-02, -1.8115e-01, -2.1822e-02,\n",
       "                      -7.2806e-02, -1.3430e-01, -2.3765e-01, -1.2065e-01, -1.5004e-01,\n",
       "                      -3.8046e-02, -5.5647e-02, -3.6525e-02, -7.4698e-02, -6.2055e-02,\n",
       "                      -1.2443e-01, -2.1293e-01, -5.3072e-02,  1.9322e-02, -1.5306e-01,\n",
       "                       3.8788e-02, -4.1097e-02, -1.5344e-02,  4.4912e-02, -5.5911e-02,\n",
       "                       6.5826e-02, -1.7338e-01, -5.8083e-02,  9.2085e-02,  5.1029e-02,\n",
       "                      -1.1904e-01, -1.1197e-01, -7.0060e-02, -1.8581e-01,  6.6616e-05,\n",
       "                      -5.3989e-02, -9.9057e-02, -1.2613e-01, -1.3650e-01, -4.4824e-02,\n",
       "                       1.3528e-01, -5.5319e-03, -8.9012e-02, -1.2801e-02, -1.7756e-01,\n",
       "                      -4.0063e-02,  4.0924e-02, -5.6046e-02, -6.7709e-03, -4.5815e-02,\n",
       "                      -1.8276e-01, -1.8162e-01, -1.4775e-01, -3.7101e-02, -1.1626e-01,\n",
       "                      -8.1401e-02, -3.9299e-02, -1.0056e-01, -3.3130e-02, -1.5482e-01,\n",
       "                      -1.3992e-04, -1.4190e-01, -1.0942e-01,  3.1663e-02, -1.1218e-01,\n",
       "                      -6.7776e-02, -4.8323e-02, -6.4608e-02, -4.1824e-03, -1.7430e-01,\n",
       "                      -1.1604e-01, -1.4885e-01])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.5857,  0.1588, -0.0959,  ..., -0.2318,  0.1313, -0.0700],\n",
       "                      [ 0.2694, -0.0674, -0.5318,  ...,  0.1110,  0.0594, -0.1153],\n",
       "                      [ 0.3455, -0.2001, -0.3616,  ...,  0.1937,  0.6576, -0.0449],\n",
       "                      ...,\n",
       "                      [-0.9075,  0.2038, -0.1144,  ...,  1.0395,  0.5309, -0.6462],\n",
       "                      [ 0.0234, -0.2524, -0.0384,  ...,  0.5230,  0.0303,  0.0884],\n",
       "                      [ 0.0164, -0.4786,  0.1033,  ...,  0.0991, -0.3885, -0.6985]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1744,  0.0363, -0.1457,  ..., -0.0372, -0.0700,  0.1246],\n",
       "                      [ 0.1909,  0.1008, -0.1654,  ...,  0.0626,  0.0781,  0.1306],\n",
       "                      [-0.0463, -0.1055,  0.1236,  ...,  0.2565,  0.0564,  0.0040],\n",
       "                      ...,\n",
       "                      [-0.0876, -0.1975,  0.1598,  ..., -0.0736, -0.0459, -0.0470],\n",
       "                      [-0.1055,  0.3507,  0.0305,  ..., -0.0606,  0.1687,  0.2539],\n",
       "                      [ 0.1890, -0.1615, -0.1647,  ..., -0.1519, -0.1768, -0.1387]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0891,  0.0354,  0.3202,  0.1683,  0.1231,  0.1998,  0.0607,  0.1412,\n",
       "                       0.0226,  0.0211,  0.2677,  0.1770,  0.2080,  0.1072,  0.1179,  0.1831,\n",
       "                      -0.0035, -0.0780,  0.0317, -0.0028,  0.2589,  0.1479,  0.0833,  0.2263,\n",
       "                      -0.0617,  0.1739,  0.1036,  0.2694,  0.1127,  0.1781,  0.1636,  0.2078,\n",
       "                       0.1195,  0.0779, -0.0164,  0.3180,  0.2149,  0.0076,  0.0674,  0.0222,\n",
       "                       0.1798,  0.0374,  0.0914,  0.2068,  0.0217,  0.0480,  0.1869,  0.2825,\n",
       "                       0.2211,  0.0700,  0.2236, -0.0618,  0.0477,  0.0595,  0.1599, -0.0076,\n",
       "                       0.1740,  0.0633,  0.2782, -0.0288,  0.1870,  0.3316,  0.1530,  0.0893,\n",
       "                      -0.0009,  0.2887,  0.2248,  0.0632,  0.0286,  0.1088,  0.3298,  0.2424,\n",
       "                       0.0725, -0.0557, -0.0275,  0.0530,  0.1992,  0.1113,  0.1905,  0.2152,\n",
       "                      -0.0329,  0.2641,  0.1310,  0.0133,  0.2245,  0.2391,  0.0317,  0.2310,\n",
       "                       0.1593,  0.0736,  0.0950,  0.1871,  0.1639,  0.1251,  0.1005,  0.0328,\n",
       "                       0.2023,  0.0805,  0.1882,  0.1463,  0.1108,  0.2686, -0.0886,  0.0651,\n",
       "                      -0.0102, -0.0696,  0.1493,  0.2375,  0.1045,  0.3010,  0.1986,  0.1802,\n",
       "                       0.1205,  0.2086,  0.1986, -0.1150,  0.0933,  0.1497,  0.1907,  0.1174,\n",
       "                       0.2571,  0.1806,  0.0169,  0.1991,  0.1843,  0.0181,  0.0731,  0.3050,\n",
       "                       0.0542, -0.0890,  0.0375,  0.0599,  0.1426, -0.0009,  0.0128,  0.1294,\n",
       "                       0.0517,  0.0506, -0.0027,  0.1000,  0.1529,  0.0304,  0.0338, -0.0051,\n",
       "                      -0.0575,  0.0123, -0.0507,  0.0513,  0.0566,  0.0755,  0.0539,  0.0144,\n",
       "                       0.0707, -0.0255,  0.0875, -0.0008, -0.0383,  0.0381,  0.0910,  0.1596,\n",
       "                       0.0262,  0.0598, -0.0864, -0.0425,  0.0202,  0.1361, -0.0340,  0.1359,\n",
       "                       0.0039,  0.1984, -0.0415,  0.1663,  0.0189,  0.0174,  0.2100,  0.0473,\n",
       "                       0.1567, -0.0195, -0.0044,  0.0080,  0.1033,  0.1883, -0.0608,  0.0368,\n",
       "                       0.1024, -0.0208,  0.0504, -0.0929,  0.1428, -0.1128,  0.1897,  0.0618,\n",
       "                      -0.0279,  0.0084, -0.0103,  0.0954,  0.0079,  0.1932, -0.0091,  0.0172,\n",
       "                       0.0698,  0.0555,  0.0418,  0.0241,  0.0628,  0.1111,  0.0852, -0.0578,\n",
       "                       0.0442,  0.0944, -0.0094,  0.0757, -0.0308, -0.0112,  0.1135, -0.0181,\n",
       "                       0.0080, -0.0421, -0.0712,  0.0625, -0.0226, -0.0592,  0.0203,  0.0435,\n",
       "                       0.0325,  0.0050,  0.0875,  0.0582, -0.0066,  0.1000,  0.1821,  0.1797,\n",
       "                      -0.0304,  0.0533,  0.0146, -0.0417,  0.0519, -0.0787, -0.0632, -0.1066,\n",
       "                       0.1506,  0.0367,  0.2098,  0.0789,  0.0718,  0.0822,  0.0317,  0.0779,\n",
       "                      -0.0120, -0.0151,  0.0648, -0.0209,  0.0053,  0.1724,  0.0062,  0.0179,\n",
       "                       0.1088, -0.0210,  0.0425, -0.0312, -0.0172,  0.0128,  0.0104,  0.0644,\n",
       "                      -0.0405, -0.0587,  0.1058, -0.0636,  0.0394, -0.0502,  0.0086,  0.0310,\n",
       "                       0.0060, -0.0216,  0.0687, -0.0049, -0.0343, -0.0360,  0.1393,  0.0276,\n",
       "                       0.0334,  0.1349, -0.0663, -0.0275, -0.0158,  0.0166, -0.0344, -0.0311,\n",
       "                      -0.0488,  0.0397, -0.0395, -0.0040, -0.1341, -0.0595, -0.0438, -0.0997,\n",
       "                      -0.0128, -0.1106, -0.0266, -0.0100, -0.0395,  0.1209, -0.0089, -0.0400,\n",
       "                       0.0131,  0.1816,  0.0439,  0.0525,  0.0030,  0.1082, -0.0104,  0.0027,\n",
       "                       0.0235,  0.1058,  0.0912,  0.0027,  0.0817,  0.0389,  0.0542, -0.0254,\n",
       "                      -0.0777, -0.1176,  0.0507,  0.2306, -0.0339,  0.0258, -0.0286,  0.0210,\n",
       "                       0.0818,  0.0343,  0.0759, -0.0920, -0.0341,  0.0053, -0.0152, -0.0106,\n",
       "                      -0.1278,  0.0330, -0.1338,  0.0008,  0.0187, -0.0112,  0.0470,  0.0956,\n",
       "                      -0.0178, -0.0106,  0.1262, -0.0209, -0.1165, -0.0398, -0.1166, -0.0443,\n",
       "                       0.0355,  0.0613, -0.0916,  0.0156,  0.1067, -0.0087,  0.0411,  0.0285,\n",
       "                       0.0106,  0.0930, -0.0852, -0.0338,  0.0443, -0.0781, -0.0912, -0.0043,\n",
       "                      -0.0344,  0.0706,  0.0570, -0.0163,  0.0939, -0.0263, -0.0258,  0.0465,\n",
       "                       0.0085, -0.0616, -0.0662,  0.0075, -0.1042, -0.0114,  0.0528, -0.0445,\n",
       "                       0.2261,  0.1970,  0.1898,  0.2889,  0.1087,  0.3711,  0.0720,  0.2262,\n",
       "                       0.0147,  0.1178,  0.0049,  0.0098,  0.1146,  0.3115,  0.1323,  0.2448,\n",
       "                       0.0553,  0.1115,  0.0806,  0.1666,  0.1285,  0.3154,  0.0326,  0.2339,\n",
       "                       0.1142,  0.3523,  0.0487,  0.2094,  0.1027,  0.1940,  0.0825, -0.0224,\n",
       "                       0.2311,  0.2214,  0.0015,  0.2295,  0.0889,  0.0225,  0.0167,  0.1609,\n",
       "                       0.1553,  0.2163,  0.1374,  0.1686,  0.1322,  0.0334,  0.1917,  0.1804,\n",
       "                       0.2092, -0.0278, -0.0285,  0.1828,  0.1402,  0.1885,  0.1368,  0.1112,\n",
       "                       0.0307, -0.0652,  0.3860,  0.1001,  0.2679,  0.3633,  0.1874,  0.2987,\n",
       "                       0.0104,  0.0045,  0.2364,  0.1515,  0.2728,  0.0614,  0.1137,  0.1603,\n",
       "                       0.0450,  0.0383,  0.0546,  0.0700,  0.1993,  0.1556,  0.1261,  0.3137,\n",
       "                      -0.1017,  0.1176,  0.0431, -0.0145,  0.0478,  0.1673,  0.0575,  0.0557,\n",
       "                       0.0793,  0.2953,  0.2260,  0.1368,  0.0370,  0.0107,  0.1506,  0.1070,\n",
       "                       0.2763,  0.1477,  0.0902,  0.1422,  0.1663,  0.2822,  0.0681,  0.1803,\n",
       "                      -0.0251, -0.0556,  0.2148,  0.1282,  0.4331,  0.1927,  0.1451,  0.2086,\n",
       "                       0.1765,  0.1974,  0.1707,  0.1922,  0.0927,  0.1780, -0.0239,  0.0130,\n",
       "                       0.1588,  0.2137,  0.1791,  0.1142,  0.1072,  0.3419,  0.1789,  0.1173])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 0.2034,  0.1618,  0.2492,  0.1435,  0.1289,  0.2384,  0.0348,  0.0711,\n",
       "                      -0.0142,  0.1325,  0.2727,  0.0786,  0.1228,  0.1162,  0.1163,  0.1652,\n",
       "                      -0.0641,  0.0018,  0.0687,  0.0638,  0.2945,  0.1458,  0.0538,  0.1942,\n",
       "                      -0.0985,  0.2915,  0.1411,  0.2296,  0.1322,  0.2363,  0.2126,  0.2269,\n",
       "                       0.1897, -0.0419,  0.0367,  0.3385,  0.2401,  0.0184,  0.0177,  0.0785,\n",
       "                       0.0602,  0.0583,  0.1161,  0.1823,  0.1248,  0.0863,  0.0703,  0.0141,\n",
       "                       0.3583,  0.0654,  0.0802, -0.0042,  0.1166,  0.0205,  0.1550,  0.0177,\n",
       "                       0.0429,  0.0059,  0.2562, -0.0257,  0.3255,  0.3274,  0.1253,  0.2634,\n",
       "                       0.1743,  0.1861,  0.2392, -0.1355,  0.0593,  0.0984,  0.4262,  0.1642,\n",
       "                       0.1905,  0.0653, -0.0562, -0.0089,  0.1883,  0.2426,  0.1464,  0.2070,\n",
       "                      -0.0753,  0.2005,  0.0811, -0.0328,  0.1821,  0.0665, -0.0547,  0.1624,\n",
       "                       0.0232,  0.1369,  0.2377,  0.1316,  0.0965,  0.1729,  0.0644, -0.0302,\n",
       "                       0.1680,  0.2380,  0.1965,  0.1241,  0.1209,  0.1635,  0.0207,  0.1522,\n",
       "                       0.0924,  0.0176,  0.2860,  0.2366,  0.1573,  0.2870, -0.0322,  0.1122,\n",
       "                       0.1790,  0.3291,  0.1557,  0.0710,  0.0553,  0.1988,  0.0894,  0.0455,\n",
       "                       0.3492,  0.3151,  0.0928,  0.1050,  0.2025,  0.0920,  0.2074,  0.2185,\n",
       "                      -0.0471,  0.0943, -0.0018, -0.0723, -0.0187, -0.0427,  0.0546, -0.0628,\n",
       "                      -0.1098, -0.0708,  0.1638,  0.0736,  0.0353,  0.0490,  0.0221, -0.0162,\n",
       "                      -0.0086,  0.0119, -0.0069,  0.0485,  0.1554,  0.0354,  0.1038, -0.2434,\n",
       "                      -0.0234,  0.0082,  0.0651, -0.0694, -0.0067,  0.1227,  0.0026,  0.0918,\n",
       "                       0.0229,  0.0895,  0.0064, -0.0722,  0.0847,  0.1167, -0.0494, -0.0109,\n",
       "                      -0.0614,  0.1595, -0.1752,  0.0057,  0.1314,  0.1151,  0.1700,  0.1076,\n",
       "                       0.1655,  0.1161,  0.0871, -0.0527,  0.0033,  0.0656,  0.0370,  0.0102,\n",
       "                      -0.0150, -0.0918,  0.2126,  0.0529, -0.0303, -0.0075, -0.0600,  0.0801,\n",
       "                       0.0207,  0.1401, -0.0350,  0.1476,  0.0530,  0.0874,  0.0736,  0.0309,\n",
       "                       0.1151,  0.1625, -0.0152, -0.0513,  0.0212,  0.0353,  0.1132, -0.0328,\n",
       "                       0.0454, -0.0508, -0.0207,  0.0864, -0.0369,  0.0146,  0.0187,  0.0594,\n",
       "                      -0.0142, -0.0157, -0.0023, -0.0402, -0.0075, -0.0532, -0.0530,  0.1658,\n",
       "                       0.0676, -0.0892, -0.0895, -0.0785,  0.0446,  0.1094,  0.1390,  0.0651,\n",
       "                      -0.0064,  0.0429, -0.1123, -0.0335,  0.0222,  0.0781, -0.0768,  0.0450,\n",
       "                       0.0696,  0.0820,  0.0307, -0.0347,  0.0803,  0.0644,  0.0295,  0.0841,\n",
       "                      -0.1270, -0.0048, -0.0532,  0.0864,  0.1437,  0.1442,  0.0202,  0.0117,\n",
       "                      -0.0139, -0.0823, -0.0096, -0.1606,  0.0365, -0.0038,  0.0276, -0.0534,\n",
       "                      -0.0860, -0.1663, -0.1242, -0.0390,  0.0477,  0.0798, -0.0090,  0.0986,\n",
       "                       0.1294,  0.0604,  0.0534, -0.0773, -0.0472,  0.1137,  0.1921, -0.0933,\n",
       "                       0.0312,  0.0417,  0.0327, -0.0752,  0.0336, -0.0719, -0.0129, -0.0157,\n",
       "                       0.0230, -0.0849, -0.0143, -0.0187, -0.0764,  0.0815,  0.0601,  0.0020,\n",
       "                       0.0372,  0.0378,  0.0093, -0.0588, -0.0452, -0.0364,  0.0274, -0.0859,\n",
       "                      -0.0093,  0.0722,  0.1013,  0.0321, -0.0209, -0.0749,  0.0902,  0.0244,\n",
       "                       0.0479,  0.1155, -0.0087, -0.0125,  0.0335,  0.0292,  0.0698, -0.0827,\n",
       "                      -0.0166,  0.0179, -0.0354,  0.0071, -0.0402, -0.0609,  0.0815, -0.0192,\n",
       "                       0.0202,  0.0197,  0.0375, -0.0627,  0.0292, -0.0448, -0.0493, -0.0394,\n",
       "                       0.0629, -0.0338,  0.0366,  0.0488,  0.0218,  0.0734,  0.1221, -0.0371,\n",
       "                       0.0570, -0.2089,  0.0158, -0.0763, -0.0094, -0.0843, -0.1798, -0.0422,\n",
       "                       0.0088,  0.1567, -0.0738, -0.0239,  0.1256, -0.0113,  0.0225, -0.0686,\n",
       "                      -0.0251,  0.1254, -0.0059, -0.0505,  0.0690, -0.0291, -0.0134, -0.0143,\n",
       "                       0.1278, -0.1045,  0.0737, -0.0322, -0.0907,  0.0254,  0.0335,  0.0411,\n",
       "                       0.0690,  0.0630,  0.0845, -0.0720, -0.0488,  0.0403, -0.0113, -0.0481,\n",
       "                       0.0555,  0.1440,  0.0188,  0.1986,  0.0529,  0.2497,  0.3292,  0.1980,\n",
       "                       0.0138,  0.1246, -0.0076,  0.1711,  0.1074,  0.3005,  0.2143,  0.2399,\n",
       "                       0.0658,  0.1639, -0.0005,  0.2126,  0.1292,  0.1825,  0.0911,  0.2515,\n",
       "                       0.0471,  0.1810,  0.0385,  0.3116,  0.1038,  0.1785,  0.1835,  0.0635,\n",
       "                       0.3243,  0.1011,  0.0973,  0.2160,  0.3113, -0.0455,  0.0750,  0.0675,\n",
       "                       0.1002,  0.2162,  0.2356,  0.0717,  0.0322,  0.0038,  0.0857,  0.2519,\n",
       "                       0.3822,  0.1038,  0.0488,  0.3090,  0.0945,  0.2086,  0.1268,  0.0507,\n",
       "                       0.1663, -0.0133,  0.3525,  0.1563,  0.0935,  0.3367,  0.2218,  0.2928,\n",
       "                       0.1635,  0.0553,  0.4151,  0.1039,  0.2044,  0.0817,  0.2502,  0.0619,\n",
       "                      -0.0209,  0.0237,  0.0792,  0.0826,  0.0720,  0.0620,  0.2732,  0.1948,\n",
       "                       0.0996,  0.1715,  0.2814, -0.0387,  0.1135,  0.1923,  0.2036, -0.0099,\n",
       "                       0.0106,  0.1763,  0.1021,  0.0998,  0.1097,  0.0176,  0.1673,  0.0767,\n",
       "                       0.2089,  0.0896,  0.1001,  0.1092,  0.1001,  0.3813,  0.0281,  0.1992,\n",
       "                       0.0534,  0.0122,  0.2688,  0.1110,  0.2998,  0.1374,  0.0735,  0.1474,\n",
       "                      -0.0099,  0.0874,  0.2288,  0.0748,  0.0577,  0.1027,  0.2651,  0.0642,\n",
       "                       0.2672,  0.2903,  0.0633,  0.2288,  0.2567,  0.1927,  0.1918,  0.1452])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.0661,  0.0827,  0.0011,  ..., -0.0924,  0.1855, -0.1338],\n",
       "                      [ 0.1508,  0.0521, -0.1321,  ..., -0.0635,  0.0743, -0.3385],\n",
       "                      [-0.5132, -0.4302, -0.1141,  ..., -0.0127,  0.1073, -0.0066],\n",
       "                      ...,\n",
       "                      [-0.0939,  0.0425, -0.3363,  ...,  0.3583,  0.0427, -0.2478],\n",
       "                      [ 0.1597,  0.0895, -0.5283,  ..., -0.3793,  0.0058,  0.2343],\n",
       "                      [-0.2497, -0.1582,  0.0368,  ..., -0.0106,  0.2053, -0.2758]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[-1.5755e-01,  6.0751e-02,  7.5721e-02,  ..., -4.2715e-04,\n",
       "                       -1.4833e-01,  1.7909e-01],\n",
       "                      [-1.5211e-01,  1.0514e-01,  1.6583e-01,  ..., -2.5862e-01,\n",
       "                       -1.3649e-01, -3.9156e-01],\n",
       "                      [-6.6405e-02, -2.9907e-02,  2.1707e-01,  ...,  2.8475e-01,\n",
       "                        3.1658e-02, -6.1775e-02],\n",
       "                      ...,\n",
       "                      [ 8.9855e-02,  2.3105e-01, -1.9556e-01,  ..., -1.0631e-01,\n",
       "                        1.4344e-01, -1.6761e-01],\n",
       "                      [ 1.2743e-01,  9.8320e-02,  1.4621e-01,  ..., -1.1194e-01,\n",
       "                       -7.0856e-02, -8.4424e-02],\n",
       "                      [-4.7524e-01, -1.5044e-01,  1.5873e-02,  ..., -4.2208e-02,\n",
       "                        1.5527e-02, -1.5191e-01]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([-0.1511,  0.0667,  0.1568,  0.1665, -0.0486, -0.0313,  0.0233, -0.0752,\n",
       "                       0.0391,  0.0050, -0.1225, -0.0245,  0.0511, -0.1328,  0.0745,  0.0482,\n",
       "                       0.0600,  0.0606, -0.0913, -0.0266, -0.1334,  0.0350,  0.0681, -0.0043,\n",
       "                       0.1132,  0.1156,  0.0755, -0.0007,  0.0963,  0.0357,  0.0458, -0.0080,\n",
       "                       0.0933, -0.0502, -0.0444,  0.1694,  0.0692,  0.1221,  0.0044,  0.1166,\n",
       "                      -0.0552,  0.0226, -0.0415, -0.0093, -0.0630, -0.1793,  0.0560, -0.0156,\n",
       "                       0.0320, -0.0348,  0.0674, -0.0068,  0.0070,  0.0743, -0.0263,  0.1433,\n",
       "                       0.1088,  0.0846,  0.1535,  0.0007,  0.0539, -0.0986,  0.0325,  0.0112,\n",
       "                       0.1026,  0.2387, -0.0646,  0.0317,  0.2045, -0.1040, -0.0202, -0.1102,\n",
       "                      -0.0063,  0.0023,  0.0910, -0.0429,  0.1607, -0.0397,  0.0166,  0.0312,\n",
       "                       0.0305, -0.0535,  0.1602,  0.1009, -0.1054, -0.0158, -0.0737, -0.0548,\n",
       "                       0.1127, -0.0599,  0.1493, -0.0715,  0.1564,  0.0197,  0.0949, -0.0033,\n",
       "                       0.1213, -0.0953, -0.0353,  0.0074,  0.0960,  0.0181,  0.0557, -0.0515,\n",
       "                      -0.1332,  0.0528,  0.1979, -0.0015,  0.0266,  0.0197, -0.0543, -0.0451,\n",
       "                      -0.0331,  0.0058,  0.0043,  0.0663,  0.0054, -0.1194, -0.0298,  0.0029,\n",
       "                      -0.0657, -0.0340, -0.0385,  0.0738, -0.0814, -0.0514, -0.0251,  0.0558,\n",
       "                       0.0389, -0.0973, -0.0377, -0.1077, -0.0855, -0.0021, -0.0988,  0.0767,\n",
       "                      -0.0612,  0.1250, -0.1296, -0.1385, -0.0493, -0.0141, -0.0256, -0.1748,\n",
       "                      -0.1717,  0.1293, -0.0496,  0.0408, -0.1350, -0.1493,  0.0522,  0.1073,\n",
       "                      -0.0603,  0.0193, -0.0816,  0.0267,  0.0495, -0.0638,  0.0409, -0.0719,\n",
       "                       0.0605, -0.0440, -0.0762, -0.0238,  0.0838, -0.0206, -0.0426,  0.0818,\n",
       "                      -0.0534, -0.1498,  0.0655,  0.0652, -0.1475,  0.0423,  0.1480,  0.1006,\n",
       "                      -0.0518,  0.0047, -0.0167, -0.0237,  0.0238, -0.1343, -0.0843,  0.0368,\n",
       "                      -0.0301, -0.0318, -0.0065, -0.0190, -0.0577, -0.1481, -0.1691,  0.1063,\n",
       "                      -0.0258,  0.0017, -0.1443,  0.0557,  0.0131, -0.1433, -0.0976, -0.0535,\n",
       "                       0.0131,  0.0949, -0.1287, -0.1047, -0.1124,  0.0516, -0.1489,  0.0064,\n",
       "                      -0.1139, -0.1078, -0.0114,  0.0982, -0.0404, -0.0685,  0.0341,  0.0090,\n",
       "                      -0.0482,  0.0079, -0.0440,  0.0516,  0.0701,  0.0845, -0.0058, -0.0310,\n",
       "                      -0.0644, -0.1131, -0.0639, -0.0350, -0.0347,  0.0254,  0.0071,  0.0186,\n",
       "                      -0.1330, -0.0306, -0.0547, -0.0857, -0.0872,  0.1201,  0.0355,  0.0283,\n",
       "                       0.1178, -0.0331,  0.0589, -0.0876,  0.0872, -0.1219,  0.0384, -0.1753,\n",
       "                      -0.0852, -0.1078,  0.0228,  0.0299,  0.0278, -0.0778,  0.2056,  0.0417,\n",
       "                      -0.0174,  0.0154, -0.1347,  0.0369,  0.0335, -0.0832,  0.0246, -0.0178,\n",
       "                      -0.0108,  0.0059, -0.0568, -0.0185, -0.0367,  0.0184, -0.1082,  0.0069,\n",
       "                      -0.0285, -0.0289,  0.0240,  0.0192,  0.0590, -0.0793,  0.0231,  0.0491,\n",
       "                      -0.0145,  0.0838,  0.0260, -0.0330,  0.0057,  0.0885,  0.0998,  0.0603,\n",
       "                      -0.0270, -0.0246, -0.0396, -0.0706,  0.0997, -0.0075,  0.1154, -0.0612,\n",
       "                      -0.0446, -0.0442, -0.0229, -0.0205,  0.0363,  0.0938, -0.0664, -0.0338,\n",
       "                       0.0707, -0.0171, -0.0889,  0.0046, -0.0099,  0.0521, -0.1366, -0.0907,\n",
       "                       0.0443, -0.0505, -0.0862,  0.0144,  0.0207,  0.0531, -0.0453, -0.0519,\n",
       "                       0.0391, -0.0559,  0.0739,  0.0144,  0.0502,  0.0123,  0.0430, -0.1270,\n",
       "                       0.0158, -0.0467,  0.0014,  0.0889, -0.0523, -0.0531,  0.0659,  0.0382,\n",
       "                       0.1093,  0.0532, -0.0076, -0.0257,  0.1481,  0.0061,  0.0050, -0.0790,\n",
       "                      -0.0017, -0.0729,  0.0599, -0.0320,  0.0093, -0.0344, -0.0654,  0.0232,\n",
       "                       0.0952,  0.0116,  0.0790,  0.0364,  0.0162, -0.0206,  0.1156, -0.0841,\n",
       "                      -0.0019, -0.0664, -0.0130, -0.0796, -0.0275, -0.0098, -0.0591,  0.1234,\n",
       "                       0.0117,  0.0171, -0.0609,  0.0168,  0.0294, -0.0598,  0.0418, -0.0391,\n",
       "                      -0.0061, -0.0044, -0.1040, -0.0768,  0.0175, -0.0453, -0.0703, -0.0216,\n",
       "                      -0.0874,  0.2143,  0.0969,  0.1341, -0.0328, -0.0803, -0.0139, -0.0957,\n",
       "                      -0.0043, -0.0428, -0.1892, -0.0872,  0.0785,  0.0644,  0.0512, -0.0385,\n",
       "                       0.0719, -0.1010, -0.0239,  0.0519, -0.0877, -0.0285,  0.0004, -0.0129,\n",
       "                       0.1258,  0.0886,  0.0468,  0.0866,  0.0836,  0.0314,  0.0601, -0.0675,\n",
       "                       0.1697,  0.1270,  0.0066,  0.1343,  0.1064,  0.1694,  0.0613,  0.1240,\n",
       "                      -0.1044,  0.0232,  0.0114, -0.0515, -0.0555,  0.0703,  0.1661,  0.0477,\n",
       "                       0.0240,  0.0787, -0.0674, -0.0542,  0.1434,  0.1319,  0.1371,  0.0651,\n",
       "                      -0.0432, -0.0421,  0.1089, -0.0419,  0.1144, -0.1419, -0.0925, -0.0318,\n",
       "                      -0.0062,  0.1395,  0.0270,  0.0424,  0.1496, -0.0944, -0.0193, -0.2151,\n",
       "                      -0.0089, -0.0559,  0.2257, -0.0065, -0.0706, -0.1022, -0.0338, -0.0317,\n",
       "                      -0.0973,  0.0096,  0.1366,  0.0570,  0.0097, -0.0377,  0.1571, -0.0165,\n",
       "                      -0.0409, -0.0616,  0.0934, -0.1339,  0.1017,  0.0193,  0.1074,  0.0712,\n",
       "                      -0.0107,  0.0445, -0.0136,  0.1112, -0.1029,  0.0509, -0.0164,  0.1248,\n",
       "                      -0.0997,  0.0883,  0.0155, -0.0433,  0.0190, -0.0474, -0.0509, -0.0873,\n",
       "                      -0.0062,  0.0880, -0.0019,  0.1015,  0.0470, -0.1199,  0.1274,  0.0824,\n",
       "                      -0.1433,  0.0391,  0.1277,  0.0297,  0.0234,  0.0324,  0.1151,  0.0365])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-6.5845e-02,  1.7928e-01,  8.7811e-02,  5.1025e-02,  1.0886e-01,\n",
       "                      -1.0284e-01,  1.7573e-02, -4.3204e-02,  3.7202e-02, -5.4174e-02,\n",
       "                      -1.2048e-01,  3.3653e-02,  5.8888e-02,  1.6179e-02,  2.2513e-02,\n",
       "                      -1.8560e-02,  5.3358e-02,  1.3386e-01, -7.3213e-02,  5.2656e-02,\n",
       "                      -2.0112e-01, -1.2809e-01,  7.8069e-02, -6.4740e-02,  1.2223e-01,\n",
       "                       1.3124e-01,  1.4388e-02,  1.2121e-02,  5.8891e-02,  9.5614e-02,\n",
       "                      -5.3350e-02, -8.9961e-02,  6.4521e-02,  4.6941e-02,  1.5799e-01,\n",
       "                       1.2661e-01,  8.8336e-02,  1.3243e-01, -4.9480e-02,  8.4532e-02,\n",
       "                      -1.4708e-01, -4.5916e-02,  1.6142e-01,  7.1244e-02, -7.9004e-02,\n",
       "                      -1.1143e-01,  6.1884e-02,  4.6312e-02,  1.5419e-02,  2.6509e-02,\n",
       "                       3.5692e-02, -2.4109e-02,  1.1011e-01,  1.2118e-01,  7.7858e-02,\n",
       "                       4.5753e-02,  5.5289e-02,  2.1001e-02,  1.0037e-01, -3.7478e-02,\n",
       "                      -4.4459e-04, -8.5267e-02, -1.5495e-02,  2.9003e-02,  1.6928e-03,\n",
       "                       1.7441e-01,  9.0577e-02, -7.0513e-03, -1.7466e-02, -7.7107e-02,\n",
       "                      -1.2197e-01, -2.0109e-01,  2.9534e-04,  2.8282e-02,  1.2078e-01,\n",
       "                       5.8669e-02, -3.7377e-02,  8.8782e-02,  1.8656e-02,  8.4982e-02,\n",
       "                      -1.0215e-01,  3.5000e-02,  2.0985e-01,  4.1564e-02, -1.4029e-01,\n",
       "                       5.8371e-02,  3.8370e-02, -7.9363e-02,  8.4530e-02,  2.1362e-02,\n",
       "                       2.0503e-01, -8.8946e-02,  8.1519e-04,  8.3708e-02, -7.0717e-02,\n",
       "                       6.6754e-02,  1.3886e-02, -2.2250e-02, -2.8464e-02,  8.6983e-02,\n",
       "                      -1.5272e-02, -3.2940e-02,  4.2733e-02,  1.2449e-01, -1.0078e-01,\n",
       "                       8.5501e-02,  8.1492e-02,  8.3459e-02,  1.6467e-01,  1.8313e-03,\n",
       "                      -1.3132e-01, -1.8474e-01,  1.2480e-02,  3.1831e-02, -2.7927e-02,\n",
       "                       5.0241e-02, -8.0942e-02, -1.1378e-03, -2.9400e-03, -5.2172e-02,\n",
       "                      -5.4076e-02, -1.2384e-01,  5.8036e-02, -1.0240e-01,  7.7216e-02,\n",
       "                      -1.1893e-01,  9.9787e-04,  6.8279e-02, -7.5875e-02, -9.0575e-02,\n",
       "                       6.4834e-02,  1.8590e-04, -7.7729e-02, -3.4115e-02, -9.7344e-02,\n",
       "                       1.3475e-02, -1.7387e-01, -6.4666e-02, -1.4614e-01, -1.0357e-01,\n",
       "                      -1.2020e-02, -1.0561e-01,  2.6366e-02, -1.6328e-01, -2.3187e-01,\n",
       "                      -1.4418e-02, -6.9633e-02,  1.1661e-02,  6.5259e-03, -1.0412e-01,\n",
       "                       6.4104e-02,  8.2275e-02,  7.4857e-02,  4.5383e-02, -1.2488e-01,\n",
       "                       2.8094e-02,  6.0072e-02, -4.0657e-02,  4.3123e-02, -1.1709e-01,\n",
       "                       6.4725e-02, -1.6672e-01, -6.4612e-02, -5.0914e-02,  2.3210e-02,\n",
       "                       9.0553e-03, -1.4809e-01,  6.2411e-02, -1.1023e-01, -1.8839e-01,\n",
       "                       2.9360e-02, -1.0707e-01, -4.7699e-02, -2.4409e-02,  1.2027e-01,\n",
       "                      -6.6931e-02, -2.9417e-02,  1.0165e-01,  7.8906e-03,  1.7967e-02,\n",
       "                      -1.0141e-01, -1.1094e-01, -7.5156e-02, -2.3321e-02,  4.6810e-03,\n",
       "                       7.0932e-02,  4.1317e-02,  1.4579e-02, -3.2881e-02, -3.5396e-02,\n",
       "                      -6.8653e-02,  1.0251e-01, -1.0064e-01, -7.9406e-02, -8.3487e-02,\n",
       "                       8.1213e-02, -6.9539e-02, -2.3718e-01, -2.9287e-02, -1.9382e-01,\n",
       "                      -4.5504e-02,  7.1446e-02, -5.9981e-02, -1.7750e-01, -5.2099e-02,\n",
       "                      -4.7067e-02, -1.2373e-02,  6.5523e-02, -1.4710e-01,  8.8575e-02,\n",
       "                      -6.5512e-02,  2.1085e-02, -1.0664e-01, -9.9664e-02,  6.1964e-02,\n",
       "                      -6.6606e-02, -1.2807e-01,  1.0943e-04,  5.1724e-02, -4.6281e-02,\n",
       "                       7.1204e-02,  9.3552e-02,  7.9310e-02,  8.5569e-02, -2.1994e-02,\n",
       "                      -1.1062e-01, -5.4564e-02, -1.2286e-01, -5.9790e-02,  7.0768e-03,\n",
       "                      -2.5698e-02,  3.2816e-02, -1.1599e-01,  1.6330e-01, -1.7059e-01,\n",
       "                      -9.9337e-02, -7.3283e-02,  5.8531e-02,  9.6292e-02,  1.4906e-02,\n",
       "                      -5.7481e-02, -2.4573e-03,  2.4317e-02, -2.5291e-02, -1.2218e-01,\n",
       "                      -2.0642e-01, -1.1131e-01, -1.1198e-01, -3.8062e-02,  1.8479e-02,\n",
       "                       1.7723e-01, -8.2756e-02, -7.2459e-02, -2.4970e-02,  6.3913e-02,\n",
       "                       8.3707e-02, -3.0996e-02,  1.0968e-01, -8.0619e-02, -1.7984e-02,\n",
       "                       3.3053e-02, -7.3251e-02, -5.7059e-02, -2.7864e-02, -5.6100e-03,\n",
       "                      -1.3335e-03, -1.2338e-01,  3.6088e-02, -1.5990e-02, -3.7243e-02,\n",
       "                       5.4694e-02,  5.4833e-03, -5.0563e-02,  3.2835e-02, -3.8411e-03,\n",
       "                      -1.2330e-02,  6.0296e-02, -2.3840e-02,  1.3459e-01,  2.0374e-02,\n",
       "                      -5.5024e-03,  4.7163e-02,  4.3772e-02, -3.5249e-02, -2.8698e-02,\n",
       "                      -1.7335e-03, -5.9399e-02, -7.0185e-02, -3.7808e-02, -5.1618e-02,\n",
       "                      -5.8947e-02,  5.0669e-02,  6.1038e-02,  9.7197e-02,  3.2724e-02,\n",
       "                      -1.3427e-03, -3.2133e-02, -6.1442e-02, -5.1005e-02, -1.0809e-01,\n",
       "                      -2.8531e-02,  1.0199e-01,  1.9192e-02,  9.9372e-02, -7.2435e-02,\n",
       "                      -2.7937e-02, -6.2068e-02,  1.2720e-01, -3.2792e-02,  2.1122e-02,\n",
       "                       1.7278e-02, -2.6858e-02,  2.2628e-02,  2.0989e-02, -9.0530e-02,\n",
       "                       1.7469e-02, -1.7053e-02, -5.9327e-02, -7.1921e-02, -5.8116e-02,\n",
       "                       1.0874e-03,  8.3996e-02, -3.9843e-02, -1.7068e-02, -6.1107e-02,\n",
       "                      -4.4884e-02, -8.1393e-03,  1.7315e-03,  6.8226e-02,  7.2327e-03,\n",
       "                      -3.9537e-03,  2.4010e-02,  1.8119e-01,  3.0521e-02, -3.3672e-03,\n",
       "                       2.3388e-02, -3.2001e-02, -1.0424e-01, -8.0488e-02,  1.9734e-02,\n",
       "                       6.9542e-02, -6.5666e-03, -2.8332e-02,  5.8425e-02, -2.9375e-02,\n",
       "                       8.0278e-02,  1.8880e-02,  7.0274e-02,  5.7093e-02,  6.3261e-02,\n",
       "                       5.9305e-02,  1.2014e-01,  6.9600e-02,  2.4245e-02,  1.5020e-02,\n",
       "                       2.0881e-02,  3.9367e-02, -6.1000e-02,  4.7745e-02,  9.3586e-02,\n",
       "                      -2.0667e-03,  7.9353e-02, -6.8944e-03,  3.9867e-02, -5.6441e-02,\n",
       "                       9.4441e-02,  2.0523e-02, -6.5349e-02, -4.8276e-02,  2.8900e-02,\n",
       "                       1.0112e-01,  5.3966e-02,  7.3170e-02,  7.2307e-02,  3.7694e-02,\n",
       "                       4.8901e-04,  3.3641e-02, -5.4995e-02, -1.7940e-02,  4.1674e-02,\n",
       "                       3.5275e-02, -7.0625e-02,  3.3427e-02,  4.7556e-02,  2.1448e-02,\n",
       "                       5.6389e-02, -9.9035e-03,  8.0572e-02,  3.7412e-03,  2.7190e-03,\n",
       "                      -1.4912e-01, -1.3121e-01, -2.6423e-02, -9.2976e-02, -5.2077e-02,\n",
       "                       2.7868e-02,  1.8406e-01, -1.2449e-01,  1.2093e-01,  9.4194e-02,\n",
       "                      -3.1065e-02,  4.7361e-02, -5.9578e-02,  3.4238e-02, -1.9126e-01,\n",
       "                      -1.3301e-01,  8.5477e-02, -1.2020e-02,  2.1116e-01,  6.2610e-02,\n",
       "                       7.6621e-02,  1.0515e-01, -4.9730e-02,  1.6729e-01,  4.7748e-02,\n",
       "                      -4.0999e-03,  1.1316e-01,  3.8577e-02, -4.8785e-04,  2.6446e-02,\n",
       "                       4.6078e-03,  1.0285e-01, -3.8640e-02,  8.1629e-02, -6.1352e-02,\n",
       "                       2.7454e-03, -6.9320e-02,  3.0577e-02, -4.8750e-02,  6.4506e-02,\n",
       "                       1.4388e-01, -3.0175e-02,  1.4561e-01,  2.4811e-01,  8.3064e-02,\n",
       "                      -3.1674e-03,  2.8941e-02,  8.1341e-02,  1.4477e-01,  5.1653e-02,\n",
       "                      -6.3776e-02, -1.1339e-01,  1.3736e-01,  5.8430e-02,  1.3319e-01,\n",
       "                      -1.0058e-01, -5.8827e-02,  7.4319e-03,  7.0209e-02,  2.2601e-01,\n",
       "                      -1.0964e-02, -1.0760e-01,  2.2038e-01, -1.0360e-01, -2.1416e-02,\n",
       "                      -1.6031e-01, -1.6823e-01,  2.9419e-02,  2.4653e-01, -7.4625e-02,\n",
       "                      -6.0111e-03, -4.6988e-02, -1.1747e-01, -1.4440e-02,  1.0343e-01,\n",
       "                      -5.0358e-02,  1.2920e-01,  2.7420e-02, -1.8910e-02, -7.0137e-02,\n",
       "                      -4.3852e-02,  8.4037e-02,  9.3494e-02, -1.2043e-01,  1.4443e-01,\n",
       "                      -1.5905e-01, -9.9707e-02, -4.2525e-02,  5.0375e-02,  1.0007e-01,\n",
       "                       9.8690e-02, -1.4221e-02, -6.4064e-02,  5.9012e-02, -7.2389e-02,\n",
       "                       3.6200e-02, -4.9319e-02,  2.4244e-03, -1.1891e-01,  1.0165e-02,\n",
       "                       9.6005e-02, -1.1524e-01,  4.2021e-02, -2.7848e-02,  5.9922e-02,\n",
       "                      -9.4145e-02,  6.7742e-03,  9.3523e-02, -1.0024e-02, -2.3504e-02,\n",
       "                      -2.3123e-02,  7.1280e-02, -3.7198e-02,  7.0218e-03, -1.0605e-01,\n",
       "                      -2.9845e-02,  1.4310e-01, -2.4984e-02, -3.7281e-02,  9.5072e-02,\n",
       "                      -5.6301e-02, -5.0148e-02])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0304, -0.1769,  0.1057,  ..., -0.1949, -0.2425, -0.1684],\n",
       "                      [-0.0132, -0.2353, -0.1366,  ..., -0.1505,  0.2433, -0.0851],\n",
       "                      [ 0.0436, -0.1189, -0.2042,  ..., -0.3438,  0.2010,  0.0510],\n",
       "                      ...,\n",
       "                      [-0.0709,  0.0503, -0.2277,  ...,  0.1580, -0.1494,  0.1193],\n",
       "                      [-0.0359, -0.3115,  0.2396,  ..., -0.3766, -0.3189, -0.2962],\n",
       "                      [ 0.1269, -0.0782,  0.0363,  ..., -0.0840, -0.0385, -0.2602]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0449,  0.0913, -0.2035,  ..., -0.1823,  0.0566, -0.0375],\n",
       "                      [-0.4131, -0.2231,  0.2329,  ...,  0.2179, -0.1447, -0.0894],\n",
       "                      [-0.0063, -0.0906,  0.0070,  ..., -0.0080, -0.1248,  0.1348],\n",
       "                      ...,\n",
       "                      [ 0.1758, -0.2001, -0.0346,  ..., -0.1426,  0.1379,  0.0515],\n",
       "                      [ 0.2621, -0.1108,  0.1748,  ...,  0.2655, -0.0187,  0.4432],\n",
       "                      [-0.0163, -0.0204,  0.0848,  ...,  0.0944,  0.0581, -0.2346]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 1.5093e-03, -1.7294e-02,  1.0708e-03, -5.3248e-02,  6.3307e-02,\n",
       "                      -1.2603e-02,  9.4873e-02, -2.4703e-03,  1.3095e-01,  4.8634e-02,\n",
       "                      -5.7723e-03,  5.3149e-02,  2.9843e-02,  7.2618e-02,  1.0935e-01,\n",
       "                      -4.0510e-02,  9.9283e-02,  1.0730e-01,  1.3932e-01,  1.0731e-01,\n",
       "                       1.4244e-02,  1.1081e-01,  7.1203e-02,  2.7677e-02,  4.4840e-02,\n",
       "                       1.1384e-01, -8.8352e-03, -4.8068e-02,  4.8375e-02,  1.8878e-01,\n",
       "                       4.4588e-02,  9.0698e-02,  2.3539e-01, -3.6502e-02,  3.6602e-02,\n",
       "                       6.8586e-02,  9.8086e-02,  7.9602e-02,  1.2806e-01, -4.7439e-03,\n",
       "                      -1.3698e-04,  1.3353e-01,  1.5342e-01,  1.4301e-01,  8.7192e-02,\n",
       "                      -1.5217e-04,  1.4759e-01,  4.5763e-02,  4.0132e-02, -7.5354e-02,\n",
       "                       8.5357e-02, -2.6871e-03,  1.2825e-02, -7.5945e-02, -6.8372e-02,\n",
       "                      -2.1714e-02, -2.1410e-02,  1.0797e-01,  4.2020e-02,  5.1391e-02,\n",
       "                       1.2813e-01,  9.8605e-02,  1.4042e-01,  9.3565e-02, -9.7080e-02,\n",
       "                       1.3207e-01,  7.4054e-02,  5.5300e-02,  9.2100e-02,  1.4423e-01,\n",
       "                       5.5815e-02,  4.5854e-02,  8.3925e-03,  8.1298e-02,  1.2731e-02,\n",
       "                       4.5880e-02, -6.8756e-03,  9.7722e-02,  4.8124e-02,  1.1402e-01,\n",
       "                      -1.6904e-02,  2.2102e-01, -4.4136e-02,  2.3783e-01,  7.4766e-02,\n",
       "                       3.7895e-02, -3.6558e-02,  1.4059e-01,  6.2565e-02,  2.8518e-01,\n",
       "                       8.5747e-02,  7.2156e-02,  1.7925e-01,  3.0184e-02, -4.8955e-02,\n",
       "                       1.0172e-01,  2.8487e-02,  1.2674e-01, -7.7445e-02,  4.9634e-02,\n",
       "                      -6.0700e-02,  2.4423e-02,  5.4910e-02, -5.5859e-02,  2.5897e-01,\n",
       "                       3.3345e-02,  8.6706e-02,  1.4567e-01,  3.1926e-02,  9.9320e-02,\n",
       "                       2.1509e-01,  1.4401e-03,  1.2647e-01,  1.0891e-01, -9.5392e-02,\n",
       "                       1.1601e-01,  2.4992e-01,  7.8538e-02, -3.8929e-02,  1.4676e-01,\n",
       "                       5.0814e-02,  2.1292e-01, -1.3312e-02, -8.4604e-02,  1.0842e-01,\n",
       "                       1.2154e-01,  7.7950e-04,  5.3355e-02,  2.1811e-02, -7.7077e-02,\n",
       "                      -2.2775e-02, -3.5353e-02,  1.3295e-01,  2.7428e-02, -7.6513e-02,\n",
       "                       1.3465e-01, -7.5282e-02, -9.3710e-02,  4.4209e-02, -3.0959e-02,\n",
       "                       2.5940e-02,  2.4107e-02, -1.8101e-01, -1.4293e-01,  2.6945e-02,\n",
       "                      -3.2236e-02,  3.1623e-02, -9.6254e-02,  6.5081e-02, -1.3932e-01,\n",
       "                       2.7506e-02,  3.7817e-02, -1.1986e-01, -1.0545e-01, -1.9801e-01,\n",
       "                      -9.3348e-02, -1.1813e-01,  3.1965e-02, -1.0566e-01, -1.5925e-02,\n",
       "                      -8.4040e-02, -2.4694e-02, -1.3335e-01,  1.0904e-02, -1.0890e-01,\n",
       "                      -1.5485e-01,  9.6546e-02,  2.4938e-02,  2.4220e-02, -2.1037e-02,\n",
       "                      -2.7786e-03, -8.6101e-02, -7.9445e-02,  1.4496e-01, -6.1950e-04,\n",
       "                      -2.3440e-02,  9.2375e-02, -2.6860e-02, -9.0744e-02, -1.9061e-01,\n",
       "                       4.4416e-02, -9.8072e-02, -1.2163e-01, -9.3357e-03, -2.2913e-02,\n",
       "                       5.6521e-03, -1.3483e-01, -7.9379e-02, -1.2363e-01, -4.1868e-02,\n",
       "                      -1.5241e-01, -5.4930e-02, -1.5349e-02, -7.2216e-02, -6.6404e-02,\n",
       "                      -7.3809e-02, -1.0979e-01, -3.6056e-02, -8.1437e-02, -7.9552e-02,\n",
       "                       4.5747e-02,  9.4898e-02, -2.4569e-02, -6.0916e-02, -6.6724e-02,\n",
       "                       1.1992e-01,  5.6942e-02, -4.0980e-02,  1.1524e-01, -1.8892e-01,\n",
       "                      -1.8597e-02, -1.3348e-01,  1.5141e-01, -7.5557e-02,  3.5236e-02,\n",
       "                      -8.7492e-02,  7.7650e-02, -1.2686e-02, -1.0329e-01, -1.2160e-01,\n",
       "                      -5.5368e-02, -1.2847e-01, -5.7733e-02,  1.2607e-01,  1.1367e-01,\n",
       "                      -5.0794e-02,  1.7654e-01, -1.1722e-01, -7.8890e-02, -3.4965e-02,\n",
       "                      -1.0395e-01,  5.6797e-03,  1.0022e-01, -8.9833e-02, -1.4291e-01,\n",
       "                       3.6662e-02, -8.0579e-02, -1.1140e-01, -2.2208e-01,  3.4975e-03,\n",
       "                      -1.8697e-01, -1.5035e-01, -1.7551e-02, -1.1718e-01,  2.9260e-02,\n",
       "                      -1.2583e-01, -1.2003e-01,  6.4035e-02,  1.0764e-01, -1.9405e-01,\n",
       "                      -1.0490e-01, -1.3970e-01, -1.5993e-01,  1.1589e-01, -1.1402e-01,\n",
       "                      -7.8770e-02, -2.5310e-02, -8.4794e-03, -5.4288e-02,  7.2217e-03,\n",
       "                       1.7535e-02, -4.6074e-02, -2.1503e-02, -4.0235e-02,  4.6489e-02,\n",
       "                       4.4710e-03,  4.6259e-02, -4.3626e-02,  3.6318e-02, -7.2375e-03,\n",
       "                       9.3506e-02,  1.8840e-01, -8.6321e-02,  3.4023e-02, -5.9771e-02,\n",
       "                       5.8580e-02, -2.1777e-02,  9.1821e-02, -5.8934e-02,  2.8301e-03,\n",
       "                      -5.6291e-02, -2.5250e-02, -1.0667e-01, -5.8053e-02,  4.2100e-02,\n",
       "                      -2.9827e-02,  5.0131e-02,  1.0422e-01,  1.5899e-02, -7.5560e-03,\n",
       "                       6.3965e-02, -1.5779e-02, -2.4437e-02,  8.1918e-02,  4.6901e-02,\n",
       "                      -5.1177e-03, -3.2244e-02, -1.4293e-02,  4.0257e-02,  3.4132e-02,\n",
       "                      -5.5024e-02,  3.9586e-02,  4.6965e-02,  8.1061e-02,  2.2405e-02,\n",
       "                       4.0875e-04,  4.1478e-02, -5.6407e-02,  8.8308e-02, -4.9918e-02,\n",
       "                      -5.8082e-02,  6.7190e-02, -3.3498e-02, -3.7239e-02, -3.3232e-02,\n",
       "                      -2.4246e-02,  8.3665e-03,  3.9855e-02,  4.3569e-02,  4.9873e-02,\n",
       "                      -6.3369e-02, -5.4277e-02,  1.5583e-02, -3.3970e-02,  3.6071e-02,\n",
       "                       1.2037e-01,  7.2149e-02,  2.1563e-02,  4.4871e-02, -1.2229e-01,\n",
       "                      -3.1064e-02, -1.4939e-01, -6.8264e-02, -5.4456e-02,  6.5914e-02,\n",
       "                      -1.2514e-01,  7.7896e-02,  9.5690e-02, -1.4925e-03,  1.3332e-02,\n",
       "                       1.8769e-02, -2.9225e-02, -1.2124e-02,  1.9632e-02, -5.2202e-02,\n",
       "                       8.5632e-02,  1.4753e-01, -3.5901e-02,  1.4133e-01, -8.2350e-02,\n",
       "                       4.9219e-02,  1.1466e-01, -1.5411e-01, -7.5470e-02,  4.1044e-02,\n",
       "                       2.4097e-02,  4.2025e-02, -2.2252e-02, -2.3966e-02, -1.1663e-01,\n",
       "                      -4.1250e-02,  4.6990e-02, -1.0944e-01,  2.6714e-02,  1.8686e-02,\n",
       "                       3.3819e-02,  8.0353e-02, -1.2870e-01,  4.5852e-02,  6.9607e-02,\n",
       "                      -2.4323e-02,  5.1878e-02,  8.7183e-03,  9.2923e-02, -7.2161e-02,\n",
       "                      -5.3858e-02,  1.5911e-02, -5.8285e-02, -3.9282e-02, -2.7424e-02,\n",
       "                       4.6533e-02, -3.8983e-02,  4.0675e-02,  8.0219e-02,  6.2050e-02,\n",
       "                       8.2626e-02,  1.5124e-01,  3.7826e-02,  6.3704e-02,  6.0997e-02,\n",
       "                      -1.1874e-01, -2.7926e-02,  9.3122e-02, -3.2659e-02, -1.0609e-01,\n",
       "                       7.2279e-02, -1.7081e-02, -2.8115e-02,  1.4395e-02,  5.8757e-02,\n",
       "                       1.4259e-02,  4.8953e-02,  2.0199e-01,  1.1684e-01, -5.4048e-02,\n",
       "                       1.0850e-01, -1.4989e-02,  1.7088e-02, -7.2919e-02,  1.4503e-02,\n",
       "                       5.5466e-02,  3.6512e-02,  1.4899e-01,  1.1216e-01,  1.0279e-01,\n",
       "                      -1.2373e-02,  1.5697e-01, -2.9343e-02,  6.6502e-02,  9.6213e-02,\n",
       "                       3.6345e-02,  1.1127e-01,  7.2630e-02,  7.6516e-02, -3.3268e-02,\n",
       "                       3.5753e-02,  1.2916e-01,  9.3016e-02,  7.5433e-02,  5.3255e-02,\n",
       "                       3.3032e-01,  5.6215e-02,  8.2035e-02,  1.0961e-02, -1.9535e-02,\n",
       "                      -8.8605e-02, -8.0214e-02,  5.2077e-02,  9.7842e-02,  3.7266e-02,\n",
       "                       5.3089e-02,  8.7498e-03,  1.5310e-01,  2.2861e-01, -2.7502e-02,\n",
       "                       1.2067e-01,  1.0588e-01, -6.9551e-02, -4.6636e-02,  2.5329e-01,\n",
       "                      -5.2886e-03,  5.1580e-03,  2.6898e-01,  1.1208e-01,  6.4235e-02,\n",
       "                       9.9949e-02,  1.5406e-01,  8.4600e-02,  1.2118e-01,  2.5048e-02,\n",
       "                       2.9314e-02,  1.2370e-01,  6.7594e-02,  1.7354e-01,  7.0930e-02,\n",
       "                       9.3190e-02,  1.0882e-01,  1.3042e-01, -1.1642e-03, -8.9057e-02,\n",
       "                      -6.3763e-03,  1.5163e-01,  1.4309e-01,  3.0163e-01,  8.8911e-03,\n",
       "                      -3.5706e-02,  7.1465e-02, -9.9378e-02,  2.1306e-02,  1.9353e-01,\n",
       "                       1.9190e-01,  2.0942e-02,  4.9682e-02,  7.7077e-02,  7.2656e-02,\n",
       "                      -9.7039e-03,  1.7361e-01,  6.2355e-02,  2.4332e-01,  2.0158e-01,\n",
       "                      -1.4124e-02,  1.3226e-01,  3.1729e-02,  2.9971e-01,  8.0252e-02,\n",
       "                      -3.9724e-02,  1.2998e-01, -6.3385e-02,  1.0954e-01,  1.3016e-01,\n",
       "                       8.8742e-02,  7.1221e-02, -6.1736e-03,  9.7091e-02,  1.3575e-01,\n",
       "                      -2.1153e-04,  8.9946e-02,  5.1785e-02,  1.4378e-02,  1.2320e-02,\n",
       "                      -9.3788e-02,  1.7264e-02])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-0.0028,  0.1016,  0.1440,  0.0959,  0.0402, -0.0354,  0.1140, -0.0117,\n",
       "                       0.0790, -0.0604, -0.0685,  0.1527,  0.1382, -0.0692,  0.1429, -0.0203,\n",
       "                       0.0627, -0.0110,  0.1329,  0.0841,  0.0422,  0.0279,  0.0479,  0.1189,\n",
       "                       0.0177,  0.0826, -0.0334, -0.0823,  0.0397,  0.1603,  0.2222,  0.0961,\n",
       "                       0.2136, -0.0257,  0.1822,  0.0322, -0.0712,  0.0769,  0.1016,  0.1283,\n",
       "                       0.0237,  0.0809,  0.0556,  0.1631,  0.0180,  0.0287,  0.0597, -0.0378,\n",
       "                       0.0080,  0.0206,  0.0075, -0.1622,  0.0503, -0.0538, -0.0472, -0.0659,\n",
       "                      -0.0410,  0.0256,  0.0318,  0.1044,  0.0500,  0.1398,  0.1675,  0.1321,\n",
       "                      -0.1113,  0.1506,  0.1203,  0.1005,  0.1230,  0.2444,  0.0577,  0.1826,\n",
       "                       0.0875,  0.1428, -0.0265,  0.1076,  0.0285,  0.0717,  0.1151,  0.0109,\n",
       "                       0.0022,  0.1669,  0.0147,  0.1552,  0.0499, -0.0571, -0.0913,  0.1466,\n",
       "                       0.1274,  0.1804,  0.1811,  0.0699,  0.0849, -0.0097,  0.0082,  0.1537,\n",
       "                       0.1631,  0.1915, -0.0517,  0.0962,  0.0537, -0.0676,  0.1127,  0.0369,\n",
       "                       0.1496,  0.1706, -0.0016,  0.1780,  0.1183,  0.2880,  0.1661,  0.1203,\n",
       "                      -0.0031,  0.0187, -0.0605,  0.0870, -0.0248,  0.0428,  0.0167,  0.0701,\n",
       "                       0.0283,  0.0053,  0.0645, -0.0896,  0.0373, -0.0216,  0.0191, -0.0244,\n",
       "                      -0.0629, -0.1490, -0.0696, -0.0745, -0.0217,  0.0918, -0.2033,  0.2251,\n",
       "                      -0.1159, -0.0640, -0.1310, -0.0702,  0.0021,  0.0915,  0.0083, -0.0714,\n",
       "                      -0.0248, -0.2238,  0.0862, -0.0426,  0.0034, -0.0870, -0.0405, -0.0417,\n",
       "                       0.1250, -0.1507,  0.0317,  0.0223, -0.0567,  0.0653, -0.0422,  0.1081,\n",
       "                      -0.0043, -0.1249, -0.0249, -0.1085, -0.0724, -0.0925, -0.0753,  0.0213,\n",
       "                      -0.0602, -0.0023,  0.0444,  0.0344, -0.0843,  0.2141, -0.0829, -0.0208,\n",
       "                      -0.0872, -0.0569, -0.1134, -0.0471,  0.0243, -0.0977,  0.0753,  0.0408,\n",
       "                       0.0527, -0.0197, -0.1625, -0.0398,  0.0101, -0.1122, -0.2203, -0.0189,\n",
       "                      -0.1281, -0.1132, -0.1660,  0.0397, -0.0976, -0.0653, -0.1614, -0.1041,\n",
       "                       0.0239,  0.1780,  0.0613, -0.0730, -0.1353, -0.0360,  0.0098, -0.0430,\n",
       "                      -0.0479, -0.1206,  0.0606,  0.0117,  0.0297, -0.0406, -0.0149, -0.1191,\n",
       "                      -0.0568,  0.0199, -0.0375, -0.0748, -0.0355, -0.0622, -0.0235,  0.0084,\n",
       "                       0.0621, -0.1098,  0.0518, -0.0939, -0.0493, -0.1034, -0.0619,  0.0160,\n",
       "                      -0.0228, -0.0755, -0.0637,  0.0505, -0.1579, -0.1484, -0.0960, -0.0672,\n",
       "                      -0.0893, -0.0127, -0.0764, -0.1440, -0.0612, -0.0669, -0.0134,  0.0139,\n",
       "                      -0.1204, -0.1161, -0.0466, -0.0517, -0.0333,  0.1692, -0.0214, -0.0217,\n",
       "                      -0.0214, -0.0195,  0.0337,  0.0215, -0.0674, -0.0337,  0.0122,  0.0585,\n",
       "                       0.0174,  0.1376,  0.0323,  0.0637, -0.0353,  0.1084, -0.1195, -0.0010,\n",
       "                       0.0197,  0.0715,  0.0632,  0.0990, -0.0952,  0.1362, -0.1296, -0.0443,\n",
       "                      -0.0598,  0.0080, -0.0392, -0.0011,  0.0043, -0.0548,  0.0321,  0.0373,\n",
       "                       0.0396, -0.0880, -0.0239, -0.0380,  0.0364,  0.0495, -0.0907,  0.0021,\n",
       "                      -0.0643,  0.0503, -0.0171,  0.0811,  0.0505, -0.0532,  0.0986,  0.0057,\n",
       "                       0.0156,  0.0350, -0.0512, -0.0501,  0.0555, -0.1760,  0.0388,  0.0164,\n",
       "                      -0.0182, -0.0739, -0.0995, -0.0719, -0.0977,  0.0203, -0.0166, -0.0487,\n",
       "                       0.0565, -0.0669,  0.0526, -0.0914, -0.0477,  0.0942, -0.0409,  0.1068,\n",
       "                      -0.0597, -0.0176,  0.0829,  0.1788, -0.0442, -0.0576, -0.0280, -0.0695,\n",
       "                       0.0390,  0.0779, -0.1711, -0.0438,  0.0176,  0.0272, -0.0082,  0.0460,\n",
       "                       0.0112,  0.0826,  0.0179,  0.0445, -0.0233,  0.0519,  0.0068,  0.0246,\n",
       "                      -0.0096, -0.0081, -0.0261,  0.0242, -0.0037, -0.0339,  0.0781, -0.0753,\n",
       "                      -0.0098,  0.0608, -0.0423,  0.0015,  0.0860, -0.0839,  0.0598, -0.1117,\n",
       "                      -0.0052,  0.0516,  0.0505, -0.0470, -0.0469,  0.0768,  0.0944, -0.0231,\n",
       "                      -0.0691, -0.0806, -0.0551,  0.1290, -0.0695,  0.0908,  0.0566,  0.0863,\n",
       "                       0.1139, -0.0502,  0.0698,  0.0505, -0.0374,  0.0086,  0.0324, -0.0665,\n",
       "                      -0.0122, -0.0115,  0.0877,  0.1729,  0.0613, -0.0255,  0.0810, -0.2000,\n",
       "                       0.1474,  0.0515,  0.0435,  0.1315,  0.1190,  0.0649, -0.0071,  0.0943,\n",
       "                      -0.1282,  0.0751,  0.0540, -0.0227,  0.1884,  0.0710,  0.1376,  0.0052,\n",
       "                       0.1455, -0.0971,  0.0189,  0.0130,  0.0637,  0.1357,  0.1403,  0.0380,\n",
       "                       0.1031,  0.0526,  0.0137,  0.0321,  0.0443, -0.0519,  0.0985,  0.0234,\n",
       "                       0.0908,  0.1078, -0.0069, -0.0923,  0.0028, -0.0042,  0.0987,  0.0742,\n",
       "                      -0.0167,  0.2067, -0.0673,  0.1616, -0.1936,  0.1261,  0.0825,  0.0670,\n",
       "                      -0.0526,  0.2223, -0.0770, -0.0579,  0.1460,  0.1837,  0.0108,  0.1331,\n",
       "                       0.1139,  0.1370,  0.0635,  0.0256,  0.0035,  0.0618,  0.0621,  0.0746,\n",
       "                       0.0703,  0.1920,  0.0581,  0.1403,  0.0903,  0.0193,  0.0040,  0.0182,\n",
       "                      -0.0178,  0.2375,  0.1363,  0.0034,  0.1704, -0.0572,  0.1320,  0.1828,\n",
       "                       0.0672,  0.1681,  0.0008,  0.1251,  0.0254,  0.0710, -0.0132,  0.0614,\n",
       "                       0.2053,  0.1378,  0.0435,  0.0708,  0.0016,  0.2311,  0.2823, -0.0079,\n",
       "                       0.0398, -0.1663, -0.0478,  0.1700, -0.0132, -0.0106, -0.1678,  0.1882,\n",
       "                       0.0757,  0.0120,  0.0159,  0.0669, -0.0576,  0.1328, -0.0638,  0.0965])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[-0.0115,  0.0793, -0.0221,  ..., -0.0752,  0.1619,  0.3884],\n",
       "                      [-0.1162,  0.1112,  0.0556,  ..., -0.0441, -0.0911,  0.0344],\n",
       "                      [-0.0526,  0.1666, -0.0929,  ...,  0.1075, -0.0540, -0.3754],\n",
       "                      ...,\n",
       "                      [ 0.1084, -0.0251,  0.2395,  ..., -0.1987, -0.0008,  0.2794],\n",
       "                      [ 0.0826,  0.1094, -0.0144,  ..., -0.0300, -0.0478,  0.1074],\n",
       "                      [ 0.0617,  0.0790,  0.0126,  ..., -0.1509,  0.0560, -0.2698]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-0.1600,  0.0264, -0.1437, -0.1430, -0.1375, -0.1453,  0.0985, -0.0984,\n",
       "                      -0.0511,  0.0548,  0.3189,  0.1937, -0.3089, -0.1760,  0.0398, -0.0663,\n",
       "                       0.0611, -0.1404,  0.0213, -0.0196, -0.1706,  0.1367, -0.0743,  0.0764,\n",
       "                       0.0091,  0.0574,  0.0045,  0.1706,  0.1464,  0.2044,  0.0342, -0.3102,\n",
       "                      -0.1546, -0.0708, -0.1320,  0.2015, -0.0306, -0.1486,  0.2071, -0.0828,\n",
       "                      -0.0006, -0.1102,  0.0506, -0.0075,  0.1714, -0.1715,  0.2828,  0.1498,\n",
       "                       0.0594,  0.0694,  0.3358, -0.0862,  0.0509,  0.1075, -0.0208, -0.1609,\n",
       "                      -0.0480,  0.1050,  0.0556, -0.0447,  0.1844,  0.2925, -0.1347, -0.0023])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[-0.0470,  0.0877, -0.3497,  ...,  0.2049,  0.0375, -0.2173],\n",
       "                      [ 0.0243, -0.2699,  0.1929,  ..., -0.2736, -0.0195,  0.0213],\n",
       "                      [ 0.1051,  0.0440, -0.2230,  ..., -0.1261,  0.0310,  0.0805],\n",
       "                      ...,\n",
       "                      [-0.1742, -0.1860,  0.1543,  ..., -0.0181,  0.0313,  0.1364],\n",
       "                      [-0.0505,  0.0285,  0.1201,  ..., -0.0419, -0.0330,  0.4152],\n",
       "                      [ 0.1033,  0.2520,  0.0204,  ...,  0.1144,  0.0080,  0.0865]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([ 0.0170, -0.2598, -0.0888,  0.2357, -0.0340, -0.3024,  0.1598, -0.0290,\n",
       "                      -0.2994, -0.0304,  0.0118,  0.0400,  0.1424, -0.1855,  0.0663,  0.3138,\n",
       "                      -0.0721,  0.1455, -0.0452, -0.0364,  0.1721, -0.0350,  0.0714, -0.1763,\n",
       "                      -0.0096,  0.0826, -0.2910,  0.2706, -0.0494,  0.1934,  0.0975,  0.0840,\n",
       "                      -0.2257, -0.2671, -0.1661, -0.1706, -0.4048, -0.1433,  0.3059,  0.0378,\n",
       "                      -0.0483, -0.1391,  0.0417, -0.3722, -0.3111, -0.0849, -0.0849, -0.0761,\n",
       "                       0.2144, -0.1332, -0.1327, -0.0239, -0.0985,  0.2075, -0.1870, -0.3072,\n",
       "                       0.3610, -0.2379,  0.1957, -0.2582, -0.1615, -0.0334,  0.0743, -0.1941])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[-0.0831, -0.2272,  0.1097,  ..., -0.0076, -0.2086, -0.0522],\n",
       "                      [ 0.1477,  0.1137,  0.3695,  ..., -0.2946, -0.0256,  0.0948],\n",
       "                      [ 0.1926, -0.0077,  0.1445,  ..., -0.1372, -0.2282,  0.2321],\n",
       "                      ...,\n",
       "                      [ 0.2008, -0.0024,  0.4629,  ...,  0.0308, -0.0621,  0.0854],\n",
       "                      [ 0.4329,  0.1513, -0.3038,  ..., -0.1602,  0.0019,  0.0510],\n",
       "                      [ 0.1250, -0.0684, -0.2957,  ...,  0.0296, -0.1321,  0.0479]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.4135, -0.0337, -0.1729,  ..., -0.2550, -0.1684, -0.3099]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
