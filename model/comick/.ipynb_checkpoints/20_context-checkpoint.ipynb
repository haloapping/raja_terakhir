{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=20,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 20)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 20)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.8986176e+07,  4.5605259e-41, -5.8986176e+07, ...,\n",
       "         0.0000000e+00,  2.3329049e-02,  4.5605259e-41],\n",
       "       [ 3.2362878e-02,  4.5605259e-41,  9.6507425e-41, ...,\n",
       "         4.5605259e-41,  9.6521438e-41,  0.0000000e+00],\n",
       "       [ 2.3329705e-02,  4.5605259e-41,  3.2319486e-02, ...,\n",
       "         4.5605259e-41,  3.2367885e-02,  4.5605259e-41],\n",
       "       ...,\n",
       "       [ 2.4856441e-03,  4.5605259e-41,  3.2663286e-02, ...,\n",
       "         4.5605259e-41,  3.2665670e-02,  4.5605259e-41],\n",
       "       [ 9.8268857e-41,  0.0000000e+00,  2.4857260e-03, ...,\n",
       "         0.0000000e+00,  2.4858005e-03,  4.5605259e-41],\n",
       "       [ 3.2668293e-02,  4.5605259e-41,  9.8284272e-41, ...,\n",
       "         4.5605259e-41,  9.8298285e-41,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02942f1fc7924638aa83180647377c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=4.3955 | F1Score=0.2763\n",
      "Batch-100: NLLLoss=5.5535 | F1Score=0.2875\n",
      "Batch-150: NLLLoss=4.9265 | F1Score=0.3023\n",
      "Batch-200: NLLLoss=4.7751 | F1Score=0.3271\n",
      "Batch-250: NLLLoss=3.1236 | F1Score=0.3532\n",
      "Batch-300: NLLLoss=4.4950 | F1Score=0.3720\n",
      "Batch-350: NLLLoss=5.0200 | F1Score=0.3871\n",
      "Batch-400: NLLLoss=3.5484 | F1Score=0.3999\n",
      "Batch-450: NLLLoss=3.1183 | F1Score=0.4152\n",
      "Batch-500: NLLLoss=3.9979 | F1Score=0.4303\n",
      "Batch-518: NLLLoss=2.8809 | F1Score=0.4347\n",
      "\n",
      "Mean NLLLoss: 4.4998 | Mean F1Score: 0.3450\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf5d6e3931a43b58ecaf2d56ff3432e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.6012 | F1Score=0.6000\n",
      "Batch-100: NLLLoss=2.3296 | F1Score=0.6050\n",
      "Batch-150: NLLLoss=3.9705 | F1Score=0.6088\n",
      "Batch-200: NLLLoss=1.6539 | F1Score=0.6097\n",
      "Batch-250: NLLLoss=2.4428 | F1Score=0.6113\n",
      "Batch-300: NLLLoss=3.1977 | F1Score=0.6152\n",
      "Batch-350: NLLLoss=2.8563 | F1Score=0.6240\n",
      "Batch-400: NLLLoss=3.2789 | F1Score=0.6296\n",
      "Batch-450: NLLLoss=2.1281 | F1Score=0.6340\n",
      "Batch-500: NLLLoss=1.6332 | F1Score=0.6398\n",
      "Batch-518: NLLLoss=2.7613 | F1Score=0.6412\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 2.6987 | Mean F1Score: 0.6164\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e365b573ecc74195abbbbea39542f625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.8852 | F1Score=0.7150\n",
      "Batch-100: NLLLoss=1.6940 | F1Score=0.7272\n",
      "Batch-150: NLLLoss=1.6992 | F1Score=0.7292\n",
      "Batch-200: NLLLoss=2.0828 | F1Score=0.7269\n",
      "Batch-250: NLLLoss=2.4494 | F1Score=0.7290\n",
      "Batch-300: NLLLoss=2.1653 | F1Score=0.7342\n",
      "Batch-350: NLLLoss=1.5881 | F1Score=0.7366\n",
      "Batch-400: NLLLoss=1.7584 | F1Score=0.7381\n",
      "Batch-450: NLLLoss=2.0288 | F1Score=0.7417\n",
      "Batch-500: NLLLoss=2.8514 | F1Score=0.7444\n",
      "Batch-518: NLLLoss=1.6888 | F1Score=0.7446\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.7877 | Mean F1Score: 0.7302\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7321357f38d64fdfadefdc41dae09f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.8594 | F1Score=0.8306\n",
      "Batch-100: NLLLoss=0.7579 | F1Score=0.8213\n",
      "Batch-150: NLLLoss=0.7341 | F1Score=0.8184\n",
      "Batch-200: NLLLoss=1.3693 | F1Score=0.8185\n",
      "Batch-250: NLLLoss=1.0412 | F1Score=0.8139\n",
      "Batch-300: NLLLoss=1.4440 | F1Score=0.8105\n",
      "Batch-350: NLLLoss=1.3702 | F1Score=0.8144\n",
      "Batch-400: NLLLoss=0.8197 | F1Score=0.8147\n",
      "Batch-450: NLLLoss=0.9357 | F1Score=0.8168\n",
      "Batch-500: NLLLoss=1.3106 | F1Score=0.8168\n",
      "Batch-518: NLLLoss=0.8287 | F1Score=0.8169\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.1618 | Mean F1Score: 0.8158\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636c4f73743f4f24af335c87fd70f2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.9553 | F1Score=0.8859\n",
      "Batch-100: NLLLoss=0.7529 | F1Score=0.8867\n",
      "Batch-150: NLLLoss=0.3564 | F1Score=0.8891\n",
      "Batch-200: NLLLoss=0.5915 | F1Score=0.8851\n",
      "Batch-250: NLLLoss=0.3411 | F1Score=0.8839\n",
      "Batch-300: NLLLoss=0.8736 | F1Score=0.8803\n",
      "Batch-350: NLLLoss=0.4582 | F1Score=0.8773\n",
      "Batch-400: NLLLoss=0.6173 | F1Score=0.8762\n",
      "Batch-450: NLLLoss=1.2344 | F1Score=0.8758\n",
      "Batch-500: NLLLoss=0.3545 | F1Score=0.8753\n",
      "Batch-518: NLLLoss=1.3853 | F1Score=0.8746\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.6822 | Mean F1Score: 0.8820\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f6a916ffba4d1cb9390516c3e1bbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.2062 | F1Score=0.9550\n",
      "Batch-100: NLLLoss=0.4009 | F1Score=0.9556\n",
      "Batch-150: NLLLoss=0.3510 | F1Score=0.9521\n",
      "Batch-200: NLLLoss=0.1695 | F1Score=0.9505\n",
      "Batch-250: NLLLoss=0.2732 | F1Score=0.9493\n",
      "Batch-300: NLLLoss=0.3856 | F1Score=0.9471\n",
      "Batch-350: NLLLoss=0.4976 | F1Score=0.9470\n",
      "Batch-400: NLLLoss=0.2082 | F1Score=0.9452\n",
      "Batch-450: NLLLoss=0.3190 | F1Score=0.9451\n",
      "Batch-500: NLLLoss=0.0796 | F1Score=0.9439\n",
      "Batch-518: NLLLoss=0.3234 | F1Score=0.9434\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.3252 | Mean F1Score: 0.9498\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2fb64f1b944157ab482b293d240995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0296 | F1Score=0.9866\n",
      "Batch-100: NLLLoss=0.0391 | F1Score=0.9895\n",
      "Batch-150: NLLLoss=0.1193 | F1Score=0.9885\n",
      "Batch-200: NLLLoss=0.1073 | F1Score=0.9889\n",
      "Batch-250: NLLLoss=0.1467 | F1Score=0.9887\n",
      "Batch-300: NLLLoss=0.0936 | F1Score=0.9896\n",
      "Batch-350: NLLLoss=0.2076 | F1Score=0.9901\n",
      "Batch-400: NLLLoss=0.1069 | F1Score=0.9891\n",
      "Batch-450: NLLLoss=0.2137 | F1Score=0.9888\n",
      "Batch-500: NLLLoss=0.1209 | F1Score=0.9886\n",
      "Batch-518: NLLLoss=0.1424 | F1Score=0.9882\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.1084 | Mean F1Score: 0.9888\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d3845a1b334c5996d0d60aaf0f2055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0579 | F1Score=0.9984\n",
      "Batch-100: NLLLoss=0.0319 | F1Score=0.9980\n",
      "Batch-150: NLLLoss=0.0279 | F1Score=0.9979\n",
      "Batch-200: NLLLoss=0.1093 | F1Score=0.9981\n",
      "Batch-250: NLLLoss=0.0120 | F1Score=0.9984\n",
      "Batch-300: NLLLoss=0.0136 | F1Score=0.9985\n",
      "Batch-350: NLLLoss=0.0277 | F1Score=0.9986\n",
      "Batch-400: NLLLoss=0.0572 | F1Score=0.9986\n",
      "Batch-450: NLLLoss=0.0333 | F1Score=0.9987\n",
      "Batch-500: NLLLoss=0.0072 | F1Score=0.9988\n",
      "Batch-518: NLLLoss=0.0053 | F1Score=0.9987\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0292 | Mean F1Score: 0.9983\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203ae7286b7c4371985e5c7d942e0ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0071 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0141 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0054 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0094 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0083 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0112 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0066 | F1Score=0.9995\n",
      "Batch-400: NLLLoss=0.0062 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0085 | F1Score=0.9995\n",
      "Batch-500: NLLLoss=0.0045 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0244 | F1Score=0.9993\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0108 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbbbf4967724ee69f771f749132f61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0072 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0079 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0061 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0059 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0031 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0104 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0054 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0072 | F1Score=0.9996\n",
      "Batch-450: NLLLoss=0.0051 | F1Score=0.9996\n",
      "Batch-500: NLLLoss=0.0027 | F1Score=0.9995\n",
      "Batch-518: NLLLoss=0.0068 | F1Score=0.9995\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0073 | Mean F1Score: 0.9997\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53ad2a23922438a943fd837b704b2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0040 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0036 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0050 | F1Score=0.9990\n",
      "Batch-200: NLLLoss=0.0094 | F1Score=0.9989\n",
      "Batch-250: NLLLoss=0.0042 | F1Score=0.9991\n",
      "Batch-300: NLLLoss=0.0030 | F1Score=0.9990\n",
      "Batch-350: NLLLoss=0.0062 | F1Score=0.9989\n",
      "Batch-400: NLLLoss=0.0141 | F1Score=0.9986\n",
      "Batch-450: NLLLoss=0.2654 | F1Score=0.9939\n",
      "Batch-500: NLLLoss=0.1820 | F1Score=0.9857\n",
      "Batch-518: NLLLoss=0.1023 | F1Score=0.9836\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0776 | Mean F1Score: 0.9975\n",
      "Patience = 1/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2313ba9721fe46c2bc7f1b1ddfb66f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.3117 | F1Score=0.9472\n",
      "Batch-100: NLLLoss=0.0538 | F1Score=0.9517\n",
      "Batch-150: NLLLoss=0.1324 | F1Score=0.9562\n",
      "Batch-200: NLLLoss=0.0566 | F1Score=0.9598\n",
      "Batch-250: NLLLoss=0.0620 | F1Score=0.9614\n",
      "Batch-300: NLLLoss=0.0795 | F1Score=0.9634\n",
      "Batch-350: NLLLoss=0.1344 | F1Score=0.9637\n",
      "Batch-400: NLLLoss=0.0171 | F1Score=0.9655\n",
      "Batch-450: NLLLoss=0.1525 | F1Score=0.9662\n",
      "Batch-500: NLLLoss=0.0631 | F1Score=0.9677\n",
      "Batch-518: NLLLoss=0.0671 | F1Score=0.9681\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.1418 | Mean F1Score: 0.9601\n",
      "Patience = 2/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd77578ad2042c9a2e9264de527eeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0115 | F1Score=0.9981\n",
      "Batch-100: NLLLoss=0.0157 | F1Score=0.9981\n",
      "Batch-150: NLLLoss=0.0068 | F1Score=0.9985\n",
      "Batch-200: NLLLoss=0.0043 | F1Score=0.9987\n",
      "Batch-250: NLLLoss=0.0063 | F1Score=0.9983\n",
      "Batch-300: NLLLoss=0.0206 | F1Score=0.9982\n",
      "Batch-350: NLLLoss=0.0278 | F1Score=0.9983\n",
      "Batch-400: NLLLoss=0.0147 | F1Score=0.9985\n",
      "Batch-450: NLLLoss=0.0084 | F1Score=0.9985\n",
      "Batch-500: NLLLoss=0.0291 | F1Score=0.9987\n",
      "Batch-518: NLLLoss=0.0073 | F1Score=0.9987\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0122 | Mean F1Score: 0.9984\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bdd3fb9b6345618e8984735f44a09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0028 | F1Score=0.9984\n",
      "Batch-100: NLLLoss=0.0025 | F1Score=0.9989\n",
      "Batch-150: NLLLoss=0.0031 | F1Score=0.9993\n",
      "Batch-200: NLLLoss=0.0022 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0019 | F1Score=0.9994\n",
      "Batch-300: NLLLoss=0.0013 | F1Score=0.9995\n",
      "Batch-350: NLLLoss=0.0017 | F1Score=0.9994\n",
      "Batch-400: NLLLoss=0.0015 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0019 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0017 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0014 | F1Score=0.9994\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0036 | Mean F1Score: 0.9993\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3217d888a25a4263977a502e76f4bde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0023 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0011 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0017 | F1Score=0.9997\n",
      "Batch-200: NLLLoss=0.0021 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0017 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0015 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0011 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0015 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0005 | F1Score=0.9997\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0023 | Mean F1Score: 0.9997\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fc610003b04e1bb7e71990f2b71676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0013 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0008 | F1Score=0.9992\n",
      "Batch-150: NLLLoss=0.0013 | F1Score=0.9994\n",
      "Batch-200: NLLLoss=0.0016 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0007 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0008 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0014 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0010 | F1Score=0.9997\n",
      "Batch-450: NLLLoss=0.0011 | F1Score=0.9997\n",
      "Batch-500: NLLLoss=0.0010 | F1Score=0.9997\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9997\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0021 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151d3a175bb1403ba053bf74bbc706d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0006 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0007 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0011 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0011 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd720771816487ba2b3399e0e343793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0007 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0004 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0005 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.1344 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0004 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0011 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0db0875f31409ea1bb022847a6eff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0010 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0002 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0004 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0007 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc490db9e1f5446e89bd1cf23231ee8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0004 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0006 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0006 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0003 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0003 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0064 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0049 | F1Score=0.9995\n",
      "Batch-518: NLLLoss=0.0124 | F1Score=0.9993\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.0043 | Mean F1Score: 0.9999\n",
      "Patience = 3/20â—\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0007\n",
      "Best F1Score      : 0.9999\n",
      "Training duration : 15.589 minutes.\n",
      "Training date     : 2022-10-11 11:58:43.365288+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3628634 ,  5.2715187 , -4.0305734 , ..., -3.5120769 ,\n",
       "         5.444339  ,  2.699836  ],\n",
       "       [ 0.63256687,  0.2136978 ,  2.2996113 , ..., -4.3601832 ,\n",
       "         1.5626844 , -4.088047  ],\n",
       "       [ 1.0609256 ,  0.90680003, -3.5849583 , ..., -1.7723886 ,\n",
       "         2.047829  , -3.0207515 ],\n",
       "       ...,\n",
       "       [-2.5810204 ,  0.38208953,  1.5979294 , ..., -2.3704972 ,\n",
       "        -2.4229977 , -0.5440423 ],\n",
       "       [ 3.0147274 , -2.4570172 ,  2.3459718 , ...,  0.44364333,\n",
       "        -0.43936613, -0.30108225],\n",
       "       [-0.12337767, -4.7733603 ,  1.5518495 , ..., -1.4321828 ,\n",
       "         0.15575263,  2.23222   ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.1125, -0.1299, -0.3377,  ..., -0.3665, -0.0334,  0.0417],\n",
       "                      [ 0.6546, -0.0786, -0.3622,  ...,  0.2119,  0.2850, -0.0459],\n",
       "                      [ 0.2110, -0.0516,  0.2782,  ..., -0.1750,  0.3157, -0.0359],\n",
       "                      ...,\n",
       "                      [ 0.0937, -0.2673, -0.1051,  ...,  0.1697,  0.1645,  0.0902],\n",
       "                      [ 0.2866,  0.0690,  0.0636,  ...,  0.0245, -0.2227,  0.2181],\n",
       "                      [-0.0328, -0.3319, -0.2399,  ..., -0.2339,  0.2123,  0.1744]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0514, -0.1269,  0.2115,  ...,  0.1310,  0.0313, -0.0450],\n",
       "                      [ 0.1603, -0.1435,  0.0087,  ...,  0.0387,  0.0009,  0.1024],\n",
       "                      [-0.0220,  0.1435, -0.2296,  ...,  0.0083,  0.0527,  0.0474],\n",
       "                      ...,\n",
       "                      [ 0.1267,  0.2379,  0.0109,  ...,  0.0290,  0.0496, -0.0402],\n",
       "                      [ 0.0805,  0.3842, -0.2600,  ...,  0.1201, -0.0831,  0.0251],\n",
       "                      [ 0.1781, -0.0636,  0.0486,  ..., -0.0291, -0.1675, -0.0908]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-1.9136e-02, -1.3484e-02, -1.3125e-01,  5.5375e-02, -1.2787e-02,\n",
       "                      -1.5056e-01, -6.3559e-02, -8.6473e-02, -8.4004e-02, -1.5211e-01,\n",
       "                      -1.7495e-01, -1.0446e-01,  5.1642e-02, -7.9431e-02, -4.0826e-02,\n",
       "                      -1.6749e-01, -1.7433e-02, -8.2572e-02, -6.3700e-02, -6.3251e-02,\n",
       "                      -1.7404e-02, -7.0292e-02,  1.4480e-02, -8.8471e-02, -4.7981e-02,\n",
       "                      -1.1257e-01, -9.3828e-02,  2.8137e-02,  4.4432e-02,  6.8063e-02,\n",
       "                      -1.0398e-01, -6.7502e-02,  4.1544e-02, -6.1670e-02,  3.5741e-02,\n",
       "                      -1.5079e-02,  4.7806e-02, -4.6040e-03, -1.5962e-01, -6.6388e-02,\n",
       "                      -1.0136e-01, -1.3280e-01,  4.2887e-02, -6.8574e-03, -8.0162e-02,\n",
       "                      -9.0798e-02, -6.6908e-02,  5.2724e-04, -7.8965e-02, -2.5327e-02,\n",
       "                      -1.4745e-01, -1.1872e-01, -7.2888e-02,  4.3533e-02, -3.7234e-02,\n",
       "                       7.4031e-02,  4.7316e-03,  7.8668e-02, -6.1654e-02, -9.9734e-02,\n",
       "                      -1.4684e-01, -2.3172e-02, -1.0304e-01,  4.1925e-02, -3.5715e-02,\n",
       "                      -5.9071e-02,  2.2718e-03,  7.6812e-02, -9.6498e-02,  2.3083e-02,\n",
       "                      -1.0812e-01, -7.8953e-02, -1.4920e-01, -5.8939e-02, -2.3581e-02,\n",
       "                      -8.0451e-02,  2.0979e-02, -2.8400e-01, -1.1262e-01, -1.0229e-01,\n",
       "                      -4.2407e-02, -3.0572e-03,  4.8647e-02, -2.1248e-01,  5.1329e-02,\n",
       "                       9.8940e-02, -6.3544e-02, -4.2010e-02, -1.7149e-01,  9.1388e-02,\n",
       "                      -8.5531e-03, -7.2640e-02,  5.8959e-02, -1.5146e-02, -1.0187e-01,\n",
       "                      -1.4299e-01, -1.0123e-01, -1.9317e-01,  4.0553e-03,  9.4946e-02,\n",
       "                      -3.9219e-02, -1.8593e-02, -1.2352e-01, -5.2424e-02, -6.1031e-02,\n",
       "                      -1.0320e-01,  8.3375e-02, -8.3470e-02, -8.7962e-02,  1.3023e-01,\n",
       "                      -1.2764e-01, -1.3536e-01, -7.2862e-02,  4.5437e-02, -7.2352e-02,\n",
       "                      -6.8801e-02, -1.0570e-01, -9.5193e-02,  4.7118e-02, -3.2255e-02,\n",
       "                      -5.1036e-02, -5.2898e-02,  5.0309e-04,  1.1036e-04, -7.9936e-03,\n",
       "                       2.1110e-02, -6.2930e-02, -9.7243e-02,  1.2326e-01,  3.4809e-02,\n",
       "                       1.6257e-01,  1.8029e-02, -7.6329e-02, -7.4400e-02, -1.4652e-01,\n",
       "                      -2.0179e-01, -5.6541e-02, -7.3132e-02, -7.6189e-02, -3.1239e-02,\n",
       "                       1.6574e-02, -3.9857e-02, -1.1545e-01, -2.5294e-02, -4.6074e-02,\n",
       "                      -1.1498e-01,  1.7576e-02,  3.6355e-02,  5.4747e-02, -7.0513e-02,\n",
       "                      -1.4999e-01,  6.9803e-02, -1.0338e-01, -1.2129e-01, -1.6805e-01,\n",
       "                      -3.3945e-02, -1.5533e-01,  7.1456e-03,  4.1501e-03, -9.9979e-02,\n",
       "                      -8.8940e-02, -5.0955e-02,  1.6869e-02, -1.2589e-02, -1.5777e-01,\n",
       "                      -8.1495e-03, -8.0872e-02, -6.7306e-02, -1.5611e-01, -1.2211e-01,\n",
       "                      -2.0663e-01, -1.4600e-01, -1.0116e-01, -8.7452e-02, -2.7204e-02,\n",
       "                      -6.6841e-03,  4.3389e-02, -8.2937e-02, -9.0068e-02,  2.5205e-02,\n",
       "                      -6.2968e-02, -2.2596e-02, -1.4100e-01, -6.4394e-02, -1.7985e-01,\n",
       "                       2.4423e-02, -6.7403e-02, -6.0009e-02, -6.9117e-02, -7.2604e-02,\n",
       "                      -2.6470e-02, -1.2986e-01, -2.5529e-02,  9.2191e-02, -8.4928e-02,\n",
       "                       4.7366e-02, -6.1171e-02, -1.5137e-01, -8.2733e-02, -5.4638e-02,\n",
       "                      -3.7308e-02, -1.2183e-01,  3.0288e-03, -7.0006e-03, -4.0809e-02,\n",
       "                      -8.5482e-02,  4.0963e-02, -1.0932e-01, -1.3647e-02, -1.3578e-01,\n",
       "                      -5.0704e-02, -2.3587e-01,  7.2390e-02, -8.9095e-02, -6.5433e-02,\n",
       "                       4.1238e-02, -9.5257e-02,  7.4679e-02, -5.6096e-02,  2.8434e-02,\n",
       "                       1.8944e-02, -8.5801e-02, -1.7292e-02, -8.5414e-02, -1.1432e-01,\n",
       "                      -5.1329e-02, -2.6193e-02, -1.9215e-01, -6.4786e-02, -1.1561e-01,\n",
       "                      -7.7467e-02,  3.6265e-02, -1.3655e-01, -4.6538e-02, -1.1186e-01,\n",
       "                      -6.9121e-02, -4.5728e-02, -2.1523e-02, -6.6680e-02, -2.8978e-02,\n",
       "                      -1.3386e-02,  4.4689e-02, -6.7377e-02, -4.8874e-02, -1.6498e-01,\n",
       "                      -9.6609e-02, -1.4333e-01, -7.0266e-02, -5.7711e-02,  1.9138e-03,\n",
       "                      -5.3424e-02, -1.6406e-01, -5.5733e-02, -4.5126e-03, -1.2632e-01,\n",
       "                      -1.4019e-01,  2.1447e-02, -3.9359e-02,  1.2019e-01,  3.3504e-03,\n",
       "                      -2.9687e-02,  6.7398e-02,  1.8330e-02,  4.0289e-02, -3.4313e-02,\n",
       "                      -1.4313e-01, -1.1447e-02, -2.5472e-02,  9.9534e-02,  2.7287e-02,\n",
       "                      -2.7169e-02, -1.1591e-02, -8.8346e-02,  2.6338e-02,  4.4055e-02,\n",
       "                      -3.3960e-02,  1.0814e-02,  5.1804e-03,  3.5646e-02, -9.2909e-03,\n",
       "                       1.7274e-02,  3.1872e-02,  2.9464e-02, -4.8488e-02, -6.6504e-02,\n",
       "                      -2.8454e-02,  2.6457e-03, -1.6247e-02,  1.2790e-02, -7.3078e-02,\n",
       "                      -5.8008e-02, -1.5441e-01, -3.6469e-02,  1.1591e-01,  7.4452e-02,\n",
       "                       1.1454e-02,  1.4338e-01,  2.6491e-02, -4.4582e-02, -1.3429e-02,\n",
       "                      -1.3009e-01,  3.3962e-02, -7.3141e-02, -2.8404e-02, -5.6053e-02,\n",
       "                       2.7206e-02,  7.3308e-02,  4.4740e-02,  1.2821e-01,  7.1188e-02,\n",
       "                      -7.8096e-02,  4.7018e-03,  6.0578e-02,  1.4261e-02,  9.8693e-03,\n",
       "                       2.4022e-02,  8.0676e-03,  1.2693e-01,  4.8419e-02,  4.6384e-02,\n",
       "                      -9.1242e-02, -8.7581e-03, -7.1624e-02, -7.1836e-03,  5.0222e-02,\n",
       "                      -4.1669e-02,  4.4986e-02,  2.9076e-02,  9.8215e-03,  4.1149e-03,\n",
       "                       1.6078e-02, -4.7456e-02, -1.0424e-01,  9.6638e-02,  1.1589e-01,\n",
       "                      -9.3980e-02,  1.1664e-02, -1.6540e-02,  5.3889e-02,  4.2442e-02,\n",
       "                      -4.9866e-03,  2.8789e-02, -8.1739e-02,  7.0180e-02, -3.0395e-02,\n",
       "                       8.7600e-02, -4.5327e-02, -4.8002e-02, -1.4009e-01,  7.7972e-02,\n",
       "                      -6.3015e-02,  2.7370e-02,  1.0888e-01,  9.5059e-04, -2.8097e-02,\n",
       "                      -4.0020e-02,  2.0672e-02,  1.7604e-02,  7.8802e-02, -8.4646e-02,\n",
       "                      -9.0506e-02,  6.0185e-03, -1.0459e-01,  3.0185e-02, -5.9297e-02,\n",
       "                      -3.9892e-03,  7.3813e-02, -3.4023e-02,  8.3660e-02,  1.4705e-02,\n",
       "                       7.5054e-02,  1.3515e-02,  9.7947e-02, -9.3190e-02,  9.3983e-03,\n",
       "                      -1.5224e-01, -1.2902e-02,  8.0655e-02, -5.1022e-02, -1.7714e-02,\n",
       "                      -6.2777e-02, -6.2090e-02,  3.2473e-02, -8.6300e-03, -1.4686e-02,\n",
       "                       3.3680e-02, -8.5867e-02, -1.8518e-02, -1.4781e-01,  1.0273e-01,\n",
       "                      -1.5498e-02, -1.2397e-01, -4.3308e-02, -5.6112e-02, -7.9037e-02,\n",
       "                      -1.1452e-01, -5.1567e-02,  4.1839e-02, -7.5905e-02, -7.4282e-02,\n",
       "                       6.4337e-02,  2.3492e-02, -3.9904e-02, -7.6058e-02, -2.7031e-02,\n",
       "                      -1.3771e-01,  1.9141e-02,  6.0199e-03,  4.1071e-02, -2.1254e-01,\n",
       "                      -3.5662e-02, -4.7753e-02,  6.7819e-02,  5.9336e-02, -1.3444e-01,\n",
       "                      -1.4335e-01,  5.3404e-02,  1.6329e-02, -3.2041e-02,  1.0462e-01,\n",
       "                      -5.6835e-03,  1.9370e-02, -6.6218e-02, -3.8844e-02, -2.1776e-01,\n",
       "                       1.4747e-03, -7.3022e-02,  2.8519e-02,  2.1047e-02, -1.2507e-01,\n",
       "                      -4.8229e-02, -4.3922e-02, -4.8812e-02, -9.8304e-02, -1.1569e-01,\n",
       "                      -1.0222e-02, -6.5526e-02, -9.0663e-02,  5.1796e-02,  5.5000e-02,\n",
       "                       2.4848e-02,  4.0278e-02,  7.1729e-02,  2.2204e-02,  7.8625e-02,\n",
       "                       9.8557e-02, -1.0035e-01, -1.4353e-01, -1.0360e-01, -9.1385e-02,\n",
       "                      -3.7822e-03, -4.6205e-02, -8.3647e-02, -1.1576e-01, -8.4778e-02,\n",
       "                      -5.8385e-02,  2.6463e-02, -7.9904e-02, -1.0829e-01, -1.0683e-01,\n",
       "                      -3.3306e-02, -1.6148e-01,  2.2017e-02, -4.0336e-02, -4.5518e-02,\n",
       "                      -5.2839e-02, -3.9247e-03, -1.2042e-01, -6.4624e-03, -5.2744e-02,\n",
       "                      -9.5701e-02, -9.6700e-02, -4.4724e-02,  1.1623e-01, -1.5181e-02,\n",
       "                      -2.1113e-01, -4.7512e-02, -2.0120e-02, -9.4515e-02, -7.2457e-02,\n",
       "                      -5.2747e-02, -1.2038e-01, -9.6828e-02, -2.4722e-02,  1.0674e-01,\n",
       "                       1.5600e-02, -7.4093e-02, -5.3888e-02, -4.4079e-02, -5.6732e-02,\n",
       "                      -3.4454e-02,  9.2992e-02, -1.6733e-01, -2.0021e-02, -4.6474e-02,\n",
       "                      -1.8073e-01, -2.1482e-01, -3.8049e-03, -7.2890e-02, -3.4663e-02,\n",
       "                      -1.9941e-01,  1.5423e-02,  2.1192e-02, -7.1180e-02, -1.4878e-02,\n",
       "                      -4.2887e-02, -4.8947e-02,  5.6445e-02,  5.1370e-02,  7.7550e-02,\n",
       "                      -3.8248e-02, -3.8393e-03])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([-6.3325e-02,  5.6599e-02, -3.8698e-02,  1.0698e-01, -9.1624e-02,\n",
       "                      -9.5476e-02,  8.0203e-03, -1.1186e-01, -3.8364e-02, -1.4054e-01,\n",
       "                      -3.5314e-02, -1.9024e-01, -8.9366e-02, -1.1689e-03,  6.3513e-02,\n",
       "                      -2.8428e-02, -3.7853e-02, -7.1969e-02, -4.3194e-02,  1.3079e-01,\n",
       "                      -1.0388e-01,  7.0882e-02,  1.8628e-02, -3.7602e-02, -9.9432e-02,\n",
       "                      -1.1469e-01, -1.2426e-01,  6.0762e-02, -2.5322e-02, -4.1136e-02,\n",
       "                      -1.4286e-01, -3.8968e-02, -8.0612e-02, -6.8635e-02,  4.2805e-02,\n",
       "                       4.5770e-02, -1.0402e-02, -1.7808e-01, -9.4540e-02,  8.7035e-02,\n",
       "                      -1.6196e-01, -7.7735e-02,  1.2842e-02, -6.8846e-02, -9.1744e-02,\n",
       "                      -8.0446e-02,  4.1572e-02, -8.7738e-02,  3.8890e-02, -1.4198e-01,\n",
       "                      -1.0039e-02, -1.0302e-01, -4.7464e-02, -6.2639e-02, -5.3486e-02,\n",
       "                      -2.5869e-02, -9.0582e-02,  2.4442e-02, -2.2961e-02,  5.4886e-02,\n",
       "                       1.2969e-02, -7.2223e-02, -1.7610e-02, -7.3790e-02, -1.4765e-02,\n",
       "                      -7.9744e-02,  3.5035e-02, -6.2270e-02, -1.0753e-01, -1.3294e-01,\n",
       "                      -6.0389e-02, -1.4390e-01,  2.8194e-02, -4.7445e-02,  4.1789e-02,\n",
       "                      -6.3558e-03, -3.1086e-02, -2.6311e-01,  1.6732e-02, -4.8586e-02,\n",
       "                      -4.7259e-03, -1.1681e-01,  3.8271e-02, -1.1296e-01, -3.4442e-02,\n",
       "                       5.1600e-02, -2.5164e-01, -3.2936e-03, -7.3835e-02,  9.8749e-03,\n",
       "                      -1.0135e-01, -2.1463e-02,  5.8052e-02,  1.7797e-02, -1.6872e-02,\n",
       "                      -1.0948e-01, -1.2061e-01, -9.3677e-02, -1.0777e-01,  7.7224e-02,\n",
       "                      -2.5998e-02,  7.9468e-02, -1.3792e-01, -1.2279e-01, -2.8289e-02,\n",
       "                       8.0713e-02,  1.7343e-02, -7.9121e-02,  3.4895e-02,  4.3038e-02,\n",
       "                      -2.2826e-01, -1.6757e-01, -1.4954e-01,  2.7517e-02,  2.6997e-02,\n",
       "                      -3.7958e-02, -8.1041e-02, -1.0516e-01,  6.2979e-02, -6.4822e-02,\n",
       "                       1.9452e-03,  7.6921e-02, -9.6424e-02,  3.0611e-02, -2.0905e-02,\n",
       "                      -1.4051e-01, -1.6450e-01, -1.4313e-03, -7.7652e-02, -1.6475e-01,\n",
       "                       6.1976e-02, -5.7155e-02, -1.0109e-01, -8.0358e-02, -2.2625e-02,\n",
       "                      -1.6981e-01, -8.0660e-02, -1.5171e-01, -1.0669e-01, -1.6206e-01,\n",
       "                      -5.6206e-02, -1.5249e-01,  1.0795e-01, -1.0631e-02, -9.4914e-02,\n",
       "                      -6.1193e-02, -3.7196e-02, -2.7997e-02, -2.7825e-02, -2.0604e-01,\n",
       "                      -2.1790e-02, -6.7222e-02, -7.0469e-02, -1.4359e-01, -1.8754e-02,\n",
       "                      -5.2972e-03, -1.4261e-01, -1.9733e-02, -1.0562e-01, -1.3356e-01,\n",
       "                      -1.2308e-01, -1.2811e-01, -3.1686e-02, -2.0588e-01,  2.7135e-03,\n",
       "                      -2.5514e-02, -3.7126e-02, -6.4474e-02, -1.4078e-01,  5.3468e-03,\n",
       "                      -1.4025e-01, -9.3148e-02, -6.7651e-02, -3.0656e-03,  6.7280e-02,\n",
       "                       1.9789e-02, -2.9773e-02,  2.0020e-02, -1.0400e-01,  9.2945e-02,\n",
       "                       3.0077e-02, -9.0285e-02, -9.7712e-02, -7.9400e-02, -1.6094e-01,\n",
       "                      -4.2270e-02, -1.2646e-01, -6.7685e-02, -2.6888e-02, -6.0966e-05,\n",
       "                      -2.3999e-01, -7.0731e-02, -4.0826e-02,  1.2100e-01,  1.8726e-03,\n",
       "                       2.0850e-02, -6.8180e-02, -9.7040e-02, -5.7216e-02,  1.0069e-02,\n",
       "                      -1.0345e-01, -4.0534e-02, -2.2440e-02, -5.6428e-02, -1.4607e-01,\n",
       "                      -5.0185e-02, -4.3479e-02, -3.1612e-03, -5.1344e-02, -1.1005e-01,\n",
       "                      -1.5028e-01, -1.3738e-01, -1.4021e-01,  1.7043e-03, -1.3839e-01,\n",
       "                      -4.1590e-02, -1.9423e-03,  5.7451e-02, -4.9269e-02,  8.9646e-02,\n",
       "                      -8.1793e-02, -5.4079e-02, -5.6856e-02, -7.9782e-02, -8.5087e-02,\n",
       "                      -8.6411e-02, -1.0576e-02, -1.6639e-01, -6.7384e-02, -1.2933e-02,\n",
       "                      -6.0612e-02,  1.0070e-02, -1.6827e-01,  1.7903e-01, -7.7252e-02,\n",
       "                      -1.1969e-01,  1.3254e-02, -5.4902e-02, -1.1776e-01,  4.0788e-03,\n",
       "                      -6.9322e-02, -1.5403e-02, -9.7148e-02, -9.4324e-02,  7.3229e-03,\n",
       "                      -2.0306e-01, -1.7941e-01, -2.6080e-01,  7.3981e-02, -1.3647e-01,\n",
       "                      -4.6827e-02, -8.1132e-02,  1.1765e-01,  1.5667e-01,  5.4830e-02,\n",
       "                      -2.1989e-01,  3.7766e-02, -3.0655e-02,  6.4392e-02,  1.2355e-01,\n",
       "                       8.9416e-02, -2.5808e-02, -1.0282e-01,  4.6927e-02, -4.3218e-02,\n",
       "                      -5.0728e-02, -2.2337e-02,  7.4259e-02,  6.2127e-03,  4.5368e-02,\n",
       "                      -4.1631e-02,  1.3594e-01, -3.9692e-02, -3.8343e-02, -9.1012e-02,\n",
       "                      -6.5575e-02,  5.0710e-02,  5.2208e-02,  3.4338e-02, -2.2973e-03,\n",
       "                       4.7600e-02, -2.6482e-03, -2.0269e-02, -4.9371e-02,  3.7041e-02,\n",
       "                       1.3523e-02,  4.7636e-02, -8.3359e-02,  3.6649e-02,  1.2294e-02,\n",
       "                       1.1618e-02,  6.6159e-02, -6.5440e-02, -3.9909e-02,  5.8348e-02,\n",
       "                      -1.0516e-02, -1.4963e-02, -3.9452e-02,  3.5410e-02, -7.0471e-02,\n",
       "                       2.6323e-02,  1.0440e-01,  1.1533e-01, -2.7914e-02, -5.2455e-02,\n",
       "                       5.0353e-03,  6.5375e-03, -1.7330e-03,  3.6065e-02, -2.1012e-02,\n",
       "                      -2.5243e-03,  2.1060e-02, -1.2328e-01,  2.6425e-02,  8.4454e-02,\n",
       "                      -2.8978e-02, -4.2434e-02, -1.0101e-01, -4.3792e-02, -9.4352e-02,\n",
       "                       1.4219e-01, -1.7450e-02, -2.8031e-02, -7.6766e-02, -7.5388e-03,\n",
       "                      -6.4377e-03, -5.4772e-02,  1.6183e-02, -1.4483e-02,  3.6878e-02,\n",
       "                      -1.2467e-02, -4.1564e-02, -3.5897e-02, -3.1295e-02, -7.0543e-02,\n",
       "                      -3.2919e-02,  3.2362e-02,  4.4261e-02,  6.1335e-02,  6.2051e-03,\n",
       "                       4.1418e-02,  4.5139e-02, -4.5211e-02, -1.1384e-02,  9.2781e-03,\n",
       "                      -1.0179e-01,  1.3178e-01,  4.8424e-02, -5.5430e-02, -9.1233e-02,\n",
       "                       1.5082e-01, -7.1157e-02,  2.0444e-02,  3.4684e-02, -3.6206e-02,\n",
       "                       1.6274e-02,  5.3065e-02,  3.9396e-02,  1.2260e-01, -7.6670e-02,\n",
       "                       3.1024e-02,  5.9945e-02,  4.4352e-02, -1.4959e-02, -4.2430e-02,\n",
       "                      -7.6240e-02, -3.3570e-03,  4.6976e-02,  4.8266e-03, -1.4897e-01,\n",
       "                      -7.6767e-02, -7.4879e-02,  7.6753e-02, -2.0077e-02,  3.5835e-03,\n",
       "                      -3.0566e-02,  2.7508e-02, -8.3256e-02, -4.0592e-02, -4.0869e-02,\n",
       "                       2.3723e-02, -5.8080e-02,  3.8750e-03,  7.2419e-02,  3.7682e-02,\n",
       "                       6.5841e-02, -1.3773e-01,  9.1913e-03, -7.0921e-02,  9.8433e-02,\n",
       "                       5.6090e-02,  4.1376e-02, -1.5248e-01, -6.2100e-02, -3.7124e-02,\n",
       "                      -1.1270e-01,  7.7582e-02,  1.1443e-01,  8.9924e-02, -2.6076e-02,\n",
       "                       7.2678e-02,  1.5020e-02, -4.9444e-02,  2.8162e-02,  4.8946e-02,\n",
       "                       3.3726e-02,  9.9141e-02, -7.2768e-02, -3.8461e-02, -2.1923e-01,\n",
       "                      -2.0043e-01, -4.6123e-02,  7.4319e-02, -9.0285e-03, -1.2268e-01,\n",
       "                       7.2586e-02,  1.3829e-02,  7.4116e-02, -7.9729e-02,  2.6849e-02,\n",
       "                       6.7018e-02, -3.0344e-03, -2.0096e-02, -1.4441e-01, -1.4958e-01,\n",
       "                      -4.4932e-02,  9.7642e-03, -8.4009e-02, -8.5206e-02, -1.1871e-01,\n",
       "                      -8.1234e-03, -4.7255e-02, -1.3571e-01, -1.3174e-05, -1.1379e-01,\n",
       "                      -9.2763e-02, -4.0836e-02, -1.5922e-01, -9.1613e-02,  1.0289e-02,\n",
       "                       3.5515e-02, -1.2637e-02,  6.7572e-03, -3.0685e-02, -1.0803e-01,\n",
       "                       5.6837e-02, -1.6368e-01, -1.7039e-01, -2.6716e-02, -7.0614e-02,\n",
       "                       1.2062e-01, -7.1762e-02, -1.7362e-01, -6.9478e-02, -5.8633e-03,\n",
       "                      -1.4084e-01,  6.3694e-02, -1.2125e-01, -4.1567e-02, -3.8104e-02,\n",
       "                      -3.5281e-03, -1.9879e-01,  4.8392e-02, -8.5727e-03,  3.2111e-02,\n",
       "                      -3.0388e-02,  9.7040e-03, -6.3866e-02,  7.6562e-03, -8.1707e-02,\n",
       "                      -1.3019e-01, -9.7647e-02,  3.9516e-02,  5.8749e-02, -5.9000e-02,\n",
       "                       7.0757e-03, -1.3019e-01, -5.4542e-02,  2.8506e-02, -8.9009e-02,\n",
       "                      -8.9211e-02, -8.4601e-02,  4.9583e-02, -4.1786e-02,  4.3336e-02,\n",
       "                      -8.9432e-02,  1.7135e-02, -6.2214e-03,  5.9985e-03, -7.4270e-02,\n",
       "                       7.7597e-03, -1.9744e-01, -9.8861e-02,  1.0711e-01, -9.8795e-02,\n",
       "                      -1.3613e-01, -1.8822e-01,  9.3855e-04,  2.8101e-02, -1.3106e-01,\n",
       "                      -5.8659e-02,  1.0906e-01,  1.1386e-02, -1.5413e-01, -4.9531e-02,\n",
       "                      -2.0645e-02, -4.5846e-02,  1.4076e-02,  2.3452e-02,  8.7038e-02,\n",
       "                      -1.6752e-01,  2.6186e-02])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.3360,  0.0634,  0.2573,  ..., -0.3156, -0.0126,  0.0810],\n",
       "                      [ 0.1064,  0.5058, -0.2340,  ..., -0.0359, -0.1896,  0.0032],\n",
       "                      [ 0.1691, -0.1439,  0.1149,  ..., -0.0459,  0.1200,  0.1327],\n",
       "                      ...,\n",
       "                      [ 0.0203,  0.3028,  0.1396,  ...,  0.1027, -0.3220,  0.0870],\n",
       "                      [ 0.0798, -0.2503, -0.0881,  ..., -0.2182,  0.0597, -0.4598],\n",
       "                      [-0.0813,  0.0347, -0.2848,  ..., -0.1764,  0.1962,  0.0258]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0492,  0.1485, -0.0047,  ..., -0.0255,  0.0765, -0.2620],\n",
       "                      [ 0.1058,  0.1551,  0.0062,  ...,  0.0152,  0.0452,  0.1205],\n",
       "                      [-0.1950, -0.2180,  0.2067,  ..., -0.0128,  0.0972,  0.1468],\n",
       "                      ...,\n",
       "                      [ 0.1404,  0.0833,  0.0107,  ...,  0.0228,  0.1611,  0.2049],\n",
       "                      [ 0.0104, -0.0030, -0.1578,  ...,  0.0809, -0.0629, -0.2201],\n",
       "                      [ 0.1026,  0.0912, -0.1147,  ..., -0.3737,  0.0594, -0.1051]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.1315, -0.0032,  0.0567,  0.0992,  0.2618,  0.1491, -0.0148,  0.0502,\n",
       "                      -0.0886, -0.0281, -0.0398, -0.0884,  0.0622,  0.0337, -0.0094,  0.1102,\n",
       "                       0.0328, -0.0433,  0.0846, -0.0504,  0.1830,  0.0042,  0.0350, -0.0465,\n",
       "                      -0.0313,  0.0401,  0.0830,  0.0405, -0.0880, -0.0141,  0.1175, -0.0405,\n",
       "                       0.1351, -0.1382,  0.0472,  0.1435, -0.0633,  0.0054,  0.1414, -0.0501,\n",
       "                       0.0010,  0.1835,  0.0403,  0.1205,  0.0113,  0.0767,  0.1736,  0.0901,\n",
       "                       0.0581,  0.1352, -0.1422,  0.0303,  0.1019,  0.2283,  0.0095,  0.0790,\n",
       "                      -0.0374,  0.0635,  0.1643,  0.1487, -0.0086,  0.0877,  0.0530,  0.1311,\n",
       "                      -0.0168,  0.1659,  0.1106,  0.0940,  0.1050,  0.0363,  0.1110,  0.1302,\n",
       "                       0.0590,  0.1823,  0.0111, -0.0629, -0.0375,  0.0727, -0.0752, -0.0474,\n",
       "                      -0.0292, -0.1677, -0.1002,  0.0327,  0.1532,  0.1429, -0.0195, -0.0497,\n",
       "                       0.0087,  0.2365,  0.0423,  0.0536, -0.0273,  0.0702,  0.0849, -0.0037,\n",
       "                       0.0520, -0.0822,  0.0989, -0.0577, -0.0802, -0.0910,  0.0572,  0.0996,\n",
       "                       0.1839,  0.0347,  0.0519, -0.0675,  0.2101,  0.0117,  0.1157, -0.0173,\n",
       "                      -0.0102,  0.0993, -0.0357,  0.0360,  0.1835,  0.1044,  0.1278,  0.1598,\n",
       "                      -0.0315,  0.0872,  0.0139,  0.0600, -0.1204,  0.1276, -0.0202, -0.0298,\n",
       "                       0.0263, -0.1705,  0.0228, -0.0723, -0.0452, -0.0227, -0.0723, -0.0337,\n",
       "                      -0.1670, -0.0256, -0.0078, -0.0700, -0.0127, -0.0448, -0.0589,  0.0681,\n",
       "                       0.0901,  0.1647,  0.0451,  0.0651,  0.0449, -0.0806, -0.1065, -0.1997,\n",
       "                      -0.0958,  0.0519, -0.1790, -0.1576, -0.0193, -0.0312, -0.0954, -0.0278,\n",
       "                       0.0309,  0.0290, -0.0202, -0.1477,  0.0632,  0.0405, -0.2111, -0.0545,\n",
       "                      -0.0984,  0.0053, -0.2295, -0.0199, -0.0540, -0.1040, -0.0454, -0.1043,\n",
       "                      -0.1181,  0.0569, -0.1247, -0.0613,  0.1307, -0.0588, -0.0699, -0.1356,\n",
       "                      -0.0036,  0.0155, -0.0395, -0.0733, -0.0797, -0.1918, -0.1301,  0.0894,\n",
       "                      -0.0761, -0.0108, -0.0214, -0.0940, -0.0721, -0.0129, -0.0245,  0.0387,\n",
       "                      -0.1371, -0.0577,  0.1297, -0.0210,  0.0758,  0.1250,  0.0132, -0.0556,\n",
       "                      -0.0045, -0.0765, -0.0965, -0.1520,  0.1375, -0.0410, -0.0223,  0.0656,\n",
       "                      -0.1471, -0.1969,  0.0985, -0.1830,  0.0196, -0.1824, -0.0243, -0.0540,\n",
       "                       0.0449, -0.0482, -0.0850,  0.0280, -0.2799, -0.1637,  0.0296, -0.0489,\n",
       "                       0.1841, -0.0801,  0.0507, -0.0372, -0.2448, -0.0306,  0.0006, -0.0677,\n",
       "                       0.0292, -0.0254, -0.2036,  0.0454,  0.0527,  0.0013, -0.0861, -0.0923,\n",
       "                      -0.0943,  0.0528, -0.0887, -0.0330, -0.1878, -0.0129, -0.0577, -0.0960,\n",
       "                      -0.0040, -0.0989,  0.0344, -0.0116,  0.0108,  0.0225,  0.1123, -0.1184,\n",
       "                      -0.0284, -0.0555,  0.0490, -0.0569, -0.0896, -0.0007, -0.0621,  0.0187,\n",
       "                       0.0280,  0.1529,  0.0241, -0.0203,  0.0053, -0.0102, -0.0351,  0.0818,\n",
       "                       0.0687,  0.0971, -0.0290,  0.0401,  0.0128, -0.0134, -0.0069, -0.0212,\n",
       "                      -0.0016, -0.0121, -0.0879,  0.0409,  0.1006, -0.0256, -0.0369,  0.0345,\n",
       "                      -0.0042, -0.0323,  0.0228, -0.0454,  0.0316, -0.0622, -0.0614, -0.0527,\n",
       "                      -0.0502, -0.0359,  0.0493, -0.0830,  0.0199,  0.0905,  0.0940,  0.0383,\n",
       "                      -0.1333, -0.0236, -0.0269, -0.0222, -0.0644, -0.0898,  0.0377,  0.0707,\n",
       "                       0.0784,  0.0955,  0.0693,  0.0543, -0.0440, -0.0273,  0.0596, -0.0421,\n",
       "                      -0.0446, -0.1037, -0.1321, -0.0522, -0.0469,  0.0362,  0.0046, -0.0201,\n",
       "                      -0.0333, -0.0142,  0.0206, -0.0261, -0.0197,  0.0747,  0.0753, -0.0457,\n",
       "                      -0.0192, -0.0221, -0.0824,  0.0400, -0.0029, -0.0541,  0.0738, -0.0505,\n",
       "                       0.0554,  0.0498, -0.0574, -0.0160,  0.0129,  0.0738, -0.0017,  0.0260,\n",
       "                       0.0268, -0.0840,  0.0518, -0.0307,  0.0091, -0.0289, -0.0224,  0.0280,\n",
       "                       0.0286, -0.0803, -0.0774, -0.0634,  0.0346, -0.0093,  0.0568,  0.0641,\n",
       "                      -0.0141,  0.0943, -0.0325, -0.0141, -0.0300,  0.0580,  0.0209,  0.0116,\n",
       "                       0.0324, -0.1408, -0.0030,  0.0362,  0.1318,  0.0741, -0.0843,  0.2393,\n",
       "                       0.0559,  0.0195, -0.0338,  0.0369,  0.1813, -0.0227, -0.0429,  0.0762,\n",
       "                       0.1306,  0.0957, -0.0207,  0.0121,  0.2038,  0.0237,  0.0511, -0.0355,\n",
       "                      -0.0612,  0.0446,  0.0640,  0.0991,  0.0623,  0.0808,  0.0456,  0.1507,\n",
       "                       0.0962, -0.0485,  0.0314,  0.1206, -0.0350, -0.1540,  0.1615,  0.0753,\n",
       "                       0.0057,  0.2081,  0.1469, -0.0074,  0.0119,  0.0648,  0.3108,  0.0407,\n",
       "                       0.1579,  0.1262, -0.1490,  0.1254,  0.0560,  0.0573, -0.0051,  0.0462,\n",
       "                       0.0446,  0.1542,  0.1027,  0.1572,  0.1286,  0.0781,  0.0779,  0.0340,\n",
       "                       0.0213,  0.2135, -0.0398,  0.0261,  0.0962,  0.0418,  0.1723,  0.1771,\n",
       "                       0.0504,  0.0569, -0.1406,  0.1312,  0.0076,  0.0218,  0.0181,  0.0954,\n",
       "                       0.0147,  0.0805,  0.0356, -0.0475,  0.1313,  0.1390,  0.0193,  0.1056,\n",
       "                      -0.0134,  0.2021,  0.1026, -0.0148,  0.1324,  0.0804,  0.1640, -0.0069,\n",
       "                       0.0387, -0.1049,  0.1684,  0.0032,  0.1369,  0.0637,  0.0956,  0.0197,\n",
       "                      -0.0810, -0.0134,  0.0677,  0.1154,  0.0254,  0.0104,  0.1205, -0.0536,\n",
       "                       0.0467,  0.0468, -0.0142, -0.0135,  0.1303,  0.1076,  0.0759,  0.2799,\n",
       "                       0.0540,  0.1064,  0.1248,  0.1374, -0.0125,  0.1200,  0.0301, -0.0281])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 0.1685, -0.0646, -0.2093, -0.0140,  0.2382,  0.0929, -0.0664,  0.1350,\n",
       "                       0.0114, -0.0228, -0.0804, -0.1011, -0.0120,  0.0387, -0.1667, -0.0164,\n",
       "                       0.1083,  0.0863, -0.0076, -0.0401,  0.1503, -0.0139,  0.0310, -0.0357,\n",
       "                      -0.0441,  0.0506,  0.1185, -0.0593,  0.0786,  0.0269, -0.0097, -0.0232,\n",
       "                       0.1802, -0.0840,  0.0468,  0.0546,  0.0623, -0.1871,  0.0018, -0.0581,\n",
       "                      -0.0644,  0.1616, -0.0006, -0.0212,  0.0796, -0.1003,  0.1203,  0.1492,\n",
       "                       0.1554,  0.1785, -0.0468,  0.0589,  0.0369,  0.0932,  0.0233,  0.0685,\n",
       "                       0.0869, -0.0295,  0.1400,  0.1193,  0.0097,  0.1117,  0.0901,  0.1674,\n",
       "                       0.0758,  0.1821,  0.0283,  0.0504,  0.2061, -0.0101,  0.0095,  0.0421,\n",
       "                       0.1055,  0.0532,  0.0145,  0.0108, -0.0241,  0.0045, -0.0569,  0.1218,\n",
       "                       0.0090, -0.0564, -0.0188,  0.1274,  0.1367,  0.0591, -0.0564,  0.1123,\n",
       "                      -0.0396,  0.0530,  0.0276, -0.0548,  0.1160,  0.1566,  0.0880,  0.0379,\n",
       "                       0.0304, -0.0125,  0.1110, -0.0806, -0.0410,  0.0487,  0.1095,  0.1354,\n",
       "                       0.1204, -0.0251,  0.1743,  0.0680,  0.1050,  0.0239,  0.0139, -0.0151,\n",
       "                      -0.0951,  0.0583,  0.1929,  0.0084,  0.1472, -0.0015,  0.0824,  0.1847,\n",
       "                       0.0146,  0.1300,  0.0372,  0.0172, -0.1021,  0.1746,  0.0497, -0.0275,\n",
       "                       0.1058, -0.0499,  0.0583, -0.1621, -0.0378,  0.1053, -0.0592,  0.0100,\n",
       "                      -0.0419, -0.1002, -0.0330,  0.0084, -0.1236, -0.0330,  0.0088, -0.1004,\n",
       "                       0.0567, -0.0158, -0.0113, -0.0952, -0.0281, -0.0546, -0.1200, -0.0897,\n",
       "                      -0.0983, -0.0331, -0.0746, -0.1837, -0.2009, -0.0466, -0.1090, -0.0404,\n",
       "                      -0.0092,  0.0464,  0.0082, -0.0111,  0.0037,  0.0595, -0.0942, -0.0185,\n",
       "                      -0.2006,  0.0685, -0.0575, -0.1023, -0.0918, -0.1110, -0.0553, -0.0222,\n",
       "                      -0.1755, -0.1490, -0.0379, -0.0974,  0.0382, -0.0500, -0.0057, -0.0288,\n",
       "                      -0.0539,  0.0214, -0.1533, -0.1380, -0.0666, -0.0993, -0.0988,  0.1038,\n",
       "                       0.0506, -0.0270,  0.0724,  0.0441, -0.0688, -0.1678, -0.1140, -0.0832,\n",
       "                      -0.1195,  0.0087, -0.1637, -0.1529,  0.0573,  0.0719, -0.0649, -0.0763,\n",
       "                      -0.0993, -0.0211, -0.0354, -0.2126,  0.0453, -0.1117,  0.0178, -0.0104,\n",
       "                      -0.1251, -0.0283,  0.0770, -0.2325, -0.0984, -0.0383,  0.0752,  0.0736,\n",
       "                       0.1251, -0.0465, -0.1586,  0.0235, -0.0519, -0.0476, -0.0137,  0.0555,\n",
       "                      -0.0260, -0.1687,  0.0118,  0.0349, -0.0679,  0.0182, -0.0276, -0.0611,\n",
       "                       0.1040, -0.0667, -0.0531,  0.0964,  0.1130, -0.0394, -0.1834, -0.0923,\n",
       "                      -0.1736,  0.0152,  0.1651, -0.0288, -0.1240,  0.0267, -0.1276, -0.0631,\n",
       "                       0.0179, -0.0782,  0.0021,  0.1129, -0.0835,  0.0705,  0.0152,  0.0013,\n",
       "                      -0.0619,  0.0645,  0.0869, -0.1056, -0.0855,  0.0481,  0.0289,  0.0125,\n",
       "                      -0.0675,  0.0190, -0.0445, -0.1522,  0.0055, -0.0915,  0.0062, -0.0303,\n",
       "                       0.0614,  0.0345,  0.0058, -0.0042, -0.0060,  0.0037,  0.0423,  0.0176,\n",
       "                       0.0049, -0.0433,  0.0733,  0.0181, -0.0051,  0.0927, -0.0794, -0.0165,\n",
       "                       0.0853,  0.1260, -0.1019,  0.0303,  0.0180, -0.0211,  0.0626, -0.0796,\n",
       "                      -0.0540, -0.0968, -0.0797, -0.0963, -0.0128,  0.0048, -0.0332,  0.0117,\n",
       "                       0.0910,  0.0722, -0.0353, -0.0074,  0.1389, -0.0158, -0.0637,  0.0344,\n",
       "                      -0.0015,  0.0900, -0.0547, -0.1074, -0.1315,  0.0253, -0.0147,  0.0934,\n",
       "                       0.0734, -0.0202,  0.0268, -0.0274, -0.0198,  0.0398, -0.0139,  0.0451,\n",
       "                      -0.1365, -0.0054, -0.0684, -0.0487,  0.0349, -0.0044, -0.0287, -0.0807,\n",
       "                      -0.0049, -0.0780,  0.1501, -0.0008, -0.0053,  0.0086,  0.0995,  0.1673,\n",
       "                       0.0583,  0.1155,  0.0586,  0.0811, -0.1001, -0.0127, -0.0114,  0.0455,\n",
       "                      -0.0518,  0.0619,  0.0190, -0.0101,  0.0011, -0.0511, -0.0448, -0.0341,\n",
       "                       0.0265, -0.0413, -0.0319, -0.1440,  0.0567, -0.0300,  0.0549,  0.0121,\n",
       "                      -0.0381, -0.1400,  0.0408,  0.1490, -0.0097, -0.0752,  0.0501,  0.0644,\n",
       "                       0.1748, -0.0766,  0.0162,  0.1260,  0.2106,  0.0504, -0.0377,  0.3102,\n",
       "                       0.0352,  0.0824, -0.1835, -0.0553,  0.0876,  0.0373,  0.0058,  0.1175,\n",
       "                       0.1272,  0.0048, -0.0026,  0.0709,  0.2938, -0.0356,  0.1444, -0.0454,\n",
       "                       0.0375,  0.0629,  0.1839,  0.0257,  0.0759,  0.1872, -0.0543,  0.1161,\n",
       "                       0.1306,  0.1436,  0.0166,  0.0648, -0.1035, -0.0558,  0.1202, -0.0489,\n",
       "                      -0.0258,  0.2617,  0.0140, -0.1019,  0.0093,  0.1614,  0.2312,  0.0701,\n",
       "                       0.2178,  0.2328, -0.1146,  0.0183,  0.1470,  0.2587, -0.1046,  0.0519,\n",
       "                       0.0007,  0.1268,  0.0638,  0.2227, -0.0084,  0.0772,  0.0533,  0.0254,\n",
       "                       0.0218,  0.1831, -0.0298,  0.0542,  0.2615,  0.1139,  0.0340,  0.2567,\n",
       "                       0.2169, -0.0150, -0.0538,  0.0263,  0.0997,  0.0906, -0.0526,  0.1353,\n",
       "                       0.0015,  0.0107,  0.1129,  0.1523,  0.1410,  0.0876,  0.0709,  0.1963,\n",
       "                       0.0006,  0.1658,  0.0667, -0.0383,  0.0110,  0.1415,  0.0976,  0.0687,\n",
       "                       0.0139, -0.0632,  0.1593, -0.0175, -0.1382,  0.1177, -0.0299,  0.1278,\n",
       "                      -0.0028,  0.1333,  0.2463,  0.0169,  0.0394, -0.0145, -0.0321,  0.0299,\n",
       "                       0.0674,  0.0472,  0.0853, -0.0355,  0.1767,  0.1586,  0.0771,  0.2030,\n",
       "                       0.0687,  0.0709,  0.0847,  0.1463, -0.0463,  0.0692,  0.2132,  0.0227])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.4228, -0.4466,  0.0039,  ..., -0.2523, -0.2092,  0.1171],\n",
       "                      [ 0.3367, -0.2780,  0.3419,  ...,  0.4245,  0.1132,  0.0379],\n",
       "                      [ 0.1904,  0.3030,  0.1144,  ...,  0.0327,  0.2953, -0.4965],\n",
       "                      ...,\n",
       "                      [-0.2075,  0.1707, -0.4972,  ...,  0.1871, -0.0234,  0.3167],\n",
       "                      [-0.1893,  0.1340, -0.4594,  ...,  0.2294,  0.4033, -0.2360],\n",
       "                      [ 0.3466, -0.7670,  0.0903,  ..., -0.0926, -0.1839, -0.0961]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.2012,  0.0442, -0.0368,  ..., -0.0764, -0.1541, -0.1481],\n",
       "                      [ 0.3168,  0.1308,  0.1662,  ...,  0.1321,  0.0921,  0.0414],\n",
       "                      [ 0.0836, -0.2114, -0.1612,  ..., -0.0979, -0.0109, -0.1325],\n",
       "                      ...,\n",
       "                      [-0.0255, -0.0148,  0.2105,  ...,  0.1065,  0.1148,  0.0824],\n",
       "                      [ 0.0152, -0.0724,  0.0151,  ..., -0.0815, -0.1138,  0.3069],\n",
       "                      [ 0.0716, -0.0809, -0.1840,  ...,  0.1724, -0.0338, -0.2376]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-9.6939e-02, -4.6644e-02, -1.2688e-01, -1.6410e-01, -1.5294e-01,\n",
       "                      -8.9826e-02,  3.2341e-02, -7.3925e-02, -8.7250e-02, -2.4147e-02,\n",
       "                       2.1845e-02, -2.1194e-02, -5.2793e-02, -1.1672e-01, -1.4417e-01,\n",
       "                      -1.2940e-02, -4.5853e-02, -1.7072e-02, -2.0396e-02,  1.0794e-01,\n",
       "                      -1.0085e-01, -5.9320e-02, -2.6754e-02, -1.0321e-01, -2.3651e-02,\n",
       "                      -6.4746e-02, -5.7872e-02,  7.0564e-02, -1.0205e-01, -2.0055e-01,\n",
       "                      -1.9832e-01, -5.4298e-02, -1.4129e-01, -9.4728e-03, -1.9005e-03,\n",
       "                       6.6223e-02,  5.1417e-02, -9.9854e-02,  1.9921e-02,  5.7459e-02,\n",
       "                      -2.7104e-01, -7.8538e-02, -6.3645e-02, -6.9629e-02, -3.2444e-02,\n",
       "                      -1.4328e-01, -2.3795e-01, -1.5076e-01, -1.4702e-01, -8.1476e-02,\n",
       "                      -1.5116e-01, -3.0225e-02, -1.7229e-02, -1.9604e-02,  3.0909e-02,\n",
       "                      -1.8921e-01, -8.4214e-02, -8.5220e-02, -1.8544e-01, -8.4395e-02,\n",
       "                       1.5586e-03, -3.8175e-02, -2.0713e-01, -1.6839e-01,  9.5901e-04,\n",
       "                      -8.0776e-02, -2.3841e-02, -8.4242e-02, -5.8967e-02, -1.0969e-01,\n",
       "                      -2.4286e-02, -1.4038e-01, -1.9568e-01,  9.1093e-03,  7.5499e-02,\n",
       "                      -1.5769e-01, -9.7075e-02,  5.2988e-02,  1.0307e-02, -1.7598e-03,\n",
       "                       4.6478e-02, -4.0311e-02, -1.1660e-01, -3.9616e-02, -7.0856e-02,\n",
       "                      -5.6965e-02, -6.5646e-02, -7.4606e-02, -4.6856e-02, -2.6572e-02,\n",
       "                      -1.4550e-01, -6.5994e-02, -4.8165e-02, -4.7016e-02, -6.6092e-02,\n",
       "                       5.6258e-02, -9.2236e-02, -8.3841e-02, -1.8553e-01, -4.6883e-02,\n",
       "                      -8.4356e-02, -5.5783e-02, -1.5262e-01, -9.9627e-02, -3.9194e-03,\n",
       "                      -1.7289e-01, -6.3387e-02, -1.2455e-01, -3.1999e-03,  1.2141e-01,\n",
       "                      -2.0346e-01, -1.3803e-01,  1.1772e-01, -1.8715e-01, -1.6379e-01,\n",
       "                       2.2871e-02, -1.1008e-01, -1.0069e-01,  1.1706e-01, -1.2375e-01,\n",
       "                      -7.1589e-02, -1.7853e-01, -1.5155e-01, -2.3577e-01,  7.5530e-02,\n",
       "                      -9.9828e-02, -1.5783e-01,  6.9334e-02, -9.2727e-02, -5.8399e-02,\n",
       "                       1.6117e-01, -2.6079e-01,  4.3655e-02,  1.8610e-01, -2.0192e-01,\n",
       "                      -1.5954e-01,  2.7086e-02,  3.5257e-02, -1.0901e-01, -1.5969e-01,\n",
       "                       2.8669e-03, -2.3046e-01, -1.0069e-01, -1.1287e-01, -2.3029e-02,\n",
       "                      -4.3846e-02, -5.2795e-02,  2.8943e-01, -1.7994e-01, -2.9948e-02,\n",
       "                       8.2976e-02, -8.5173e-02, -1.1886e-01,  1.1137e-02, -1.3097e-01,\n",
       "                       2.8585e-02,  2.3036e-01, -2.8750e-02, -1.0637e-01, -1.2202e-01,\n",
       "                       1.9868e-02, -9.7735e-02, -1.5084e-01,  1.8506e-01, -3.1610e-02,\n",
       "                       8.5794e-02, -9.4728e-02, -1.1890e-01, -1.3686e-01, -1.6018e-01,\n",
       "                       5.4649e-03,  9.2599e-03,  3.9511e-02, -5.5127e-02, -2.7871e-01,\n",
       "                      -2.1798e-01, -1.1690e-01,  1.6230e-01, -1.2576e-01,  1.2740e-02,\n",
       "                      -1.4711e-02, -3.6032e-02, -1.1021e-02,  4.7399e-02,  1.2513e-01,\n",
       "                      -8.6180e-02, -1.3185e-01, -1.2463e-01, -2.2231e-01, -2.5903e-01,\n",
       "                      -7.6930e-02,  5.2431e-02,  9.3548e-02, -1.1830e-01,  2.2955e-03,\n",
       "                      -1.7788e-02, -1.4733e-02, -6.1881e-02, -9.7649e-02, -5.4666e-02,\n",
       "                      -1.0625e-01, -7.1218e-02,  6.3838e-02, -1.2954e-01, -5.1921e-02,\n",
       "                       1.2035e-01, -1.5267e-01, -3.9365e-02,  1.0543e-01,  6.0691e-02,\n",
       "                      -1.3155e-02, -1.3710e-01,  4.2386e-02, -9.4349e-02,  9.9347e-02,\n",
       "                       9.5613e-02, -9.1565e-02, -1.4745e-01, -1.1843e-01, -1.4744e-01,\n",
       "                      -2.3407e-01, -7.3324e-02, -1.7065e-01, -8.1125e-02, -1.9091e-01,\n",
       "                      -2.3502e-04, -4.4431e-02, -2.6515e-02, -1.9368e-01,  8.7262e-02,\n",
       "                      -1.3061e-01, -1.9841e-01,  6.0267e-02, -9.1360e-02,  9.7296e-02,\n",
       "                       1.3030e-02,  7.9183e-02, -1.2420e-01, -2.0898e-01, -1.3838e-01,\n",
       "                       5.8891e-02, -1.7374e-02, -4.6828e-02, -1.5175e-01, -1.2058e-01,\n",
       "                       1.0073e-01, -8.3751e-02, -1.5962e-01,  7.9879e-02, -8.3100e-02,\n",
       "                      -1.2211e-01, -1.6283e-01,  1.0129e-01, -1.8704e-01,  8.1442e-02,\n",
       "                       1.5753e-01, -3.0849e-02, -1.3186e-02,  1.3106e-01, -2.4066e-02,\n",
       "                       2.0153e-02,  5.8502e-03,  3.6508e-02,  1.1107e-01,  2.3677e-02,\n",
       "                      -1.7130e-02, -7.8854e-02,  1.8891e-02,  1.4727e-02,  2.5142e-02,\n",
       "                       1.0205e-02, -2.4305e-02, -3.2651e-02, -3.7005e-02, -5.0212e-02,\n",
       "                       3.8640e-02,  1.2798e-01, -6.3936e-02,  1.2219e-02, -4.7672e-03,\n",
       "                      -2.5981e-02,  3.7053e-02, -7.9214e-02, -5.0323e-02, -1.1775e-01,\n",
       "                       7.6191e-02,  5.6456e-02, -5.2040e-02, -1.8408e-02, -5.3492e-02,\n",
       "                      -1.6931e-02,  1.9799e-02,  1.6192e-02,  8.0992e-02,  6.7877e-03,\n",
       "                      -6.5359e-02,  8.0390e-02, -4.3263e-02,  1.3587e-01, -5.6530e-02,\n",
       "                      -1.0833e-01, -2.1411e-02,  9.6434e-02,  8.6371e-02,  2.0449e-02,\n",
       "                       7.3813e-03,  3.1389e-02, -6.5963e-03, -4.5692e-02,  1.7013e-02,\n",
       "                      -2.8164e-02, -1.9532e-01, -5.9375e-02,  6.0303e-02,  4.0303e-02,\n",
       "                      -5.0316e-02,  7.9590e-03, -3.2584e-02,  6.5411e-02,  6.5209e-02,\n",
       "                       6.1524e-02,  1.0597e-01, -3.3170e-02,  2.4719e-02, -4.1441e-02,\n",
       "                      -6.4193e-02,  1.0925e-02,  1.0365e-01,  1.3106e-02,  1.1203e-02,\n",
       "                      -5.7302e-02, -5.4554e-02, -7.5816e-02,  2.8803e-02,  6.1648e-02,\n",
       "                      -2.2065e-02, -2.6372e-02,  2.6099e-02,  6.1480e-03,  1.7103e-03,\n",
       "                       9.4528e-03, -5.6067e-02, -6.7872e-02, -1.7395e-02,  1.0641e-03,\n",
       "                      -6.1297e-02, -1.0824e-02, -6.8920e-02, -1.1676e-01, -9.8655e-03,\n",
       "                      -6.0428e-03, -2.4999e-02,  4.7950e-02,  4.0721e-02, -4.5866e-02,\n",
       "                       6.1260e-02, -9.1398e-02,  1.7762e-02,  6.3604e-04,  7.1728e-02,\n",
       "                       9.5752e-03,  7.0408e-02, -1.5966e-02,  1.2193e-01, -1.8852e-02,\n",
       "                       6.4333e-02,  4.3769e-02,  1.0698e-01,  4.3331e-02,  7.4091e-02,\n",
       "                      -1.0737e-01,  7.8580e-02, -4.5428e-02,  7.4788e-02, -8.8013e-02,\n",
       "                      -1.1615e-02,  8.5675e-02,  8.4491e-02, -5.4818e-02, -5.2085e-02,\n",
       "                      -6.3489e-02, -6.2223e-02,  1.2791e-02, -2.4888e-03, -9.2198e-02,\n",
       "                      -1.7790e-01, -1.7573e-01, -2.2322e-01, -8.6682e-02,  5.5285e-03,\n",
       "                      -1.1893e-01, -9.7850e-02, -1.2961e-01, -1.1665e-01,  2.4643e-02,\n",
       "                      -2.4353e-02,  6.7895e-02, -9.0930e-02, -1.5817e-02, -9.1323e-02,\n",
       "                      -5.5050e-02, -2.2833e-01,  4.8394e-02,  2.0972e-01, -1.1161e-01,\n",
       "                       3.0643e-02,  2.3834e-02, -3.0098e-02, -1.6574e-01, -4.8259e-02,\n",
       "                      -6.1915e-02, -5.9860e-02, -4.9044e-02, -1.0415e-01, -1.5708e-01,\n",
       "                      -8.9367e-02, -1.1531e-01, -9.6914e-02, -5.9774e-02,  6.1512e-02,\n",
       "                      -7.4157e-04, -1.4719e-01, -7.0700e-02, -1.2191e-01, -8.0412e-02,\n",
       "                      -2.3135e-01, -1.0568e-01, -5.4724e-02, -2.9662e-02, -2.1161e-01,\n",
       "                      -3.1298e-01, -1.4112e-01, -9.7775e-02,  6.5627e-02,  2.2373e-02,\n",
       "                      -1.3916e-02, -1.1992e-01, -9.2227e-02, -4.4412e-02, -1.7762e-02,\n",
       "                       4.6441e-02,  3.5779e-04, -8.4363e-02, -4.5709e-02, -1.0991e-01,\n",
       "                      -1.4843e-01, -1.6674e-01,  1.0592e-01, -1.2782e-01, -4.0599e-02,\n",
       "                      -6.6783e-02, -1.0379e-01, -1.3290e-01, -1.1062e-01, -1.0456e-01,\n",
       "                      -7.8839e-02, -1.3155e-01, -1.0621e-01,  3.0764e-02, -1.6031e-01,\n",
       "                       7.6106e-02, -3.5301e-02, -9.0785e-02, -3.4766e-02, -7.1997e-02,\n",
       "                      -1.8013e-02, -5.9397e-02, -2.7623e-03, -8.6564e-02, -4.6972e-02,\n",
       "                      -4.9134e-02, -1.2419e-01, -5.0976e-02, -1.3195e-01, -9.0133e-02,\n",
       "                      -1.4285e-01, -6.9330e-02, -7.0194e-02, -3.7789e-02, -1.1618e-01,\n",
       "                      -9.1346e-02, -1.7998e-01, -7.7291e-02, -1.1336e-01,  9.8107e-03,\n",
       "                       4.8058e-03, -1.8976e-01, -6.0490e-02, -4.8811e-02, -5.8846e-02,\n",
       "                       6.2840e-03, -2.0274e-01,  1.8311e-02, -8.7558e-02, -1.6713e-01,\n",
       "                      -2.4491e-01,  4.1832e-02, -1.1379e-01, -5.0137e-02, -7.9346e-02,\n",
       "                      -3.3539e-02, -1.4803e-01, -9.8951e-03, -9.2467e-02, -1.6199e-02,\n",
       "                      -1.1215e-01, -8.2347e-02, -1.9750e-01, -3.8481e-02, -1.3631e-01,\n",
       "                      -5.5685e-02, -3.0670e-02])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-0.1492,  0.0225, -0.1233, -0.0009, -0.0439,  0.0373, -0.1092, -0.0737,\n",
       "                      -0.1199, -0.2003, -0.1547,  0.1103,  0.0086, -0.0518, -0.0520, -0.0564,\n",
       "                       0.0039, -0.1630,  0.0069,  0.0123, -0.1366, -0.0731, -0.1071, -0.0547,\n",
       "                      -0.1378, -0.0238, -0.1707, -0.0716, -0.0179, -0.0562, -0.0663, -0.1147,\n",
       "                      -0.0457, -0.0088, -0.0327, -0.0707, -0.2392, -0.1097, -0.0389,  0.0284,\n",
       "                      -0.1669, -0.1647,  0.0027, -0.0270, -0.0300, -0.1593, -0.1028, -0.1518,\n",
       "                      -0.2245,  0.0452, -0.0062, -0.1609, -0.0418, -0.1865,  0.0284, -0.0697,\n",
       "                      -0.0137, -0.0317, -0.1615, -0.0506, -0.1510, -0.1453, -0.0888, -0.0340,\n",
       "                      -0.1229, -0.0625,  0.0962, -0.0014, -0.0392, -0.1407, -0.1290, -0.0405,\n",
       "                      -0.1777, -0.1566, -0.0199, -0.1856, -0.0364, -0.0925, -0.1031, -0.0183,\n",
       "                       0.1415, -0.0202, -0.0763, -0.0875, -0.1242, -0.2219, -0.1092, -0.0330,\n",
       "                      -0.1365, -0.0493, -0.0442, -0.1470, -0.0865,  0.0615,  0.0215, -0.0062,\n",
       "                      -0.0143, -0.1406, -0.0905, -0.2095, -0.0194,  0.0415, -0.0195, -0.0833,\n",
       "                      -0.0217, -0.1489,  0.0010, -0.1643,  0.0259,  0.0502, -0.1975, -0.1333,\n",
       "                       0.0468, -0.1448, -0.0789,  0.0061, -0.0658, -0.1217,  0.1506, -0.1653,\n",
       "                      -0.0778, -0.1180, -0.0807, -0.1250,  0.1812, -0.1416, -0.1277, -0.0447,\n",
       "                      -0.0771, -0.0459,  0.0022, -0.1880, -0.2029,  0.1633, -0.1049, -0.1115,\n",
       "                       0.0531, -0.0298, -0.1282, -0.0892, -0.0752, -0.1972, -0.0134,  0.0071,\n",
       "                      -0.0356, -0.1595, -0.0266,  0.1884, -0.1923, -0.0841, -0.0315, -0.0576,\n",
       "                      -0.2194, -0.0528, -0.0493,  0.0826,  0.0161, -0.0558, -0.1061, -0.2183,\n",
       "                      -0.1206, -0.0031, -0.0648,  0.0846, -0.0534, -0.0172, -0.0862, -0.0300,\n",
       "                      -0.0849, -0.2836, -0.0705,  0.0078,  0.0339, -0.1810, -0.1145, -0.0700,\n",
       "                      -0.0345,  0.0998, -0.0226, -0.1339, -0.0575, -0.1116, -0.0735,  0.0979,\n",
       "                       0.0338, -0.1250,  0.0233,  0.0266, -0.0414, -0.1141, -0.0750,  0.0691,\n",
       "                      -0.0336, -0.1044, -0.0354, -0.2005, -0.1086, -0.1243, -0.2541, -0.1581,\n",
       "                      -0.1937, -0.1336,  0.0356, -0.1146, -0.0602,  0.1339, -0.0120, -0.0509,\n",
       "                       0.0733,  0.0021,  0.0450,  0.0416, -0.0355, -0.0619,  0.0297,  0.0778,\n",
       "                      -0.1458, -0.0344, -0.0074, -0.0705, -0.2158,  0.0046, -0.0385,  0.1168,\n",
       "                      -0.2171,  0.0421, -0.0068, -0.1392, -0.1074, -0.0129, -0.0293,  0.0155,\n",
       "                      -0.0796, -0.0899, -0.0029, -0.0944,  0.1665, -0.0400, -0.0157, -0.2063,\n",
       "                       0.2247, -0.0878, -0.0025, -0.2322, -0.0832, -0.0661, -0.1646, -0.0657,\n",
       "                      -0.0825, -0.1795, -0.0906, -0.0672, -0.0174, -0.0435, -0.0489,  0.0546,\n",
       "                       0.0457, -0.0268, -0.0228,  0.0400,  0.1329, -0.0061,  0.0494, -0.0146,\n",
       "                       0.0011, -0.0554,  0.0621, -0.0094,  0.0028, -0.0071,  0.0305, -0.0693,\n",
       "                       0.0046, -0.0641,  0.0670,  0.0835,  0.0616, -0.0618, -0.0096,  0.0198,\n",
       "                      -0.0436,  0.0287, -0.0580, -0.0066, -0.0186, -0.0102,  0.0538, -0.1418,\n",
       "                       0.0216,  0.0036,  0.1061, -0.0018,  0.0905, -0.0005,  0.1006,  0.0219,\n",
       "                       0.0433, -0.0472,  0.0072,  0.0805, -0.0141,  0.0010, -0.0223, -0.0561,\n",
       "                       0.0731,  0.0220,  0.0390, -0.0101, -0.1007,  0.1007, -0.0897, -0.0789,\n",
       "                      -0.0665,  0.0577, -0.2034, -0.0078, -0.0453,  0.1039,  0.0687,  0.0247,\n",
       "                      -0.0431, -0.0299, -0.0901,  0.1368,  0.0557,  0.0723, -0.0279, -0.0025,\n",
       "                       0.0430, -0.0329,  0.0751,  0.0072,  0.0385,  0.0464,  0.0657,  0.0184,\n",
       "                       0.0446,  0.0322,  0.0489, -0.0094,  0.0844,  0.0347,  0.0528, -0.0625,\n",
       "                      -0.0794,  0.0365, -0.0665, -0.0856, -0.0450,  0.0239, -0.1985,  0.0505,\n",
       "                      -0.0569,  0.0354, -0.0141,  0.0296, -0.0157, -0.0022, -0.0252, -0.0682,\n",
       "                       0.1060, -0.0315,  0.0145, -0.0674,  0.0090,  0.1196,  0.0628, -0.0215,\n",
       "                       0.0723, -0.1627,  0.0713,  0.0184,  0.0860,  0.0366,  0.0180,  0.0394,\n",
       "                       0.0005, -0.0070,  0.0795,  0.0350, -0.1074, -0.0169,  0.0140,  0.0328,\n",
       "                      -0.1332, -0.0462, -0.1548, -0.1214, -0.1582,  0.0222, -0.1558, -0.1433,\n",
       "                      -0.0531, -0.0804,  0.0372,  0.0007, -0.0302, -0.0680,  0.0133, -0.1396,\n",
       "                      -0.1207, -0.0142, -0.0322,  0.1521, -0.1312, -0.1052, -0.0175, -0.1411,\n",
       "                      -0.0586,  0.0092, -0.0821,  0.0433, -0.0610, -0.1352, -0.0750,  0.0154,\n",
       "                      -0.0445, -0.0552, -0.0912, -0.0525, -0.0661, -0.1085, -0.0061, -0.0322,\n",
       "                      -0.2055, -0.1879, -0.0834,  0.0287, -0.1019, -0.1609, -0.1464, -0.0874,\n",
       "                      -0.1724,  0.1774, -0.0192,  0.0906, -0.0834, -0.0325, -0.0024, -0.0782,\n",
       "                       0.0536, -0.1741, -0.1886, -0.0805, -0.1674, -0.2429, -0.0804,  0.0373,\n",
       "                      -0.1772, -0.0846,  0.0147, -0.0041,  0.0403, -0.1209, -0.0209, -0.1225,\n",
       "                      -0.0915, -0.0581,  0.0455,  0.0098,  0.0506, -0.0138, -0.0817,  0.0176,\n",
       "                       0.0447,  0.0119, -0.0660, -0.0443,  0.0895, -0.0674, -0.0158, -0.0514,\n",
       "                      -0.0849, -0.0998, -0.0664, -0.0589, -0.1048,  0.0823, -0.0293, -0.0419,\n",
       "                      -0.1048, -0.2035, -0.0616, -0.0780,  0.0093, -0.0379, -0.0354, -0.1619,\n",
       "                      -0.0462, -0.1902, -0.0460, -0.0581,  0.0035, -0.1049, -0.1316, -0.1595,\n",
       "                      -0.0439, -0.0616, -0.1797, -0.0683, -0.1031, -0.1111,  0.0167, -0.0941,\n",
       "                       0.0643, -0.1873, -0.2476, -0.2023, -0.0493, -0.0391, -0.1115,  0.0893])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.6242,  0.3185,  0.0952,  ...,  0.5443,  0.3305, -0.5938],\n",
       "                      [-0.0617, -0.2825, -0.7057,  ...,  0.6532,  0.4526, -0.1505],\n",
       "                      [ 0.3252,  0.4279, -0.1839,  ...,  0.4675,  0.4911, -0.1745],\n",
       "                      ...,\n",
       "                      [-0.6756,  0.5024, -0.6113,  ..., -0.8020, -0.0235,  0.2035],\n",
       "                      [-0.0040, -0.1060, -0.1151,  ...,  0.4312,  0.0606,  0.3143],\n",
       "                      [-0.5407, -0.0555,  0.1190,  ..., -0.5042,  0.0071,  0.3299]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.3537,  0.0314, -0.0314,  ..., -0.2007,  0.1173,  0.1361],\n",
       "                      [-0.0812, -0.2360,  0.1940,  ...,  0.0016, -0.0794,  0.0679],\n",
       "                      [-0.2055, -0.1587,  0.1519,  ...,  0.5745,  0.0411,  0.1797],\n",
       "                      ...,\n",
       "                      [ 0.1071,  0.4605,  0.1067,  ...,  0.0414,  0.2247,  0.0809],\n",
       "                      [-0.1013,  0.2596,  0.3350,  ..., -0.1579, -0.2771,  0.0613],\n",
       "                      [ 0.0685, -0.2299,  0.0734,  ..., -0.2056, -0.0745,  0.0765]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 3.3460e-02,  1.5454e-01,  2.8367e-02,  7.6385e-02, -1.1203e-01,\n",
       "                       1.3739e-01,  4.5333e-01,  1.1099e-01,  1.4277e-01,  6.3709e-02,\n",
       "                       1.1252e-01,  1.9290e-01,  1.2215e-01,  1.4400e-02,  8.8363e-02,\n",
       "                       1.4800e-01,  2.6614e-01,  2.3344e-01,  3.4961e-01,  7.0817e-02,\n",
       "                       1.2623e-01,  9.7167e-02,  2.7494e-01,  1.8704e-01,  1.8342e-01,\n",
       "                       8.3449e-02,  1.4513e-01,  1.5500e-01,  1.3491e-01,  2.3149e-01,\n",
       "                       1.1163e-01,  1.8044e-02,  1.6223e-01,  2.2330e-01, -4.1403e-03,\n",
       "                       1.8791e-01, -1.8951e-02,  6.7142e-02,  2.3685e-01,  8.0534e-02,\n",
       "                       5.7967e-02,  6.9006e-02, -6.3440e-03,  2.2078e-01,  6.2963e-02,\n",
       "                       1.1711e-01,  1.2502e-01, -7.0192e-03,  6.6835e-02,  1.6649e-01,\n",
       "                       2.3015e-01,  2.7288e-01,  2.9926e-01, -5.5722e-02,  1.9765e-01,\n",
       "                       4.9701e-02,  2.2641e-01,  2.0136e-01,  1.0535e-01,  9.9635e-02,\n",
       "                       1.0019e-01,  2.7450e-01,  4.4975e-02,  2.5804e-01,  4.9607e-02,\n",
       "                       2.6883e-01,  2.0296e-01,  8.4967e-02,  1.8566e-01,  1.1160e-01,\n",
       "                       1.6535e-01,  1.8196e-01,  9.1094e-02,  4.3959e-02,  5.5303e-02,\n",
       "                       1.5720e-01,  2.1938e-01,  1.3094e-01,  1.2823e-01,  1.9338e-01,\n",
       "                       1.6263e-01,  5.0438e-02,  1.4021e-01,  2.2333e-01,  5.5312e-02,\n",
       "                       1.5494e-01, -1.6921e-02, -1.2774e-02,  1.2188e-01,  1.0422e-01,\n",
       "                       1.2324e-01,  1.1083e-01,  2.6305e-02,  1.2348e-01,  1.7664e-01,\n",
       "                       3.1575e-02,  2.3350e-02,  1.7806e-01,  4.7872e-03,  2.4778e-01,\n",
       "                       2.2542e-01,  4.2227e-02,  7.9772e-02,  1.5085e-01,  2.8013e-01,\n",
       "                       9.2414e-02,  1.3902e-01,  1.1659e-01, -3.2802e-02,  1.3269e-01,\n",
       "                       1.4511e-01,  1.2862e-01,  8.0450e-02,  3.7070e-02,  1.6497e-01,\n",
       "                       4.0818e-02,  1.6446e-01,  3.9015e-02, -3.4089e-02,  2.8137e-01,\n",
       "                       1.0307e-01, -5.8733e-02,  7.2606e-02,  4.3615e-02,  6.6393e-02,\n",
       "                       3.8447e-01,  1.9711e-01,  1.0459e-01,  1.9632e-01, -1.9041e-03,\n",
       "                      -4.0912e-02,  8.5641e-02,  6.0207e-02,  2.3082e-01, -8.0991e-02,\n",
       "                       2.2926e-02,  1.0422e-01,  1.3587e-03, -1.8092e-02,  9.1142e-02,\n",
       "                       8.9142e-03, -5.3995e-02, -1.2041e-02,  1.0765e-01,  1.3000e-01,\n",
       "                      -3.9336e-03,  5.6435e-02, -9.0339e-03, -4.3053e-02,  6.5193e-02,\n",
       "                      -3.3939e-02,  1.5565e-01,  5.5279e-02,  1.3325e-02,  4.2154e-03,\n",
       "                       1.0091e-02,  1.6521e-01, -4.9992e-02, -8.6513e-02, -2.7947e-02,\n",
       "                      -1.5334e-02,  6.0480e-03,  3.5459e-02,  5.6947e-02,  5.5557e-02,\n",
       "                       1.9202e-01,  6.2463e-02,  1.5914e-02, -8.6332e-02,  7.7278e-03,\n",
       "                       5.5640e-02, -3.8429e-02,  4.2565e-02,  1.2734e-01, -1.2856e-03,\n",
       "                      -2.7785e-02,  3.4024e-05, -2.0177e-02, -6.9054e-03, -8.1000e-03,\n",
       "                       6.2613e-02,  6.5023e-02,  2.6769e-02,  7.3071e-02,  1.5182e-02,\n",
       "                      -1.1284e-01,  5.0954e-02,  2.2517e-02,  1.4270e-01, -3.6861e-02,\n",
       "                       1.0862e-01, -1.9442e-01, -6.5332e-02, -2.8226e-02, -5.3819e-03,\n",
       "                       5.2410e-02,  3.1047e-02,  6.5733e-02, -3.8412e-02,  4.3885e-02,\n",
       "                       1.1951e-02,  3.8006e-02, -2.2677e-02,  1.5992e-01,  2.6511e-01,\n",
       "                       8.6730e-02,  3.7730e-02,  3.8716e-02,  1.2557e-01,  8.2149e-02,\n",
       "                       6.5162e-02,  2.1393e-02,  4.4998e-02,  4.5643e-02, -9.9291e-02,\n",
       "                      -4.3527e-02,  7.4720e-03, -2.1875e-02,  1.5759e-02,  3.3241e-02,\n",
       "                      -1.3387e-02,  4.6174e-02,  1.3713e-01,  3.8028e-02,  1.2441e-01,\n",
       "                       9.0667e-02,  4.7124e-02,  7.0048e-03,  2.2827e-01, -3.7377e-02,\n",
       "                       2.3904e-01, -3.8576e-02,  1.9316e-02,  4.1769e-02,  6.7620e-02,\n",
       "                      -3.3496e-02, -4.8316e-03, -2.8214e-02, -5.1781e-02, -7.0652e-03,\n",
       "                       1.1897e-01,  4.9598e-02,  2.7451e-02,  6.1532e-02, -1.1621e-01,\n",
       "                      -2.7965e-02, -1.6686e-01,  4.4729e-03, -2.8805e-02,  2.0553e-02,\n",
       "                       2.9723e-02,  2.9520e-02,  5.1772e-02,  3.8081e-02,  9.9830e-03,\n",
       "                       7.0521e-02, -2.7421e-02,  6.8383e-03, -6.4726e-02,  7.3391e-02,\n",
       "                       7.5055e-02,  8.9051e-03,  8.8876e-02, -1.6367e-02, -1.4739e-01,\n",
       "                       4.4760e-02,  4.0174e-02,  3.3305e-02,  3.0677e-02, -2.5337e-02,\n",
       "                       9.4833e-03, -8.3901e-03,  1.5716e-02,  3.5462e-02,  5.6272e-02,\n",
       "                       2.2949e-02, -8.8719e-02, -2.8866e-02, -3.8203e-02, -9.0435e-02,\n",
       "                      -1.0965e-01,  1.0062e-01,  1.2186e-02, -3.9549e-02,  3.0165e-02,\n",
       "                      -9.7364e-02, -4.6471e-02, -1.6520e-01, -3.1840e-03, -2.1099e-02,\n",
       "                       7.5148e-02, -7.6344e-02, -6.7907e-02,  7.7298e-02, -6.6573e-02,\n",
       "                      -8.8388e-02, -8.3741e-03, -4.9309e-02,  1.8829e-01, -5.1639e-02,\n",
       "                       6.6062e-02,  8.3214e-02, -6.0951e-03,  1.0350e-02,  8.7343e-02,\n",
       "                      -1.4032e-01,  1.1417e-01, -5.7073e-03,  1.2560e-01, -6.4694e-02,\n",
       "                       4.6696e-02, -1.4802e-01, -4.0546e-04,  2.0236e-04, -7.6153e-02,\n",
       "                       4.2239e-02, -1.4179e-01, -5.8095e-02,  1.5096e-01,  9.3581e-02,\n",
       "                       9.5241e-04, -5.7894e-03,  1.4544e-01,  1.1873e-02, -9.7487e-02,\n",
       "                      -5.1875e-02,  2.9581e-02,  4.5252e-02,  1.7141e-01, -6.4522e-02,\n",
       "                       1.0124e-01, -3.4303e-02,  8.8394e-02,  2.4463e-02,  3.1799e-02,\n",
       "                       2.7862e-02,  5.0472e-02,  3.1280e-02,  5.0212e-02, -1.4819e-02,\n",
       "                      -9.2932e-02, -4.9733e-02,  1.8285e-02,  3.8051e-02, -3.2951e-02,\n",
       "                      -1.4086e-01, -1.0157e-01,  5.3616e-02,  2.5311e-02,  6.3626e-02,\n",
       "                       3.6763e-02, -3.1510e-02,  3.3012e-02,  5.6988e-03,  3.1450e-02,\n",
       "                       6.8213e-02,  6.3562e-02,  1.4726e-02,  2.2533e-02, -5.2412e-02,\n",
       "                       8.0232e-02, -2.3897e-02, -1.9001e-02,  6.1328e-02,  6.7447e-02,\n",
       "                       3.2717e-02,  9.5513e-02,  7.3499e-02,  2.0722e-02,  1.5078e-01,\n",
       "                      -3.4086e-02, -3.9896e-02,  8.0755e-02, -6.6446e-02,  1.7087e-02,\n",
       "                      -1.5948e-02,  1.1330e-01,  1.2989e-01,  7.3842e-02, -1.2745e-01,\n",
       "                      -4.2151e-02,  1.5042e-02,  1.1047e-01,  8.8653e-02,  5.1260e-02,\n",
       "                       1.5959e-01,  2.1729e-01,  5.4119e-02,  1.6010e-01,  3.2438e-01,\n",
       "                       1.1825e-01,  2.1078e-02,  6.9509e-02, -1.3744e-02,  7.7070e-02,\n",
       "                       2.6245e-01,  2.5348e-01,  5.4323e-02,  1.5763e-01,  1.8417e-01,\n",
       "                       8.9754e-02,  9.2609e-02,  1.1622e-01,  1.7538e-01,  7.5368e-02,\n",
       "                       1.2053e-01,  3.0391e-01, -2.3505e-02,  9.0702e-02,  1.3536e-01,\n",
       "                       8.1974e-02,  1.3127e-01,  4.1621e-02,  3.0367e-02,  1.1834e-01,\n",
       "                       7.0115e-02,  1.8318e-02,  1.1173e-01,  3.3364e-02,  1.9961e-01,\n",
       "                       1.1513e-01, -2.2583e-02,  2.4928e-01,  2.7386e-01,  1.1026e-01,\n",
       "                       2.1049e-01,  1.3674e-03,  1.6322e-01, -2.5282e-05,  1.1242e-01,\n",
       "                       2.2804e-01,  1.4054e-01,  1.1595e-01,  6.6698e-02,  9.8572e-02,\n",
       "                       3.5132e-02,  4.5225e-02, -8.6837e-02,  1.4099e-01, -2.2683e-02,\n",
       "                      -2.2595e-02,  1.5773e-01,  9.0227e-02,  3.1853e-01,  9.3452e-02,\n",
       "                       1.8165e-01,  5.4315e-02,  2.7621e-01,  1.7279e-01,  1.5662e-01,\n",
       "                       9.1933e-03,  9.5867e-02,  5.4678e-02,  1.8243e-01,  1.0037e-01,\n",
       "                       1.7134e-01,  1.6529e-01,  1.1850e-01,  1.0240e-02,  2.7344e-01,\n",
       "                      -4.7645e-02,  2.7322e-01,  2.0812e-01,  2.3661e-01,  9.9902e-02,\n",
       "                       1.0551e-01,  1.8062e-01,  1.3456e-01,  9.4415e-02,  2.4723e-01,\n",
       "                       1.9002e-01,  1.9491e-01,  6.8912e-02,  1.8269e-01,  1.5999e-01,\n",
       "                      -6.0252e-03,  5.8261e-02, -7.8149e-02,  8.3543e-02,  1.8605e-01,\n",
       "                       1.8849e-01,  1.7291e-01,  1.9717e-02,  2.2493e-01,  1.0399e-01,\n",
       "                       7.6517e-02,  3.3116e-02,  2.4774e-01,  1.4867e-01,  2.4799e-01,\n",
       "                      -5.9026e-02,  1.7756e-01,  1.1033e-01,  1.3123e-01,  7.3632e-02,\n",
       "                       3.5349e-02,  2.9747e-02,  7.2822e-02,  1.6574e-01,  1.5656e-01,\n",
       "                       2.2811e-02,  2.1020e-01,  3.9693e-03,  1.1102e-01, -9.6498e-02,\n",
       "                       2.0940e-01,  1.6269e-01,  5.3675e-02,  1.0649e-01,  1.4641e-01,\n",
       "                       3.2546e-01,  1.5414e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 0.0838,  0.0751,  0.0810,  0.2220,  0.1188,  0.1701,  0.2593,  0.2060,\n",
       "                       0.1193,  0.1040,  0.2033,  0.1530,  0.1671,  0.0374,  0.0809,  0.1376,\n",
       "                       0.4199,  0.1070,  0.2638,  0.0732,  0.2345,  0.1785,  0.2682,  0.2233,\n",
       "                       0.1418,  0.1535,  0.2432,  0.1190,  0.0718,  0.1718,  0.0319,  0.1184,\n",
       "                       0.1672,  0.1883, -0.0106,  0.2404,  0.0279,  0.0841,  0.1695,  0.2824,\n",
       "                       0.0339,  0.1546,  0.1106,  0.0640,  0.2551,  0.1560,  0.1108,  0.1163,\n",
       "                       0.1796,  0.1957,  0.1340,  0.1603,  0.2162,  0.0152,  0.0718,  0.1339,\n",
       "                       0.0736,  0.1242,  0.3201,  0.0756, -0.0029,  0.2624,  0.0763,  0.2333,\n",
       "                       0.0412,  0.0142,  0.2948,  0.1196,  0.1245,  0.0076,  0.2196,  0.1568,\n",
       "                       0.1859,  0.0999,  0.1067,  0.1035,  0.0626,  0.1450,  0.2155,  0.1553,\n",
       "                       0.1334,  0.0915,  0.1553,  0.1664,  0.0482,  0.1799, -0.0723, -0.0171,\n",
       "                       0.1958,  0.2571,  0.1950,  0.0823, -0.0164,  0.0785,  0.1172,  0.1225,\n",
       "                       0.0931,  0.2577,  0.1234,  0.2013,  0.1522,  0.0219, -0.0185,  0.2428,\n",
       "                       0.2209,  0.1916,  0.2096,  0.1770,  0.0621,  0.0859,  0.0726, -0.0102,\n",
       "                       0.2754,  0.0377,  0.1761,  0.1941,  0.0828,  0.0639,  0.0393,  0.2866,\n",
       "                       0.0768,  0.0897,  0.2569,  0.0804,  0.0864,  0.3960,  0.1422,  0.2159,\n",
       "                       0.0006,  0.0859,  0.0288,  0.0158,  0.1185,  0.0157,  0.1062,  0.1349,\n",
       "                       0.1521, -0.0403,  0.1612,  0.1748, -0.2104, -0.0098,  0.0170,  0.0978,\n",
       "                       0.1470,  0.0325,  0.1113, -0.0077,  0.0478,  0.0854, -0.0880,  0.0977,\n",
       "                       0.0339,  0.0334, -0.0450,  0.0435,  0.2492,  0.0168,  0.1101,  0.0516,\n",
       "                      -0.0914,  0.0640,  0.0235,  0.1201,  0.0533,  0.1055,  0.0206,  0.0851,\n",
       "                       0.0445,  0.1296,  0.0736, -0.0448,  0.0121,  0.0638, -0.0655, -0.0948,\n",
       "                       0.1023,  0.0328, -0.0785, -0.0578,  0.1142,  0.1358,  0.0253,  0.0011,\n",
       "                      -0.0170,  0.0599, -0.0245,  0.0834,  0.1228,  0.0242, -0.0337, -0.1551,\n",
       "                       0.1024, -0.1080, -0.0809,  0.1697, -0.0929,  0.1035,  0.0325, -0.0094,\n",
       "                      -0.0836,  0.0436, -0.0115,  0.1055,  0.0800, -0.0134,  0.0292, -0.0644,\n",
       "                       0.1203,  0.0415, -0.0132, -0.0667, -0.0541,  0.0434,  0.0704,  0.1306,\n",
       "                       0.0419, -0.0764,  0.0095,  0.0563, -0.0796, -0.1201,  0.0965,  0.1563,\n",
       "                       0.0469,  0.0354, -0.0005,  0.0355,  0.0572, -0.0875,  0.1077,  0.0517,\n",
       "                      -0.0544, -0.0166, -0.0032,  0.0588,  0.0735, -0.0413, -0.0252,  0.0077,\n",
       "                       0.0708,  0.0891,  0.0144, -0.0134, -0.0758,  0.0926, -0.1273, -0.0640,\n",
       "                       0.0054,  0.0542,  0.0474,  0.0329, -0.0629, -0.1766,  0.0906,  0.1776,\n",
       "                      -0.0029, -0.0365,  0.0927,  0.0032, -0.1033,  0.0324,  0.1104, -0.1079,\n",
       "                       0.0482,  0.1318, -0.0820,  0.0522,  0.0589, -0.0084, -0.0791,  0.0798,\n",
       "                       0.0148,  0.0358,  0.0488,  0.0521,  0.1293, -0.0131, -0.0276,  0.0560,\n",
       "                      -0.0704,  0.0873,  0.0407,  0.0339,  0.0175, -0.0849, -0.0663, -0.1155,\n",
       "                      -0.0221,  0.0408,  0.0623, -0.0690, -0.0601,  0.0056, -0.0255,  0.0078,\n",
       "                       0.0632, -0.0312, -0.0088, -0.0580, -0.0107,  0.0685,  0.0765, -0.0464,\n",
       "                      -0.0295, -0.0450,  0.1065, -0.0227,  0.0844, -0.0767, -0.0849,  0.0580,\n",
       "                       0.0267, -0.0542, -0.1487, -0.0138, -0.0353, -0.1452, -0.1208,  0.0589,\n",
       "                       0.0547, -0.0847,  0.0387, -0.1572,  0.0573,  0.0508, -0.0198,  0.0816,\n",
       "                      -0.0432, -0.1375,  0.0839,  0.0507,  0.1353,  0.0066,  0.0655,  0.0034,\n",
       "                       0.0601, -0.1679,  0.0166, -0.0329, -0.0436, -0.0698,  0.1318, -0.0666,\n",
       "                       0.0509,  0.1448, -0.1096,  0.1607, -0.0328,  0.0623,  0.0560,  0.0348,\n",
       "                       0.0308,  0.0174, -0.0339, -0.0076,  0.1277,  0.0923,  0.0304, -0.0075,\n",
       "                       0.0677,  0.0035,  0.0534, -0.0045,  0.0336,  0.0680, -0.0105, -0.0535,\n",
       "                      -0.0237,  0.0150,  0.0016, -0.0955,  0.0187,  0.0074,  0.0715,  0.0063,\n",
       "                       0.1000,  0.0933,  0.1565, -0.0235,  0.0459, -0.0443,  0.0158, -0.0052,\n",
       "                       0.0577,  0.2175,  0.0689,  0.1719,  0.1046,  0.1388,  0.2519,  0.0755,\n",
       "                      -0.0381,  0.0945, -0.0625,  0.2238,  0.1848,  0.0091,  0.0517,  0.1441,\n",
       "                       0.2083,  0.1271,  0.2105,  0.1415,  0.0508,  0.1676,  0.3348,  0.1235,\n",
       "                      -0.0423,  0.1042,  0.0155,  0.0071,  0.0223,  0.1594,  0.1415,  0.0577,\n",
       "                      -0.0033,  0.0873, -0.0420,  0.1192,  0.0494,  0.1258,  0.1446,  0.2541,\n",
       "                       0.1075,  0.1314, -0.0798,  0.1739,  0.0095,  0.0886,  0.2590,  0.1096,\n",
       "                       0.0366,  0.2478,  0.0986, -0.0080,  0.1073, -0.1170,  0.1430, -0.0724,\n",
       "                       0.0009,  0.2432,  0.1880,  0.1759,  0.0435,  0.2556,  0.0753,  0.2877,\n",
       "                       0.2063,  0.1258,  0.0710,  0.0964,  0.1893,  0.0614,  0.1570,  0.2594,\n",
       "                       0.1094,  0.0465,  0.0838,  0.2130,  0.0116,  0.3425,  0.0355,  0.2483,\n",
       "                       0.0168, -0.0109,  0.0476,  0.3586,  0.0290,  0.2199,  0.1415,  0.0485,\n",
       "                      -0.0038,  0.1466,  0.1433, -0.1070,  0.0822,  0.0027,  0.1438,  0.2176,\n",
       "                       0.1794,  0.1954,  0.1750,  0.2700,  0.2099,  0.1046,  0.0067,  0.1179,\n",
       "                       0.2947,  0.2547,  0.0524,  0.1197,  0.0355,  0.1429, -0.0515,  0.0826,\n",
       "                       0.1497,  0.1511,  0.2653,  0.0798,  0.1656,  0.2155,  0.0570,  0.0704,\n",
       "                      -0.0898,  0.0194,  0.1555,  0.1769,  0.1578,  0.2294,  0.3795,  0.1718])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.1041,  0.1838, -0.4158,  ...,  0.1082, -0.0354,  0.2784],\n",
       "                      [-0.0532, -0.0278, -0.1574,  ..., -0.2217,  0.1463, -0.0459],\n",
       "                      [-0.0243, -0.2201, -0.0577,  ..., -0.3150,  0.0871,  0.0711],\n",
       "                      ...,\n",
       "                      [-0.1207, -0.3040, -0.0393,  ..., -0.0469,  0.1265, -0.0289],\n",
       "                      [-0.2060,  0.1363,  0.2381,  ..., -0.3521,  0.0360, -0.2467],\n",
       "                      [-0.1607,  0.0542, -0.2068,  ..., -0.5244,  0.0456, -0.0734]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0365, -0.1010, -0.0262,  ...,  0.2397, -0.1270,  0.0193],\n",
       "                      [ 0.1057,  0.0647, -0.2140,  ..., -0.0736, -0.1247, -0.0159],\n",
       "                      [ 0.3585, -0.0984,  0.0979,  ..., -0.0746,  0.4573,  0.2707],\n",
       "                      ...,\n",
       "                      [ 0.2934, -0.1435,  0.0729,  ...,  0.1825, -0.1564,  0.2351],\n",
       "                      [ 0.0274, -0.1382, -0.0670,  ...,  0.3514,  0.1234,  0.0342],\n",
       "                      [ 0.1507,  0.0385,  0.1617,  ..., -0.0792,  0.3126, -0.0412]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([-0.1103, -0.0507,  0.0819,  0.1082, -0.1654,  0.0229, -0.1503, -0.1129,\n",
       "                      -0.1661, -0.0397,  0.0104, -0.1314, -0.0499, -0.0895, -0.0020, -0.0514,\n",
       "                      -0.0723, -0.0468, -0.0311,  0.0795, -0.0137, -0.0398, -0.0022,  0.1130,\n",
       "                      -0.0321, -0.1196, -0.1558,  0.0246,  0.0238, -0.0668, -0.0076, -0.0633,\n",
       "                      -0.0145, -0.1439, -0.0738, -0.1326,  0.0518, -0.0608, -0.0400, -0.1209,\n",
       "                       0.0217,  0.0837, -0.1461,  0.0349,  0.0010, -0.1908, -0.1141, -0.0229,\n",
       "                       0.1517, -0.0412, -0.0351, -0.0748,  0.0195,  0.1096, -0.0533, -0.0672,\n",
       "                      -0.1585, -0.1125, -0.0857, -0.1204, -0.0818, -0.0697,  0.0225, -0.0498,\n",
       "                      -0.1407, -0.0765, -0.2157, -0.1114, -0.2166,  0.0080, -0.0967, -0.0554,\n",
       "                       0.0354, -0.0315, -0.1582, -0.1550,  0.0759,  0.0868, -0.0267, -0.0303,\n",
       "                      -0.0391, -0.1043, -0.0309,  0.0083, -0.0347, -0.1308, -0.1205, -0.1716,\n",
       "                      -0.0672, -0.0277, -0.0898,  0.0161, -0.1234,  0.0456, -0.0301, -0.1064,\n",
       "                       0.0951, -0.1402, -0.0562, -0.0601,  0.0650,  0.0723, -0.0568, -0.1358,\n",
       "                      -0.0374, -0.0289, -0.1005,  0.0079, -0.0987, -0.0689,  0.0154, -0.0579,\n",
       "                      -0.1051, -0.0243,  0.1672, -0.0664,  0.0654,  0.0263, -0.1277, -0.1631,\n",
       "                       0.0105,  0.0103,  0.0006, -0.1785,  0.0907, -0.0141,  0.0010, -0.0038,\n",
       "                       0.0154, -0.1387, -0.1274,  0.0207, -0.0136, -0.0779, -0.1223, -0.0206,\n",
       "                      -0.1011, -0.1003,  0.0485, -0.1672, -0.0050, -0.0332, -0.0794, -0.0061,\n",
       "                      -0.0886, -0.0621, -0.0571, -0.2461, -0.1008, -0.0337, -0.1764,  0.0256,\n",
       "                      -0.0545, -0.1213, -0.0752, -0.0017, -0.0994,  0.0008, -0.1614, -0.1723,\n",
       "                       0.0592, -0.0824,  0.1006, -0.1359, -0.0100, -0.0546,  0.0225, -0.2074,\n",
       "                      -0.0361, -0.0612, -0.0088, -0.0589, -0.0630, -0.1084, -0.1340, -0.2233,\n",
       "                       0.0527, -0.0251,  0.0597, -0.1323, -0.0360,  0.0232, -0.0647,  0.0373,\n",
       "                      -0.0693, -0.0368, -0.0670,  0.0568, -0.0411,  0.0241, -0.1108, -0.0980,\n",
       "                      -0.0204,  0.0634, -0.0679, -0.1749, -0.0890, -0.0067,  0.0134,  0.0036,\n",
       "                      -0.0272, -0.0730,  0.0345,  0.0073,  0.0449, -0.0614, -0.0571, -0.0466,\n",
       "                      -0.0088, -0.0931, -0.1491, -0.2066, -0.0597, -0.1618, -0.0216, -0.1788,\n",
       "                      -0.0725, -0.0928,  0.1427,  0.0347, -0.0200, -0.0137,  0.0622, -0.0080,\n",
       "                      -0.0711, -0.0744,  0.0111, -0.0713, -0.0143, -0.2192, -0.0050, -0.0119,\n",
       "                       0.0606,  0.0057, -0.1355, -0.0258, -0.0400, -0.1011,  0.1768,  0.0182,\n",
       "                      -0.1011, -0.0359, -0.0493, -0.1631, -0.0225, -0.0048, -0.1881,  0.0216,\n",
       "                       0.0342, -0.0304, -0.1622, -0.1095,  0.0270,  0.0567,  0.0008, -0.1365,\n",
       "                       0.0436,  0.0597,  0.0714,  0.0997, -0.0731, -0.0366,  0.0627,  0.0572,\n",
       "                      -0.1372, -0.0215, -0.0836, -0.0289,  0.0394,  0.0796, -0.0507,  0.0735,\n",
       "                       0.0855, -0.0105, -0.0673,  0.0913, -0.0932,  0.0194, -0.0625, -0.1365,\n",
       "                      -0.1099, -0.0212, -0.0599,  0.0711,  0.0656,  0.0654, -0.1267,  0.1081,\n",
       "                       0.0228,  0.0304,  0.0985,  0.0297,  0.0596, -0.0077, -0.0361, -0.0190,\n",
       "                      -0.0015, -0.1016, -0.0266,  0.0389, -0.0639,  0.0271,  0.0375, -0.0045,\n",
       "                      -0.0030,  0.0261, -0.0310, -0.0497, -0.0720, -0.0179,  0.0020,  0.0323,\n",
       "                      -0.0604,  0.0362,  0.0623, -0.0837,  0.0471,  0.0542,  0.0012,  0.0270,\n",
       "                       0.0665,  0.0045,  0.0629,  0.0878, -0.0173, -0.0244, -0.1258, -0.0776,\n",
       "                      -0.0047, -0.0212, -0.1690,  0.0606,  0.0106, -0.0028,  0.0870, -0.0976,\n",
       "                      -0.0485, -0.0356, -0.1064, -0.1096, -0.0913, -0.0212, -0.0059,  0.0184,\n",
       "                      -0.0624, -0.0104, -0.0236, -0.0540,  0.0015, -0.0753, -0.1873, -0.1056,\n",
       "                      -0.0969, -0.0256,  0.0259,  0.0588, -0.0012, -0.0549,  0.0040,  0.0303,\n",
       "                       0.0756,  0.1035, -0.0305,  0.0576, -0.0071,  0.0812,  0.0687,  0.0447,\n",
       "                       0.0905, -0.0891, -0.0284,  0.0442, -0.0286,  0.0363, -0.0012, -0.1042,\n",
       "                       0.0706, -0.0081, -0.0525,  0.0261,  0.0146, -0.0390,  0.0237,  0.0598,\n",
       "                      -0.0579, -0.0665, -0.0086,  0.0538,  0.0561,  0.0788, -0.1336, -0.0500,\n",
       "                      -0.0672,  0.0134, -0.0787, -0.0301, -0.0310,  0.1236, -0.0196, -0.0633,\n",
       "                      -0.0800,  0.0646,  0.1212, -0.1644, -0.1366,  0.0946, -0.1981,  0.0806,\n",
       "                       0.1344, -0.1142, -0.0285, -0.1328,  0.0327, -0.0368,  0.0343, -0.0519,\n",
       "                      -0.0810, -0.1042, -0.0438, -0.1709,  0.0706,  0.0201, -0.0285, -0.0966,\n",
       "                       0.0375,  0.0312, -0.1127, -0.0505, -0.0448, -0.0152,  0.0077, -0.0870,\n",
       "                       0.0408,  0.0702, -0.0879,  0.1951,  0.1123,  0.0238, -0.0840,  0.0355,\n",
       "                       0.0248, -0.0603,  0.0060, -0.0860, -0.0315,  0.0262,  0.0358, -0.0502,\n",
       "                      -0.1349,  0.0044, -0.0911, -0.0222, -0.1817,  0.1001, -0.0717, -0.0739,\n",
       "                       0.0593, -0.0123, -0.0199, -0.1646,  0.1691,  0.0659, -0.0329, -0.1219,\n",
       "                      -0.0802, -0.0622, -0.0819, -0.0936, -0.0295, -0.0808,  0.0244, -0.1135,\n",
       "                       0.0344, -0.1593, -0.0617,  0.0004,  0.0137, -0.0857,  0.0433, -0.0245,\n",
       "                      -0.0257, -0.0135, -0.1147, -0.0510, -0.1103, -0.0422, -0.0652,  0.0213,\n",
       "                      -0.0589, -0.0743, -0.1471, -0.0048, -0.1598, -0.0835,  0.0005,  0.0338,\n",
       "                      -0.0309,  0.0631,  0.0615,  0.1396, -0.0374, -0.0235, -0.0734, -0.0508,\n",
       "                      -0.0085,  0.1118, -0.0466, -0.2136, -0.0230,  0.0525, -0.0915,  0.0108])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-1.4139e-01,  7.6267e-03,  1.1007e-01, -1.8792e-02, -1.1138e-02,\n",
       "                       4.8605e-03,  1.2845e-01, -5.7284e-02, -1.0510e-01, -9.7189e-02,\n",
       "                      -6.2401e-02,  5.3867e-02,  7.6426e-02, -2.7105e-02, -4.9198e-03,\n",
       "                      -1.2328e-01,  1.7879e-03, -1.3438e-01,  2.7943e-02,  6.2161e-03,\n",
       "                      -5.4195e-02,  1.8116e-01, -1.7148e-01,  8.6569e-02, -8.4470e-02,\n",
       "                       4.2290e-04, -2.1980e-01,  5.4772e-02, -3.8687e-02,  2.3083e-02,\n",
       "                      -3.5321e-02, -1.1544e-03, -1.0323e-01, -8.1494e-02,  2.7964e-02,\n",
       "                      -5.1720e-02,  3.1513e-03, -1.1372e-01, -1.5645e-01,  3.0153e-02,\n",
       "                      -7.4645e-02,  8.2472e-02, -1.8877e-01, -1.6070e-01,  2.1169e-02,\n",
       "                      -1.1161e-01,  6.4251e-02,  9.2038e-03,  1.3090e-01, -7.0862e-02,\n",
       "                      -3.0541e-02,  6.1938e-02,  1.6198e-02,  9.0717e-02, -2.1889e-01,\n",
       "                      -4.6511e-02, -1.0075e-01, -7.8269e-02, -1.4048e-01, -7.7290e-02,\n",
       "                      -5.6179e-02, -6.7857e-02, -1.1747e-01, -5.0448e-03, -9.0126e-02,\n",
       "                      -2.7234e-02, -1.9387e-01, -5.5861e-02, -1.9018e-01, -2.4893e-02,\n",
       "                       8.6736e-02,  7.6182e-04,  9.6050e-02,  2.2023e-02, -1.2641e-01,\n",
       "                      -1.1086e-01,  1.1392e-02,  9.4723e-04, -1.0888e-01,  8.4768e-02,\n",
       "                      -1.0312e-01, -8.5622e-02,  3.0357e-02, -9.0963e-02,  5.7722e-02,\n",
       "                       5.2560e-02, -1.0196e-01, -5.7187e-02, -3.4218e-02, -1.4279e-01,\n",
       "                      -1.2099e-01, -2.1387e-02, -1.1434e-01,  6.9342e-02, -5.5655e-02,\n",
       "                      -4.4667e-02,  1.3320e-01, -3.8140e-02, -5.7571e-02, -1.2924e-01,\n",
       "                      -6.6919e-02,  1.0858e-01, -1.3627e-01, -2.7344e-01,  2.1434e-02,\n",
       "                      -2.9664e-03, -1.0203e-01, -1.4867e-02, -1.9765e-01,  2.6105e-02,\n",
       "                       8.8436e-02, -5.0175e-02,  2.2377e-02, -5.8001e-03,  3.3642e-02,\n",
       "                       1.1785e-02,  6.9966e-02, -1.3002e-01, -2.1941e-01, -1.5740e-01,\n",
       "                      -1.0083e-01, -6.3428e-02,  5.6231e-02, -1.6373e-01,  5.2861e-02,\n",
       "                      -1.3960e-01, -1.5747e-02, -8.5705e-03, -1.3560e-02, -4.0702e-02,\n",
       "                      -3.9526e-03,  5.1190e-02, -1.0848e-02,  6.5885e-02, -7.4233e-02,\n",
       "                       2.1623e-02, -9.3882e-02, -6.3768e-02, -4.4685e-02, -2.5195e-02,\n",
       "                      -1.1885e-01, -7.4527e-02, -8.9677e-02, -2.3137e-02, -1.2144e-01,\n",
       "                       8.2799e-02, -1.0669e-01, -1.1192e-01, -9.4533e-02, -1.5338e-01,\n",
       "                      -1.3889e-01,  3.2504e-02, -8.0062e-02, -1.1065e-01, -7.2208e-02,\n",
       "                      -5.7718e-02, -1.7771e-01, -1.2787e-01, -5.9193e-02, -5.4624e-02,\n",
       "                      -5.0821e-03, -5.7172e-02, -4.6737e-02, -6.7265e-02, -2.9024e-02,\n",
       "                      -1.5380e-01,  5.2182e-02, -1.8416e-01, -2.3538e-02, -8.4416e-02,\n",
       "                      -7.1311e-02,  9.2331e-02, -1.5374e-01, -3.7662e-02, -7.6606e-02,\n",
       "                      -2.4693e-01, -1.0198e-01, -6.7723e-02, -1.0127e-01, -3.5404e-03,\n",
       "                      -8.1080e-02,  7.3520e-02, -4.6910e-02,  3.7973e-02, -1.1352e-01,\n",
       "                      -8.2276e-02, -1.0970e-01, -1.2357e-01, -8.9177e-02, -8.3040e-03,\n",
       "                      -1.3658e-01,  7.7294e-03, -1.3651e-01,  4.4183e-03, -8.3681e-02,\n",
       "                      -1.2817e-01, -1.4837e-01,  4.2338e-02, -1.0249e-01,  2.6524e-02,\n",
       "                      -1.2099e-01,  4.2902e-02,  9.6908e-02,  7.3892e-02, -8.7198e-02,\n",
       "                      -3.7215e-02, -1.4415e-01,  6.2555e-02, -6.9135e-02, -8.0613e-02,\n",
       "                       1.7811e-02, -2.2128e-01, -8.2011e-02, -3.1880e-02, -5.1235e-02,\n",
       "                      -1.7999e-01,  8.4158e-03, -1.4821e-02, -8.9535e-02,  1.8083e-02,\n",
       "                      -9.3727e-02, -1.1161e-01, -5.0986e-02, -8.2196e-02, -6.3696e-02,\n",
       "                      -8.0080e-02,  6.8356e-02, -4.3350e-02,  4.3664e-03, -7.1110e-02,\n",
       "                       8.1444e-02, -8.5729e-02, -9.1522e-02,  2.8992e-03, -1.3679e-01,\n",
       "                      -3.6239e-02, -4.1433e-02, -3.8196e-02,  1.1238e-01,  5.1118e-02,\n",
       "                       2.5068e-02, -5.2724e-03, -4.3421e-02,  6.2086e-03,  2.4001e-04,\n",
       "                      -2.6271e-01, -4.4888e-02,  6.2262e-02, -1.9922e-02,  9.0007e-02,\n",
       "                      -5.8847e-02, -5.9179e-02,  7.9767e-02, -5.9483e-02, -2.1329e-01,\n",
       "                      -9.2401e-02,  3.3906e-02,  3.5425e-02,  3.2695e-02, -7.8353e-02,\n",
       "                      -1.4262e-02,  2.3442e-02, -8.3603e-02, -3.8849e-02, -4.1781e-03,\n",
       "                      -2.0850e-02,  8.4707e-02,  3.9552e-02,  7.0407e-02, -1.6689e-01,\n",
       "                      -2.2146e-02,  3.7161e-02,  6.1949e-02, -1.1576e-02,  3.8887e-02,\n",
       "                      -1.7766e-01, -7.0352e-03,  1.1500e-02,  1.0162e-01,  4.1923e-02,\n",
       "                      -7.3472e-02, -2.2521e-02,  1.4298e-02, -9.4709e-03, -7.3899e-02,\n",
       "                       4.8091e-02,  3.2057e-02, -3.2121e-03,  7.1559e-02, -5.8051e-02,\n",
       "                      -7.7666e-02, -5.1263e-02, -2.5447e-02, -1.5417e-03, -1.1495e-01,\n",
       "                       5.8262e-02,  9.3697e-02, -2.5398e-02, -3.1807e-02, -8.0029e-02,\n",
       "                       1.1736e-02, -3.2834e-02, -9.6655e-02, -5.2294e-02, -2.8715e-02,\n",
       "                      -2.0226e-03,  1.0323e-02,  7.2494e-02,  1.3211e-01, -3.9444e-02,\n",
       "                      -4.1806e-02,  7.9442e-03,  5.2916e-03,  6.9056e-02, -1.0293e-01,\n",
       "                      -1.3377e-01, -2.5674e-02,  3.6889e-02,  9.8216e-02, -2.2911e-02,\n",
       "                       4.7904e-03,  1.0172e-01, -4.5568e-02,  2.5236e-03,  8.8332e-02,\n",
       "                       3.0242e-02,  7.3394e-02,  4.4709e-02,  8.6351e-02,  2.0493e-02,\n",
       "                       3.1317e-02, -2.5112e-02, -8.1164e-02,  2.0541e-02, -8.9047e-02,\n",
       "                       2.3858e-02, -1.5807e-02,  5.7803e-02,  1.2616e-01,  4.4127e-02,\n",
       "                      -7.1173e-02,  8.5935e-02,  7.7896e-03,  1.1921e-01,  9.9528e-03,\n",
       "                       2.7931e-02, -4.9001e-02, -8.9599e-02,  4.4834e-02,  1.3271e-02,\n",
       "                       3.1879e-02,  9.4006e-03, -5.4752e-02, -2.1744e-02,  5.2509e-02,\n",
       "                       6.5166e-02,  1.2120e-01,  8.6214e-02,  1.3655e-02, -3.3783e-02,\n",
       "                       4.8293e-02,  2.9606e-02,  7.1777e-02,  4.7822e-02, -6.4622e-02,\n",
       "                       3.1661e-03,  2.2494e-02,  1.3543e-01,  9.0244e-03, -3.4058e-02,\n",
       "                       1.3494e-02,  4.8644e-02, -2.0209e-02,  5.4597e-04, -6.8909e-02,\n",
       "                       2.3695e-02,  8.4997e-02,  3.0957e-02,  1.2339e-01, -2.9587e-02,\n",
       "                       4.5277e-02, -3.0563e-02, -4.3286e-02, -1.2375e-02, -1.1578e-01,\n",
       "                      -1.5172e-01,  1.4760e-02,  1.1228e-03, -1.3857e-02,  5.3330e-02,\n",
       "                      -1.0123e-01,  4.5003e-03, -9.1675e-02,  6.7080e-02, -9.1629e-02,\n",
       "                      -1.2341e-01,  2.1393e-02,  8.2674e-02,  4.7516e-02, -5.5275e-02,\n",
       "                       3.2859e-02,  2.5418e-02,  5.7415e-02, -8.0697e-02, -9.5662e-02,\n",
       "                      -3.4240e-02, -9.2156e-02,  5.7693e-03,  6.7836e-02, -2.7133e-02,\n",
       "                      -1.3202e-01, -9.8885e-02,  4.2084e-02, -6.9805e-03,  1.3752e-02,\n",
       "                       2.4339e-02, -1.5333e-01, -2.8811e-02, -5.1788e-02, -1.1962e-01,\n",
       "                       1.3764e-01, -1.0420e-01, -1.6855e-02, -1.2020e-01, -9.0884e-02,\n",
       "                       8.7425e-02, -1.3351e-01,  1.7767e-02, -6.8292e-02, -1.0869e-01,\n",
       "                      -5.8747e-02, -3.6481e-02, -7.9517e-02, -8.4112e-02, -1.0866e-01,\n",
       "                       3.8477e-02, -3.4511e-03,  1.2468e-01, -1.1852e-01,  4.4812e-02,\n",
       "                       4.4527e-02, -1.3629e-02, -4.9002e-02, -1.5749e-01, -1.6298e-02,\n",
       "                      -1.4317e-01, -1.3659e-02, -1.0342e-01, -1.4482e-01, -2.7324e-02,\n",
       "                      -1.3480e-01, -1.3208e-01, -2.9579e-01, -3.5567e-02,  4.1931e-02,\n",
       "                      -2.8692e-02,  7.0474e-02, -2.9101e-02,  1.1319e-02, -6.4830e-02,\n",
       "                       1.0101e-01, -5.7971e-02, -1.9313e-01,  3.8333e-02, -9.0151e-02,\n",
       "                      -2.2344e-01, -9.2504e-02, -1.7302e-01,  2.9817e-02, -1.3983e-01,\n",
       "                      -4.2067e-02, -1.4758e-01,  7.1933e-02, -4.6789e-02,  1.4856e-02,\n",
       "                       2.3851e-01, -1.3595e-01, -8.0557e-03, -7.2939e-03, -5.0923e-02,\n",
       "                       3.1001e-02, -8.7006e-03, -3.2499e-03, -2.8211e-02,  9.7501e-02,\n",
       "                       4.1706e-02, -2.7265e-02,  5.2420e-02,  5.5998e-02,  1.7519e-02,\n",
       "                      -1.0020e-01,  1.1839e-01,  1.6663e-02, -1.0762e-01,  1.4032e-01,\n",
       "                      -2.9193e-02, -1.0208e-01,  3.8423e-02,  5.2246e-02, -9.1770e-03,\n",
       "                       6.5039e-02, -1.0787e-02, -1.7731e-02, -2.3762e-02, -4.6053e-02,\n",
       "                       5.7514e-02,  2.6248e-02, -4.9991e-02, -1.7872e-02, -3.0025e-04,\n",
       "                       8.7570e-03, -8.1934e-02])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 5.6040e-02, -3.3442e-02, -3.1841e-01,  ..., -1.1254e-01,\n",
       "                        2.4188e-01, -3.7440e-01],\n",
       "                      [ 1.9127e-01, -2.2565e-01,  2.4166e-02,  ...,  2.0856e-01,\n",
       "                        1.2512e-01, -4.4327e-02],\n",
       "                      [ 2.6275e-02,  1.3997e-01,  2.5264e-01,  ..., -2.3124e-01,\n",
       "                       -9.2324e-03, -2.1254e-01],\n",
       "                      ...,\n",
       "                      [-3.8911e-02,  1.0959e-01, -3.4150e-01,  ...,  1.1907e-02,\n",
       "                       -4.6158e-03, -3.9454e-01],\n",
       "                      [-9.1662e-02, -2.6160e-01, -1.5665e-01,  ...,  1.5010e-01,\n",
       "                        1.4408e-01, -2.3807e-02],\n",
       "                      [-2.8545e-01, -1.5819e-01, -1.5615e-04,  ..., -7.4147e-02,\n",
       "                        1.2005e-01, -1.0321e-01]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.2607, -0.2502,  0.1049,  ..., -0.0292, -0.1901, -0.2232],\n",
       "                      [-0.1074,  0.1125,  0.0095,  ...,  0.0219,  0.2884, -0.4294],\n",
       "                      [-0.1492, -0.1506,  0.1571,  ..., -0.1285,  0.0019,  0.1336],\n",
       "                      ...,\n",
       "                      [ 0.4591, -0.1920, -0.0194,  ..., -0.0421,  0.0528,  0.1155],\n",
       "                      [-0.3301, -0.1702, -0.1452,  ...,  0.2426,  0.2349, -0.0663],\n",
       "                      [-0.0525, -0.1029,  0.0766,  ...,  0.1096,  0.0995, -0.0195]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0340,  0.1165,  0.1839,  0.1316,  0.0922,  0.0201,  0.0303, -0.0263,\n",
       "                      -0.1060,  0.0060,  0.1620, -0.0659,  0.1176,  0.1387,  0.0997,  0.1020,\n",
       "                       0.0663,  0.2404, -0.0301,  0.1196, -0.0549, -0.0256,  0.0537,  0.0865,\n",
       "                       0.1613, -0.0600,  0.0307, -0.0596,  0.2238,  0.1230,  0.1692,  0.0632,\n",
       "                      -0.1714,  0.1168,  0.2197, -0.0338,  0.1754,  0.2981,  0.0217,  0.1244,\n",
       "                      -0.0233, -0.0266,  0.0080,  0.0133, -0.0025,  0.0347,  0.0693,  0.1164,\n",
       "                       0.1027, -0.0161,  0.1532,  0.0294,  0.0904,  0.1251, -0.0571, -0.0099,\n",
       "                       0.0227, -0.1046,  0.0241, -0.0007, -0.0069,  0.0422,  0.0969, -0.0512,\n",
       "                       0.2368,  0.0148, -0.0605,  0.0573,  0.1109,  0.1040,  0.0484,  0.1653,\n",
       "                      -0.1763,  0.1950, -0.1337,  0.0294, -0.1334,  0.0883,  0.2024,  0.1850,\n",
       "                       0.0022,  0.0009,  0.0907, -0.0072,  0.3267,  0.0042,  0.0707,  0.1722,\n",
       "                       0.2243,  0.3016,  0.0357,  0.1486,  0.1622,  0.0651,  0.2927,  0.0451,\n",
       "                       0.0308,  0.0431,  0.1019,  0.1522, -0.0055,  0.0122,  0.0316,  0.1032,\n",
       "                       0.1293,  0.0137,  0.0857,  0.0305, -0.0306,  0.1218, -0.0683, -0.0291,\n",
       "                       0.0911, -0.0603, -0.0638, -0.1597, -0.0500,  0.0853,  0.1373,  0.0679,\n",
       "                       0.0681,  0.0401,  0.2052, -0.0206,  0.1207,  0.1182, -0.0697,  0.1037,\n",
       "                       0.0233, -0.0356,  0.0195, -0.1242,  0.0228, -0.0128, -0.0024, -0.1274,\n",
       "                      -0.0733, -0.0616, -0.1377,  0.1113, -0.0987, -0.1741, -0.1345, -0.0733,\n",
       "                       0.0242, -0.1099, -0.0390, -0.1154, -0.0589, -0.0448, -0.1337,  0.0701,\n",
       "                      -0.0046, -0.1354,  0.0956, -0.0502, -0.1104, -0.0222, -0.0261, -0.1628,\n",
       "                       0.0588, -0.0057, -0.0616, -0.0672,  0.0660, -0.0581,  0.0748, -0.2496,\n",
       "                      -0.0071, -0.1621,  0.0413,  0.1241, -0.0952, -0.1975, -0.1571, -0.0957,\n",
       "                      -0.0124,  0.0390, -0.0762, -0.1485, -0.0387, -0.1340, -0.0896,  0.0025,\n",
       "                      -0.0010,  0.0455,  0.1020, -0.1757, -0.0141, -0.0192,  0.0129, -0.1089,\n",
       "                      -0.0761,  0.0869,  0.0457,  0.0181,  0.0318, -0.0963, -0.1247,  0.0490,\n",
       "                       0.0628, -0.0997, -0.0009, -0.0275,  0.0025, -0.0172, -0.0245, -0.0231,\n",
       "                      -0.0157, -0.0693, -0.1302, -0.0232, -0.0325, -0.1176, -0.0112, -0.0841,\n",
       "                      -0.0244,  0.0492, -0.1352, -0.0347, -0.0180, -0.2018, -0.0758, -0.0876,\n",
       "                      -0.0722,  0.0626, -0.1087, -0.1016, -0.0484, -0.0182, -0.0152, -0.1278,\n",
       "                      -0.1858, -0.0439,  0.0028,  0.0794, -0.0567, -0.0914, -0.1613, -0.1465,\n",
       "                       0.0701, -0.1797, -0.0503, -0.0868, -0.0969, -0.0694, -0.1045, -0.0510,\n",
       "                       0.1142, -0.0133,  0.0076, -0.1060,  0.0215,  0.0141,  0.0692, -0.0536,\n",
       "                      -0.0860,  0.0404, -0.0012,  0.1318, -0.0156, -0.0134, -0.0689,  0.0207,\n",
       "                       0.0325, -0.0362, -0.0516,  0.0597, -0.0216, -0.0325,  0.0617, -0.1247,\n",
       "                       0.0821,  0.0069, -0.0624, -0.1527, -0.0239,  0.0404,  0.0779,  0.1739,\n",
       "                      -0.0479,  0.0090,  0.0456,  0.0446, -0.1271, -0.1293, -0.1036, -0.0986,\n",
       "                      -0.0706,  0.0803, -0.0132,  0.0917, -0.0835,  0.1550, -0.1572,  0.0133,\n",
       "                       0.1145,  0.0429,  0.1076, -0.0288,  0.0036,  0.0318, -0.0271,  0.0043,\n",
       "                       0.0024, -0.0776,  0.0254, -0.1671,  0.0190,  0.0085,  0.0144, -0.0845,\n",
       "                       0.0204,  0.0645, -0.0768, -0.1038, -0.0380,  0.0403,  0.0165,  0.0164,\n",
       "                      -0.0207,  0.0469,  0.0463,  0.1590, -0.0224,  0.0202, -0.0052, -0.0055,\n",
       "                       0.0018, -0.1588, -0.1593, -0.0303, -0.0045, -0.0543, -0.0091, -0.1300,\n",
       "                       0.0047, -0.0009, -0.0907, -0.0420,  0.0273, -0.1163,  0.0449, -0.0774,\n",
       "                       0.0023,  0.0033, -0.0902,  0.0097, -0.0517, -0.0549,  0.0485, -0.0075,\n",
       "                      -0.0048, -0.1027,  0.0443, -0.1070, -0.0818,  0.0945, -0.0479, -0.0018,\n",
       "                       0.0478,  0.1013, -0.0325, -0.0127,  0.0527,  0.0064, -0.1110, -0.0993,\n",
       "                      -0.0103, -0.1054, -0.0497, -0.1206,  0.0104,  0.0385,  0.0024, -0.0463,\n",
       "                      -0.0104, -0.0045, -0.0599, -0.1369, -0.0488, -0.1086, -0.0290, -0.0830,\n",
       "                       0.0366,  0.1533,  0.0940,  0.1416,  0.0764,  0.1262, -0.1074,  0.0131,\n",
       "                       0.1309, -0.0092,  0.1042,  0.0583,  0.0917, -0.0126,  0.1388,  0.1271,\n",
       "                      -0.0265,  0.1224, -0.0573, -0.0153, -0.0086, -0.0778,  0.2430,  0.0410,\n",
       "                       0.0016,  0.0722,  0.1543,  0.0038,  0.2130,  0.1305,  0.1358,  0.0800,\n",
       "                       0.0927,  0.1256,  0.1895, -0.0016,  0.0996,  0.1086, -0.0863,  0.0652,\n",
       "                       0.0327, -0.0767,  0.1731,  0.1148, -0.0899,  0.0553,  0.0735,  0.1174,\n",
       "                       0.2251,  0.0268,  0.0700,  0.0487,  0.0319,  0.0962,  0.0594,  0.0934,\n",
       "                      -0.0485,  0.0518, -0.1018, -0.0519, -0.0937,  0.2013,  0.1189,  0.0406,\n",
       "                       0.1594,  0.0523, -0.0534,  0.1076,  0.1887,  0.1549,  0.0957,  0.2710,\n",
       "                      -0.0449,  0.1555, -0.1027,  0.0339,  0.0347,  0.1199,  0.1129,  0.1380,\n",
       "                      -0.0946, -0.0511,  0.0821, -0.0115,  0.0982,  0.0824,  0.0172,  0.2607,\n",
       "                       0.0670,  0.1250, -0.0163,  0.1540,  0.0862, -0.0978,  0.1100,  0.1265,\n",
       "                      -0.1484,  0.0740,  0.2111,  0.1237, -0.0979, -0.0283,  0.1826,  0.1395,\n",
       "                       0.2503, -0.0422,  0.2501, -0.0351,  0.0235,  0.0835,  0.0859,  0.0700,\n",
       "                       0.2086,  0.1056, -0.0711,  0.0270,  0.0895,  0.1886,  0.1938,  0.0954,\n",
       "                       0.2675,  0.0608,  0.0680,  0.0147,  0.0942,  0.0273, -0.1167,  0.1291])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 7.3697e-02,  5.5147e-02,  1.2359e-01,  1.4924e-01,  1.3545e-01,\n",
       "                       1.4119e-01, -6.5062e-02, -7.9166e-02,  1.0158e-01,  4.7653e-02,\n",
       "                       1.0234e-01, -1.5767e-02,  1.4217e-01,  5.5265e-02,  8.9604e-03,\n",
       "                       4.8604e-02, -1.0741e-01,  2.0686e-01,  9.0155e-02, -1.3540e-02,\n",
       "                      -5.1334e-02, -6.3102e-02,  1.3439e-01,  4.6890e-02,  1.3157e-01,\n",
       "                       1.4946e-01,  1.4449e-01, -3.7772e-02,  3.3805e-01,  2.3224e-01,\n",
       "                       1.7611e-01,  1.2783e-01, -1.0279e-01,  9.4974e-02,  2.2028e-01,\n",
       "                      -1.3668e-01,  5.1200e-02,  2.5084e-01, -4.4986e-02,  1.8214e-01,\n",
       "                       1.1401e-01, -7.6383e-02,  1.1836e-01,  7.1794e-02,  7.2518e-02,\n",
       "                       2.3370e-02, -2.5240e-04,  1.5711e-01,  1.2277e-01,  5.3322e-03,\n",
       "                       5.0085e-03,  2.7830e-03,  2.6964e-02,  1.5883e-01,  1.4271e-03,\n",
       "                       1.7752e-02, -9.9603e-03, -2.6701e-02,  1.9120e-03,  5.2044e-02,\n",
       "                       1.3312e-01,  1.7083e-01,  5.7472e-02, -7.4132e-02,  4.2791e-02,\n",
       "                       5.3112e-03,  8.9911e-03, -1.4967e-02,  1.7618e-01,  5.4673e-02,\n",
       "                       1.1631e-01,  4.6435e-02, -3.2006e-02,  1.8647e-01, -1.6822e-01,\n",
       "                       2.9634e-02,  9.8321e-02,  3.3717e-02,  5.9268e-02,  9.3349e-02,\n",
       "                       3.6171e-03,  1.6039e-01,  1.1946e-01, -1.6722e-01,  3.4038e-01,\n",
       "                       6.5160e-02,  6.7005e-02,  1.2928e-01,  1.4158e-01,  1.2149e-01,\n",
       "                       1.5514e-02,  6.5805e-02,  1.1704e-01,  5.5633e-02,  2.1039e-01,\n",
       "                       9.6254e-02, -7.1947e-02,  6.6219e-02,  2.4285e-01,  8.0361e-03,\n",
       "                       6.4876e-02,  4.8145e-02, -1.4313e-03,  2.5180e-01,  1.6422e-01,\n",
       "                      -4.3286e-02,  1.4943e-01,  3.6555e-02, -7.3343e-02,  7.2243e-02,\n",
       "                      -6.0434e-02,  4.1314e-02,  5.4937e-02,  8.5273e-02, -1.0358e-01,\n",
       "                       5.9922e-02,  4.9453e-03,  2.0434e-01,  8.6851e-03,  1.1130e-01,\n",
       "                       3.5199e-02,  1.7254e-01,  1.6106e-01, -1.5324e-02,  1.7523e-01,\n",
       "                       1.0122e-01,  1.6514e-02,  1.6121e-01,  1.4294e-01, -1.0506e-01,\n",
       "                      -3.9922e-02, -1.4113e-01,  4.0316e-02,  1.0041e-01, -6.3443e-02,\n",
       "                      -8.8688e-02, -3.8318e-02, -8.4112e-02, -1.2116e-01,  1.3109e-01,\n",
       "                      -1.1565e-01,  1.9003e-03, -1.5942e-03, -1.7084e-01, -2.1306e-02,\n",
       "                      -1.4448e-01,  1.3898e-02, -7.5452e-02,  2.6977e-02,  4.5649e-02,\n",
       "                       3.3556e-02, -1.1153e-02, -5.6194e-02, -5.9887e-02, -8.1445e-02,\n",
       "                      -4.0897e-02, -2.9533e-01, -1.4500e-01,  9.5782e-02, -2.1979e-01,\n",
       "                      -1.6724e-01, -5.2649e-02,  4.7271e-02, -1.2919e-01, -1.8071e-02,\n",
       "                       1.4550e-01,  1.4060e-01, -1.8132e-01, -4.8066e-02, -1.3348e-02,\n",
       "                       3.4801e-02, -1.5320e-02, -8.1001e-02, -2.2193e-01, -5.6955e-02,\n",
       "                      -1.3092e-01, -1.2465e-01,  2.9616e-02, -1.1093e-01, -1.3513e-01,\n",
       "                      -8.2052e-02, -8.1858e-02, -5.5973e-02, -2.9255e-02, -1.3093e-01,\n",
       "                      -1.0780e-01, -1.0848e-02, -6.9952e-02,  1.6568e-02, -7.7305e-02,\n",
       "                      -1.2562e-01, -1.5763e-01, -2.3495e-01, -5.6487e-03,  1.0309e-01,\n",
       "                      -6.0331e-02, -2.9349e-02, -6.3909e-02, -9.9746e-02, -6.6615e-03,\n",
       "                       3.5447e-02, -4.6503e-02, -1.2673e-02, -1.5208e-01,  1.4480e-04,\n",
       "                      -4.5940e-03,  2.7918e-02, -8.4516e-03,  3.8076e-02, -6.7106e-02,\n",
       "                      -3.2763e-02, -5.6108e-02, -5.6467e-02, -1.8238e-01, -1.5595e-01,\n",
       "                      -1.3517e-01, -2.5722e-01, -1.0963e-01,  1.6017e-01, -1.0110e-01,\n",
       "                       2.7666e-02, -5.3683e-02, -7.3154e-02, -3.2748e-02, -3.1205e-02,\n",
       "                       1.1203e-01,  4.0881e-02,  8.6354e-02, -8.8365e-02,  1.2354e-01,\n",
       "                      -2.5440e-02,  5.9868e-02, -8.3648e-02,  1.2119e-02, -3.3341e-02,\n",
       "                      -3.0612e-02,  8.7627e-02,  8.7286e-02, -1.1941e-01, -2.6541e-02,\n",
       "                      -7.9389e-03,  4.2874e-02, -7.3029e-03, -2.2094e-01, -8.1246e-02,\n",
       "                      -1.1040e-01, -1.0446e-01, -1.6575e-02,  2.6777e-02, -1.4428e-01,\n",
       "                      -8.2393e-02, -4.7653e-02, -1.1799e-01,  1.0008e-01,  1.6583e-02,\n",
       "                      -1.8580e-02, -6.2816e-02,  2.4644e-03, -3.5619e-02,  1.0019e-01,\n",
       "                      -7.0749e-02, -3.0315e-02,  3.6030e-02, -6.2773e-02,  1.5775e-01,\n",
       "                       6.4134e-02, -9.6482e-03,  7.0871e-02,  3.5087e-02, -1.2647e-01,\n",
       "                       5.4559e-02,  2.5910e-02, -2.1867e-02,  1.6510e-01, -1.6739e-02,\n",
       "                       4.6992e-03,  9.4349e-03,  4.8943e-02,  4.9463e-02,  5.5733e-02,\n",
       "                       3.1621e-02, -1.8056e-02, -1.2411e-01,  7.1529e-03, -6.9098e-02,\n",
       "                       9.5714e-03, -5.6118e-02, -4.2259e-02,  1.6819e-02, -7.3601e-02,\n",
       "                       2.7268e-02,  1.0859e-02,  7.6437e-02,  9.8791e-02, -3.5230e-03,\n",
       "                      -5.4773e-02, -2.2482e-02,  6.5565e-02,  9.3402e-02, -3.0786e-03,\n",
       "                       2.5745e-03,  9.2645e-02, -7.6152e-03, -1.2931e-02, -1.1313e-01,\n",
       "                       1.6639e-02,  1.0547e-01,  7.9889e-03, -7.3979e-02,  4.4388e-02,\n",
       "                       3.2799e-02,  1.9599e-02,  4.5915e-02, -4.1947e-02, -8.1572e-02,\n",
       "                      -3.2764e-02, -4.8793e-02,  4.0935e-02, -1.3832e-01,  1.0357e-01,\n",
       "                       1.0018e-01, -1.9027e-02,  2.6542e-02,  5.8819e-02, -8.4529e-03,\n",
       "                      -2.5615e-03,  1.2355e-01, -8.7935e-02, -3.3884e-03,  3.4063e-02,\n",
       "                       3.7213e-02, -1.5770e-02,  1.8989e-02, -4.9853e-02, -7.4580e-02,\n",
       "                      -4.3300e-02, -6.5781e-02, -4.9997e-02, -1.5275e-01, -2.8452e-02,\n",
       "                      -8.3051e-03,  5.4155e-02,  1.7092e-02, -1.8772e-01,  5.8318e-02,\n",
       "                       5.7487e-02,  6.4404e-02,  1.9992e-01,  6.1162e-02, -4.2472e-02,\n",
       "                      -3.5837e-02, -3.7665e-02, -1.1763e-01,  4.3905e-03, -5.0660e-03,\n",
       "                      -8.6901e-02,  1.3553e-01,  6.4631e-02, -1.6611e-02, -9.3750e-02,\n",
       "                      -1.8243e-02,  5.7896e-02, -6.7549e-02,  1.6167e-01,  8.9709e-02,\n",
       "                       1.0620e-01,  4.7385e-02,  2.5474e-02, -6.3693e-02, -6.4508e-02,\n",
       "                       2.6700e-02, -5.0187e-02, -6.6737e-02,  6.5737e-02, -1.4605e-01,\n",
       "                       4.1436e-02, -8.0461e-03,  3.0382e-03, -1.3857e-01, -5.1077e-02,\n",
       "                      -8.5661e-02, -8.9354e-02, -1.0799e-02, -4.6071e-02, -3.3040e-02,\n",
       "                       5.7141e-02,  9.0621e-02,  1.4332e-01,  7.1494e-02,  1.6064e-01,\n",
       "                       5.6086e-02, -1.8806e-02, -1.4599e-01, -2.6441e-03,  1.4867e-01,\n",
       "                       5.0286e-02,  1.2691e-01, -5.7650e-02, -4.5157e-03,  2.5478e-02,\n",
       "                       3.4596e-02,  1.2589e-01, -1.2641e-01,  8.8610e-03,  5.6262e-02,\n",
       "                      -4.8068e-03,  2.5002e-01,  9.9999e-02,  1.3971e-01,  2.9064e-02,\n",
       "                       1.8518e-01,  3.7313e-03,  3.0060e-01,  1.9096e-01,  1.2001e-01,\n",
       "                      -4.4351e-02,  7.8114e-02,  1.1114e-01,  2.0278e-01, -1.1303e-01,\n",
       "                       5.8070e-02,  1.6258e-01, -6.2560e-02,  1.9601e-01,  1.0805e-01,\n",
       "                      -5.5283e-02,  5.2212e-02,  1.9880e-02, -2.9063e-02,  4.7000e-02,\n",
       "                      -1.9924e-02,  4.1204e-02,  1.5312e-01,  3.9737e-02,  1.6048e-01,\n",
       "                      -1.5865e-02,  4.4614e-02,  4.5476e-02,  3.2760e-02,  1.2517e-01,\n",
       "                       2.1506e-02, -9.6741e-02, -1.9294e-02,  3.3221e-02, -4.7718e-02,\n",
       "                       2.0998e-01,  2.7484e-01,  9.9201e-03,  1.5185e-01, -8.8978e-02,\n",
       "                      -5.9384e-02,  7.6700e-02,  1.7339e-01,  2.2997e-01,  8.2856e-02,\n",
       "                       1.2960e-01,  3.6756e-02,  1.0218e-01,  5.3710e-03,  8.4715e-02,\n",
       "                       1.9260e-01,  4.6688e-02,  8.3657e-02,  1.2121e-01, -9.9361e-02,\n",
       "                      -3.9148e-02,  4.8919e-02, -2.1680e-03,  4.5809e-02, -3.9620e-02,\n",
       "                       1.1562e-01,  1.5818e-01,  8.7957e-02,  1.8170e-01,  7.2182e-02,\n",
       "                       2.1206e-01,  1.0660e-01, -1.0530e-01,  2.0444e-01,  6.6951e-02,\n",
       "                      -1.0520e-01,  6.3014e-02,  2.3932e-01,  1.4721e-02, -4.7263e-02,\n",
       "                       7.9250e-02,  1.2226e-01,  1.4140e-01,  1.5721e-01, -1.2692e-02,\n",
       "                       2.0730e-01, -1.0619e-01, -1.0640e-02,  5.7650e-02, -4.6010e-02,\n",
       "                       6.2492e-02,  8.4168e-02, -2.0426e-02,  5.8472e-02,  5.8441e-02,\n",
       "                      -1.5436e-02,  8.0891e-02,  1.4683e-01,  3.9675e-02,  1.9192e-01,\n",
       "                       5.1451e-02,  8.1474e-03, -2.6162e-02,  2.2652e-01, -5.0468e-02,\n",
       "                      -8.1537e-02, -4.3585e-03])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[ 0.0141, -0.1799, -0.0097,  ...,  0.0998, -0.1537, -0.2842],\n",
       "                      [-0.0057, -0.3097,  0.0696,  ...,  0.3327,  0.0010, -0.2708],\n",
       "                      [-0.0060,  0.0584,  0.2254,  ..., -0.1426,  0.0963,  0.3199],\n",
       "                      ...,\n",
       "                      [-0.0331,  0.1002,  0.0732,  ...,  0.0791,  0.2727, -0.0636],\n",
       "                      [ 0.0344, -0.1259, -0.1711,  ..., -0.1546,  0.4533,  0.1531],\n",
       "                      [-0.0319, -0.1113,  0.0727,  ..., -0.2035,  0.0845,  0.0888]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-0.1760,  0.1588, -0.3367,  0.0764,  0.2242,  0.2892, -0.1051,  0.0872,\n",
       "                      -0.0724,  0.1673,  0.0023,  0.0256, -0.0100, -0.0629, -0.1366, -0.2067,\n",
       "                       0.3819,  0.0796,  0.0093,  0.1520, -0.0216, -0.3039,  0.1547, -0.1833,\n",
       "                      -0.1168, -0.2608, -0.1726, -0.0174, -0.0073, -0.1996,  0.1106,  0.2140,\n",
       "                       0.1457,  0.2177,  0.0946,  0.1113, -0.1986,  0.0351,  0.2080, -0.1045,\n",
       "                       0.0059,  0.1104,  0.0448,  0.0159, -0.0583,  0.0085, -0.0166,  0.0476,\n",
       "                      -0.2660,  0.2065, -0.1107, -0.0005, -0.0714,  0.0121,  0.0433,  0.0782,\n",
       "                       0.0413,  0.0825,  0.0351,  0.2419,  0.0726, -0.0109, -0.3193, -0.3184])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[-0.1360,  0.1003, -0.2431,  ...,  0.0278,  0.1238, -0.0515],\n",
       "                      [ 0.0222,  0.0251, -0.0998,  ..., -0.0981,  0.1512,  0.1861],\n",
       "                      [-0.1197, -0.0606,  0.2375,  ...,  0.1622, -0.2011, -0.0205],\n",
       "                      ...,\n",
       "                      [ 0.1967,  0.0571,  0.0057,  ..., -0.0675,  0.0028, -0.2021],\n",
       "                      [-0.2222,  0.0140, -0.1664,  ..., -0.0469,  0.0489, -0.1255],\n",
       "                      [-0.2557, -0.0696,  0.1477,  ..., -0.0710,  0.0062,  0.0234]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([-0.0024,  0.2094, -0.0826, -0.1526, -0.1163,  0.1255, -0.0135, -0.3143,\n",
       "                      -0.1850, -0.1269, -0.1593,  0.1158, -0.0485, -0.1039,  0.2737,  0.0427,\n",
       "                       0.1031, -0.0435, -0.0883, -0.1131,  0.1402,  0.0280, -0.1018,  0.3896,\n",
       "                       0.3590, -0.1927,  0.1807,  0.1417,  0.2478, -0.0712, -0.0861,  0.0195,\n",
       "                      -0.3232, -0.1981, -0.1428,  0.2018,  0.0097,  0.0095, -0.1064,  0.1791,\n",
       "                      -0.0132, -0.0865,  0.1437, -0.0888, -0.0180,  0.2265,  0.0399, -0.0394,\n",
       "                      -0.1489,  0.0237,  0.0357,  0.0963, -0.0484, -0.3650,  0.0045,  0.2580,\n",
       "                      -0.0952,  0.0096,  0.2616, -0.2600,  0.1968, -0.0581,  0.0487,  0.0241])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[-0.1457,  0.2191, -0.0789,  ...,  0.1529, -0.1725,  0.0934],\n",
       "                      [ 0.2022, -0.0841,  0.1340,  ...,  0.1295,  0.1330, -0.4050],\n",
       "                      [-0.0427, -0.0223, -0.1900,  ...,  0.0190,  0.1262, -0.1886],\n",
       "                      ...,\n",
       "                      [-0.2599, -0.0649, -0.0848,  ...,  0.0361, -0.5507,  0.0815],\n",
       "                      [ 0.2239, -0.0437, -0.0562,  ..., -0.0235, -0.2400, -0.1563],\n",
       "                      [ 0.1092, -0.2005, -0.2034,  ..., -0.1735, -0.2017, -0.0575]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.7117, -0.0623, -0.1118,  ..., -0.1168, -0.2420, -0.2238]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
