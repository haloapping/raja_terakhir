{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=40,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 40)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 40)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.3283049e-30,  4.5556213e-41, -4.3283049e-30, ...,\n",
       "         4.5554812e-41,  1.0480031e-40,  0.0000000e+00],\n",
       "       [-6.0659576e+37,  4.5554812e-41, -1.0820081e+38, ...,\n",
       "         4.5554812e-41, -1.0820666e+38,  4.5554812e-41],\n",
       "       [ 1.0481572e-40,  0.0000000e+00, -6.0661361e+37, ...,\n",
       "         0.0000000e+00, -6.0662983e+37,  4.5554812e-41],\n",
       "       ...,\n",
       "       [ 1.0654773e-40,  0.0000000e+00, -7.9007388e+36, ...,\n",
       "         0.0000000e+00, -7.9009417e+36,  4.5554812e-41],\n",
       "       [-1.0906403e+38,  4.5554812e-41,  1.0656314e-40, ...,\n",
       "         4.5554812e-41,  1.0657716e-40,  0.0000000e+00],\n",
       "       [-7.9011648e+36,  4.5554812e-41, -1.0907117e+38, ...,\n",
       "         4.5554812e-41, -1.0907766e+38,  4.5554812e-41]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6d53333e46429fbd74514868cfb184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=6.2199 | F1Score=0.2700\n",
      "Batch-100: NLLLoss=5.6037 | F1Score=0.2919\n",
      "Batch-150: NLLLoss=5.1191 | F1Score=0.3223\n",
      "Batch-200: NLLLoss=5.3037 | F1Score=0.3463\n",
      "Batch-250: NLLLoss=3.7531 | F1Score=0.3641\n",
      "Batch-300: NLLLoss=4.8755 | F1Score=0.3808\n",
      "Batch-350: NLLLoss=3.9307 | F1Score=0.3970\n",
      "Batch-400: NLLLoss=4.0302 | F1Score=0.4103\n",
      "Batch-450: NLLLoss=3.8817 | F1Score=0.4226\n",
      "Batch-500: NLLLoss=2.6488 | F1Score=0.4347\n",
      "Batch-518: NLLLoss=4.6665 | F1Score=0.4394\n",
      "\n",
      "Mean NLLLoss: 4.4866 | Mean F1Score: 0.3544\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c803506720405c8a5ac34e93fc65a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=3.6690 | F1Score=0.5987\n",
      "Batch-100: NLLLoss=2.8972 | F1Score=0.6051\n",
      "Batch-150: NLLLoss=2.6695 | F1Score=0.6051\n",
      "Batch-200: NLLLoss=2.4640 | F1Score=0.6094\n",
      "Batch-250: NLLLoss=2.5673 | F1Score=0.6177\n",
      "Batch-300: NLLLoss=3.5517 | F1Score=0.6233\n",
      "Batch-350: NLLLoss=2.5056 | F1Score=0.6276\n",
      "Batch-400: NLLLoss=2.2161 | F1Score=0.6324\n",
      "Batch-450: NLLLoss=2.5203 | F1Score=0.6373\n",
      "Batch-500: NLLLoss=2.9539 | F1Score=0.6414\n",
      "Batch-518: NLLLoss=2.5672 | F1Score=0.6438\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 2.6820 | Mean F1Score: 0.6180\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3071177ce914458fbea1acf494bde1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.9111 | F1Score=0.7169\n",
      "Batch-100: NLLLoss=2.0774 | F1Score=0.7181\n",
      "Batch-150: NLLLoss=2.0717 | F1Score=0.7200\n",
      "Batch-200: NLLLoss=1.2998 | F1Score=0.7241\n",
      "Batch-250: NLLLoss=1.9750 | F1Score=0.7287\n",
      "Batch-300: NLLLoss=1.4596 | F1Score=0.7302\n",
      "Batch-350: NLLLoss=1.8174 | F1Score=0.7324\n",
      "Batch-400: NLLLoss=1.0962 | F1Score=0.7364\n",
      "Batch-450: NLLLoss=1.7826 | F1Score=0.7403\n",
      "Batch-500: NLLLoss=2.1421 | F1Score=0.7435\n",
      "Batch-518: NLLLoss=0.9067 | F1Score=0.7452\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.7844 | Mean F1Score: 0.7282\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2329323e961f44cbbaaeaff78b6f108b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.2474 | F1Score=0.8219\n",
      "Batch-100: NLLLoss=0.8562 | F1Score=0.8173\n",
      "Batch-150: NLLLoss=1.3083 | F1Score=0.8161\n",
      "Batch-200: NLLLoss=2.0849 | F1Score=0.8154\n",
      "Batch-250: NLLLoss=1.5020 | F1Score=0.8134\n",
      "Batch-300: NLLLoss=1.7822 | F1Score=0.8151\n",
      "Batch-350: NLLLoss=1.2784 | F1Score=0.8118\n",
      "Batch-400: NLLLoss=1.1490 | F1Score=0.8147\n",
      "Batch-450: NLLLoss=1.7197 | F1Score=0.8160\n",
      "Batch-500: NLLLoss=1.2731 | F1Score=0.8166\n",
      "Batch-518: NLLLoss=0.5068 | F1Score=0.8170\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 1.1657 | Mean F1Score: 0.8141\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90748499f3045a78fdf06cf786f438d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.4854 | F1Score=0.8863\n",
      "Batch-100: NLLLoss=0.9423 | F1Score=0.8797\n",
      "Batch-150: NLLLoss=0.8478 | F1Score=0.8797\n",
      "Batch-200: NLLLoss=0.2579 | F1Score=0.8787\n",
      "Batch-250: NLLLoss=0.9493 | F1Score=0.8753\n",
      "Batch-300: NLLLoss=0.8632 | F1Score=0.8718\n",
      "Batch-350: NLLLoss=0.3548 | F1Score=0.8713\n",
      "Batch-400: NLLLoss=1.0905 | F1Score=0.8707\n",
      "Batch-450: NLLLoss=0.7949 | F1Score=0.8697\n",
      "Batch-500: NLLLoss=0.1889 | F1Score=0.8697\n",
      "Batch-518: NLLLoss=0.2103 | F1Score=0.8699\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.6994 | Mean F1Score: 0.8770\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e07705afb54404294f820a6a5ef6761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.2109 | F1Score=0.9475\n",
      "Batch-100: NLLLoss=0.3653 | F1Score=0.9523\n",
      "Batch-150: NLLLoss=0.0460 | F1Score=0.9520\n",
      "Batch-200: NLLLoss=0.2043 | F1Score=0.9467\n",
      "Batch-250: NLLLoss=0.4314 | F1Score=0.9446\n",
      "Batch-300: NLLLoss=0.4535 | F1Score=0.9411\n",
      "Batch-350: NLLLoss=0.4997 | F1Score=0.9406\n",
      "Batch-400: NLLLoss=0.4348 | F1Score=0.9397\n",
      "Batch-450: NLLLoss=0.4703 | F1Score=0.9374\n",
      "Batch-500: NLLLoss=0.1652 | F1Score=0.9364\n",
      "Batch-518: NLLLoss=0.0760 | F1Score=0.9358\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.3490 | Mean F1Score: 0.9447\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a70105af6854e65b72cd463986f3c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1214 | F1Score=0.9869\n",
      "Batch-100: NLLLoss=0.1061 | F1Score=0.9881\n",
      "Batch-150: NLLLoss=0.0621 | F1Score=0.9879\n",
      "Batch-200: NLLLoss=0.0789 | F1Score=0.9880\n",
      "Batch-250: NLLLoss=0.1228 | F1Score=0.9885\n",
      "Batch-300: NLLLoss=0.1234 | F1Score=0.9883\n",
      "Batch-350: NLLLoss=0.1078 | F1Score=0.9881\n",
      "Batch-400: NLLLoss=0.2006 | F1Score=0.9873\n",
      "Batch-450: NLLLoss=0.2465 | F1Score=0.9874\n",
      "Batch-500: NLLLoss=0.1450 | F1Score=0.9869\n",
      "Batch-518: NLLLoss=0.1675 | F1Score=0.9867\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.1178 | Mean F1Score: 0.9880\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995d2be5daf342fe97f8ee4b87eed8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0127 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.1373 | F1Score=0.9975\n",
      "Batch-150: NLLLoss=0.0248 | F1Score=0.9975\n",
      "Batch-200: NLLLoss=0.0201 | F1Score=0.9977\n",
      "Batch-250: NLLLoss=0.0428 | F1Score=0.9978\n",
      "Batch-300: NLLLoss=0.0284 | F1Score=0.9981\n",
      "Batch-350: NLLLoss=0.0156 | F1Score=0.9980\n",
      "Batch-400: NLLLoss=0.0365 | F1Score=0.9980\n",
      "Batch-450: NLLLoss=0.0559 | F1Score=0.9982\n",
      "Batch-500: NLLLoss=0.0500 | F1Score=0.9980\n",
      "Batch-518: NLLLoss=0.1249 | F1Score=0.9979\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0319 | Mean F1Score: 0.9981\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4874d70a01bc43bba481343efad933af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0132 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0092 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0101 | F1Score=0.9995\n",
      "Batch-200: NLLLoss=0.0070 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0112 | F1Score=0.9994\n",
      "Batch-300: NLLLoss=0.0109 | F1Score=0.9995\n",
      "Batch-350: NLLLoss=0.0091 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0056 | F1Score=0.9996\n",
      "Batch-450: NLLLoss=0.0082 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0080 | F1Score=0.9993\n",
      "Batch-518: NLLLoss=0.0095 | F1Score=0.9993\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0125 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eae1335ca184133840c96f6492ee506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0052 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0058 | F1Score=0.9995\n",
      "Batch-150: NLLLoss=0.0028 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0053 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0044 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0087 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0042 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0059 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0043 | F1Score=0.9995\n",
      "Batch-500: NLLLoss=0.0072 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0014 | F1Score=0.9996\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0067 | Mean F1Score: 0.9996\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52d5bb830b942a388ea1129f8db36b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0032 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0031 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0069 | F1Score=0.9992\n",
      "Batch-200: NLLLoss=0.0041 | F1Score=0.9994\n",
      "Batch-250: NLLLoss=0.0028 | F1Score=0.9995\n",
      "Batch-300: NLLLoss=0.0066 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0036 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0033 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0030 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0056 | F1Score=0.9994\n",
      "Batch-518: NLLLoss=0.0047 | F1Score=0.9994\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0062 | Mean F1Score: 0.9992\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a603e5dbba45dcb09894d7c91d30dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0044 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0018 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0009 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0032 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0023 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0036 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0043 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0025 | F1Score=0.9997\n",
      "Batch-450: NLLLoss=0.0019 | F1Score=0.9997\n",
      "Batch-500: NLLLoss=0.0028 | F1Score=0.9997\n",
      "Batch-518: NLLLoss=0.0032 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0042 | Mean F1Score: 0.9997\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc5c44288264f90a395c027bfd6d33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0014 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0011 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0013 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0019 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0014 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0016 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0016 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0014 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0019 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0040 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0017 | F1Score=0.9998\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0024 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4722a2be21044a3ca13bef18290f19c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0031 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0016 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0014 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0022 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0237 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0170 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.4329 | F1Score=0.9824\n",
      "Batch-400: NLLLoss=0.7749 | F1Score=0.9653\n",
      "Batch-450: NLLLoss=0.3389 | F1Score=0.9567\n",
      "Batch-500: NLLLoss=0.2446 | F1Score=0.9523\n",
      "Batch-518: NLLLoss=0.0041 | F1Score=0.9506\n",
      "\n",
      "Huft ðŸ˜¥! Model not improved.\n",
      "Mean NLLLoss: 0.2026 | Mean F1Score: 0.9869\n",
      "Patience = 1/20â—\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05256332e7064b0b9013d09ea62d81ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.3095 | F1Score=0.9475\n",
      "Batch-100: NLLLoss=0.2243 | F1Score=0.9522\n",
      "Batch-150: NLLLoss=0.0555 | F1Score=0.9575\n",
      "Batch-200: NLLLoss=0.0998 | F1Score=0.9602\n",
      "Batch-250: NLLLoss=0.0894 | F1Score=0.9626\n",
      "Batch-300: NLLLoss=0.1753 | F1Score=0.9658\n",
      "Batch-350: NLLLoss=0.1999 | F1Score=0.9667\n",
      "Batch-400: NLLLoss=0.0409 | F1Score=0.9684\n",
      "Batch-450: NLLLoss=0.1224 | F1Score=0.9694\n",
      "Batch-500: NLLLoss=0.2354 | F1Score=0.9698\n",
      "Batch-518: NLLLoss=0.0348 | F1Score=0.9704\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.1235 | Mean F1Score: 0.9597\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d2c3b1e5cd423a82ada9f818489979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0085 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0086 | F1Score=0.9977\n",
      "Batch-150: NLLLoss=0.0015 | F1Score=0.9984\n",
      "Batch-200: NLLLoss=0.0039 | F1Score=0.9988\n",
      "Batch-250: NLLLoss=0.0050 | F1Score=0.9991\n",
      "Batch-300: NLLLoss=0.0049 | F1Score=0.9992\n",
      "Batch-350: NLLLoss=0.0017 | F1Score=0.9992\n",
      "Batch-400: NLLLoss=0.0028 | F1Score=0.9991\n",
      "Batch-450: NLLLoss=0.0016 | F1Score=0.9991\n",
      "Batch-500: NLLLoss=0.0006 | F1Score=0.9991\n",
      "Batch-518: NLLLoss=0.0013 | F1Score=0.9991\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0088 | Mean F1Score: 0.9989\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fde5d5ed58f4e61b4e9558ecdfeaa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0018 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0028 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0008 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0023 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0011 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0018 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0017 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0018 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0019 | Mean F1Score: 0.9998\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8d2dcd0aa642e390de303c7d9c162d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0017 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0012 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0009 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0016 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0014 | F1Score=1.0000\n",
      "Batch-300: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-350: NLLLoss=0.0013 | F1Score=1.0000\n",
      "Batch-400: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-450: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0011 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0012 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0011 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b2ee2f1447482d858e5a2f7b500109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0012 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0011 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0011 | F1Score=1.0000\n",
      "Batch-300: NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-350: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-400: NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-450: NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-500: NLLLoss=0.0013 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0008 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0009 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95aa18188d854a968b3331b791edd288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0004 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0015 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0003 | F1Score=0.9999\n",
      "\n",
      "Yeah ðŸŽ‰ðŸ˜„! Model improved.\n",
      "Mean NLLLoss: 0.0007 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0007\n",
      "Best F1Score      : 1.0000\n",
      "Training duration : 25.675 minutes.\n",
      "Training date     : 2022-10-11 13:47:43.838849+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah ðŸŽ‰ðŸ˜„! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft ðŸ˜¥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft ðŸ˜¥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}â—\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}â—\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.9588763 , -4.208417  , -4.2313395 , ..., -0.54904115,\n",
       "        -1.1717591 ,  0.3354122 ],\n",
       "       [-2.670443  ,  0.28272033, -1.650678  , ..., -3.8783855 ,\n",
       "         3.4852262 ,  5.5110903 ],\n",
       "       [-5.8808265 ,  3.3411117 ,  0.3063033 , ...,  0.6387613 ,\n",
       "         1.6119611 ,  2.2606153 ],\n",
       "       ...,\n",
       "       [-7.0227976 ,  3.7781048 , -2.1947558 , ..., -1.6572292 ,\n",
       "         2.0973978 ,  2.3124058 ],\n",
       "       [-2.4176395 , -8.586711  , -2.8833697 , ...,  1.3964037 ,\n",
       "         0.7760628 , -2.7612195 ],\n",
       "       [-4.5948033 ,  4.3068867 , -0.42587656, ...,  3.8149922 ,\n",
       "         3.9430692 ,  3.647541  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[-1.0647e-01, -2.5548e-01, -2.2549e-01,  ...,  3.8735e-01,\n",
       "                        1.6080e-02, -6.1852e-01],\n",
       "                      [ 2.1382e-01,  1.9758e-03, -4.1307e-02,  ..., -1.5636e-04,\n",
       "                        5.5709e-01, -2.6966e-01],\n",
       "                      [-1.0535e-01, -1.1995e-01,  1.1441e-01,  ...,  3.3186e-02,\n",
       "                        8.2207e-02, -1.2483e-01],\n",
       "                      ...,\n",
       "                      [ 4.0204e-01, -2.2198e-01, -2.6634e-01,  ..., -4.6819e-01,\n",
       "                       -2.9717e-01, -3.2120e-01],\n",
       "                      [-9.2598e-02,  3.3359e-02, -1.7449e-01,  ..., -1.8969e-01,\n",
       "                       -2.4914e-01, -3.1022e-01],\n",
       "                      [ 2.0461e-01, -1.6637e-01,  4.7581e-01,  ..., -2.8355e-02,\n",
       "                       -1.1433e-01,  9.1966e-03]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.2400,  0.1157, -0.1030,  ..., -0.0318, -0.1059, -0.0334],\n",
       "                      [-0.0739,  0.3162,  0.2248,  ..., -0.1336, -0.2520, -0.1822],\n",
       "                      [-0.0507, -0.1489,  0.1235,  ...,  0.0141,  0.2597, -0.4708],\n",
       "                      ...,\n",
       "                      [ 0.1001,  0.0282,  0.2149,  ..., -0.2215,  0.0098, -0.2870],\n",
       "                      [ 0.2222, -0.0127, -0.1234,  ...,  0.2534,  0.2793,  0.0341],\n",
       "                      [ 0.2978,  0.0593,  0.0144,  ..., -0.0095, -0.1036, -0.1136]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-1.0428e-01, -1.5103e-01, -4.5362e-02, -8.2273e-02, -1.8909e-01,\n",
       "                      -1.8295e-01, -6.1188e-02, -8.3344e-02, -2.0168e-01, -1.3989e-01,\n",
       "                      -2.0345e-01, -1.2625e-01, -1.6175e-01, -5.6528e-02, -1.0648e-01,\n",
       "                      -1.9046e-01,  8.0597e-02, -1.3029e-01, -5.9251e-02, -9.0929e-02,\n",
       "                      -1.8009e-01, -2.3980e-01, -1.8613e-02, -1.5449e-01, -2.0912e-01,\n",
       "                      -2.1118e-01, -1.1927e-01, -1.1143e-01, -5.9343e-02, -1.0101e-01,\n",
       "                      -4.6297e-02, -1.0825e-01, -2.3072e-01, -3.9782e-02, -1.7787e-01,\n",
       "                      -8.3632e-02, -2.2585e-01, -2.0018e-01, -1.2554e-02, -7.4074e-03,\n",
       "                      -1.7332e-01,  4.9594e-02, -1.4828e-01, -1.7626e-01, -1.8347e-01,\n",
       "                      -1.5788e-01, -2.4369e-01, -2.0544e-02,  5.2499e-03, -1.7943e-01,\n",
       "                      -1.1409e-01, -1.9691e-01, -1.2734e-01, -2.7319e-01, -1.5107e-01,\n",
       "                      -7.6891e-03, -2.5992e-01, -5.6023e-02, -9.6008e-02, -1.9498e-01,\n",
       "                      -1.2006e-01, -2.8499e-01, -3.1967e-02, -1.9773e-01, -2.3267e-02,\n",
       "                      -1.0658e-01, -1.1103e-01, -4.2933e-02, -1.4557e-02, -1.5112e-01,\n",
       "                      -1.3856e-01, -1.6031e-01, -1.0589e-01, -7.4112e-02, -2.6690e-01,\n",
       "                      -2.2675e-01, -3.5672e-02, -1.3994e-01, -1.9552e-01, -1.5499e-01,\n",
       "                      -1.0954e-01, -1.1350e-01, -3.2498e-01, -1.2627e-01, -1.8875e-01,\n",
       "                      -8.3065e-02, -1.9674e-01, -5.2790e-02, -8.9508e-02, -1.9664e-01,\n",
       "                      -7.6235e-02, -5.5234e-02, -9.9777e-02, -1.2305e-01, -2.0639e-01,\n",
       "                      -1.6036e-01, -1.8151e-01, -2.0399e-01, -1.3006e-01, -1.0516e-01,\n",
       "                      -2.5793e-01, -1.6611e-01, -1.5428e-02, -1.5391e-01, -1.0708e-01,\n",
       "                      -8.0429e-02, -1.4867e-01, -1.3069e-01, -3.1485e-01, -2.1376e-02,\n",
       "                      -1.3338e-01, -1.5398e-01, -1.9591e-01, -9.1393e-02, -6.6328e-02,\n",
       "                      -2.7825e-02, -1.9129e-01, -1.9056e-01, -1.0698e-01, -2.1029e-01,\n",
       "                      -1.9168e-01, -1.9075e-01, -1.3973e-01, -7.6199e-02, -2.1610e-01,\n",
       "                      -2.1357e-01,  1.8544e-02, -1.2150e-01, -5.1278e-03, -1.3208e-01,\n",
       "                      -7.7375e-03, -9.9596e-02, -1.9480e-01, -8.4767e-03, -1.5043e-01,\n",
       "                      -5.8725e-02,  1.6528e-02, -1.3860e-01, -1.2370e-01, -1.5732e-01,\n",
       "                      -1.4634e-01, -2.0256e-03, -1.1598e-01, -6.4744e-02, -8.4581e-02,\n",
       "                      -1.4004e-01, -1.9838e-01, -5.1660e-02, -2.5032e-01, -3.3835e-02,\n",
       "                      -5.5908e-02, -7.8144e-02,  4.6253e-02, -2.1915e-01, -1.5455e-01,\n",
       "                      -9.7099e-02, -1.2369e-01, -8.1556e-02, -1.1875e-01, -2.2049e-01,\n",
       "                      -1.4756e-01, -1.7241e-01, -2.1033e-01, -1.2555e-01, -1.3982e-01,\n",
       "                      -1.7746e-01, -1.5047e-01, -2.1934e-01, -1.3883e-02, -1.1738e-01,\n",
       "                      -1.7999e-01, -7.4299e-02, -8.0877e-02, -1.0555e-01,  9.3196e-03,\n",
       "                       3.0343e-03, -1.1867e-01, -2.4431e-01, -2.1748e-01, -6.2685e-03,\n",
       "                      -1.3071e-01, -2.2191e-01,  1.1495e-02, -1.1906e-01, -1.4822e-01,\n",
       "                      -6.8747e-02, -1.7027e-01, -1.5963e-01, -7.4483e-02, -7.5286e-02,\n",
       "                      -9.1685e-02, -1.1858e-01, -9.3772e-02, -9.5798e-02, -1.5728e-01,\n",
       "                      -1.0578e-01, -1.8769e-01, -2.4126e-01,  6.5312e-02, -1.1547e-01,\n",
       "                      -1.7427e-02, -2.0477e-01, -1.2814e-01, -1.6230e-02,  6.6437e-02,\n",
       "                      -1.2765e-01, -7.6837e-02, -6.5938e-02, -1.5680e-01, -1.4157e-01,\n",
       "                      -1.4491e-02, -1.0898e-01, -6.1905e-02, -8.5998e-05, -2.2110e-01,\n",
       "                      -1.6147e-01, -8.5684e-02, -8.2378e-03, -8.8601e-02, -1.3499e-01,\n",
       "                      -1.3455e-01, -1.0767e-01, -1.7787e-01, -8.9902e-02, -1.1325e-01,\n",
       "                      -1.8520e-01,  2.4461e-02, -1.4635e-01, -1.1126e-01, -4.8108e-02,\n",
       "                       7.8968e-02, -4.4758e-02, -7.0150e-02, -1.3494e-01, -8.5023e-02,\n",
       "                      -1.6395e-01, -3.0493e-03, -1.2699e-01,  8.1356e-02, -5.9557e-02,\n",
       "                      -2.1647e-01, -1.0241e-01, -5.8317e-02, -4.0358e-02, -1.2690e-01,\n",
       "                      -1.8636e-01, -4.6851e-02, -1.8916e-01, -5.5001e-02, -2.3910e-01,\n",
       "                      -1.5050e-01, -9.6281e-02, -1.8894e-01, -1.0568e-01, -8.7376e-02,\n",
       "                      -6.5494e-02,  4.3752e-02,  3.0842e-02, -3.4631e-02, -6.6378e-02,\n",
       "                      -6.2448e-02, -2.9218e-02, -6.0456e-02,  7.7972e-03, -1.2866e-01,\n",
       "                       1.0368e-01,  7.1324e-02, -4.1315e-02, -7.2434e-02, -9.4042e-03,\n",
       "                      -1.9932e-02, -2.0445e-02,  7.5502e-02, -3.6249e-02,  2.5698e-02,\n",
       "                       4.5421e-03,  5.0332e-02, -8.6081e-02, -6.9961e-02, -9.7886e-02,\n",
       "                       4.4395e-02,  5.9236e-02, -5.4427e-02, -1.6166e-02, -8.2498e-02,\n",
       "                      -8.1789e-02,  3.6204e-02, -1.7961e-02, -5.3509e-02,  2.3702e-02,\n",
       "                       4.5924e-02,  3.9368e-02, -1.2673e-01, -9.1288e-03, -8.6661e-02,\n",
       "                      -3.9770e-02,  1.3174e-02,  1.7715e-02, -1.6166e-02,  3.2299e-02,\n",
       "                      -7.6264e-02,  6.9648e-02, -1.5300e-02, -1.0659e-01,  2.5192e-02,\n",
       "                       1.3995e-02,  2.4481e-02,  9.8272e-02, -5.5493e-02,  1.9065e-02,\n",
       "                      -1.1199e-01, -2.5510e-02,  2.0417e-02,  1.7817e-02, -3.7427e-02,\n",
       "                       1.9224e-02, -1.3692e-01,  6.6837e-02, -3.2783e-03,  1.3285e-02,\n",
       "                      -5.0998e-02,  1.5930e-02, -1.2121e-01, -3.5395e-02, -2.8479e-02,\n",
       "                       4.8068e-02, -1.3983e-02, -4.5539e-02,  3.2158e-03, -4.7053e-02,\n",
       "                       2.4928e-02,  1.1722e-02, -1.8057e-02, -2.4419e-02,  1.8328e-02,\n",
       "                      -6.5987e-02,  3.7120e-02, -7.9222e-03,  1.2045e-02,  1.5319e-01,\n",
       "                       1.5174e-02,  1.5298e-02,  5.6993e-02,  3.3442e-02, -1.5180e-02,\n",
       "                      -5.4159e-02,  1.1762e-01,  2.2504e-03, -9.9219e-02,  1.9524e-02,\n",
       "                       2.5583e-02,  1.1331e-03,  3.4331e-02,  1.3591e-02, -4.5462e-02,\n",
       "                      -1.8736e-02,  8.2773e-02, -2.0355e-02,  1.2887e-02,  4.1559e-02,\n",
       "                       9.3021e-02, -6.7211e-02,  6.5893e-02, -4.8923e-02, -4.4658e-02,\n",
       "                      -1.3696e-02, -6.2349e-02, -3.5690e-02, -8.4932e-02,  8.8563e-02,\n",
       "                      -6.9329e-02, -5.1225e-02, -2.3651e-02, -1.7556e-02, -5.9059e-03,\n",
       "                      -1.1174e-02,  7.9845e-02,  1.6973e-02, -1.0549e-01,  4.3749e-02,\n",
       "                       3.7179e-02, -6.3502e-02, -7.3731e-02,  5.5913e-02, -9.8762e-02,\n",
       "                      -1.0495e-01, -1.3867e-01, -6.9643e-02, -1.0791e-01, -1.3401e-01,\n",
       "                      -1.4137e-01, -1.0485e-01, -1.6190e-01, -7.9921e-02, -3.6779e-01,\n",
       "                      -1.4221e-01, -1.3323e-01, -1.9630e-01, -9.9024e-02, -2.2788e-01,\n",
       "                      -3.5198e-02, -1.8955e-01,  6.3317e-02, -1.3966e-01, -1.1341e-01,\n",
       "                      -8.9737e-02, -1.2181e-01, -1.1750e-01, -2.2869e-01, -1.8971e-01,\n",
       "                      -1.0423e-01, -2.3961e-01, -1.2432e-01, -1.1467e-01, -1.3605e-01,\n",
       "                      -5.3882e-02, -9.0925e-02, -2.1639e-01, -1.3183e-01, -1.8169e-01,\n",
       "                      -1.1301e-02, -1.1612e-01, -1.5689e-01, -1.4196e-01, -1.8712e-01,\n",
       "                      -1.2221e-01, -1.9898e-01, -5.0609e-02, -8.2871e-02, -1.0739e-01,\n",
       "                      -1.8424e-01, -1.3023e-01, -4.4734e-02, -1.6223e-01, -2.2311e-01,\n",
       "                      -3.6375e-02, -1.2819e-01, -2.3543e-01, -8.9421e-02, -6.9277e-02,\n",
       "                      -6.2090e-02, -9.0499e-02, -1.5015e-01, -2.3754e-01, -6.9668e-02,\n",
       "                      -8.4393e-02, -1.1492e-02, -1.8857e-01,  7.2970e-03, -8.2592e-02,\n",
       "                      -3.7070e-03, -1.9863e-01, -1.4120e-01, -2.5536e-01, -1.0632e-01,\n",
       "                      -9.7192e-02, -1.9500e-01, -1.4848e-01, -2.9007e-01, -2.1990e-01,\n",
       "                      -7.3431e-03, -1.0786e-01, -2.0631e-01, -9.5730e-02, -1.1751e-01,\n",
       "                      -1.6377e-01, -9.7383e-02, -1.1470e-01, -1.1032e-01, -4.2774e-02,\n",
       "                       4.5646e-02, -1.7031e-01, -2.2258e-01, -1.6549e-01, -2.0202e-01,\n",
       "                      -1.8916e-01, -1.0716e-02, -6.2285e-02, -2.1174e-01, -9.3582e-02,\n",
       "                      -1.8423e-01, -3.2117e-01, -8.3580e-02, -4.6833e-02, -1.8770e-01,\n",
       "                      -4.3638e-02, -1.8289e-01, -1.3313e-01, -1.0393e-01, -8.4071e-02,\n",
       "                      -1.3700e-01, -1.4635e-01, -1.6904e-01, -6.1425e-02,  2.1173e-02,\n",
       "                      -2.7584e-01, -1.7139e-01, -1.2420e-01, -2.5328e-02, -4.6338e-02,\n",
       "                      -9.5185e-02, -1.5773e-01,  1.6815e-02, -1.4905e-01, -2.6716e-02,\n",
       "                      -1.4034e-01, -1.3929e-02, -1.2401e-01, -1.7755e-01, -1.5271e-01,\n",
       "                      -2.0668e-01, -1.0718e-01])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([-2.3455e-01, -1.7354e-01, -1.2643e-01, -9.7203e-02, -1.2343e-01,\n",
       "                      -2.5304e-01, -7.1524e-02, -5.5814e-02, -2.2313e-02, -1.5786e-02,\n",
       "                      -2.5634e-01, -1.6309e-01, -8.4588e-02, -1.1054e-01, -1.1286e-01,\n",
       "                      -2.0332e-01,  2.6554e-02, -1.3602e-01, -9.8449e-02, -1.0933e-01,\n",
       "                      -5.2474e-02, -9.8826e-02, -1.6739e-01, -8.9113e-02, -2.1520e-01,\n",
       "                      -2.1253e-01,  8.0882e-03, -5.8792e-03, -5.3738e-02, -7.8454e-02,\n",
       "                      -1.3633e-01, -1.2918e-01, -1.9271e-01, -1.6298e-01, -1.4919e-01,\n",
       "                      -7.7310e-02, -1.9092e-01, -4.3628e-02, -1.1445e-01,  3.5665e-02,\n",
       "                      -5.9396e-02, -2.4183e-02, -2.2280e-01, -1.8077e-01, -2.1690e-01,\n",
       "                      -1.2285e-01, -6.6903e-02, -1.4158e-01, -2.1618e-01, -1.9180e-01,\n",
       "                      -1.8329e-01, -7.7582e-02,  8.0947e-03, -2.2305e-01, -8.4690e-02,\n",
       "                      -6.4915e-02, -2.4472e-01, -2.2218e-01, -1.6347e-01, -1.4209e-01,\n",
       "                      -1.0904e-01, -2.2378e-01, -9.7679e-02, -1.5650e-01, -1.6464e-01,\n",
       "                      -8.3175e-02, -2.4368e-01, -2.8480e-01, -1.5462e-01, -1.8896e-01,\n",
       "                      -1.1055e-01, -5.7768e-02, -8.9295e-03, -1.7751e-01, -2.3916e-01,\n",
       "                      -1.6592e-01, -8.6630e-02, -1.0532e-01, -5.7135e-02, -1.1656e-01,\n",
       "                      -8.9772e-02, -5.8778e-02, -1.6986e-01, -1.6904e-01, -4.3174e-02,\n",
       "                      -1.1857e-01, -8.7460e-02, -6.8834e-02, -1.2641e-01, -5.2457e-02,\n",
       "                      -4.3925e-02, -1.1097e-01, -1.2903e-01, -5.1727e-02, -1.5379e-01,\n",
       "                      -2.4062e-01, -1.8991e-01, -1.6902e-01, -1.5752e-01, -1.5946e-01,\n",
       "                      -2.0314e-01, -8.0149e-02, -5.6298e-02, -6.7523e-02, -2.5408e-02,\n",
       "                      -2.6882e-02, -6.8899e-02, -5.9822e-02, -1.2773e-01, -6.4002e-02,\n",
       "                      -1.0889e-01, -6.0457e-02, -1.8234e-01, -2.1299e-01, -1.0242e-01,\n",
       "                      -5.8508e-02, -1.3019e-01, -7.2432e-02, -5.1789e-02, -8.5695e-02,\n",
       "                      -2.4258e-02, -2.2496e-01, -1.0022e-01, -2.5358e-01, -1.6630e-01,\n",
       "                      -1.7136e-01, -1.3308e-01, -5.4467e-02, -1.3251e-02, -1.1618e-01,\n",
       "                      -1.1959e-01, -1.0995e-01, -1.5455e-01, -1.6229e-01, -3.0477e-02,\n",
       "                      -9.0916e-02, -2.0905e-02,  2.3042e-02, -2.4800e-01, -9.3556e-02,\n",
       "                      -8.5736e-02, -4.0021e-02, -3.6784e-02, -1.4003e-01, -5.2862e-02,\n",
       "                      -1.3672e-01, -9.7650e-02, -1.2492e-01, -2.2029e-01, -2.0170e-01,\n",
       "                      -1.8562e-01, -1.3520e-01, -5.1358e-02, -2.5067e-01, -2.0799e-01,\n",
       "                      -1.3941e-01, -2.1661e-01, -1.2188e-01, -3.9750e-02, -1.2011e-01,\n",
       "                      -9.1359e-02, -1.6418e-01, -1.3579e-01, -1.1039e-01, -1.5647e-01,\n",
       "                      -9.0329e-02, -1.3409e-01, -1.2489e-01, -1.4613e-01, -1.4697e-01,\n",
       "                      -1.1160e-01,  1.1352e-02, -3.2123e-02, -4.9156e-02, -1.8616e-01,\n",
       "                      -1.0266e-01, -3.0415e-01, -1.1273e-01, -1.9017e-01, -1.8705e-02,\n",
       "                      -2.6526e-02, -1.4227e-01, -1.2778e-01, -5.9521e-02, -2.6608e-01,\n",
       "                      -1.8999e-01, -1.8514e-01, -2.5308e-01, -1.7153e-01, -1.9099e-01,\n",
       "                      -1.4270e-01, -2.0515e-01, -2.0381e-01, -1.7369e-02, -2.0148e-01,\n",
       "                      -2.9297e-02, -1.0672e-01, -2.5475e-01, -1.4403e-02,  4.8285e-03,\n",
       "                      -8.1201e-02, -1.5467e-01, -8.9473e-02,  2.4491e-02, -1.3861e-02,\n",
       "                      -9.3464e-02, -3.7933e-02, -1.4220e-01, -1.5504e-01, -2.0605e-01,\n",
       "                      -3.5175e-02, -9.4048e-02, -2.0237e-01, -8.6317e-02, -5.2704e-02,\n",
       "                      -7.9434e-02, -1.6347e-01, -1.1163e-01, -2.0869e-02,  2.1990e-02,\n",
       "                      -1.3279e-01, -1.0176e-01, -5.6937e-02, -1.9454e-01, -8.4537e-02,\n",
       "                      -2.1408e-01,  5.6175e-02, -8.5572e-02, -1.7891e-01, -4.9971e-02,\n",
       "                      -1.1013e-01, -2.0433e-01, -1.5121e-01, -1.3451e-01, -1.3457e-01,\n",
       "                      -9.7293e-02, -1.2526e-01, -1.1464e-01,  4.8503e-02, -4.5946e-02,\n",
       "                      -2.6335e-01, -1.2738e-01, -1.6846e-01, -1.0734e-01, -7.6600e-02,\n",
       "                      -5.4881e-02, -7.8752e-02, -1.4596e-01, -1.0704e-01, -1.8679e-01,\n",
       "                       3.9807e-02, -7.9437e-02, -1.5459e-01, -1.5540e-01, -6.8953e-02,\n",
       "                      -1.1534e-01,  2.6848e-02, -3.7425e-02, -1.9220e-02, -1.0658e-02,\n",
       "                      -3.1570e-02, -1.4208e-01,  9.3663e-02,  1.1339e-01, -2.2796e-03,\n",
       "                       2.6182e-02,  6.1095e-02,  1.0924e-01, -6.2877e-02, -6.7422e-02,\n",
       "                      -6.1945e-02,  2.4383e-02,  2.8862e-02,  4.0174e-02,  1.9725e-02,\n",
       "                       1.2802e-02,  3.2893e-03,  2.1339e-02,  5.3941e-03, -7.5240e-03,\n",
       "                      -4.1572e-02,  5.3308e-02,  1.0934e-01,  5.9118e-02,  1.0850e-01,\n",
       "                       3.4720e-03, -8.5837e-02,  4.5366e-02, -3.8875e-02, -3.0665e-02,\n",
       "                      -3.8163e-02, -5.0117e-02, -2.0700e-02, -4.6979e-02,  1.0277e-01,\n",
       "                      -6.5968e-02,  6.6211e-02, -3.3187e-02,  1.6279e-01, -1.6061e-02,\n",
       "                      -1.0861e-02, -5.8476e-02, -3.3591e-02, -3.1323e-02,  5.3533e-03,\n",
       "                      -1.1204e-01, -8.3602e-02,  7.3756e-02,  4.2139e-02,  2.5567e-02,\n",
       "                      -1.9401e-02,  1.4756e-01, -1.3121e-02, -2.6205e-02, -2.0318e-02,\n",
       "                      -1.7765e-02,  4.2665e-04, -6.9511e-02, -4.3314e-02,  6.9299e-02,\n",
       "                      -2.1836e-03, -3.0560e-02, -1.7859e-03, -4.8143e-02,  3.5790e-02,\n",
       "                       3.5553e-02,  3.1581e-02,  1.3816e-02,  8.5743e-03,  7.8106e-02,\n",
       "                      -3.4825e-02,  4.2129e-02, -4.8809e-02,  1.1357e-02,  5.8795e-02,\n",
       "                      -2.1891e-02,  1.8740e-02, -3.5064e-02, -6.8040e-02, -1.6175e-02,\n",
       "                      -1.9260e-02,  7.6685e-02, -3.9699e-03,  2.5552e-02, -1.5126e-02,\n",
       "                      -2.8952e-02, -7.1113e-02,  7.9575e-03, -1.6315e-02, -1.5949e-02,\n",
       "                      -2.4932e-03, -1.1462e-01,  8.5370e-02,  3.8770e-02,  4.0586e-02,\n",
       "                       1.0983e-01, -5.0120e-02, -5.6068e-02,  3.7906e-02, -7.1630e-03,\n",
       "                       1.5350e-02, -1.1133e-01,  1.1238e-01, -6.3101e-02,  8.1159e-02,\n",
       "                      -4.0728e-02,  5.1976e-02,  2.4886e-02, -7.7184e-02, -8.5303e-03,\n",
       "                      -9.8355e-02, -6.7738e-02,  8.2200e-02, -1.8388e-02, -1.5577e-02,\n",
       "                       8.1778e-02, -1.9430e-02,  1.4567e-02,  4.6035e-03,  1.8542e-02,\n",
       "                      -5.3566e-02, -5.9515e-02,  8.7639e-02, -3.6818e-02, -2.5890e-01,\n",
       "                      -1.9141e-01, -1.2597e-01, -6.0304e-02, -9.3714e-02, -1.1110e-01,\n",
       "                      -1.4226e-01, -1.4121e-01, -1.7152e-01, -3.1583e-02, -1.9567e-01,\n",
       "                      -1.4773e-01, -1.4617e-01,  2.3601e-02,  5.1421e-02, -1.2478e-01,\n",
       "                      -5.8099e-02, -1.6820e-01, -1.0316e-01, -1.8617e-01, -2.0742e-01,\n",
       "                      -3.7987e-02, -1.5936e-01, -7.9659e-02, -1.8718e-01, -2.3326e-01,\n",
       "                       2.7275e-02, -6.3306e-02, -5.1181e-02, -8.1673e-02, -8.5145e-02,\n",
       "                      -1.5114e-01, -8.9627e-02, -2.2087e-01, -1.7073e-02, -9.9974e-02,\n",
       "                      -4.0142e-02, -1.4580e-01, -1.5582e-01, -1.6435e-02, -1.3565e-01,\n",
       "                      -7.7289e-02, -1.9081e-01, -1.3508e-01, -2.7820e-01, -1.0829e-01,\n",
       "                      -2.0076e-01,  6.4658e-03, -1.4755e-01, -1.5171e-01, -2.4870e-01,\n",
       "                       2.8110e-02, -6.3323e-02, -1.4767e-01, -8.6112e-02, -2.1181e-01,\n",
       "                      -2.6845e-01, -9.5676e-02, -1.6369e-01, -1.9582e-01, -7.5186e-02,\n",
       "                      -2.5203e-01, -1.3222e-01, -2.0465e-01, -9.5274e-02, -1.0548e-01,\n",
       "                      -1.5048e-01, -2.2855e-01, -1.2934e-01, -4.3905e-02, -1.5634e-01,\n",
       "                      -3.6091e-02, -1.3365e-01, -1.0050e-01, -9.1392e-02, -1.5035e-01,\n",
       "                      -1.1841e-01, -3.9907e-02, -7.5942e-02,  3.1539e-02, -2.2029e-01,\n",
       "                      -1.2177e-01, -8.9550e-02, -1.1543e-01, -9.6433e-02, -2.4499e-01,\n",
       "                      -1.6336e-01, -9.6215e-02, -9.7502e-02, -5.8681e-02, -1.2481e-01,\n",
       "                      -8.1768e-02, -3.6653e-02, -5.0982e-02, -1.5216e-01, -9.7135e-02,\n",
       "                       1.1670e-04, -2.8617e-01, -1.7516e-01, -1.3085e-01, -2.3116e-01,\n",
       "                      -7.7449e-02, -1.2483e-01, -1.7237e-01, -1.6092e-01, -1.5341e-01,\n",
       "                      -1.9180e-01, -5.9939e-02, -1.6423e-01, -1.3611e-01, -3.0990e-03,\n",
       "                      -9.6413e-02, -1.1556e-01, -1.0480e-01, -1.0741e-01, -1.0732e-01,\n",
       "                      -7.2058e-02, -1.1770e-01, -1.2817e-01, -1.5519e-01, -6.5236e-02,\n",
       "                      -1.7083e-01, -1.1401e-01, -1.1597e-01, -8.2292e-02, -1.5007e-01,\n",
       "                      -1.1703e-01, -1.4950e-02])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.0520,  0.0144, -0.1176,  ...,  0.0174, -0.0976,  0.0517],\n",
       "                      [ 0.1691, -0.1507, -0.0968,  ...,  0.1862,  0.0137, -0.1948],\n",
       "                      [-0.0297, -0.4124, -0.1098,  ..., -0.1953,  0.3784, -0.0452],\n",
       "                      ...,\n",
       "                      [-0.0818,  0.0322,  0.2505,  ...,  0.2184,  0.1799, -0.0553],\n",
       "                      [-0.2252,  0.0899, -0.2486,  ..., -0.2650, -0.0713, -0.3984],\n",
       "                      [ 0.3485, -0.0527, -0.0239,  ..., -0.1502,  0.2978, -0.3045]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0183,  0.0643, -0.0377,  ..., -0.0215, -0.0288,  0.1960],\n",
       "                      [-0.4037,  0.0860, -0.1345,  ...,  0.0906,  0.0526, -0.2213],\n",
       "                      [-0.3690,  0.0491,  0.3050,  ...,  0.1685, -0.0935, -0.0490],\n",
       "                      ...,\n",
       "                      [-0.3034, -0.1969,  0.2989,  ...,  0.1428,  0.2259,  0.0355],\n",
       "                      [ 0.1586,  0.0607, -0.0996,  ...,  0.1673,  0.1332,  0.0986],\n",
       "                      [-0.0631, -0.1931, -0.0119,  ..., -0.0292, -0.1229, -0.3933]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.1027,  0.0891,  0.1265,  0.0941,  0.0039, -0.0580, -0.0239,  0.0074,\n",
       "                       0.0650,  0.0137, -0.0944,  0.0103, -0.0446,  0.0158, -0.1237, -0.0798,\n",
       "                       0.0161, -0.0604,  0.1041,  0.0551,  0.0681, -0.0739,  0.0655,  0.2677,\n",
       "                      -0.0095,  0.1603, -0.0696,  0.0803, -0.0071,  0.1890, -0.0771, -0.0080,\n",
       "                      -0.0984, -0.0837,  0.1270,  0.0342,  0.0550,  0.0657, -0.0715, -0.0126,\n",
       "                       0.0104, -0.0859,  0.1743,  0.1225,  0.0956, -0.1303,  0.1528, -0.0088,\n",
       "                       0.1177,  0.0554, -0.0125,  0.2387, -0.0115,  0.0595, -0.0639,  0.1145,\n",
       "                       0.0473,  0.0545, -0.0039, -0.0156, -0.0159,  0.0655, -0.0078,  0.0250,\n",
       "                       0.1252,  0.2312,  0.1927,  0.1543, -0.0900,  0.0485,  0.0728,  0.0890,\n",
       "                      -0.0411,  0.0292,  0.0040,  0.1268, -0.0498,  0.0154,  0.1000, -0.0020,\n",
       "                      -0.1366,  0.1002,  0.0059,  0.0947,  0.1744, -0.0186,  0.0874, -0.0808,\n",
       "                       0.1143,  0.1177,  0.1242, -0.0545,  0.0518,  0.0052,  0.1252,  0.2060,\n",
       "                       0.1567,  0.1105,  0.1422,  0.0615,  0.0522,  0.3121,  0.0904,  0.0410,\n",
       "                      -0.0024,  0.1027,  0.0010, -0.1430,  0.0272, -0.0216,  0.1437, -0.0753,\n",
       "                      -0.0612, -0.0267,  0.2271,  0.0371,  0.0113, -0.0156,  0.0753,  0.1011,\n",
       "                      -0.0655,  0.1281,  0.1604,  0.1424,  0.1327, -0.0409,  0.0557,  0.1477,\n",
       "                      -0.2049, -0.0424, -0.1632, -0.2099, -0.0623, -0.0064, -0.0710, -0.0311,\n",
       "                       0.1343,  0.0483, -0.0907,  0.1483, -0.0470, -0.0512,  0.0681, -0.2134,\n",
       "                      -0.0863, -0.1358, -0.0163, -0.1320,  0.0386, -0.0183, -0.0436, -0.1159,\n",
       "                       0.0488,  0.0871, -0.0846,  0.0204,  0.0724, -0.1312, -0.0494, -0.0485,\n",
       "                      -0.1477, -0.0825, -0.0455, -0.0889,  0.0090, -0.1824, -0.1428,  0.0126,\n",
       "                      -0.0794, -0.0267,  0.0037,  0.0558, -0.0217, -0.1412,  0.1594, -0.0054,\n",
       "                      -0.1286, -0.0433, -0.1010, -0.1255, -0.0957, -0.1490, -0.0543, -0.1966,\n",
       "                      -0.0730, -0.0040, -0.0651, -0.1063, -0.1642, -0.1383,  0.0338, -0.0287,\n",
       "                       0.1199, -0.0053, -0.0372, -0.0313, -0.1236, -0.0711, -0.0458, -0.0467,\n",
       "                      -0.1309,  0.0025,  0.0258, -0.0633, -0.0728, -0.0481, -0.0523, -0.0246,\n",
       "                      -0.1563,  0.0308, -0.1387, -0.0846, -0.1687, -0.2435,  0.0706, -0.0976,\n",
       "                      -0.0388,  0.0332, -0.0394, -0.1368, -0.0424,  0.0176, -0.1053, -0.1160,\n",
       "                      -0.0747, -0.0907,  0.0268,  0.1207, -0.1359, -0.0525, -0.1018, -0.0358,\n",
       "                       0.0137, -0.3388, -0.0021, -0.1112,  0.0525, -0.0353, -0.1814, -0.1917,\n",
       "                      -0.0167, -0.0160, -0.2424, -0.1030, -0.1107,  0.0727, -0.0104, -0.1226,\n",
       "                      -0.0282, -0.0860, -0.0973,  0.0529, -0.1545, -0.0639,  0.0948, -0.0484,\n",
       "                       0.0260, -0.0457,  0.0483,  0.0412,  0.1071, -0.0450,  0.0420,  0.0330,\n",
       "                      -0.0343, -0.0899, -0.0182, -0.0399,  0.0175,  0.0432, -0.0178, -0.0626,\n",
       "                      -0.0186,  0.0552, -0.0021,  0.0336,  0.0278,  0.1028,  0.0554,  0.2235,\n",
       "                      -0.1145, -0.1990, -0.0800, -0.0617, -0.0883, -0.0387,  0.0392, -0.0379,\n",
       "                       0.0133, -0.1607, -0.0734, -0.0122,  0.1447,  0.0766,  0.0669, -0.0053,\n",
       "                      -0.0217,  0.0313,  0.0130, -0.0798,  0.1651, -0.0876, -0.0189,  0.1080,\n",
       "                      -0.0338, -0.0376, -0.0519,  0.1125,  0.0663,  0.0695, -0.0529, -0.0269,\n",
       "                       0.0487, -0.0206,  0.0027,  0.1365, -0.0968, -0.0168,  0.0284, -0.0450,\n",
       "                      -0.0298,  0.0052, -0.1148,  0.0702,  0.0391,  0.0269, -0.0784, -0.0499,\n",
       "                      -0.0070, -0.0668,  0.0407,  0.0888, -0.0199, -0.0126,  0.0964, -0.0576,\n",
       "                       0.0322,  0.0592, -0.0578, -0.0405, -0.0327,  0.0323,  0.0329, -0.0595,\n",
       "                       0.0033,  0.0183,  0.0005, -0.0403,  0.0338,  0.1569, -0.0287,  0.1334,\n",
       "                      -0.0322,  0.0969, -0.0500, -0.0656, -0.0675, -0.0165, -0.0984,  0.0009,\n",
       "                      -0.0333, -0.0040,  0.0300, -0.0367,  0.0772,  0.0718,  0.0469, -0.1100,\n",
       "                      -0.0059,  0.0274, -0.0544,  0.0324,  0.0785,  0.1212,  0.0683,  0.0092,\n",
       "                       0.0188,  0.0770, -0.0209,  0.0480,  0.0174,  0.0327, -0.0195, -0.0287,\n",
       "                       0.2376,  0.2796,  0.2043,  0.0167,  0.0978,  0.0962,  0.0266,  0.0821,\n",
       "                       0.1390,  0.1228,  0.0488,  0.1476,  0.0792, -0.0067,  0.1135, -0.1364,\n",
       "                      -0.0490,  0.0141,  0.2431,  0.0647,  0.0474,  0.0586, -0.0400,  0.0427,\n",
       "                       0.0472,  0.1587,  0.0689,  0.0992,  0.0734,  0.1547, -0.0719,  0.1227,\n",
       "                       0.0599, -0.0356,  0.0855,  0.0900, -0.0486,  0.0827, -0.0359,  0.1394,\n",
       "                       0.0926,  0.0663,  0.0547, -0.0749,  0.1061, -0.1946,  0.1376, -0.0849,\n",
       "                       0.1052,  0.0217, -0.0240,  0.1626, -0.0616,  0.1400,  0.1169,  0.0454,\n",
       "                       0.1489, -0.0026, -0.1043,  0.0529,  0.0668,  0.1409,  0.0271,  0.0964,\n",
       "                       0.0286,  0.2227,  0.0285,  0.1134,  0.1629,  0.1245, -0.0729,  0.1242,\n",
       "                       0.1743,  0.0264,  0.1983,  0.0910,  0.0916,  0.1149,  0.1375,  0.1173,\n",
       "                      -0.0125,  0.1241,  0.0364,  0.1176,  0.0841,  0.1022, -0.0206, -0.1408,\n",
       "                      -0.0107,  0.0900,  0.0711,  0.0309,  0.0188, -0.0237, -0.0145,  0.2068,\n",
       "                       0.1344, -0.0426,  0.0356,  0.1314, -0.0266,  0.1082,  0.1382,  0.0402,\n",
       "                      -0.0521, -0.0106,  0.0416, -0.0076,  0.0196,  0.1667, -0.0024, -0.1891,\n",
       "                      -0.0119,  0.0986,  0.1337, -0.0223, -0.0082,  0.0555,  0.0452,  0.1599,\n",
       "                       0.0549,  0.0996,  0.1279,  0.1617,  0.1025, -0.1514, -0.0365,  0.0567])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 1.1986e-01,  8.1126e-02, -9.1270e-03,  1.5080e-01,  3.3389e-02,\n",
       "                      -3.5039e-02, -2.1461e-02, -6.6295e-02,  5.2731e-02,  3.3257e-02,\n",
       "                       4.8978e-02, -1.1464e-02,  1.8462e-01,  1.0518e-02, -6.2906e-03,\n",
       "                      -1.0131e-01, -7.9401e-02, -3.6224e-02,  1.1983e-01,  9.1131e-02,\n",
       "                       6.2000e-02,  6.3230e-02, -3.8602e-02,  2.3118e-01,  9.5371e-02,\n",
       "                       1.4371e-01,  9.3900e-02,  8.3162e-02, -3.8967e-02,  7.7767e-02,\n",
       "                      -3.2371e-02,  4.6440e-02, -7.0286e-02,  3.7990e-04, -4.4312e-02,\n",
       "                      -9.0038e-02,  1.3568e-01,  1.4541e-01, -3.6126e-02,  6.3921e-02,\n",
       "                       2.1079e-01, -9.0768e-02,  1.7168e-01,  1.1153e-02, -6.7800e-02,\n",
       "                      -1.7090e-01, -1.2760e-02,  3.3481e-02,  1.1234e-01,  7.7052e-02,\n",
       "                       1.8023e-02,  6.4154e-02, -1.4936e-02,  1.4295e-01, -1.5134e-02,\n",
       "                       4.9855e-02,  9.6562e-02, -6.4134e-02,  3.2704e-02, -1.7851e-02,\n",
       "                      -5.9735e-02,  1.1400e-01,  4.7257e-02,  1.9896e-01,  4.5616e-02,\n",
       "                       1.7946e-01,  7.8721e-02,  1.9323e-01, -2.6721e-02,  8.2745e-02,\n",
       "                       9.5970e-02,  5.8501e-02,  6.6737e-02, -8.1923e-02,  9.2789e-02,\n",
       "                       1.8266e-01,  6.9975e-02, -7.7190e-03,  1.5570e-01,  2.3733e-01,\n",
       "                      -2.5222e-01,  7.3094e-02,  6.5838e-02,  8.8658e-02,  2.1630e-01,\n",
       "                       4.0348e-02,  1.2939e-01,  3.1691e-02,  1.7060e-01,  8.9685e-02,\n",
       "                       1.3647e-01,  4.7890e-02,  4.1296e-02, -2.6780e-02,  7.4219e-02,\n",
       "                       2.1122e-01,  8.1034e-02,  8.2887e-02,  1.5430e-01,  1.0048e-01,\n",
       "                      -5.3308e-02,  1.5393e-01,  1.3264e-01, -2.0827e-02,  1.1360e-01,\n",
       "                      -1.8685e-02, -1.0841e-02, -9.7176e-03,  1.0933e-02, -1.2721e-01,\n",
       "                       2.1931e-02, -9.4640e-02, -9.2513e-02,  1.4962e-01,  1.2703e-01,\n",
       "                      -1.9971e-02,  1.5750e-01,  2.4801e-03,  1.1492e-01,  4.5092e-02,\n",
       "                       9.7851e-02,  8.0201e-02,  2.0049e-01,  5.6981e-02,  1.8395e-01,\n",
       "                      -7.0502e-02,  1.7251e-01,  4.9384e-02, -1.8912e-01, -8.6091e-02,\n",
       "                      -1.1565e-01, -1.0474e-01, -6.0941e-02, -6.9278e-02,  3.0730e-02,\n",
       "                      -9.6571e-02,  4.2648e-02, -5.3855e-02, -5.9744e-02,  1.8828e-01,\n",
       "                      -1.6970e-01, -3.1818e-03,  2.2856e-02, -2.2099e-01,  1.7335e-01,\n",
       "                      -1.6593e-01, -1.5608e-01, -1.6121e-02, -4.1846e-02, -1.3077e-02,\n",
       "                       4.2586e-02, -1.7979e-01,  1.4189e-01, -8.1466e-02, -1.4749e-01,\n",
       "                      -3.4693e-02,  4.2564e-02, -2.8455e-02, -4.6571e-02, -3.2937e-02,\n",
       "                      -4.5704e-02, -4.3389e-02,  3.7131e-03, -1.2917e-02, -8.0715e-02,\n",
       "                      -1.4813e-01, -1.6060e-02,  5.7862e-02, -2.9205e-02,  8.4648e-02,\n",
       "                      -1.0778e-01, -4.7514e-02,  7.4248e-02, -4.1740e-02,  1.9464e-01,\n",
       "                      -5.3232e-02, -3.1791e-02, -4.6112e-02, -4.9970e-02, -7.8858e-02,\n",
       "                      -9.1504e-02, -7.3334e-02, -2.7308e-02, -5.5580e-02, -5.2725e-02,\n",
       "                       2.8567e-02, -1.0992e-01, -5.2876e-03, -1.6059e-01, -5.0780e-02,\n",
       "                       2.5316e-01,  1.9964e-02,  4.7627e-02,  5.2300e-02,  5.7888e-02,\n",
       "                      -7.5816e-03, -7.1216e-02, -4.4911e-02, -1.0362e-01, -4.2007e-02,\n",
       "                      -1.3139e-01,  2.6563e-02, -2.0551e-02, -4.0528e-02, -6.8844e-02,\n",
       "                       3.0593e-02, -8.6444e-02, -1.2997e-01,  5.2818e-02, -3.9376e-03,\n",
       "                      -1.9535e-01, -4.4648e-03, -1.1212e-01, -9.7724e-02, -4.8960e-02,\n",
       "                       7.6716e-02, -1.0289e-01,  6.3968e-03, -4.5100e-02, -9.7993e-02,\n",
       "                       6.9765e-02, -1.0903e-01, -1.7835e-01,  5.9568e-02, -1.6572e-01,\n",
       "                      -8.9103e-02,  1.4516e-01, -1.3870e-02,  1.0155e-01, -1.4654e-01,\n",
       "                       3.1915e-02,  1.7016e-01, -7.8266e-02, -8.3788e-02,  1.0352e-01,\n",
       "                      -7.4448e-02, -5.8855e-02, -3.2538e-02, -2.5348e-02, -1.9729e-01,\n",
       "                       9.9237e-02, -8.7082e-02, -1.4436e-01,  1.0025e-02, -8.7350e-02,\n",
       "                      -2.3954e-02, -2.5044e-02, -1.0648e-01, -8.8106e-02, -2.4613e-02,\n",
       "                      -1.4462e-01, -6.3763e-02, -1.1268e-01, -1.2365e-01,  4.9088e-02,\n",
       "                      -5.9687e-02,  6.4398e-02, -7.7066e-02, -1.4173e-01, -1.1939e-01,\n",
       "                      -4.2754e-02, -3.6653e-02,  1.8751e-02,  9.2221e-03,  6.9741e-02,\n",
       "                       1.2211e-02,  3.5231e-02, -1.1981e-02,  3.5225e-02, -5.2983e-02,\n",
       "                       8.0192e-02,  9.2001e-02, -3.6061e-02, -6.8886e-02, -4.8888e-02,\n",
       "                       9.0520e-02,  5.6100e-02,  8.1287e-02,  1.9524e-02,  1.1299e-01,\n",
       "                      -1.9321e-02,  4.6512e-02, -8.3596e-02, -3.0943e-02,  1.3005e-04,\n",
       "                      -1.3169e-01, -6.7345e-02,  1.0911e-01,  1.7326e-02,  3.6054e-02,\n",
       "                      -1.9049e-02, -1.3949e-02, -8.6536e-03,  1.0427e-02,  7.0143e-02,\n",
       "                       7.6888e-02, -6.6451e-02,  1.4515e-02, -5.9355e-03, -3.6005e-02,\n",
       "                       1.6631e-02,  1.0852e-01, -6.6893e-02, -6.0448e-02, -7.1087e-02,\n",
       "                      -6.5150e-02,  1.1559e-02,  6.3326e-02,  4.4926e-02, -3.7684e-02,\n",
       "                      -1.8050e-02, -1.0661e-01,  1.3698e-01,  4.9372e-02, -1.3574e-01,\n",
       "                       8.4195e-02,  6.4325e-02,  9.9209e-02,  6.3390e-02, -3.7386e-02,\n",
       "                      -7.0174e-03,  6.2369e-03, -7.2739e-02,  3.4189e-02, -1.0922e-01,\n",
       "                      -4.1300e-02,  6.5515e-03,  2.9231e-02, -6.8790e-02, -3.9162e-02,\n",
       "                      -1.0777e-02, -6.4242e-02,  7.3158e-02, -1.8625e-02, -7.6656e-02,\n",
       "                      -7.2717e-02,  4.8539e-02,  3.6293e-02,  4.8628e-04,  6.1837e-02,\n",
       "                      -9.3529e-02, -7.4795e-02,  1.0231e-01, -1.7705e-01, -1.8306e-02,\n",
       "                      -6.7916e-02, -3.0946e-03,  8.8151e-02, -1.3636e-02,  5.9174e-02,\n",
       "                      -7.6208e-02, -6.9878e-02,  8.2864e-02,  1.7245e-01, -4.6575e-02,\n",
       "                       1.9685e-02,  1.7139e-01,  7.2015e-02,  3.1629e-02, -5.1060e-02,\n",
       "                      -2.1006e-02,  1.5265e-03,  8.7238e-03,  6.1606e-02,  1.6356e-02,\n",
       "                      -2.1691e-02, -1.4133e-02,  1.0535e-01, -2.3784e-02,  1.5391e-02,\n",
       "                      -3.3066e-03, -3.5324e-02,  8.4912e-03, -6.2410e-02, -2.1684e-02,\n",
       "                      -4.6161e-03,  7.5225e-02, -1.1587e-02, -4.1004e-02,  5.8268e-02,\n",
       "                      -6.7183e-02,  6.8429e-02, -1.4264e-01, -6.1893e-03,  4.6169e-02,\n",
       "                       1.2875e-01,  7.8897e-02,  7.1100e-02,  1.2828e-01,  7.8516e-02,\n",
       "                       1.3478e-02, -3.2198e-02,  1.2225e-01, -6.3810e-04, -5.1822e-03,\n",
       "                       9.3994e-02,  1.1143e-01,  4.7859e-02,  1.5182e-01, -1.5045e-01,\n",
       "                       1.2331e-02,  1.2293e-01,  1.7921e-01,  5.3699e-02,  1.7110e-02,\n",
       "                      -1.5023e-01,  4.8560e-03,  8.6361e-02, -2.8694e-02,  3.0273e-01,\n",
       "                       5.8250e-02,  1.9877e-01,  1.3307e-01,  1.2570e-01,  3.6004e-02,\n",
       "                       1.1701e-02,  2.7572e-03,  1.2747e-01,  9.9950e-02,  3.8762e-02,\n",
       "                      -1.8223e-03,  1.0683e-01, -2.2281e-02,  8.8546e-02,  2.5449e-02,\n",
       "                       3.8345e-02,  7.5546e-02,  7.6603e-02,  1.4527e-01, -8.4690e-02,\n",
       "                       8.4523e-02,  3.9919e-02,  1.3665e-02,  1.7899e-01, -5.9532e-03,\n",
       "                       1.2877e-01, -5.7042e-02,  9.3485e-02,  5.6647e-02,  1.0516e-01,\n",
       "                      -2.9258e-02, -4.4440e-02,  5.4457e-02,  5.5197e-02,  1.3542e-01,\n",
       "                       8.3404e-02,  5.1819e-02,  3.8052e-02, -6.2589e-02,  2.2323e-01,\n",
       "                       1.8709e-01,  9.4975e-02,  1.4891e-01,  4.1627e-02, -6.7537e-03,\n",
       "                       1.0834e-01, -8.6768e-02,  3.9603e-02,  1.2425e-01,  1.4001e-01,\n",
       "                       1.3928e-02,  2.1199e-03,  2.2506e-01,  1.1841e-01, -5.4440e-02,\n",
       "                       8.6122e-02,  6.2922e-02,  1.2787e-01,  1.5196e-01,  6.3558e-02,\n",
       "                       8.2399e-02, -2.1572e-02,  6.3177e-02,  3.5883e-03, -6.4401e-02,\n",
       "                      -1.2610e-01,  3.2456e-02, -5.0871e-03, -1.3746e-02,  1.7747e-01,\n",
       "                      -1.3141e-02,  1.1670e-01,  1.0150e-01,  7.8452e-02,  1.2181e-01,\n",
       "                       8.9761e-02,  6.7067e-02, -6.5114e-02,  8.6953e-02,  4.4731e-02,\n",
       "                      -8.2937e-03,  3.9248e-02,  1.0089e-01,  3.7007e-02,  2.0654e-02,\n",
       "                      -7.4478e-02, -3.3141e-02,  6.4173e-02,  1.7483e-01,  1.8306e-02,\n",
       "                       1.9988e-01, -3.0605e-02,  1.0919e-01,  1.7478e-01, -3.0485e-02,\n",
       "                       8.1690e-02,  8.8936e-02,  7.1977e-02,  1.6104e-01, -7.8485e-02,\n",
       "                       2.4264e-02,  1.9704e-01])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.3984,  0.1353, -0.4508,  ..., -0.5712,  0.1809,  0.1677],\n",
       "                      [-0.3545,  0.1375, -0.7451,  ...,  0.0865, -0.0884, -0.6123],\n",
       "                      [-0.8429, -0.3154,  0.4892,  ..., -0.7225, -0.0559,  0.0133],\n",
       "                      ...,\n",
       "                      [-0.3275,  0.5677,  0.3161,  ..., -0.4596, -0.2642, -0.0026],\n",
       "                      [-0.0962,  0.0628, -0.3302,  ...,  0.4684, -0.1167, -0.3020],\n",
       "                      [ 0.0454, -0.1537, -0.1046,  ..., -0.2235, -0.2816, -0.2212]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.0735,  0.0483,  0.2867,  ...,  0.1521, -0.0767, -0.0546],\n",
       "                      [-0.0565, -0.1772,  0.2210,  ...,  0.0666, -0.3070,  0.0514],\n",
       "                      [-0.0869,  0.0117,  0.0784,  ..., -0.0893,  0.0070, -0.0052],\n",
       "                      ...,\n",
       "                      [ 0.0392, -0.0177, -0.0282,  ...,  0.0580,  0.2313, -0.1014],\n",
       "                      [ 0.2620,  0.1181, -0.1058,  ...,  0.1998,  0.0186, -0.0073],\n",
       "                      [-0.0862, -0.0498, -0.1005,  ...,  0.1181, -0.1216,  0.1500]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-0.1333, -0.0434, -0.0475, -0.0858, -0.1631, -0.1577, -0.1965, -0.1210,\n",
       "                      -0.1484, -0.1059, -0.1055, -0.2253, -0.1294, -0.1548, -0.0434, -0.0834,\n",
       "                       0.0389, -0.1405, -0.1730, -0.0518, -0.0184, -0.1476, -0.0515, -0.1111,\n",
       "                      -0.1710, -0.0316, -0.0552, -0.0441, -0.1302, -0.1194, -0.2081,  0.0498,\n",
       "                      -0.1352,  0.0582, -0.0495, -0.0076, -0.0064, -0.0465, -0.1154,  0.0626,\n",
       "                      -0.1094,  0.0433, -0.0665, -0.2499, -0.0438, -0.1773, -0.1225, -0.1389,\n",
       "                      -0.1254, -0.1661, -0.1767,  0.0514, -0.0300, -0.1513, -0.0769, -0.0626,\n",
       "                      -0.1011, -0.0974, -0.1539, -0.1511, -0.1292, -0.0471, -0.1336, -0.1038,\n",
       "                       0.0402, -0.2853, -0.1219, -0.1829, -0.0776, -0.0928, -0.0760, -0.1044,\n",
       "                      -0.2177,  0.0116, -0.0665, -0.1590, -0.1668, -0.1046, -0.1130, -0.1169,\n",
       "                      -0.1054,  0.0796,  0.0209, -0.1353, -0.0830, -0.2137, -0.2054, -0.0997,\n",
       "                      -0.1344, -0.0082, -0.2653, -0.1265, -0.1594, -0.0903, -0.1654, -0.0824,\n",
       "                      -0.1867,  0.0421,  0.0253, -0.0044, -0.2662, -0.1056, -0.1719, -0.1806,\n",
       "                      -0.0859, -0.2278, -0.0756, -0.0924,  0.0314, -0.0645, -0.0320, -0.1345,\n",
       "                      -0.2007, -0.1822, -0.0011, -0.0765, -0.0353, -0.0807, -0.0949, -0.2421,\n",
       "                      -0.0598, -0.0234, -0.2340, -0.0379, -0.0071, -0.1710, -0.1895, -0.0869,\n",
       "                      -0.1749, -0.1996, -0.1302, -0.0213, -0.1111, -0.1644,  0.0315, -0.0654,\n",
       "                      -0.1504, -0.1137,  0.1286, -0.0672, -0.0494, -0.0567,  0.0239, -0.1012,\n",
       "                       0.0224, -0.0878, -0.0376, -0.0877, -0.0412, -0.1165, -0.1169, -0.2780,\n",
       "                      -0.0938, -0.0373,  0.0176, -0.0611,  0.1555, -0.0790, -0.1141,  0.0738,\n",
       "                      -0.1546, -0.0858, -0.0865, -0.1593, -0.0673, -0.1105, -0.1003,  0.0786,\n",
       "                      -0.1102, -0.0477,  0.0088, -0.0088, -0.1126,  0.0899, -0.2712,  0.0607,\n",
       "                      -0.1227, -0.1490, -0.0154, -0.0475, -0.0571, -0.1301, -0.0560, -0.0567,\n",
       "                      -0.0757,  0.0188,  0.0372, -0.0905, -0.0272, -0.0644, -0.0632, -0.1570,\n",
       "                      -0.0364,  0.0999, -0.1567, -0.2033, -0.1045, -0.1207, -0.0028, -0.1649,\n",
       "                      -0.0668,  0.0599, -0.1239, -0.1595, -0.0040, -0.0610, -0.1820, -0.1462,\n",
       "                      -0.0399, -0.0132, -0.0499, -0.0976, -0.0257, -0.0640, -0.0578,  0.0930,\n",
       "                      -0.0591, -0.1185, -0.0346,  0.0986, -0.0872, -0.1289, -0.0433, -0.1144,\n",
       "                      -0.0815, -0.0770, -0.0897,  0.0238, -0.1231, -0.0677, -0.1388, -0.0584,\n",
       "                      -0.0731, -0.0908, -0.1321, -0.1776,  0.0258, -0.1582,  0.0810, -0.1754,\n",
       "                      -0.0229,  0.0322, -0.1860, -0.0122,  0.0656,  0.0142,  0.0885, -0.1004,\n",
       "                      -0.0064, -0.0369, -0.2136, -0.0161, -0.0197, -0.0665, -0.1034,  0.0047,\n",
       "                       0.0785,  0.0234, -0.0260, -0.0107,  0.0740, -0.0321, -0.0199, -0.0742,\n",
       "                       0.1252, -0.0637, -0.0819,  0.0531,  0.0095,  0.0532, -0.0508,  0.0911,\n",
       "                       0.0191,  0.0808,  0.1239, -0.0042,  0.1173,  0.0242,  0.0850, -0.0430,\n",
       "                       0.0125,  0.0008,  0.0095,  0.0711,  0.0142, -0.0707, -0.0120,  0.0029,\n",
       "                      -0.1291, -0.1323,  0.1488,  0.0038, -0.1326, -0.0192,  0.0218,  0.0230,\n",
       "                       0.0841,  0.1170, -0.0880, -0.0862, -0.0730,  0.0158, -0.0196,  0.0811,\n",
       "                      -0.0509,  0.0999, -0.0986,  0.0257,  0.0453,  0.0553, -0.0627,  0.0656,\n",
       "                       0.0145,  0.0083,  0.0437,  0.0580, -0.0924, -0.0650, -0.0216, -0.0936,\n",
       "                       0.0588, -0.0086,  0.0544, -0.0357, -0.0687, -0.0101, -0.0015,  0.1600,\n",
       "                       0.0762, -0.1334, -0.0048,  0.1195, -0.0165,  0.0537,  0.1734, -0.0887,\n",
       "                       0.0520,  0.0205, -0.0891, -0.1428,  0.0523, -0.0108,  0.0326,  0.1149,\n",
       "                      -0.1304, -0.0310, -0.0020, -0.0485, -0.1755, -0.1410,  0.0689, -0.0572,\n",
       "                      -0.0263,  0.0871,  0.0643,  0.0348,  0.0111,  0.0225, -0.0286, -0.0237,\n",
       "                      -0.0247, -0.0523, -0.0366,  0.0115, -0.1389, -0.0569,  0.0083, -0.0870,\n",
       "                       0.0654, -0.0425, -0.0649, -0.0530, -0.0716, -0.0768, -0.0059, -0.0453,\n",
       "                       0.0046,  0.0519,  0.0504, -0.1245,  0.1012, -0.0343, -0.1415,  0.0204,\n",
       "                      -0.1740, -0.1918, -0.0155,  0.0923, -0.1000, -0.0540, -0.0245, -0.1062,\n",
       "                      -0.1495,  0.0288, -0.1048, -0.1194, -0.1589, -0.0745, -0.0242, -0.1187,\n",
       "                      -0.0108, -0.1387, -0.1886, -0.0943, -0.1885, -0.1407, -0.1323, -0.0932,\n",
       "                      -0.1552,  0.0298, -0.0244, -0.0462, -0.1197, -0.1271, -0.1067, -0.0383,\n",
       "                       0.0570, -0.0709, -0.1280, -0.0523, -0.0243, -0.0949, -0.0875, -0.0658,\n",
       "                      -0.1305, -0.1289, -0.0360, -0.0479,  0.0632, -0.0962, -0.1490, -0.1675,\n",
       "                      -0.1181, -0.0905, -0.1072, -0.1222, -0.0472, -0.0562, -0.0707, -0.0618,\n",
       "                       0.0105,  0.1359, -0.1150, -0.0584, -0.1568, -0.0473, -0.2145, -0.0628,\n",
       "                      -0.0554, -0.0671, -0.1290, -0.1696,  0.0501, -0.0710, -0.0318, -0.0985,\n",
       "                      -0.1573,  0.0032, -0.1399, -0.1159, -0.0876, -0.0900, -0.0518, -0.0299,\n",
       "                       0.0791,  0.0131, -0.0399, -0.1356, -0.0771, -0.0905, -0.1032,  0.0738,\n",
       "                      -0.0882, -0.1013, -0.1548, -0.1054, -0.1089, -0.0819, -0.1788, -0.1349,\n",
       "                      -0.0625,  0.0094,  0.0114,  0.0221, -0.0942, -0.0205, -0.1692, -0.1266,\n",
       "                      -0.2323, -0.1682, -0.0718, -0.1668, -0.0117, -0.0714,  0.0485, -0.0966,\n",
       "                      -0.1848, -0.0060, -0.1160,  0.0370, -0.1299, -0.0280, -0.0391, -0.1509,\n",
       "                      -0.1200, -0.1074, -0.1006,  0.0372, -0.0189,  0.1049, -0.0415,  0.0113])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-1.4344e-01, -1.8751e-01, -3.2270e-02, -1.0417e-01, -1.3269e-01,\n",
       "                      -1.2300e-01, -1.2978e-01, -1.2115e-01, -1.7676e-01, -1.4283e-01,\n",
       "                      -2.5862e-02, -2.6074e-01, -1.2048e-01, -9.5930e-02, -1.9895e-01,\n",
       "                      -1.4677e-01,  7.9051e-02, -1.4993e-01, -4.7753e-02, -1.6052e-01,\n",
       "                      -1.9924e-01, -1.3090e-01, -6.3836e-03, -9.0057e-02, -8.4649e-02,\n",
       "                      -1.2192e-01, -2.1353e-01, -4.0719e-02, -3.8154e-02, -6.8489e-02,\n",
       "                      -2.3853e-02, -1.0228e-01, -8.7292e-02, -5.7081e-03, -1.3250e-01,\n",
       "                      -1.7407e-01,  1.3677e-02, -1.1759e-01, -1.8663e-01,  7.4917e-02,\n",
       "                      -2.0923e-01,  7.7961e-02,  3.0335e-02, -1.6790e-01, -2.1385e-02,\n",
       "                      -1.4363e-01, -7.6395e-02, -3.8706e-01, -8.9768e-02, -1.0655e-01,\n",
       "                      -1.1106e-01, -3.8393e-02, -1.9327e-01, -2.2162e-01, -1.0986e-01,\n",
       "                      -3.8732e-02,  6.7227e-02, -1.5783e-01, -1.2038e-01, -1.4646e-01,\n",
       "                      -1.3319e-01, -1.4088e-01, -1.8528e-01, -6.2481e-02, -3.0038e-02,\n",
       "                      -3.6726e-02, -2.0596e-01, -5.8465e-03, -4.6602e-02, -2.1409e-01,\n",
       "                      -1.5512e-02, -1.6729e-01, -7.2821e-02, -8.8114e-03, -8.0487e-02,\n",
       "                      -1.0368e-01, -1.1974e-01, -2.6986e-02, -2.5605e-01, -6.7092e-02,\n",
       "                      -6.3832e-02, -2.3976e-02, -3.1845e-02, -2.2142e-01,  1.3657e-02,\n",
       "                      -4.4858e-02, -1.9632e-01,  5.6148e-02, -1.7642e-01, -1.0450e-01,\n",
       "                      -1.3820e-01, -9.9589e-02, -6.5417e-02, -6.8666e-02, -1.4600e-01,\n",
       "                      -1.0373e-01, -1.9089e-01, -1.0803e-01, -1.7322e-01, -6.3476e-02,\n",
       "                      -1.1184e-01, -1.1582e-01, -1.7388e-01, -1.9905e-01, -1.3343e-01,\n",
       "                      -1.9503e-01, -1.6083e-01, -6.8125e-02, -3.8945e-02, -1.1000e-01,\n",
       "                      -8.2071e-02, -1.2679e-01, -1.0760e-01, -5.6784e-02, -1.5582e-01,\n",
       "                      -8.8770e-02, -8.4715e-02, -8.3638e-02,  8.8351e-02, -2.5379e-01,\n",
       "                      -1.7442e-01, -2.3652e-02, -1.4518e-01, -1.1061e-01, -2.5023e-02,\n",
       "                      -6.5793e-02, -8.3471e-02, -8.0874e-02,  4.1291e-03, -1.3533e-01,\n",
       "                      -1.1853e-01,  1.0655e-01, -1.8413e-01, -2.1643e-02, -5.3968e-02,\n",
       "                      -2.0552e-01, -1.0555e-01, -1.5165e-01,  2.2892e-02, -1.0398e-01,\n",
       "                      -2.5259e-02, -8.2087e-02, -1.0037e-01, -8.0451e-02,  3.9754e-02,\n",
       "                      -1.6970e-02, -9.7676e-02, -1.0574e-01, -1.0340e-01, -3.3654e-02,\n",
       "                       3.7293e-02, -1.3907e-01, -8.1303e-02, -6.5648e-02, -2.2150e-01,\n",
       "                      -8.3408e-02,  1.6908e-02, -3.7736e-03,  8.0506e-03, -1.3595e-02,\n",
       "                      -1.7228e-01, -1.8581e-01, -7.7448e-02, -8.0567e-02,  1.6735e-02,\n",
       "                      -1.5227e-01, -7.5212e-02,  8.1835e-02,  1.2158e-02, -4.9463e-03,\n",
       "                       2.3419e-02, -8.8614e-02, -8.4901e-02,  8.6463e-02,  3.7548e-03,\n",
       "                      -1.0471e-01, -1.8851e-01, -6.0859e-02, -7.6621e-02, -6.0973e-03,\n",
       "                      -9.6379e-02, -1.7549e-01, -1.2495e-01,  2.1160e-02,  7.8501e-02,\n",
       "                       9.1969e-02, -1.3974e-01, -5.4845e-02, -1.1539e-01, -1.0312e-01,\n",
       "                      -5.3540e-02, -1.1970e-01, -9.0576e-02,  1.1904e-01, -1.0146e-01,\n",
       "                      -6.8406e-02,  1.5990e-02, -7.5891e-02,  1.6007e-01, -4.5366e-02,\n",
       "                      -1.9134e-01,  1.0853e-01,  8.7588e-03, -4.2797e-02, -9.5525e-02,\n",
       "                      -6.7895e-02, -1.5449e-01, -1.6367e-01,  3.2819e-03,  1.3144e-02,\n",
       "                      -1.4936e-01, -1.3774e-01, -1.9054e-01, -1.7290e-01, -1.7991e-01,\n",
       "                      -5.8535e-02, -1.2579e-01, -9.4708e-02,  5.4733e-03,  2.4732e-02,\n",
       "                      -1.8858e-01, -1.0768e-01, -2.3426e-01, -8.4559e-02, -4.8958e-02,\n",
       "                      -9.9059e-02,  5.3597e-03,  1.0477e-02, -8.3693e-02, -3.8235e-02,\n",
       "                      -3.9424e-02, -2.1725e-01, -1.2657e-01, -1.5650e-02, -2.2082e-01,\n",
       "                      -1.5764e-01,  3.2126e-02,  1.5160e-02, -4.4537e-02, -3.8165e-02,\n",
       "                      -1.2993e-01,  7.2438e-02, -9.4181e-02,  1.3798e-01, -8.4462e-02,\n",
       "                       8.4644e-02,  1.0854e-01, -1.7666e-01, -1.5243e-02, -5.2512e-02,\n",
       "                      -7.0585e-02, -7.1224e-02, -8.7494e-02, -1.5015e-01, -1.7375e-01,\n",
       "                      -1.0236e-01, -1.2034e-02, -1.0905e-01, -7.2222e-03,  1.4299e-01,\n",
       "                       1.3679e-02,  7.4969e-02, -1.1511e-01,  4.0834e-03,  7.3524e-02,\n",
       "                      -1.0306e-01, -9.5655e-04, -1.3896e-02,  9.6549e-03,  1.1400e-01,\n",
       "                      -2.1083e-02,  1.3764e-01,  6.7733e-02,  1.0386e-01,  1.2340e-01,\n",
       "                       9.2857e-02,  8.0360e-02,  6.6405e-02,  4.0283e-03, -6.3662e-02,\n",
       "                       3.1328e-02,  1.1781e-01,  6.0290e-02, -4.6710e-02,  8.1997e-02,\n",
       "                      -3.9173e-02,  2.0823e-01, -2.1409e-02, -1.5481e-03, -1.0271e-02,\n",
       "                       1.4808e-02, -6.9444e-02, -1.6908e-01, -3.8328e-02, -8.4383e-02,\n",
       "                      -4.4307e-02,  2.4287e-02,  1.4555e-02,  1.1225e-02,  3.4221e-02,\n",
       "                       4.7047e-02, -4.8757e-02, -9.2760e-02, -1.0058e-01, -9.2394e-02,\n",
       "                      -1.1625e-01,  7.4499e-02,  5.3243e-03, -3.0570e-05, -4.0814e-02,\n",
       "                      -2.0338e-02,  1.1153e-01, -4.8161e-02, -8.4829e-02,  6.7128e-02,\n",
       "                       9.6126e-02,  7.5721e-02,  2.7889e-02, -6.8687e-02, -9.6624e-02,\n",
       "                       6.5660e-02, -3.8712e-03,  5.3480e-02,  6.9647e-02,  2.6017e-02,\n",
       "                      -4.2124e-02, -4.7999e-02,  7.0715e-02,  6.1177e-03, -4.5186e-02,\n",
       "                       6.1136e-02, -1.6324e-02, -4.5207e-02,  5.6928e-03,  4.2602e-02,\n",
       "                      -4.5329e-02,  4.3643e-02,  7.4663e-02,  5.2628e-04, -6.5475e-02,\n",
       "                      -2.2597e-02,  1.7210e-01, -1.3086e-02, -2.3127e-04, -9.2606e-02,\n",
       "                       3.8899e-02,  7.4400e-03, -5.6188e-02, -9.9580e-03, -3.1259e-02,\n",
       "                      -6.3039e-02,  9.3451e-02,  6.7875e-02,  2.3882e-01, -7.4576e-02,\n",
       "                       1.2437e-02, -5.2861e-02,  5.3445e-02,  6.9122e-02,  3.6673e-02,\n",
       "                      -4.5585e-02, -5.0429e-02, -6.9855e-02,  4.4939e-02,  1.6648e-02,\n",
       "                       8.4675e-02,  8.9873e-02,  1.6335e-01,  1.0017e-01,  3.0431e-02,\n",
       "                       4.8213e-02,  3.9459e-02, -4.4037e-02, -5.0195e-02,  6.9971e-02,\n",
       "                       6.7840e-02, -1.1354e-01, -3.7379e-02, -7.5633e-02, -7.6683e-02,\n",
       "                      -1.4436e-01,  9.5124e-02, -2.1650e-02,  2.1880e-02, -1.7463e-01,\n",
       "                      -1.8469e-01, -6.6564e-02, -1.1298e-02, -1.3405e-01, -3.1857e-02,\n",
       "                       7.1122e-02, -1.2766e-01, -3.0170e-01, -7.6891e-02, -6.6230e-02,\n",
       "                      -9.0923e-02, -1.9527e-01, -1.4157e-01, -6.7059e-02, -2.3531e-02,\n",
       "                      -9.4788e-02, -9.7679e-02, -1.0480e-01, -9.6737e-02, -1.2589e-01,\n",
       "                      -1.2491e-01, -1.7736e-01,  3.7116e-02, -1.8175e-01, -9.7790e-02,\n",
       "                       3.8979e-02, -6.2836e-02, -1.4825e-01, -8.7411e-02, -6.3642e-02,\n",
       "                      -6.7960e-02, -5.5563e-02, -8.5483e-02, -1.1294e-01, -5.4427e-02,\n",
       "                      -4.5453e-02, -1.3256e-01,  1.1140e-02, -5.6974e-02, -6.0743e-02,\n",
       "                       1.6233e-03, -3.2872e-02, -1.3951e-01, -6.1771e-02, -2.0316e-01,\n",
       "                      -6.4434e-02, -1.2704e-01, -1.2066e-01, -9.5905e-02, -1.0299e-01,\n",
       "                      -1.1011e-01, -2.9088e-01, -2.2573e-01, -1.6524e-01, -6.7204e-02,\n",
       "                      -4.6672e-03,  1.1473e-02, -1.6829e-02, -1.3876e-01, -4.7350e-03,\n",
       "                      -6.2478e-02, -1.5352e-01, -8.9429e-02,  4.4472e-02,  2.4631e-02,\n",
       "                      -9.7878e-02,  9.0992e-03, -1.8308e-02, -1.7312e-01, -4.5466e-02,\n",
       "                      -6.7430e-02, -1.5114e-01, -3.3981e-02, -6.2535e-02, -9.1267e-02,\n",
       "                      -1.4955e-01, -8.8213e-02, -7.6001e-02, -1.1102e-01, -8.4044e-02,\n",
       "                      -3.4562e-02, -6.2506e-02, -3.9251e-02, -1.0059e-01, -1.1716e-01,\n",
       "                      -1.8857e-01, -4.0283e-02, -1.1502e-01, -4.6869e-02, -1.5340e-01,\n",
       "                      -1.2805e-03, -2.9462e-02, -3.3655e-02, -1.4942e-01, -1.3084e-02,\n",
       "                      -4.8631e-03,  4.1004e-02, -1.0730e-01, -5.9707e-02, -8.0332e-02,\n",
       "                      -1.3564e-01, -1.1380e-01, -9.2697e-02, -4.1950e-02,  8.9229e-02,\n",
       "                      -9.4267e-02, -1.3600e-01, -6.3308e-02, -1.1143e-01,  3.3186e-02,\n",
       "                      -2.1777e-02, -1.0885e-01, -5.9664e-02, -1.6265e-01, -2.0709e-01,\n",
       "                       6.9419e-02,  1.1741e-02,  1.3356e-01, -1.9869e-01, -1.2636e-01,\n",
       "                      -7.0039e-02, -1.4219e-01, -6.0862e-02, -3.0317e-02, -6.2869e-02,\n",
       "                      -9.6143e-02,  1.8942e-02])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.5607, -0.1328,  0.0931,  ...,  0.5006,  0.6271, -0.3399],\n",
       "                      [ 0.4630, -0.4652,  0.6690,  ..., -0.3926,  0.5066,  0.1162],\n",
       "                      [-0.0515,  0.3609, -0.3620,  ...,  0.1627,  0.3742,  0.0503],\n",
       "                      ...,\n",
       "                      [ 0.3010,  0.0508, -0.1873,  ...,  0.4592,  0.0080,  0.2215],\n",
       "                      [ 0.3719,  0.1729, -0.2023,  ...,  0.5456,  0.1541, -0.2407],\n",
       "                      [-0.1914, -0.0744, -0.4500,  ..., -0.0323, -0.4575, -0.0662]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1796,  0.0817,  0.0893,  ...,  0.0420,  0.1447,  0.3877],\n",
       "                      [-0.1121,  0.2299, -0.2036,  ..., -0.3588,  0.3891,  0.0043],\n",
       "                      [ 0.1766,  0.0456, -0.0777,  ..., -0.2285,  0.0615,  0.0646],\n",
       "                      ...,\n",
       "                      [-0.1838,  0.1089,  0.2119,  ...,  0.2011,  0.0086,  0.0592],\n",
       "                      [-0.1028,  0.2741,  0.0389,  ...,  0.2744, -0.2974,  0.0453],\n",
       "                      [-0.0340,  0.0604,  0.0932,  ...,  0.2493,  0.0530, -0.2902]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 5.4344e-02,  5.0452e-02,  2.4835e-01,  4.2986e-01,  1.5364e-01,\n",
       "                       1.8144e-02,  1.4223e-01,  2.9553e-01,  1.9415e-01,  1.3913e-01,\n",
       "                       1.6469e-01,  2.3117e-01,  1.3068e-01,  8.3867e-02,  1.6222e-01,\n",
       "                       2.4701e-01,  5.2228e-02,  4.2671e-02,  1.5321e-01,  8.4411e-02,\n",
       "                       3.8769e-02, -4.6365e-02,  1.2119e-01,  1.3114e-01,  9.1854e-03,\n",
       "                       7.8755e-02,  9.3511e-02,  1.1131e-01,  1.3807e-01,  3.2239e-03,\n",
       "                       3.2153e-02, -9.3898e-03,  2.2071e-01, -1.2831e-02,  1.2578e-01,\n",
       "                       1.1010e-01,  2.2354e-01,  1.5207e-01,  3.4878e-02,  8.7076e-02,\n",
       "                       1.4052e-01,  2.5721e-01,  1.7658e-01,  1.0654e-01,  2.3281e-01,\n",
       "                       1.3697e-01,  2.2967e-01,  2.3489e-01,  1.2805e-01,  1.3904e-01,\n",
       "                       2.3262e-01,  1.5757e-01,  1.2813e-01,  3.8943e-02, -4.3242e-02,\n",
       "                       2.2960e-01,  6.1853e-02,  1.0631e-01,  1.2390e-01,  9.0961e-02,\n",
       "                       2.2567e-01,  8.3133e-02,  2.9104e-01,  1.6443e-01,  1.5988e-01,\n",
       "                      -1.9073e-02,  1.6329e-01,  5.1817e-02,  1.0960e-01,  3.1132e-02,\n",
       "                       1.3800e-01,  3.1637e-02, -1.0643e-01,  1.6522e-01,  2.5529e-01,\n",
       "                       1.0225e-01,  4.7747e-02,  1.9448e-01,  1.6234e-01,  2.1245e-01,\n",
       "                       2.4087e-01,  3.7538e-02,  2.2910e-01, -4.4560e-04,  3.0506e-01,\n",
       "                      -2.6875e-02,  6.5682e-02,  1.7939e-01,  8.7234e-02,  2.3259e-01,\n",
       "                       1.5257e-01,  2.4236e-01,  3.5284e-02,  1.6676e-01,  6.7401e-02,\n",
       "                       1.8483e-01,  3.4708e-02,  1.0579e-01, -3.5004e-02,  1.1124e-01,\n",
       "                      -1.9778e-02,  1.2829e-01,  2.8338e-01,  1.8143e-01,  2.0573e-01,\n",
       "                       2.4606e-01,  9.1295e-02,  7.0501e-02,  2.6686e-01,  1.5917e-02,\n",
       "                       1.4822e-01,  1.8216e-01,  1.1489e-01,  8.1375e-02, -8.4146e-03,\n",
       "                       7.6132e-02,  7.6199e-02,  3.0060e-01,  1.5235e-01,  1.4380e-01,\n",
       "                      -5.9005e-03,  1.5886e-01,  3.2487e-01,  8.0293e-02,  1.1097e-01,\n",
       "                       1.3189e-01,  1.0752e-01,  9.2529e-02,  5.3995e-02,  1.5694e-01,\n",
       "                      -1.2172e-02, -6.4531e-02,  1.5903e-01, -1.2068e-01,  1.1568e-01,\n",
       "                       8.0934e-02,  5.5488e-02,  1.3156e-02,  9.6251e-02,  1.8676e-01,\n",
       "                       6.9719e-02, -7.0628e-03,  9.7020e-02,  1.5247e-01, -2.6127e-02,\n",
       "                       1.1762e-01,  2.8367e-02,  3.8794e-02,  6.6216e-02, -3.1032e-02,\n",
       "                       1.4598e-01, -7.2335e-02, -1.2800e-01,  1.5772e-01,  5.0790e-03,\n",
       "                       9.4091e-02,  9.0823e-02,  1.8491e-01, -1.4475e-01, -3.1504e-02,\n",
       "                       6.3891e-02,  3.2680e-03, -4.2757e-03, -1.5013e-01,  1.6206e-02,\n",
       "                       1.1194e-01,  4.3209e-02, -3.0372e-02,  1.1247e-01, -3.8888e-02,\n",
       "                      -2.9403e-03,  8.1552e-02, -4.0231e-03, -5.6191e-02,  1.2354e-01,\n",
       "                      -8.6351e-02, -3.6879e-02,  8.6252e-02,  1.4369e-01, -6.0903e-02,\n",
       "                       1.5058e-01,  5.3801e-02, -6.0432e-02,  3.0515e-02,  3.1995e-02,\n",
       "                       1.0447e-03, -8.7838e-02,  7.5802e-02,  9.2515e-03, -5.6899e-02,\n",
       "                       1.5257e-01, -1.1276e-02, -3.9282e-02,  4.5141e-02,  1.9472e-01,\n",
       "                       1.8992e-02,  9.0631e-02,  1.6110e-01,  4.7464e-02, -8.0677e-02,\n",
       "                       8.5769e-02,  8.9209e-02,  7.7525e-02,  2.0234e-01, -1.7853e-02,\n",
       "                       4.0082e-02, -2.9069e-03,  8.2037e-03,  2.2695e-02,  8.2237e-02,\n",
       "                       1.3665e-02, -5.2571e-02, -4.7973e-02, -2.8580e-02,  6.0634e-02,\n",
       "                       3.4299e-02,  3.5368e-02,  2.2944e-02,  2.8482e-02,  2.5788e-02,\n",
       "                       1.1523e-01,  1.0670e-01, -8.2121e-02,  2.1012e-02,  1.2214e-01,\n",
       "                       9.0590e-02,  3.0883e-02, -4.2593e-02,  8.3886e-02,  4.7966e-02,\n",
       "                       3.3115e-02,  5.2463e-02,  1.3618e-01,  1.5861e-01,  1.1685e-02,\n",
       "                       1.4974e-01, -2.4539e-02,  1.4366e-01, -5.5998e-02,  7.0103e-02,\n",
       "                      -1.0643e-03,  7.2286e-02,  1.2773e-01,  8.4739e-02,  8.9975e-02,\n",
       "                      -1.4026e-01,  1.1368e-01,  1.2190e-02, -6.3424e-02,  8.6844e-02,\n",
       "                       1.8653e-02,  6.7993e-02,  3.9845e-02, -1.7167e-02,  1.8852e-01,\n",
       "                       9.0587e-02, -8.0592e-02,  8.1142e-02, -2.2920e-02,  7.7366e-02,\n",
       "                      -9.8324e-02, -5.1371e-03,  5.8074e-02,  1.9395e-02,  4.3053e-02,\n",
       "                      -5.2826e-02,  4.3809e-02,  1.7235e-03,  1.1255e-01, -2.5106e-02,\n",
       "                       5.6884e-02,  8.6789e-02,  8.3801e-02, -2.4625e-02,  1.8884e-02,\n",
       "                       5.5373e-02, -5.4090e-02,  2.7407e-02, -4.7039e-02,  7.2994e-02,\n",
       "                       2.0934e-02, -1.1015e-02,  8.3047e-02, -5.9983e-02,  1.2513e-01,\n",
       "                       1.2562e-01,  1.8029e-01, -7.8157e-02, -1.3956e-01, -1.1560e-01,\n",
       "                      -4.5806e-02, -3.4474e-02, -8.0181e-02, -2.8154e-02,  4.5068e-02,\n",
       "                      -7.6864e-02,  8.8526e-02, -8.2020e-02,  3.2262e-03,  1.2351e-01,\n",
       "                       1.0559e-01,  4.6259e-02,  3.1950e-02,  1.0553e-02, -3.4319e-02,\n",
       "                      -3.5060e-02,  4.4366e-02,  2.5013e-02, -6.4130e-02, -1.1232e-01,\n",
       "                      -4.9819e-02, -1.5251e-01,  8.0426e-02, -2.3856e-02, -1.5295e-02,\n",
       "                       1.6263e-01,  4.8082e-02,  5.2357e-03, -2.4290e-02, -1.6134e-02,\n",
       "                      -5.1913e-02, -3.1407e-02,  5.9386e-02, -3.8413e-02,  2.1850e-02,\n",
       "                      -9.0450e-02, -6.4105e-02, -1.0736e-01, -1.0153e-01, -3.1141e-02,\n",
       "                      -1.3681e-01,  4.8635e-03, -5.3377e-02, -7.3238e-02,  7.5927e-03,\n",
       "                      -7.2209e-03, -2.1562e-02, -7.2493e-02,  6.3046e-02, -2.3655e-02,\n",
       "                       1.5882e-02,  8.7111e-02,  1.1010e-01, -1.3185e-01,  9.5875e-02,\n",
       "                       1.6796e-02,  2.9413e-02,  8.3334e-02, -6.3023e-02,  3.7710e-02,\n",
       "                      -4.1487e-03, -6.9412e-02, -1.2645e-01, -2.4532e-02, -6.2975e-02,\n",
       "                      -1.3060e-01, -1.5671e-02,  4.6671e-02, -6.4415e-02, -8.7450e-02,\n",
       "                       1.0477e-01,  1.0801e-01,  6.6094e-02,  4.9873e-02, -1.0834e-01,\n",
       "                      -1.0822e-01, -7.0079e-02, -6.3293e-03,  1.2712e-01,  8.6961e-02,\n",
       "                      -5.2275e-02, -2.4867e-02, -6.4578e-02,  1.3915e-04, -1.3539e-01,\n",
       "                      -7.5916e-02, -3.4894e-02, -8.9865e-02,  6.0855e-02, -7.3176e-04,\n",
       "                       9.9603e-02,  1.8694e-02,  7.2770e-02, -1.2215e-01,  3.5656e-01,\n",
       "                       1.4856e-01,  9.2099e-02,  4.3871e-01,  1.0690e-01,  8.6237e-02,\n",
       "                       1.7435e-01,  1.9399e-01,  1.2592e-01,  1.8825e-01,  1.9404e-01,\n",
       "                       1.3614e-01,  2.1760e-02,  1.9697e-01,  1.5074e-01,  2.3027e-01,\n",
       "                       3.5953e-02,  9.5423e-02,  1.2296e-01,  7.4955e-02,  1.0173e-01,\n",
       "                       2.2734e-01,  3.8546e-04,  1.4872e-01,  5.1836e-02,  2.6479e-01,\n",
       "                       1.8595e-01,  1.2056e-01,  1.7989e-01, -7.2927e-02,  1.8897e-01,\n",
       "                       1.9058e-02,  1.0757e-01,  8.1040e-02,  6.7554e-03,  2.7457e-01,\n",
       "                       1.0712e-01,  2.8325e-01, -5.5744e-03,  6.1899e-02,  6.0871e-02,\n",
       "                       2.9030e-01,  1.2314e-01,  2.2117e-01,  1.1500e-01,  2.7508e-01,\n",
       "                       2.4515e-01,  1.2164e-01,  1.9492e-01,  8.4007e-02,  2.2485e-01,\n",
       "                       3.1012e-01,  1.6183e-01, -1.1181e-02,  7.7708e-02,  1.3507e-01,\n",
       "                       1.5315e-02,  1.4609e-01,  2.1094e-01,  1.3644e-01,  9.5437e-02,\n",
       "                       1.5553e-01,  9.5410e-03,  8.9379e-02,  3.7696e-01, -6.8988e-02,\n",
       "                       6.1047e-02,  2.5068e-01,  2.3294e-01, -3.6839e-02,  9.9981e-02,\n",
       "                       1.1054e-01,  9.8682e-02,  1.1772e-01,  3.4853e-02, -1.6319e-02,\n",
       "                       1.1739e-01,  1.7199e-02,  7.8430e-02,  2.7017e-01,  7.9649e-03,\n",
       "                       1.1129e-01,  7.4587e-02, -1.3307e-02,  3.3346e-01, -1.5137e-01,\n",
       "                       4.4905e-02,  1.8675e-01,  1.2932e-01,  1.4886e-01,  3.3773e-01,\n",
       "                       2.7433e-01,  6.7793e-02,  2.0542e-01,  1.0325e-01,  1.8224e-01,\n",
       "                       3.0500e-01,  1.7987e-01,  1.2393e-02,  1.1455e-01,  5.8012e-02,\n",
       "                       2.8281e-01,  2.8416e-01,  7.2434e-02,  2.0800e-01, -1.0672e-02,\n",
       "                       1.7697e-01,  1.1294e-01,  4.6183e-01, -5.5741e-02,  2.0549e-01,\n",
       "                       2.4664e-02,  1.1284e-01,  2.1079e-01,  3.8059e-02,  1.1923e-01,\n",
       "                       2.3990e-02,  1.8261e-01,  1.7382e-01,  1.7710e-01,  1.2572e-01,\n",
       "                       1.4234e-01,  1.8359e-01,  6.2846e-02,  1.6123e-01,  4.3169e-03,\n",
       "                       2.7265e-01,  1.4165e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 1.0352e-01,  2.9094e-03,  3.0913e-01,  3.3543e-01,  2.4643e-01,\n",
       "                      -1.0441e-01,  1.0870e-01,  1.3019e-01,  2.1415e-01,  9.7060e-02,\n",
       "                       2.4358e-02,  2.1495e-01,  1.6665e-01,  7.1734e-02,  1.9694e-01,\n",
       "                       1.4084e-01,  3.9878e-02,  7.6200e-02,  2.1799e-01,  1.7799e-01,\n",
       "                       7.3394e-02,  1.0719e-01,  1.3379e-01,  7.4514e-02,  1.4131e-01,\n",
       "                       1.5250e-01,  1.0441e-01,  1.4269e-01,  7.1034e-02,  3.1541e-02,\n",
       "                      -2.5916e-02,  8.6284e-02,  2.1591e-01,  2.2091e-02, -3.3445e-03,\n",
       "                       2.1349e-01,  3.6294e-01, -6.7211e-02,  3.0290e-02,  1.5918e-01,\n",
       "                       1.7519e-01,  2.8848e-01,  6.5218e-02,  2.4896e-01,  1.7184e-01,\n",
       "                       8.6829e-02,  1.3455e-01,  1.8380e-01,  8.0477e-02,  4.5882e-02,\n",
       "                       2.0162e-01,  2.3097e-01,  2.1313e-01,  9.8137e-02,  1.4218e-01,\n",
       "                       8.3873e-02,  4.2163e-02, -5.1086e-02,  9.8313e-02,  3.3255e-02,\n",
       "                       2.0661e-01,  1.2244e-01,  5.3690e-02,  1.6313e-01,  1.8313e-01,\n",
       "                       2.4791e-02, -1.1105e-01,  2.1841e-01,  3.4754e-02,  1.0325e-01,\n",
       "                       2.0951e-01,  1.1414e-01,  1.2436e-01, -4.6897e-02,  2.6165e-01,\n",
       "                       1.5545e-01,  9.0352e-03,  1.3126e-01,  2.1021e-01,  1.4801e-01,\n",
       "                       6.9452e-02,  5.0836e-02,  2.1988e-01,  7.5874e-03,  3.9226e-01,\n",
       "                       7.3604e-02,  9.2843e-02,  1.6168e-01,  9.8741e-02,  2.1879e-01,\n",
       "                       1.4457e-01,  2.4399e-01,  1.2785e-02,  1.7207e-01,  1.9011e-01,\n",
       "                       1.7784e-01,  2.4551e-01,  1.6739e-01,  2.6240e-02,  1.0169e-01,\n",
       "                       4.1851e-02,  1.5386e-01,  3.0519e-01,  1.3382e-01,  1.1029e-01,\n",
       "                       1.1424e-01,  1.9091e-01,  8.9187e-02,  4.4908e-01,  1.9342e-01,\n",
       "                       2.1320e-01,  2.8677e-01,  1.2868e-01,  1.5905e-01,  1.2521e-01,\n",
       "                       1.1519e-01, -3.2874e-02,  1.7422e-01,  8.2903e-02,  1.5752e-01,\n",
       "                       6.1505e-02,  6.0925e-02,  2.1808e-01,  1.1412e-01,  2.3014e-01,\n",
       "                       1.0675e-01,  2.0708e-01,  6.4106e-02,  9.3045e-03,  1.1255e-01,\n",
       "                       5.8271e-02, -1.2178e-01, -8.7083e-03, -9.2396e-02,  1.5361e-01,\n",
       "                       1.0955e-01,  1.8749e-02,  7.0708e-03, -1.2301e-01, -1.1609e-02,\n",
       "                       8.1443e-02,  2.8534e-02,  3.7876e-02,  4.4952e-02, -9.1810e-02,\n",
       "                       1.1118e-01,  1.1765e-01, -8.3643e-02, -3.4526e-02, -5.5519e-02,\n",
       "                       7.9145e-02,  7.1546e-02, -1.6417e-02,  1.2227e-01,  1.1812e-01,\n",
       "                       1.0321e-01,  1.0894e-01,  2.1199e-01, -4.5104e-02,  5.9914e-02,\n",
       "                      -1.1146e-01,  7.2780e-02,  9.6011e-02, -1.4756e-01,  1.0835e-01,\n",
       "                       1.1648e-01, -7.7777e-02, -1.8100e-03,  1.8002e-01, -9.7581e-02,\n",
       "                       1.7883e-01,  5.6023e-02,  7.5772e-03,  4.3322e-02,  5.3664e-02,\n",
       "                       1.5971e-02,  3.6817e-02,  7.5900e-02,  9.6427e-02,  3.1602e-02,\n",
       "                       5.8490e-02,  1.3809e-01, -3.7825e-02,  9.4299e-02,  8.7621e-03,\n",
       "                       1.1703e-01, -1.4013e-01,  5.7913e-02,  1.5596e-01,  4.0131e-03,\n",
       "                      -9.8868e-02,  5.8271e-02,  1.9830e-04,  1.1290e-02, -4.0634e-02,\n",
       "                       1.2035e-01,  6.9940e-02,  1.1705e-01, -6.8758e-02, -6.5629e-02,\n",
       "                       6.9342e-02,  1.1351e-01, -2.5078e-02,  8.7468e-02, -7.6370e-02,\n",
       "                       1.0661e-01,  2.2485e-02, -9.1518e-03,  5.9161e-02,  5.9298e-02,\n",
       "                      -4.1053e-02,  4.4563e-02, -1.1425e-01, -1.0030e-01,  1.3770e-01,\n",
       "                      -6.7470e-02,  4.3805e-02, -6.2610e-02,  6.4431e-02,  1.2001e-02,\n",
       "                       1.1291e-01,  9.9888e-02,  2.9579e-02, -4.8678e-02,  3.8844e-02,\n",
       "                       9.6120e-02,  1.7618e-02,  1.3060e-01,  9.1593e-03,  1.7590e-02,\n",
       "                       1.2901e-01,  1.3190e-01,  7.5587e-02,  1.0618e-01, -9.4493e-02,\n",
       "                       1.3103e-01,  7.6503e-02,  5.5669e-02,  2.9844e-02, -8.5548e-02,\n",
       "                       1.1182e-01,  8.8466e-02,  1.0272e-01, -2.0896e-02,  5.6405e-02,\n",
       "                      -1.1634e-01,  8.9796e-02,  4.9148e-02,  3.6918e-02, -2.9708e-02,\n",
       "                      -6.7727e-02,  1.7030e-01, -3.0502e-02,  1.8029e-03,  4.0313e-02,\n",
       "                      -4.0748e-02,  3.5980e-02,  1.0486e-01,  4.2782e-02, -4.1477e-02,\n",
       "                      -5.6783e-02, -6.6234e-02, -1.3008e-02,  2.5089e-02,  4.2885e-03,\n",
       "                      -1.3264e-02, -5.3845e-02, -8.1925e-03,  9.2979e-02,  2.3483e-02,\n",
       "                       5.3782e-02, -4.3548e-02,  2.0834e-02, -4.8169e-02,  2.6710e-02,\n",
       "                       4.6143e-02,  6.4584e-02, -2.5227e-02, -1.5473e-01,  5.7113e-02,\n",
       "                      -4.5938e-02, -5.7388e-03,  5.5689e-02, -8.6619e-02,  3.4275e-02,\n",
       "                       9.5379e-02, -9.6269e-02, -7.4215e-02,  5.5746e-02, -1.5529e-01,\n",
       "                      -3.4959e-02, -7.1760e-02,  9.3037e-03, -3.7166e-02,  1.5087e-01,\n",
       "                      -4.4381e-02,  6.0818e-02,  4.3910e-03, -2.4332e-02,  1.1716e-01,\n",
       "                      -4.8614e-02, -7.3612e-02,  1.0589e-01,  5.9070e-02,  4.0262e-02,\n",
       "                       6.2127e-02,  2.7231e-02, -7.4502e-03, -1.9071e-02,  5.2978e-02,\n",
       "                      -1.0145e-02, -6.1339e-02, -1.0115e-02, -9.8622e-02, -1.2994e-02,\n",
       "                       1.6733e-01,  3.6863e-02, -4.0819e-02, -1.5292e-02, -7.9462e-02,\n",
       "                      -3.8599e-02,  4.3034e-02,  5.9476e-02, -4.5169e-02,  7.6102e-02,\n",
       "                      -3.8632e-02, -8.9001e-05,  4.4597e-02,  1.1386e-02,  6.7799e-02,\n",
       "                      -1.0225e-01, -5.4967e-02, -1.1115e-01, -4.9019e-03,  1.3843e-01,\n",
       "                      -2.0163e-02, -1.0835e-01,  3.5975e-02,  3.0618e-02,  5.6643e-02,\n",
       "                      -7.7147e-02, -2.4856e-03,  3.5007e-02,  1.6094e-02, -9.6879e-03,\n",
       "                       1.7382e-02, -4.8238e-03,  7.0918e-02,  1.1633e-01,  3.7927e-02,\n",
       "                      -9.6082e-03,  1.6700e-02, -7.2048e-02, -7.2887e-03,  6.7656e-02,\n",
       "                      -1.8344e-02, -1.1470e-02, -5.7462e-02, -2.4039e-02,  2.6443e-02,\n",
       "                       6.1558e-02,  4.8227e-03, -2.1660e-02,  6.7456e-02,  5.9223e-02,\n",
       "                      -7.6669e-02,  5.6739e-02,  3.0086e-02, -2.8525e-04,  1.5347e-01,\n",
       "                      -9.3142e-02, -5.7753e-03, -7.1151e-02, -5.1529e-02, -7.0447e-02,\n",
       "                       1.2393e-02, -3.4660e-02, -1.3374e-02, -1.2245e-02, -1.3253e-02,\n",
       "                      -7.0155e-03, -2.6446e-03,  4.8313e-02,  3.9500e-02,  3.1222e-01,\n",
       "                       1.7238e-01,  1.4869e-01,  2.7693e-01,  8.5154e-02,  1.4019e-01,\n",
       "                       1.7526e-01,  2.3064e-01,  8.7871e-02, -1.8134e-05,  1.7347e-01,\n",
       "                       7.7363e-02,  1.6337e-02,  2.1797e-01,  1.1190e-01,  1.2714e-01,\n",
       "                       7.6918e-02,  3.7467e-02,  2.6053e-01,  1.7318e-01,  7.8311e-02,\n",
       "                       2.0307e-01,  6.3857e-02,  1.2869e-01,  9.7437e-02,  2.4077e-01,\n",
       "                       5.2064e-02,  1.1129e-01,  1.7983e-01,  6.0705e-02,  2.4848e-01,\n",
       "                       6.0037e-02,  8.6179e-02,  4.5421e-02,  6.4395e-02,  1.7486e-01,\n",
       "                       2.9192e-01,  2.6658e-01, -5.9916e-02,  1.6358e-01,  5.2471e-02,\n",
       "                       2.2980e-01,  5.0445e-02,  1.2689e-01,  1.8634e-02,  1.7106e-01,\n",
       "                       1.2192e-01,  7.7993e-02,  2.0668e-01,  2.0389e-01,  9.4417e-02,\n",
       "                       1.8745e-01,  1.7718e-01,  8.6830e-02,  1.6833e-01,  1.4553e-01,\n",
       "                       1.0423e-01,  1.5586e-01,  1.5405e-01,  2.0603e-02,  5.4294e-02,\n",
       "                       8.3304e-02,  1.5347e-01,  9.3361e-02,  2.2728e-01,  6.9721e-02,\n",
       "                      -1.2312e-02,  2.3571e-01,  1.9539e-01,  9.4561e-02,  1.5013e-01,\n",
       "                       1.4537e-01,  1.0398e-01,  2.3030e-02,  1.4639e-01, -8.7989e-03,\n",
       "                       1.1404e-01,  1.5696e-01,  1.1516e-01,  9.3561e-02,  8.1423e-02,\n",
       "                       1.5588e-01,  2.4449e-01,  7.2163e-02,  2.2539e-01, -4.3243e-03,\n",
       "                       4.7930e-02,  2.0993e-01,  1.6852e-01,  2.8163e-01,  2.5229e-01,\n",
       "                       2.3536e-01,  6.3881e-03,  1.7355e-01,  1.4852e-01,  2.2383e-01,\n",
       "                       2.1534e-01,  1.8817e-01,  9.1485e-02,  3.6321e-02,  2.0397e-02,\n",
       "                       1.1435e-01,  3.1478e-01,  2.1856e-01,  3.1737e-01,  7.5020e-02,\n",
       "                       2.5497e-01,  1.5345e-01,  5.2271e-01,  7.5720e-02,  3.0725e-01,\n",
       "                       1.4537e-01,  1.2306e-01,  1.8289e-01,  2.9188e-04,  5.0834e-02,\n",
       "                       1.0894e-01,  2.9003e-01,  2.9177e-01,  1.1599e-01,  7.6885e-02,\n",
       "                       6.5854e-02,  2.0616e-01,  4.4646e-02,  2.6386e-01,  2.0657e-01,\n",
       "                       2.9488e-01,  8.5024e-02])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0768, -0.0140,  0.2755,  ..., -0.1413, -0.1870, -0.3093],\n",
       "                      [ 0.2347, -0.1985, -0.2542,  ..., -0.1143, -0.0378, -0.0796],\n",
       "                      [-0.0598,  0.1111,  0.1938,  ..., -0.2797,  0.1752, -0.0861],\n",
       "                      ...,\n",
       "                      [-0.0448,  0.0555, -0.0958,  ...,  0.0776,  0.0446, -0.0421],\n",
       "                      [ 0.3566,  0.1382, -0.0601,  ..., -0.0810,  0.5259, -0.1925],\n",
       "                      [ 0.3037,  0.2400, -0.1018,  ..., -0.2876,  0.1100, -0.2391]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.1712,  0.1706,  0.1209,  ...,  0.0606, -0.0481, -0.0365],\n",
       "                      [ 0.3703,  0.2277,  0.1393,  ..., -0.1826,  0.0185,  0.0103],\n",
       "                      [-0.1045,  0.2247, -0.1335,  ...,  0.0973,  0.0672, -0.0376],\n",
       "                      ...,\n",
       "                      [-0.1605, -0.0275, -0.1346,  ...,  0.2154, -0.1718, -0.1779],\n",
       "                      [-0.0009,  0.0847,  0.1480,  ...,  0.1077,  0.1090, -0.0851],\n",
       "                      [ 0.1300, -0.1362, -0.1243,  ...,  0.1612, -0.0311,  0.0040]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([-1.0574e-01, -1.7306e-01, -1.0313e-01, -2.4585e-01, -1.1710e-01,\n",
       "                      -1.8968e-01, -5.1440e-02, -1.1684e-01, -3.0059e-01, -2.3206e-01,\n",
       "                      -1.8870e-01, -7.3998e-02, -2.2188e-01, -9.7548e-02, -1.7852e-01,\n",
       "                      -9.2003e-02, -4.7816e-02, -1.3485e-01, -1.5937e-01, -6.3951e-02,\n",
       "                      -2.0071e-01, -1.2463e-01, -9.2284e-02,  2.1436e-02, -2.0284e-01,\n",
       "                      -1.1763e-01, -1.4225e-01, -6.1603e-02, -1.1879e-01, -6.4027e-02,\n",
       "                      -1.1283e-01, -1.5380e-01, -7.2573e-02, -1.4233e-01, -1.4453e-01,\n",
       "                      -2.4402e-02,  2.2803e-02, -1.9373e-01, -1.2049e-01, -2.1187e-02,\n",
       "                      -1.3582e-01, -1.8626e-01, -1.0213e-01, -2.7529e-01, -6.8818e-02,\n",
       "                      -1.1739e-01, -1.6336e-01, -2.2249e-01, -2.1873e-02, -1.3072e-01,\n",
       "                      -1.8824e-01, -1.8544e-01,  8.0716e-03, -1.4719e-01, -1.4760e-01,\n",
       "                      -1.7268e-01,  3.2549e-02, -1.3523e-01, -4.4717e-02, -6.1618e-02,\n",
       "                      -1.3790e-01, -6.5748e-02, -1.7718e-01, -8.8003e-02, -1.7188e-01,\n",
       "                      -1.4877e-01, -1.2813e-01, -1.4872e-01, -1.9209e-01, -2.4213e-01,\n",
       "                      -1.5560e-01, -1.5237e-01, -1.0360e-01, -2.3533e-01, -8.5389e-02,\n",
       "                      -1.9195e-01, -1.0953e-01, -2.5645e-01, -1.0549e-01, -1.1133e-01,\n",
       "                      -2.3802e-01, -1.4263e-01, -7.8290e-02, -2.1845e-01, -9.6970e-02,\n",
       "                      -1.5626e-01, -1.6458e-01, -7.5064e-02, -2.5498e-01, -1.0710e-01,\n",
       "                      -6.2197e-02, -1.7062e-01, -1.3341e-01, -1.0034e-01, -7.3557e-02,\n",
       "                      -2.0468e-01, -1.7636e-01, -2.1119e-01, -2.4165e-01, -1.2443e-01,\n",
       "                      -2.0432e-01,  6.3702e-03, -1.9817e-01, -1.7170e-01, -1.4285e-01,\n",
       "                      -6.0717e-03, -1.9027e-01, -2.2755e-01, -6.1192e-02, -7.8934e-02,\n",
       "                      -2.5055e-01, -1.2865e-01, -1.4093e-01, -3.2118e-01, -6.2682e-02,\n",
       "                      -1.7842e-01, -1.1961e-01, -1.9792e-01, -1.5902e-01, -2.3371e-01,\n",
       "                      -1.1209e-01, -1.8667e-01, -5.4190e-02, -1.7978e-01, -1.3018e-01,\n",
       "                      -2.8249e-01, -1.6302e-01, -7.5730e-02, -1.2608e-01, -1.4516e-01,\n",
       "                      -4.9712e-02, -6.5619e-02, -1.6813e-01, -1.7420e-02, -2.2978e-01,\n",
       "                      -1.3884e-01, -1.6463e-01, -6.7756e-02, -2.4459e-01, -5.2873e-02,\n",
       "                      -1.5388e-01, -7.0594e-02, -1.0758e-01, -1.0155e-01, -8.1131e-02,\n",
       "                      -1.1906e-01, -4.5472e-02, -3.4550e-02, -1.2263e-01, -9.8709e-02,\n",
       "                      -1.1629e-01, -1.0275e-01, -1.8918e-01, -8.9276e-02, -2.6948e-01,\n",
       "                      -1.7544e-01, -2.5147e-01, -8.0548e-02, -7.5764e-02,  2.1075e-02,\n",
       "                      -1.6034e-01, -8.1366e-02, -1.5642e-01, -7.0343e-02, -6.3743e-02,\n",
       "                      -5.4516e-02, -5.9413e-02, -2.1471e-01, -2.2809e-01,  2.5979e-02,\n",
       "                      -1.8538e-01, -1.4917e-01, -4.6915e-02, -1.1232e-01, -7.3720e-02,\n",
       "                      -7.7551e-02, -3.7932e-02, -1.6455e-01, -1.9234e-01, -9.2046e-02,\n",
       "                      -1.5099e-01, -1.5887e-01, -1.6966e-01, -3.7172e-02, -6.6555e-02,\n",
       "                      -1.9334e-01, -1.4045e-01, -1.1334e-01, -1.6562e-01, -1.7912e-01,\n",
       "                      -1.6175e-01, -5.1928e-02,  2.1584e-03, -7.9550e-02, -1.6787e-01,\n",
       "                      -2.8035e-01, -1.2473e-01, -1.5254e-01, -1.3314e-01,  4.7741e-02,\n",
       "                      -5.5097e-02, -1.5805e-01, -1.3883e-01, -7.3724e-02, -1.2757e-01,\n",
       "                      -1.0548e-01, -1.0321e-01, -3.3246e-02, -1.3328e-01, -6.1058e-02,\n",
       "                      -1.3724e-01, -1.3550e-01, -1.3518e-01, -1.4339e-01, -8.3208e-02,\n",
       "                       5.9792e-04, -2.3147e-01, -1.1626e-01, -1.9531e-01, -1.0291e-01,\n",
       "                      -1.2064e-01,  3.5956e-02, -7.8613e-02, -1.7241e-01, -2.4539e-01,\n",
       "                      -1.3371e-01, -1.3006e-01, -7.2286e-02, -2.3798e-01, -2.1647e-01,\n",
       "                      -1.4933e-01,  4.1164e-02, -9.4395e-02, -8.8647e-02, -2.8850e-01,\n",
       "                      -1.5954e-01, -1.4901e-01, -3.3830e-02, -3.3575e-02, -1.9686e-01,\n",
       "                      -1.3915e-01, -1.9763e-01, -1.8623e-01,  2.0234e-02, -1.2840e-01,\n",
       "                      -1.7764e-01, -8.8799e-02, -2.5916e-01, -3.0594e-02, -1.8842e-02,\n",
       "                      -5.2818e-02,  6.9034e-02, -8.0165e-02,  8.2393e-03, -1.1567e-01,\n",
       "                      -1.4402e-01,  3.4375e-02,  5.8732e-03, -9.0501e-02,  1.7948e-02,\n",
       "                      -1.1671e-03,  9.6396e-02,  6.1218e-02, -6.1699e-02,  3.8980e-02,\n",
       "                      -6.5924e-02, -8.7114e-02, -6.5853e-02,  1.1075e-03, -2.8007e-02,\n",
       "                      -1.3409e-03, -9.3009e-02, -4.8453e-02, -5.2787e-02, -4.9029e-02,\n",
       "                      -2.4772e-02, -1.6979e-02,  6.3110e-03,  7.5441e-02,  1.3777e-03,\n",
       "                      -2.1793e-02, -8.1313e-02, -8.7866e-02, -3.9681e-02, -1.7214e-02,\n",
       "                       4.3168e-03, -7.5183e-02, -8.8604e-03,  1.0165e-01,  6.1524e-02,\n",
       "                      -8.2345e-02, -7.7725e-03, -8.1152e-05, -4.7699e-03, -7.8485e-02,\n",
       "                       6.1026e-02,  5.9367e-02,  9.4845e-02,  4.5830e-02,  6.0526e-02,\n",
       "                       4.1409e-02, -1.2497e-01, -1.3159e-02,  7.9599e-02,  4.1347e-02,\n",
       "                      -1.3450e-01,  7.2540e-03, -1.0273e-03,  3.9094e-02,  5.0654e-03,\n",
       "                       5.1725e-02, -3.4763e-02,  2.2514e-02,  2.2127e-02, -2.4244e-02,\n",
       "                      -4.0216e-02,  8.1295e-02, -1.0900e-03,  2.8741e-02, -4.1865e-02,\n",
       "                       1.1930e-02,  4.7628e-02, -3.4036e-02,  9.8703e-02, -6.5287e-02,\n",
       "                       2.0937e-02, -3.2061e-02,  1.0221e-01, -6.9861e-02, -1.2350e-02,\n",
       "                      -2.6508e-02,  1.0972e-02,  3.0127e-02, -1.7894e-02, -1.0613e-01,\n",
       "                       1.1798e-02,  2.7362e-02, -1.2115e-01, -6.8432e-02,  2.5516e-02,\n",
       "                       1.8136e-02, -9.6496e-02, -3.4929e-02,  7.6416e-02, -1.0478e-01,\n",
       "                      -1.5323e-01,  3.0767e-02, -7.9575e-02, -2.3139e-02,  2.2904e-02,\n",
       "                       1.4146e-01, -7.9157e-02,  3.5686e-02,  8.5192e-03,  1.1645e-01,\n",
       "                      -1.6348e-02,  4.5426e-02, -5.2451e-03,  6.8000e-02,  2.6526e-02,\n",
       "                       5.9314e-02, -4.5036e-02,  1.1931e-01,  8.8039e-02,  6.9991e-02,\n",
       "                       1.2070e-01,  3.3876e-02,  8.7936e-02,  8.0994e-03,  1.4417e-02,\n",
       "                      -1.2841e-02,  7.8690e-02, -1.0657e-01, -4.7451e-02, -4.9584e-02,\n",
       "                      -9.0641e-02,  6.3438e-02,  2.1981e-02,  6.3747e-03, -3.6258e-02,\n",
       "                      -7.8206e-02,  7.3762e-02, -5.7536e-02,  6.3355e-02, -4.1537e-03,\n",
       "                      -2.2192e-01, -1.3115e-01, -7.7635e-03, -2.2861e-01, -1.8563e-01,\n",
       "                      -1.6537e-01, -4.9052e-02, -4.9258e-03, -4.4536e-02, -1.7867e-01,\n",
       "                      -5.2642e-02, -9.3195e-02,  1.0228e-01, -7.9766e-03, -1.1481e-01,\n",
       "                      -2.3558e-02, -6.5949e-04, -1.5731e-01, -6.6170e-02, -9.6507e-02,\n",
       "                      -1.2674e-01, -6.7263e-02, -1.1917e-01, -1.4364e-01, -6.3493e-02,\n",
       "                      -1.8452e-01, -1.7715e-01, -1.1957e-01, -1.4681e-01, -1.1730e-01,\n",
       "                      -1.0851e-01, -1.2763e-01, -1.3950e-01, -7.8111e-02, -1.0705e-01,\n",
       "                      -1.3010e-02, -2.3361e-02, -9.2243e-02, -5.1995e-02, -1.6884e-01,\n",
       "                      -1.6233e-01, -4.1499e-02, -1.5611e-01, -2.9060e-02, -2.6534e-01,\n",
       "                      -1.8626e-01, -1.6131e-01, -1.7335e-02, -9.0406e-02, -9.2718e-02,\n",
       "                      -5.6531e-02, -1.2658e-01, -7.6206e-02, -1.9063e-01, -4.1346e-02,\n",
       "                      -2.5152e-01, -5.3894e-02, -1.2365e-01, -9.6919e-02, -1.5249e-01,\n",
       "                      -1.0796e-01, -2.7016e-01, -2.0433e-01, -2.6525e-01, -1.0197e-02,\n",
       "                      -9.6031e-02, -2.1665e-01, -1.6451e-01, -1.3162e-01, -1.6821e-01,\n",
       "                      -1.7067e-02,  6.9099e-02, -2.9981e-01, -1.3451e-01, -5.1662e-02,\n",
       "                      -2.6564e-01, -1.3464e-01, -1.6227e-01, -1.9816e-01, -1.0878e-01,\n",
       "                      -1.1378e-01, -2.7644e-01, -1.7239e-01, -9.2000e-02, -1.8625e-01,\n",
       "                      -7.7102e-02, -8.9426e-02, -1.8384e-01, -3.7319e-02, -8.3046e-02,\n",
       "                      -1.4358e-01, -9.1770e-02, -6.8641e-02, -5.3791e-03, -1.7655e-01,\n",
       "                      -1.4551e-01, -2.4611e-01, -1.4016e-01, -9.3542e-02, -1.1963e-01,\n",
       "                      -1.9651e-01, -2.0316e-01, -7.8176e-02, -1.0741e-01, -1.5339e-01,\n",
       "                      -1.2885e-01, -2.8800e-02, -1.5060e-01, -6.2351e-02, -1.4250e-01,\n",
       "                      -2.3145e-01, -9.2197e-02, -1.4829e-01,  3.4468e-02, -1.4147e-01,\n",
       "                      -1.5184e-01, -2.4296e-01, -1.7499e-01, -1.4274e-01, -1.5893e-01,\n",
       "                      -5.6594e-02, -5.0591e-02, -1.6229e-01, -7.7378e-02, -8.6028e-02,\n",
       "                      -1.4946e-01, -8.7630e-02])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-9.1792e-02, -2.9479e-01, -1.4602e-01, -7.2441e-02, -1.0870e-01,\n",
       "                      -2.1615e-02, -1.0571e-01, -9.6532e-02, -7.2076e-02, -1.6825e-01,\n",
       "                      -1.7768e-01, -3.6459e-02, -2.3210e-01, -8.9297e-02, -1.7870e-01,\n",
       "                      -3.3697e-02, -1.5445e-02, -3.0378e-01, -1.7460e-01, -1.3297e-01,\n",
       "                      -1.6450e-01, -6.7709e-02, -1.2981e-01, -1.4474e-01, -1.0859e-01,\n",
       "                      -1.8416e-01, -1.2376e-01, -6.6811e-02, -1.3380e-01, -1.9577e-01,\n",
       "                      -9.1706e-02, -8.2646e-02, -1.6099e-01, -1.3443e-01, -2.6604e-02,\n",
       "                      -1.2776e-01,  9.6594e-03, -1.8029e-01, -9.1435e-02, -5.8426e-02,\n",
       "                      -1.9818e-01, -6.3882e-02, -1.0467e-01, -3.7555e-01, -6.2538e-02,\n",
       "                      -1.3945e-01, -1.8126e-01, -1.7748e-01, -1.3092e-01, -1.3704e-01,\n",
       "                      -2.1534e-01, -1.1724e-01, -7.7674e-02, -1.6380e-01, -1.0332e-01,\n",
       "                      -2.4504e-01, -2.1503e-01, -1.8687e-01, -1.1623e-01, -8.6289e-02,\n",
       "                      -1.2818e-01,  2.7055e-02, -1.9900e-01, -7.1671e-02, -2.0578e-01,\n",
       "                      -2.2051e-01, -3.0250e-01, -2.3244e-01, -1.9253e-01, -1.7300e-01,\n",
       "                      -3.6349e-01, -1.2507e-01,  2.7317e-04, -1.4719e-01, -1.5287e-01,\n",
       "                      -1.4518e-01, -1.8606e-01, -1.8693e-01, -2.0618e-01, -2.4591e-01,\n",
       "                      -1.2404e-01,  3.9285e-02, -1.7226e-01, -1.2776e-01, -1.6158e-01,\n",
       "                      -3.9909e-02, -7.5715e-02, -8.9642e-02, -2.8654e-01, -1.4334e-01,\n",
       "                      -1.1702e-01, -1.1167e-01, -2.7818e-01, -9.4375e-02,  6.3943e-02,\n",
       "                      -2.0641e-01, -2.1032e-01, -2.1229e-01, -1.9191e-01, -1.3059e-01,\n",
       "                      -9.4845e-02, -6.7948e-02, -1.5263e-01, -1.4934e-01, -1.3782e-01,\n",
       "                      -2.7092e-02, -7.8995e-02,  1.4999e-02, -1.1032e-01, -4.3567e-02,\n",
       "                      -1.9675e-01, -1.0840e-01, -1.0654e-01, -3.3171e-01, -8.5118e-02,\n",
       "                      -8.8248e-02, -1.2610e-01, -2.0228e-01, -1.8919e-01, -1.2581e-01,\n",
       "                      -1.8349e-01, -6.6380e-02, -2.3441e-02, -2.0748e-01, -1.6527e-01,\n",
       "                      -1.2783e-01, -1.3901e-01, -1.1360e-01, -1.3458e-02, -1.8129e-01,\n",
       "                       1.2264e-01, -5.3970e-02, -1.2285e-01, -1.2779e-01, -7.5767e-02,\n",
       "                      -1.7977e-01, -1.0688e-01, -1.3091e-01, -7.1191e-02, -2.2978e-01,\n",
       "                      -2.1639e-01, -1.7325e-02, -6.1794e-02, -1.1293e-01,  6.3340e-02,\n",
       "                       2.1176e-02, -1.7967e-01, -5.3893e-02, -1.4306e-01, -2.3616e-01,\n",
       "                      -6.2049e-02,  4.7636e-02, -1.5915e-02, -1.9194e-01, -1.9122e-01,\n",
       "                       5.7226e-02, -1.4615e-01, -1.1191e-01,  2.0564e-02, -1.1113e-02,\n",
       "                      -1.4601e-01, -6.4906e-02, -1.8779e-01, -7.2140e-02,  5.8993e-03,\n",
       "                      -1.5251e-01, -2.7418e-02, -1.7315e-03, -1.7771e-01, -1.2516e-01,\n",
       "                      -5.1045e-02, -2.5296e-01, -5.9986e-02, -8.9833e-02, -5.2786e-02,\n",
       "                      -1.1227e-01, -6.7777e-02, -1.0101e-01, -2.5710e-01, -3.1613e-02,\n",
       "                      -9.9218e-02, -2.3122e-02, -1.0172e-01, -1.2992e-01, -1.0535e-01,\n",
       "                      -1.6967e-01, -5.1468e-02, -1.2023e-01, -2.7589e-02, -2.1530e-01,\n",
       "                      -2.1512e-01, -6.0176e-02, -1.2452e-01, -1.0890e-01, -1.7490e-01,\n",
       "                      -1.5833e-01, -6.0248e-02, -2.4793e-01, -2.0733e-01, -1.1785e-02,\n",
       "                      -4.8215e-03, -1.2082e-01, -1.2850e-02, -1.5965e-01, -9.0243e-02,\n",
       "                      -1.9946e-01, -6.5665e-02, -1.3401e-02, -1.4451e-01, -7.2666e-03,\n",
       "                      -1.1952e-01, -1.2354e-01, -9.4301e-02, -4.3822e-02, -1.5312e-01,\n",
       "                      -1.8102e-01, -2.2451e-01, -8.3746e-02, -1.8207e-01, -8.2957e-02,\n",
       "                      -3.2919e-01, -1.7217e-02, -1.4027e-01, -1.0856e-01, -1.1764e-01,\n",
       "                      -8.6067e-02, -1.9424e-01, -8.6241e-03,  2.4113e-02, -2.3823e-01,\n",
       "                      -2.3590e-01,  6.7836e-02, -2.0245e-01, -1.8064e-01, -2.8036e-01,\n",
       "                      -1.8032e-01, -1.8144e-01, -1.2901e-01, -6.6144e-03, -1.4463e-01,\n",
       "                      -2.2986e-02, -8.2947e-02, -6.3866e-02, -1.7373e-01, -4.2709e-02,\n",
       "                      -8.2223e-02, -2.1478e-01, -7.9391e-02, -1.5457e-01, -8.6252e-02,\n",
       "                      -4.2014e-02, -1.2696e-01, -1.4448e-02,  1.3246e-03, -5.6898e-02,\n",
       "                      -8.6013e-02,  3.7106e-02,  4.7540e-02,  1.7766e-02, -9.4196e-02,\n",
       "                      -3.8520e-02,  9.1443e-02, -1.0625e-01, -2.2060e-02, -3.3185e-02,\n",
       "                      -1.1869e-02, -9.6923e-02, -4.1456e-02,  1.8109e-03, -8.7756e-02,\n",
       "                      -1.0886e-01, -8.7472e-03, -2.8971e-02,  6.9002e-03,  5.4376e-03,\n",
       "                       6.2349e-03,  2.8738e-02, -9.3358e-02,  9.8202e-02,  6.3309e-03,\n",
       "                      -1.0395e-01,  2.3854e-02,  2.7116e-02,  1.0593e-01,  1.0900e-02,\n",
       "                       6.3140e-02, -9.2891e-02, -6.2097e-02, -3.4017e-02, -5.9484e-02,\n",
       "                       6.0162e-02,  1.1515e-01, -4.5226e-02,  9.1471e-02, -8.6785e-02,\n",
       "                       2.0411e-02, -4.4385e-02, -6.9673e-02,  3.9228e-02, -1.3506e-02,\n",
       "                      -5.0973e-02,  6.2976e-02,  1.0310e-01, -7.9599e-02,  5.3483e-02,\n",
       "                      -1.2655e-02,  1.1479e-01,  1.0561e-01, -1.9666e-03, -1.8905e-02,\n",
       "                      -6.1331e-02,  4.1366e-02,  1.2222e-02,  4.0588e-02, -4.2627e-03,\n",
       "                       9.6982e-02,  2.6060e-02, -3.6120e-04,  6.9415e-02,  5.6359e-02,\n",
       "                      -1.1069e-01, -8.7013e-02,  2.2625e-03, -3.6742e-02,  4.9463e-02,\n",
       "                      -2.8477e-02, -1.1917e-02, -6.6656e-02,  7.1281e-02,  5.2526e-02,\n",
       "                      -5.3521e-02, -4.0305e-02, -9.0409e-02,  1.2639e-02,  4.6059e-02,\n",
       "                       2.6054e-03, -7.4114e-02, -1.7305e-02,  2.1164e-02, -4.2718e-03,\n",
       "                      -3.9922e-02, -1.0327e-01, -1.1810e-01, -4.0016e-02,  3.5782e-02,\n",
       "                       9.4421e-02,  2.0927e-02,  6.1806e-02,  1.0912e-01, -8.0353e-03,\n",
       "                      -7.4694e-02,  3.1790e-03,  1.5337e-02, -6.3463e-02, -4.8215e-03,\n",
       "                       6.0751e-02,  1.7215e-02,  2.7998e-03, -4.0786e-02, -1.2704e-01,\n",
       "                       1.8034e-02,  8.2107e-02, -1.4245e-01, -1.6688e-02, -3.5846e-02,\n",
       "                      -1.0997e-01,  1.5890e-02,  3.1125e-02,  2.5371e-03, -2.5836e-03,\n",
       "                      -6.2824e-02, -5.2442e-02, -8.7049e-02, -6.3274e-03,  5.0812e-02,\n",
       "                       9.7107e-02,  3.2679e-02, -7.2542e-02,  3.9665e-02,  1.4122e-01,\n",
       "                       5.1526e-02,  1.0944e-03,  3.8692e-02, -2.8479e-02, -1.8845e-01,\n",
       "                      -2.2905e-01, -1.3159e-01, -2.0992e-02, -4.5927e-02, -1.2726e-01,\n",
       "                      -1.4282e-01, -7.7432e-02, -1.2134e-02, -2.1766e-01, -2.3954e-01,\n",
       "                      -1.1743e-01, -6.6278e-03, -5.8025e-02, -6.7206e-02, -2.9659e-02,\n",
       "                      -6.2439e-02, -1.8821e-01, -4.7197e-03, -1.3411e-01, -1.5799e-01,\n",
       "                      -1.5067e-02, -6.1110e-02, -1.1174e-01, -1.2091e-01, -1.0344e-01,\n",
       "                      -1.4990e-01, -4.1857e-02, -1.8630e-01, -1.4612e-01, -1.2578e-02,\n",
       "                      -1.8060e-01, -1.6762e-01, -1.8192e-01, -1.0832e-01, -2.3348e-01,\n",
       "                      -1.1919e-01, -8.8848e-03, -1.5343e-01, -2.7323e-02, -1.9150e-01,\n",
       "                      -2.1016e-01, -2.4160e-01, -2.0763e-01, -1.9575e-01, -1.0890e-01,\n",
       "                      -2.1503e-01,  1.2155e-01, -1.6518e-01, -1.6161e-01, -1.0919e-01,\n",
       "                      -8.9295e-02, -2.1176e-01, -1.2169e-01, -1.4545e-01, -1.0536e-01,\n",
       "                      -1.6007e-01, -1.8311e-01, -8.5466e-02, -8.7932e-02, -2.1519e-01,\n",
       "                      -9.8933e-02, -2.6757e-01, -5.6154e-02, -1.6208e-01, -4.9327e-02,\n",
       "                      -1.1699e-01, -2.0186e-01, -2.1176e-01, -1.6679e-01, -2.0667e-01,\n",
       "                      -1.8147e-01, -1.5465e-01, -1.7455e-01,  7.3659e-02, -1.0984e-01,\n",
       "                      -1.0833e-01, -1.6857e-01, -2.4494e-01, -8.1693e-02, -2.0677e-01,\n",
       "                       6.4703e-03, -1.0239e-01, -1.5484e-01, -1.2200e-01, -2.2719e-01,\n",
       "                      -1.6300e-01,  4.2209e-02, -2.7033e-01, -9.6009e-02, -1.4377e-01,\n",
       "                      -1.8017e-02, -3.2311e-01, -1.5444e-01, -7.9704e-03, -2.7557e-01,\n",
       "                      -1.4660e-01, -1.6879e-01, -1.1272e-01, -4.2894e-02, -1.4022e-01,\n",
       "                      -1.5603e-01, -2.5641e-01, -7.8022e-02, -3.3194e-02, -6.0523e-02,\n",
       "                      -1.0898e-01, -1.2549e-01, -1.5347e-01, -2.2527e-01, -1.5447e-01,\n",
       "                      -7.6083e-02, -3.1983e-02, -1.4297e-01, -1.0849e-02, -6.9798e-02,\n",
       "                       2.9750e-02, -1.1030e-01, -6.0545e-02, -2.0706e-01, -9.4773e-02,\n",
       "                      -1.6646e-01, -8.6361e-02, -1.9940e-01, -2.2507e-01, -1.8794e-01,\n",
       "                      -1.9428e-01, -1.9159e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.4915, -0.2520, -0.3017,  ..., -0.0585,  0.0401,  0.0935],\n",
       "                      [ 0.3584,  0.3037, -0.0254,  ..., -0.3330,  0.0552, -0.3514],\n",
       "                      [ 0.2299,  0.3251, -0.0759,  ...,  0.3223, -0.1088,  0.1027],\n",
       "                      ...,\n",
       "                      [ 0.3679,  0.2689,  0.2263,  ...,  0.4076, -0.0467,  0.0278],\n",
       "                      [ 0.0995,  0.2108, -0.0029,  ...,  0.1826, -0.2003, -0.2236],\n",
       "                      [-0.2997,  0.2042, -0.5310,  ...,  0.0451, -0.1877, -0.4631]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.1813,  0.3650,  0.0741,  ..., -0.0742,  0.1593, -0.2399],\n",
       "                      [ 0.0150,  0.2246,  0.2314,  ...,  0.0942, -0.1828,  0.0094],\n",
       "                      [-0.0945, -0.0404, -0.0323,  ...,  0.1113, -0.2919, -0.0149],\n",
       "                      ...,\n",
       "                      [-0.0994,  0.0321, -0.0676,  ...,  0.0815,  0.0339, -0.0891],\n",
       "                      [-0.2811, -0.0922,  0.2132,  ..., -0.1028,  0.0575,  0.1477],\n",
       "                      [-0.0769,  0.1083,  0.2518,  ..., -0.0841, -0.1769, -0.0376]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0865, -0.0268,  0.0947,  0.1081, -0.0025,  0.0464,  0.0342,  0.0351,\n",
       "                       0.0783,  0.1472,  0.0388,  0.0429, -0.1303, -0.0590,  0.0308,  0.2301,\n",
       "                       0.0726,  0.0453,  0.1838,  0.0196,  0.2672, -0.0146,  0.1602,  0.0024,\n",
       "                      -0.1363,  0.0596,  0.0393,  0.0576, -0.1544,  0.1031,  0.2521,  0.0288,\n",
       "                       0.1322, -0.0391,  0.0348,  0.0266,  0.0138,  0.0875, -0.0319,  0.0917,\n",
       "                       0.1092,  0.0119,  0.1447,  0.2525,  0.1455,  0.0687,  0.1920,  0.1739,\n",
       "                       0.0578,  0.1107,  0.0646, -0.0026, -0.0653,  0.0142,  0.1569,  0.0223,\n",
       "                       0.0760,  0.0227,  0.0127,  0.1122,  0.1681, -0.0632, -0.0521, -0.0539,\n",
       "                       0.1662,  0.1712,  0.0820, -0.1095, -0.1104,  0.0395,  0.0786,  0.2551,\n",
       "                      -0.0492,  0.1238, -0.0528,  0.0524,  0.0409, -0.0509, -0.0873,  0.0724,\n",
       "                      -0.0216, -0.0032,  0.2921, -0.0047,  0.0159,  0.0870,  0.1147,  0.0657,\n",
       "                       0.0931,  0.0566, -0.0452,  0.1357,  0.1010,  0.1465, -0.0785,  0.0614,\n",
       "                      -0.0520, -0.0571,  0.1288,  0.1209,  0.0811, -0.2229,  0.3499,  0.1496,\n",
       "                      -0.0840, -0.0735, -0.0909,  0.1356,  0.2048,  0.0133, -0.0134,  0.0190,\n",
       "                       0.1943,  0.0590,  0.2876,  0.0676,  0.0703,  0.0442,  0.1105,  0.1968,\n",
       "                       0.0481,  0.0794,  0.0980, -0.0521,  0.1481, -0.0358,  0.0243,  0.0319,\n",
       "                      -0.0074, -0.0636,  0.0558, -0.1445, -0.0755, -0.1962, -0.1164, -0.0399,\n",
       "                      -0.0783, -0.0466,  0.0420, -0.0814, -0.0766,  0.0062, -0.1748, -0.1099,\n",
       "                      -0.1014,  0.0028, -0.2258, -0.0283,  0.0203,  0.0297,  0.0261, -0.0142,\n",
       "                      -0.1566, -0.0829,  0.0706,  0.0071, -0.1169, -0.1808, -0.1011, -0.0968,\n",
       "                      -0.1616, -0.0626, -0.1638, -0.0200, -0.0872,  0.0802, -0.0187, -0.0456,\n",
       "                       0.0256,  0.0550, -0.0771, -0.1391,  0.0178, -0.0442, -0.1325, -0.0903,\n",
       "                       0.0399, -0.1132, -0.0793, -0.0397, -0.0411, -0.0134,  0.1280, -0.0917,\n",
       "                       0.0684,  0.0453,  0.0117, -0.0413,  0.1136, -0.1835, -0.0266,  0.0084,\n",
       "                       0.0284, -0.1537, -0.1192, -0.1709,  0.1246, -0.0766,  0.0017, -0.0413,\n",
       "                      -0.1533, -0.1527, -0.1462, -0.1190, -0.0109, -0.0343, -0.1297, -0.0449,\n",
       "                       0.0882, -0.0702,  0.1430, -0.0410,  0.1793, -0.1713, -0.0129, -0.1433,\n",
       "                       0.0062, -0.0975, -0.0397, -0.0537, -0.0156, -0.0161, -0.1626, -0.0092,\n",
       "                      -0.1837, -0.0970, -0.0768, -0.0903, -0.0282, -0.1936, -0.1409, -0.1074,\n",
       "                      -0.1242, -0.0585,  0.0183, -0.0482, -0.0305, -0.1204, -0.0469, -0.1204,\n",
       "                      -0.1353,  0.0191, -0.0333,  0.1392, -0.1109,  0.0187, -0.1134, -0.0615,\n",
       "                      -0.0637,  0.0761,  0.0529, -0.0304,  0.0266, -0.1796, -0.0333,  0.0011,\n",
       "                       0.0380,  0.0928, -0.0027, -0.0265,  0.0406,  0.0145, -0.0469,  0.1223,\n",
       "                       0.0232,  0.1104, -0.0397, -0.0182,  0.0954, -0.0237,  0.0067, -0.0334,\n",
       "                      -0.1203, -0.0701,  0.1162, -0.0277, -0.0533, -0.1036,  0.0067,  0.0511,\n",
       "                      -0.0575,  0.1119,  0.0406, -0.0993,  0.1569, -0.0169,  0.0844,  0.0641,\n",
       "                      -0.0355, -0.0186,  0.1626, -0.0575, -0.0601,  0.1728,  0.0550, -0.1080,\n",
       "                      -0.0048,  0.0710, -0.1173,  0.0302,  0.0344, -0.1369,  0.0081, -0.0588,\n",
       "                       0.0061, -0.0944,  0.0699,  0.0465,  0.0747,  0.0953,  0.1397,  0.0510,\n",
       "                      -0.0385, -0.0407,  0.0160,  0.1780,  0.0311,  0.0259, -0.0026, -0.0841,\n",
       "                       0.1582,  0.1407, -0.1061,  0.0603, -0.0265, -0.0253, -0.0350,  0.0175,\n",
       "                       0.0840, -0.0612, -0.0155, -0.0663,  0.0515,  0.0502,  0.0945, -0.0483,\n",
       "                       0.0396,  0.0511, -0.0178,  0.0239, -0.0031,  0.0175, -0.1358, -0.0494,\n",
       "                       0.0697, -0.0685,  0.0704,  0.0095,  0.0200,  0.0580, -0.0507, -0.0680,\n",
       "                      -0.0817,  0.0172, -0.1051, -0.0215,  0.0357,  0.0171, -0.0538,  0.0030,\n",
       "                       0.1488, -0.0805,  0.0296,  0.0862,  0.0367,  0.0429,  0.0716,  0.0078,\n",
       "                      -0.0478,  0.0089, -0.0184,  0.1093,  0.0667,  0.0060, -0.0123, -0.1002,\n",
       "                      -0.1707, -0.0388, -0.0154,  0.0623, -0.1048,  0.1009, -0.0677,  0.1471,\n",
       "                       0.0443,  0.1185,  0.0652, -0.0199,  0.1869,  0.1243,  0.1583,  0.0045,\n",
       "                       0.0246,  0.0627,  0.1275,  0.0124,  0.0158, -0.1521,  0.0288,  0.0778,\n",
       "                       0.0320,  0.0774,  0.1274, -0.1119,  0.0403,  0.0929,  0.0018,  0.1378,\n",
       "                      -0.1334, -0.0177,  0.0898,  0.0289, -0.0111,  0.1829,  0.1960,  0.0563,\n",
       "                       0.0949, -0.0004,  0.0511, -0.1032, -0.0511,  0.2103, -0.0676,  0.0262,\n",
       "                       0.1047, -0.0460,  0.2665,  0.2167, -0.0784,  0.1036,  0.2062,  0.1077,\n",
       "                       0.0191,  0.0824, -0.0735,  0.0719, -0.1458,  0.0737,  0.2274,  0.0179,\n",
       "                       0.2147,  0.1029,  0.1262,  0.2089,  0.0653, -0.0523,  0.0375, -0.0666,\n",
       "                       0.0075,  0.0465,  0.0920, -0.1013, -0.0406,  0.1513,  0.1878, -0.0139,\n",
       "                       0.1074,  0.1379, -0.0103,  0.1825,  0.1117, -0.0022, -0.0077,  0.1136,\n",
       "                       0.0326,  0.0117,  0.1564, -0.0063, -0.0252,  0.1811,  0.0489, -0.0275,\n",
       "                       0.0323, -0.0137,  0.0383,  0.0882,  0.0335, -0.0334,  0.0575,  0.1232,\n",
       "                      -0.1497,  0.0789,  0.1626,  0.2122,  0.1711,  0.0832,  0.1633,  0.1649,\n",
       "                      -0.0842, -0.0259, -0.0131, -0.0660,  0.2116,  0.0031, -0.0275,  0.0484,\n",
       "                       0.1639,  0.1670,  0.1615, -0.0621,  0.1557,  0.1055,  0.1249,  0.1943,\n",
       "                       0.0254, -0.0020,  0.0080,  0.0494,  0.2859, -0.0646,  0.0512,  0.0242])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-3.1649e-02, -7.6712e-02, -4.9764e-02,  1.3673e-01,  1.0744e-02,\n",
       "                       8.0518e-02, -1.1849e-03,  1.1741e-01,  6.6279e-02,  1.7334e-01,\n",
       "                       1.4361e-01, -8.5577e-02, -3.3587e-02, -4.3742e-02, -5.3785e-02,\n",
       "                       2.1294e-01, -7.7433e-02,  6.3290e-02,  2.0832e-01, -4.6055e-02,\n",
       "                       1.9012e-01, -6.0083e-02,  2.4632e-01,  2.5225e-02, -1.8412e-02,\n",
       "                       7.8800e-02, -2.3299e-02,  3.1490e-02, -4.3849e-02,  2.3897e-02,\n",
       "                       1.1695e-01,  1.3143e-01,  2.1893e-01, -6.8023e-03,  1.1776e-01,\n",
       "                       1.1218e-02, -8.7559e-02,  1.3126e-01, -7.7845e-02, -3.4797e-02,\n",
       "                      -4.7742e-02, -1.3173e-02,  1.3175e-01,  3.2488e-01, -6.2122e-02,\n",
       "                       7.3019e-02,  7.0481e-02,  1.1783e-01,  4.2366e-02,  5.1032e-03,\n",
       "                       1.2260e-01, -8.1647e-02, -1.2711e-01,  7.7033e-02,  1.2377e-01,\n",
       "                       6.2371e-02,  1.2599e-01,  7.1691e-03,  7.9926e-02,  5.2056e-02,\n",
       "                       8.9262e-02,  1.9965e-02, -1.1220e-01, -1.2228e-01,  6.5209e-02,\n",
       "                       1.0161e-01,  8.7741e-02,  3.0328e-02,  1.0816e-01,  1.2778e-01,\n",
       "                       1.3753e-01,  2.5773e-01,  3.5013e-03,  2.4211e-01, -3.2817e-02,\n",
       "                       1.4088e-01,  1.0033e-01,  6.8549e-02, -1.7759e-01, -3.4266e-02,\n",
       "                       3.7501e-02, -2.0842e-02,  1.5653e-01,  1.0265e-01,  3.1496e-02,\n",
       "                      -1.9383e-03, -2.7952e-02,  1.5154e-01,  8.1895e-02,  1.6134e-01,\n",
       "                      -8.0068e-02,  9.6847e-02,  6.4880e-02,  7.9134e-02,  9.3737e-03,\n",
       "                      -8.1097e-02, -1.2034e-02,  1.0260e-02,  8.3297e-02,  1.5371e-01,\n",
       "                       7.8180e-02, -9.6129e-03,  2.2753e-01,  7.7991e-02, -1.4434e-01,\n",
       "                       5.9049e-02, -8.2756e-03,  8.5038e-02,  4.1920e-01,  6.3951e-02,\n",
       "                       9.8035e-02,  4.4379e-02,  1.5701e-01,  1.2251e-01,  2.1848e-01,\n",
       "                       7.1595e-02,  3.6652e-02,  8.2915e-02,  4.0301e-02,  2.6073e-01,\n",
       "                       5.9724e-02,  4.4526e-02,  3.4857e-03, -2.9064e-02,  2.0069e-01,\n",
       "                      -7.6974e-02, -1.7026e-02, -2.7940e-02,  5.3300e-03, -2.1138e-01,\n",
       "                      -7.2095e-02, -8.8798e-02,  1.6459e-02, -1.5214e-01, -1.4900e-01,\n",
       "                      -7.6804e-02, -9.3865e-02, -6.5644e-02, -6.5240e-03, -1.2963e-01,\n",
       "                      -1.3657e-01,  8.3425e-03, -1.7762e-01, -1.3112e-01, -1.8731e-02,\n",
       "                       4.4002e-02, -4.4319e-02,  3.7471e-02, -5.8065e-02,  3.4819e-02,\n",
       "                      -1.3243e-02, -1.4850e-01, -1.3907e-01,  3.4298e-02,  9.2198e-02,\n",
       "                       4.9702e-02, -1.2055e-01, -5.1144e-02, -1.1510e-01,  1.9926e-02,\n",
       "                       6.0801e-02, -2.0679e-02, -1.3119e-01, -1.4268e-03, -8.8021e-02,\n",
       "                       2.6553e-02, -1.3735e-02, -6.9774e-02, -3.2667e-02, -1.1547e-01,\n",
       "                       2.8580e-02, -1.0942e-01, -4.0795e-02, -3.8623e-02, -1.4481e-01,\n",
       "                      -1.6585e-01, -1.2406e-01, -1.4248e-01, -2.5248e-02, -3.2718e-02,\n",
       "                      -8.7544e-02,  3.9588e-03, -1.5306e-02,  1.5097e-02, -3.5227e-02,\n",
       "                      -2.6671e-02, -2.1387e-01, -4.4332e-03, -1.5717e-01, -7.0900e-02,\n",
       "                      -3.7197e-02, -2.7371e-02,  2.8556e-02, -1.3819e-01, -3.2317e-02,\n",
       "                      -5.5578e-02,  7.2407e-02, -2.6204e-02, -2.6248e-02, -4.4912e-02,\n",
       "                      -3.4837e-02, -1.1664e-01, -4.8921e-02, -4.8322e-02, -5.6586e-02,\n",
       "                      -1.1017e-02, -1.2888e-01,  1.1431e-02,  5.8157e-02, -5.0194e-02,\n",
       "                       7.0167e-02, -1.7443e-01,  5.5630e-02, -1.2224e-01, -4.2798e-02,\n",
       "                      -1.5221e-01, -7.6288e-02,  2.2369e-02,  4.4419e-04, -1.9010e-02,\n",
       "                      -8.3955e-02, -1.8417e-01, -1.3099e-01,  1.0624e-02, -6.4542e-02,\n",
       "                       4.7640e-03, -2.9026e-02, -6.3597e-02,  3.9728e-02, -1.0176e-01,\n",
       "                      -8.8575e-02, -1.2771e-01, -1.7666e-02, -3.2201e-02,  4.4664e-02,\n",
       "                      -8.5402e-02, -4.0513e-02, -8.0060e-03, -1.3612e-01, -7.9074e-02,\n",
       "                      -9.7129e-02,  5.1123e-02, -6.0132e-02,  1.4212e-01, -2.9109e-02,\n",
       "                      -6.9809e-02, -1.4320e-01, -2.0555e-01, -1.3106e-01,  3.1706e-02,\n",
       "                      -2.7344e-02, -8.9374e-02, -1.1926e-01, -1.0386e-01,  2.1024e-02,\n",
       "                      -2.3795e-01,  1.8465e-02,  8.5488e-02, -8.0709e-03, -8.0069e-02,\n",
       "                       6.0517e-02, -6.2568e-03, -6.9040e-02, -1.0876e-02,  2.1887e-02,\n",
       "                       1.1527e-01,  2.7715e-02,  9.4249e-02, -6.4073e-02, -5.1756e-02,\n",
       "                      -3.5649e-02, -3.2486e-02, -3.2597e-02, -9.4791e-03,  1.1257e-01,\n",
       "                      -1.1240e-02, -1.3522e-01, -3.3414e-02,  3.7101e-02, -3.8519e-02,\n",
       "                      -1.0940e-01,  7.3231e-02, -1.0013e-01,  3.1244e-02,  4.9932e-02,\n",
       "                      -1.0034e-01,  6.9171e-02,  1.1065e-01,  2.8745e-02,  2.7243e-02,\n",
       "                      -1.4233e-02, -4.2470e-02, -2.8550e-02,  1.3896e-01, -2.4879e-02,\n",
       "                      -3.8900e-02,  9.2280e-02, -1.7680e-01, -1.2748e-02,  8.2263e-02,\n",
       "                       1.5501e-01, -4.8500e-02,  3.5267e-02,  4.8213e-02, -1.4832e-02,\n",
       "                       1.0485e-02,  1.3007e-02,  5.9634e-02,  2.7591e-02, -1.2898e-01,\n",
       "                       1.4902e-01,  3.9524e-02, -1.1151e-01, -6.1125e-02,  5.4967e-02,\n",
       "                       2.7189e-02,  1.3077e-01,  4.4808e-02,  6.5097e-02,  5.4725e-02,\n",
       "                      -1.3449e-02,  3.4979e-02, -1.9285e-01,  3.6135e-02, -1.6627e-02,\n",
       "                       9.7986e-02,  5.8440e-02, -7.8705e-03, -5.1874e-02,  2.1708e-02,\n",
       "                      -7.4127e-02, -1.4563e-01,  1.3447e-02,  5.5098e-02,  4.7760e-02,\n",
       "                      -8.0343e-03, -2.5968e-03, -9.9945e-03, -3.9469e-02, -5.3358e-02,\n",
       "                      -6.5683e-02,  9.0156e-02, -2.1540e-02,  1.7608e-02, -4.3097e-02,\n",
       "                      -6.3675e-02,  3.9656e-02,  2.7005e-02,  1.7466e-02,  5.8946e-02,\n",
       "                      -1.8853e-02, -4.6687e-02, -8.7583e-02, -3.0841e-02, -1.2870e-01,\n",
       "                       6.4743e-02,  7.5557e-02, -1.1954e-01,  7.5109e-02, -3.0114e-02,\n",
       "                      -7.8965e-02, -1.9153e-02,  2.9468e-02,  2.5137e-02,  2.1245e-01,\n",
       "                       8.1845e-02,  2.2567e-02, -6.7044e-02, -1.3764e-02,  1.9369e-01,\n",
       "                       7.6303e-02, -1.4074e-01, -5.1823e-05,  2.5593e-02, -1.0659e-01,\n",
       "                      -1.8635e-03,  7.5936e-02, -9.2616e-02,  5.4797e-03, -4.1987e-02,\n",
       "                      -2.0201e-02, -6.1156e-02, -9.9317e-03,  5.0557e-02,  6.8946e-02,\n",
       "                      -4.1886e-02,  1.7002e-03, -4.4200e-02,  5.0783e-02,  8.3399e-02,\n",
       "                       7.4529e-02,  3.3537e-02,  2.2680e-04,  1.4579e-01,  2.0343e-01,\n",
       "                      -5.3053e-02, -7.0466e-02,  3.6563e-02,  4.5627e-02,  1.5015e-01,\n",
       "                       7.4365e-02,  1.4777e-02,  1.4509e-01,  1.7606e-02,  3.5584e-02,\n",
       "                       7.1782e-02,  8.7586e-02,  1.1980e-01, -5.3658e-02, -7.8075e-02,\n",
       "                       2.3620e-01, -6.8566e-02,  3.3777e-02,  6.8616e-02,  3.0112e-01,\n",
       "                       1.6240e-02,  1.6904e-01,  2.9411e-02,  1.1391e-01, -6.0733e-02,\n",
       "                      -5.5739e-02,  1.1888e-01, -2.4058e-02,  3.9144e-03,  7.3726e-02,\n",
       "                      -1.0383e-01,  2.7848e-01,  2.3082e-01,  8.4506e-02,  1.4442e-01,\n",
       "                       5.5491e-02,  2.3962e-01,  8.0004e-02,  1.3960e-01, -7.3326e-02,\n",
       "                       4.4494e-02, -3.4616e-02, -3.4506e-02,  2.0478e-01,  3.3294e-02,\n",
       "                       1.5242e-01, -3.1326e-02,  1.7630e-03,  1.7988e-01,  5.9732e-02,\n",
       "                       4.6823e-02,  1.3568e-01, -6.1010e-02, -8.3163e-02,  1.1017e-01,\n",
       "                       6.8392e-02, -7.9290e-02, -4.8032e-02,  1.2116e-01,  1.3691e-01,\n",
       "                       1.5459e-01,  9.4066e-02,  1.0857e-01,  2.4306e-02,  7.1220e-02,\n",
       "                       1.5668e-02, -4.1139e-02, -1.5921e-02,  1.8502e-01, -1.5869e-02,\n",
       "                      -5.2451e-02,  1.4260e-01,  1.8188e-01,  6.9847e-02,  7.5788e-02,\n",
       "                      -3.4521e-03,  1.3977e-02, -2.7291e-02,  9.2618e-02, -6.2691e-02,\n",
       "                       4.6833e-02,  6.7628e-02, -1.6989e-02,  4.0381e-02,  9.2700e-02,\n",
       "                      -6.9695e-02,  2.2112e-01,  3.9028e-02,  1.5397e-01,  1.3102e-02,\n",
       "                       6.8427e-02,  1.9230e-01,  1.9290e-01, -3.7015e-02,  1.1988e-01,\n",
       "                      -8.7942e-02, -5.9557e-02,  3.2500e-01, -6.7041e-02,  5.8050e-02,\n",
       "                      -6.1466e-02,  7.2830e-03,  4.4742e-02,  4.1174e-01,  7.1215e-02,\n",
       "                       4.4797e-02,  4.1423e-02,  2.1748e-01,  1.2251e-01,  6.7463e-02,\n",
       "                       1.5059e-01, -6.3210e-02,  1.1950e-03,  1.6942e-01,  8.3505e-02,\n",
       "                       1.3392e-01,  9.7254e-02])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[ 0.0135, -0.0765, -0.0224,  ...,  0.0723,  0.2658, -0.0531],\n",
       "                      [-0.1568, -0.2150, -0.0600,  ...,  0.1061,  0.0785,  0.0244],\n",
       "                      [ 0.0248,  0.0572, -0.0565,  ..., -0.2462,  0.0114, -0.0962],\n",
       "                      ...,\n",
       "                      [-0.0853, -0.0571,  0.0694,  ...,  0.2325,  0.0738,  0.1098],\n",
       "                      [-0.0214, -0.0696,  0.1371,  ...,  0.1469, -0.0386,  0.2526],\n",
       "                      [-0.0184, -0.0200,  0.0585,  ...,  0.1773, -0.0579,  0.1864]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-0.0843,  0.1378,  0.0658,  0.1290,  0.0206, -0.1418, -0.2388, -0.1108,\n",
       "                       0.0618,  0.0377, -0.0743,  0.0652, -0.0923,  0.0729,  0.0938, -0.2196,\n",
       "                       0.0876, -0.0698, -0.1109,  0.1696,  0.0951, -0.0720,  0.0876,  0.0858,\n",
       "                      -0.1813, -0.0239,  0.1492,  0.0351, -0.0925, -0.1253, -0.1894, -0.0314,\n",
       "                      -0.0346, -0.3361, -0.1390, -0.1020, -0.2614, -0.0904,  0.0563,  0.0309,\n",
       "                      -0.1927,  0.2423,  0.0726, -0.1110, -0.1358,  0.0778,  0.0776, -0.1113,\n",
       "                      -0.0094,  0.0459,  0.1024, -0.2518, -0.1170, -0.0489, -0.1828, -0.1122,\n",
       "                      -0.2070, -0.2430, -0.0080, -0.1217,  0.0357,  0.0848,  0.0690, -0.0913])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[ 0.0881,  0.0869, -0.2711,  ...,  0.0537,  0.0151,  0.0337],\n",
       "                      [-0.0236,  0.4862, -0.0339,  ...,  0.1007,  0.2231,  0.0122],\n",
       "                      [ 0.0443,  0.0426, -0.2240,  ..., -0.1380, -0.0851,  0.1626],\n",
       "                      ...,\n",
       "                      [ 0.1150, -0.1983, -0.1472,  ..., -0.2253, -0.0976, -0.2576],\n",
       "                      [-0.0598, -0.0354,  0.4369,  ..., -0.0195,  0.1173, -0.2392],\n",
       "                      [ 0.0774, -0.0167,  0.0043,  ..., -0.2543, -0.2161,  0.1284]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([-0.1397,  0.1467, -0.1006, -0.2611, -0.1864,  0.1082, -0.0240, -0.1927,\n",
       "                      -0.0817, -0.2422, -0.0409,  0.1199,  0.2745, -0.0287,  0.1693, -0.0636,\n",
       "                       0.1616,  0.2161, -0.1850, -0.0186, -0.0646,  0.2301, -0.2714, -0.1693,\n",
       "                       0.0421, -0.2096, -0.1626, -0.0761,  0.0023,  0.0701, -0.1277, -0.3317,\n",
       "                      -0.1925,  0.2089, -0.0243,  0.2152, -0.0997, -0.3386, -0.1431,  0.0364,\n",
       "                       0.0302,  0.0074,  0.1521, -0.3734,  0.2029, -0.2931, -0.0719,  0.1888,\n",
       "                       0.0390, -0.0185, -0.2273, -0.0419, -0.2117, -0.1078, -0.0136, -0.2026,\n",
       "                       0.1053,  0.0322, -0.2543, -0.0293, -0.0408, -0.0693,  0.0423, -0.2159])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[-0.3739, -0.0610, -0.2918,  ..., -0.2967,  0.0022,  0.0064],\n",
       "                      [ 0.1176, -0.0474, -0.1256,  ..., -0.1887,  0.2178,  0.3341],\n",
       "                      [ 0.0644,  0.1985,  0.0198,  ..., -0.0668,  0.1559,  0.1218],\n",
       "                      ...,\n",
       "                      [-0.1416,  0.2193, -0.1548,  ..., -0.1239,  0.1892,  0.0558],\n",
       "                      [-0.0644, -0.4979, -0.1281,  ...,  0.1252,  0.0268, -0.1890],\n",
       "                      [ 0.2396,  0.2114, -0.1335,  ..., -0.0030,  0.3017, -0.0019]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.6808, -0.0778, -0.2836,  ..., -0.0524, -0.2647, -0.1174]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
