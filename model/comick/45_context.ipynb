{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=45,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 45)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 45)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.0411188e-08,  4.5909340e-41, -5.0411188e-08, ...,\n",
       "         4.5909340e-41,  1.3690898e-17,  4.5909340e-41],\n",
       "       [ 1.0401698e-40,  0.0000000e+00,  5.2419759e-18, ...,\n",
       "         0.0000000e+00,  5.2421082e-18,  4.5909340e-41],\n",
       "       [ 1.3691533e-17,  4.5909340e-41,  1.0403240e-40, ...,\n",
       "         4.5909340e-41,  1.0404641e-40,  0.0000000e+00],\n",
       "       ...,\n",
       "       [ 1.3326991e-17,  4.5909340e-41,  1.0576440e-40, ...,\n",
       "         4.5909340e-41,  1.0577842e-40,  0.0000000e+00],\n",
       "       [ 6.2362387e-19,  4.5909340e-41,  1.3327521e-17, ...,\n",
       "         4.5909340e-41,  1.3328050e-17,  4.5909340e-41],\n",
       "       [ 1.0579383e-40,  0.0000000e+00,  6.2364538e-19, ...,\n",
       "         0.0000000e+00,  6.2366192e-19,  4.5909340e-41]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131c353a49ed4d6cba2f532982107cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=4.9092 | F1Score=0.2850\n",
      "Batch-100: NLLLoss=4.3160 | F1Score=0.2969\n",
      "Batch-150: NLLLoss=3.8761 | F1Score=0.3140\n",
      "Batch-200: NLLLoss=5.5985 | F1Score=0.3394\n",
      "Batch-250: NLLLoss=4.1595 | F1Score=0.3599\n",
      "Batch-300: NLLLoss=2.8949 | F1Score=0.3797\n",
      "Batch-350: NLLLoss=4.9429 | F1Score=0.3951\n",
      "Batch-400: NLLLoss=3.4803 | F1Score=0.4080\n",
      "Batch-450: NLLLoss=4.4500 | F1Score=0.4221\n",
      "Batch-500: NLLLoss=2.8024 | F1Score=0.4341\n",
      "Batch-518: NLLLoss=3.4996 | F1Score=0.4381\n",
      "\n",
      "Mean NLLLoss: 4.4936 | Mean F1Score: 0.3549\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0478786b2d94cdcb9df6be76e3b1c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=3.3596 | F1Score=0.5775\n",
      "Batch-100: NLLLoss=2.5762 | F1Score=0.5894\n",
      "Batch-150: NLLLoss=2.7779 | F1Score=0.5946\n",
      "Batch-200: NLLLoss=3.0564 | F1Score=0.6011\n",
      "Batch-250: NLLLoss=2.4704 | F1Score=0.6077\n",
      "Batch-300: NLLLoss=2.7496 | F1Score=0.6162\n",
      "Batch-350: NLLLoss=2.5731 | F1Score=0.6223\n",
      "Batch-400: NLLLoss=3.4567 | F1Score=0.6294\n",
      "Batch-450: NLLLoss=3.0090 | F1Score=0.6357\n",
      "Batch-500: NLLLoss=2.1327 | F1Score=0.6400\n",
      "Batch-518: NLLLoss=0.8287 | F1Score=0.6412\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 2.6601 | Mean F1Score: 0.6085\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce63093f7c2343b9ab3a9e193beb4190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.4660 | F1Score=0.7256\n",
      "Batch-100: NLLLoss=1.8661 | F1Score=0.7297\n",
      "Batch-150: NLLLoss=1.2312 | F1Score=0.7240\n",
      "Batch-200: NLLLoss=0.9283 | F1Score=0.7280\n",
      "Batch-250: NLLLoss=2.0193 | F1Score=0.7301\n",
      "Batch-300: NLLLoss=0.7845 | F1Score=0.7354\n",
      "Batch-350: NLLLoss=1.6324 | F1Score=0.7362\n",
      "Batch-400: NLLLoss=1.0957 | F1Score=0.7394\n",
      "Batch-450: NLLLoss=1.3798 | F1Score=0.7438\n",
      "Batch-500: NLLLoss=0.6400 | F1Score=0.7475\n",
      "Batch-518: NLLLoss=1.6639 | F1Score=0.7489\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 1.7467 | Mean F1Score: 0.7320\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e982a83f069941eaa44ef68803c42360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.4468 | F1Score=0.8163\n",
      "Batch-100: NLLLoss=1.0886 | F1Score=0.8178\n",
      "Batch-150: NLLLoss=2.3303 | F1Score=0.8169\n",
      "Batch-200: NLLLoss=1.0294 | F1Score=0.8164\n",
      "Batch-250: NLLLoss=1.2476 | F1Score=0.8168\n",
      "Batch-300: NLLLoss=1.6593 | F1Score=0.8167\n",
      "Batch-350: NLLLoss=0.9318 | F1Score=0.8169\n",
      "Batch-400: NLLLoss=0.7722 | F1Score=0.8174\n",
      "Batch-450: NLLLoss=0.3007 | F1Score=0.8196\n",
      "Batch-500: NLLLoss=0.5737 | F1Score=0.8210\n",
      "Batch-518: NLLLoss=0.3365 | F1Score=0.8216\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 1.1285 | Mean F1Score: 0.8179\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bee4d0d14744a4cba5708b066d4baa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.5448 | F1Score=0.8919\n",
      "Batch-100: NLLLoss=0.6659 | F1Score=0.8888\n",
      "Batch-150: NLLLoss=0.4425 | F1Score=0.8846\n",
      "Batch-200: NLLLoss=0.4820 | F1Score=0.8816\n",
      "Batch-250: NLLLoss=0.7827 | F1Score=0.8811\n",
      "Batch-300: NLLLoss=1.0251 | F1Score=0.8785\n",
      "Batch-350: NLLLoss=0.7153 | F1Score=0.8783\n",
      "Batch-400: NLLLoss=0.6445 | F1Score=0.8774\n",
      "Batch-450: NLLLoss=0.3527 | F1Score=0.8775\n",
      "Batch-500: NLLLoss=0.4609 | F1Score=0.8759\n",
      "Batch-518: NLLLoss=0.3806 | F1Score=0.8762\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.6654 | Mean F1Score: 0.8830\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e27480b79e4eac89186979b190d96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.2844 | F1Score=0.9619\n",
      "Batch-100: NLLLoss=0.0513 | F1Score=0.9588\n",
      "Batch-150: NLLLoss=0.5236 | F1Score=0.9519\n",
      "Batch-200: NLLLoss=0.1940 | F1Score=0.9517\n",
      "Batch-250: NLLLoss=0.4651 | F1Score=0.9491\n",
      "Batch-300: NLLLoss=0.1950 | F1Score=0.9477\n",
      "Batch-350: NLLLoss=0.1997 | F1Score=0.9478\n",
      "Batch-400: NLLLoss=0.6904 | F1Score=0.9463\n",
      "Batch-450: NLLLoss=0.0935 | F1Score=0.9458\n",
      "Batch-500: NLLLoss=0.3128 | F1Score=0.9441\n",
      "Batch-518: NLLLoss=0.2196 | F1Score=0.9435\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.3219 | Mean F1Score: 0.9512\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af22d38695c4bedb9518eb89b8e86c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0496 | F1Score=0.9881\n",
      "Batch-100: NLLLoss=0.0521 | F1Score=0.9881\n",
      "Batch-150: NLLLoss=0.1286 | F1Score=0.9893\n",
      "Batch-200: NLLLoss=0.1273 | F1Score=0.9890\n",
      "Batch-250: NLLLoss=0.2018 | F1Score=0.9893\n",
      "Batch-300: NLLLoss=0.0631 | F1Score=0.9887\n",
      "Batch-350: NLLLoss=0.0703 | F1Score=0.9886\n",
      "Batch-400: NLLLoss=0.2453 | F1Score=0.9885\n",
      "Batch-450: NLLLoss=0.0616 | F1Score=0.9884\n",
      "Batch-500: NLLLoss=0.0358 | F1Score=0.9886\n",
      "Batch-518: NLLLoss=0.1327 | F1Score=0.9886\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.1073 | Mean F1Score: 0.9886\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030078776e4d43f1ab32cc8608c43c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0398 | F1Score=0.9984\n",
      "Batch-100: NLLLoss=0.0346 | F1Score=0.9978\n",
      "Batch-150: NLLLoss=0.0276 | F1Score=0.9979\n",
      "Batch-200: NLLLoss=0.0177 | F1Score=0.9978\n",
      "Batch-250: NLLLoss=0.0162 | F1Score=0.9981\n",
      "Batch-300: NLLLoss=0.0258 | F1Score=0.9980\n",
      "Batch-350: NLLLoss=0.0259 | F1Score=0.9981\n",
      "Batch-400: NLLLoss=0.0229 | F1Score=0.9980\n",
      "Batch-450: NLLLoss=0.0111 | F1Score=0.9979\n",
      "Batch-500: NLLLoss=0.0200 | F1Score=0.9977\n",
      "Batch-518: NLLLoss=0.0394 | F1Score=0.9978\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0308 | Mean F1Score: 0.9980\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb577083c24d3ab2f9918af60f1be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0102 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0042 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0138 | F1Score=0.9998\n",
      "Batch-200: NLLLoss=0.0092 | F1Score=0.9991\n",
      "Batch-250: NLLLoss=0.0085 | F1Score=0.9991\n",
      "Batch-300: NLLLoss=0.0049 | F1Score=0.9992\n",
      "Batch-350: NLLLoss=0.0094 | F1Score=0.9993\n",
      "Batch-400: NLLLoss=0.0111 | F1Score=0.9993\n",
      "Batch-450: NLLLoss=0.0152 | F1Score=0.9992\n",
      "Batch-500: NLLLoss=0.0178 | F1Score=0.9992\n",
      "Batch-518: NLLLoss=0.0043 | F1Score=0.9992\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0133 | Mean F1Score: 0.9994\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c8c4dedb5f450d91c5715bb892324b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0060 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0096 | F1Score=0.9991\n",
      "Batch-150: NLLLoss=0.0052 | F1Score=0.9992\n",
      "Batch-200: NLLLoss=0.0047 | F1Score=0.9992\n",
      "Batch-250: NLLLoss=0.0040 | F1Score=0.9991\n",
      "Batch-300: NLLLoss=0.0054 | F1Score=0.9992\n",
      "Batch-350: NLLLoss=0.0062 | F1Score=0.9993\n",
      "Batch-400: NLLLoss=0.0037 | F1Score=0.9994\n",
      "Batch-450: NLLLoss=0.0072 | F1Score=0.9994\n",
      "Batch-500: NLLLoss=0.0035 | F1Score=0.9995\n",
      "Batch-518: NLLLoss=0.0036 | F1Score=0.9995\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0078 | Mean F1Score: 0.9991\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249fb2b523414bfcaf88306d5b542a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0036 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0052 | F1Score=0.9995\n",
      "Batch-150: NLLLoss=0.0058 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0033 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0041 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0021 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0052 | F1Score=0.9995\n",
      "Batch-400: NLLLoss=0.0031 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0048 | F1Score=0.9995\n",
      "Batch-500: NLLLoss=0.0008 | F1Score=0.9995\n",
      "Batch-518: NLLLoss=0.0025 | F1Score=0.9995\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0052 | Mean F1Score: 0.9995\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b39201777704b539942c793fd3cdc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0042 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0042 | F1Score=0.9991\n",
      "Batch-150: NLLLoss=0.0040 | F1Score=0.9994\n",
      "Batch-200: NLLLoss=0.0011 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0036 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.2786 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0026 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0041 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0030 | F1Score=0.9996\n",
      "Batch-500: NLLLoss=0.0057 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0053 | F1Score=0.9996\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0056 | Mean F1Score: 0.9995\n",
      "Patience = 1/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90854eeee2654abeb447ffea9175202a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0018 | F1Score=0.9997\n",
      "Batch-100: NLLLoss=0.0012 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0019 | F1Score=0.9997\n",
      "Batch-200: NLLLoss=0.0012 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0031 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0368 | F1Score=0.9995\n",
      "Batch-350: NLLLoss=0.2199 | F1Score=0.9984\n",
      "Batch-400: NLLLoss=0.9748 | F1Score=0.9846\n",
      "Batch-450: NLLLoss=0.8728 | F1Score=0.9710\n",
      "Batch-500: NLLLoss=0.4679 | F1Score=0.9641\n",
      "Batch-518: NLLLoss=0.2497 | F1Score=0.9617\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.1591 | Mean F1Score: 0.9924\n",
      "Patience = 2/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12aa826b84564cdb83d81b5e3d5c1b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1106 | F1Score=0.9397\n",
      "Batch-100: NLLLoss=0.0818 | F1Score=0.9436\n",
      "Batch-150: NLLLoss=0.0294 | F1Score=0.9524\n",
      "Batch-200: NLLLoss=0.0534 | F1Score=0.9557\n",
      "Batch-250: NLLLoss=0.1481 | F1Score=0.9579\n",
      "Batch-300: NLLLoss=0.1318 | F1Score=0.9614\n",
      "Batch-350: NLLLoss=0.0308 | F1Score=0.9631\n",
      "Batch-400: NLLLoss=0.0639 | F1Score=0.9653\n",
      "Batch-450: NLLLoss=0.0070 | F1Score=0.9671\n",
      "Batch-500: NLLLoss=0.1437 | F1Score=0.9682\n",
      "Batch-518: NLLLoss=0.0271 | F1Score=0.9685\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.1375 | Mean F1Score: 0.9562\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1b349f37084faabf38e323883815ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0112 | F1Score=0.9969\n",
      "Batch-100: NLLLoss=0.0301 | F1Score=0.9977\n",
      "Batch-150: NLLLoss=0.0101 | F1Score=0.9982\n",
      "Batch-200: NLLLoss=0.0053 | F1Score=0.9987\n",
      "Batch-250: NLLLoss=0.0022 | F1Score=0.9987\n",
      "Batch-300: NLLLoss=0.0051 | F1Score=0.9986\n",
      "Batch-350: NLLLoss=0.0031 | F1Score=0.9985\n",
      "Batch-400: NLLLoss=0.0105 | F1Score=0.9987\n",
      "Batch-450: NLLLoss=0.0047 | F1Score=0.9989\n",
      "Batch-500: NLLLoss=0.0051 | F1Score=0.9990\n",
      "Batch-518: NLLLoss=0.0036 | F1Score=0.9990\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0098 | Mean F1Score: 0.9982\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81414333ed17461994a73e338fb8bbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0023 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0016 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0018 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0024 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-300: NLLLoss=0.0023 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0014 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0013 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0019 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0018 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0015 | F1Score=0.9999\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0019 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d339f3df0641d393f31e71b0dc5118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0017 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0011 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0005 | F1Score=0.9997\n",
      "Batch-200: NLLLoss=0.0017 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0010 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0007 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0010 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0019 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0008 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0014 | F1Score=0.9998\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0019 | Mean F1Score: 0.9998\n",
      "Patience = 3/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177c40be3c5848fab38c806df2eb9fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0012 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0007 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9998\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0015 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a87bbf049c946a2bd9c6d7b40de15ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0010 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0010 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-250: NLLLoss=0.0010 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0003 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0007 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0009 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0009 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0010 | F1Score=0.9998\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0015 | Mean F1Score: 0.9998\n",
      "Patience = 4/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ec9f6873de4e209461af4d8aef0ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-250: NLLLoss=0.0003 | F1Score=1.0000\n",
      "Batch-300: NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-350: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0011 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0008 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0010 | F1Score=0.9998\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0011 | Mean F1Score: 1.0000\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0011\n",
      "Best F1Score      : 1.0000\n",
      "Training duration : 29.535 minutes.\n",
      "Training date     : 2022-10-11 14:39:31.899657+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAFhCAYAAAAImPmbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABQEUlEQVR4nO3deXxU5dn/8c+VjX1fAgKyCYhIAMXdaopal1attj/XYt3K08WntdVWbftYa7WtbbXa1i7Ura5oq1VqXVHjijshCggCIovshCVs2a7fH+dEx5iEkGTmzJn5vn3Na84253xnMubmyrnPfczdERERERERkfSVE3UAERERERERaZoKNxERERERkTSnwk1ERERERCTNqXATERERERFJcyrcRERERERE0pwKNxERERERkTSnwk1EJEbM7HEz+3pbb5sMZna2mT3VxPpiM1ueykzpaleflYiIiOk+biIiyWVmFQmzHYGdQE04/z/ufk/qU6WemTkwwt0XhvPFwN3uPjDCTOcCF7r74em0r3RgZgXAbKBL4s8o/DluA+r+ATHN3S+MIKKISFbJizqAiEimc/fOddNmtoTgH/cz6m9nZnnuXp3KbCJN+CGwFujSwLpxdQW4iIikhrpKiohEpK6roJldZmargNvNrIeZPWpma82sPJxOPNtRYmYXhtPnmtlLZva7cNsPzOz4Fm471MxeMLMtZjbDzG42s7sbyf28mX0lnD7MzNzMvhjOH2VmpYnHDKdfCF8+28wqzOz0hP1dYmZrzGylmZ3XxOfV08xuN7OPwvfwcMK6b5jZQjPbYGbTzWyPhHVuZt80s/fNbGP43szMRgN/BQ4JM20Mt28Xfk5LzWy1mf3VzDqE6x4zs+sT9j3NzG5rbF8NvIdzzWxx+Dl/YGZnN/BZ/SjcR92jyszuCNd1M7Nbw89qhZldY2a5jX1mLWVmQ4GvAb9q632LiEjLqHATEYlWP6AnMBiYQvB7+fZwfk9gO/CnJl5/EDAf6A38BrjVzKwF294LvA70Aq4CJjdxzOeB4nD6SGAxcETC/PP1X+DudevHuXtnd78/nO8HdAMGABcAN5tZj0aOexdBV9MxQF/g9wBmNomgwDgN6A98CEyr99ovAQcAReF2x7r7POCbwMwwU/dw218DI4HxwF5htivDdecDk81sUlh0HQh8r4l9fczMOgF/AI539y7AoUBpA5/Vb8J9dAZGE5z1qvu87gCqw1wTgC8ADXZTNLOzwkK1sceeDb0u9EfgxwTfv4a8YGarzOwhMxvSxH5ERKSNqHATEYlWLfAzd9/p7tvdfb27P+ju29x9C3AtQTHUmA/d/e/uXgP8g6BwKdydbcN/wB8AXOnule7+EjC9iWM+n5DpCIKiqW6+wcKtCVXA1e5e5e6PARXAqPobmVl/4Hjgm+5eHm5fd5yzgdvc/W133wlcQXDma0jCLn7t7hvdfSnwHEFR9hlhITsF+L67bwh/Br8EzgBw91XAtwg+v5uAc8JtmqsW2NfMOrj7Snef09iG4Vm+h4Gb3P1xMysETgAudvet7r6GoHg9o6HXu/u97t69icfSRo57CpDr7v9uJNqRwBBgb+Aj4FEz06UXIiJJpsJNRCRaa919R92MmXU0s7+Z2Ydmthl4AejeRHe4VXUT7r4tnOy8m9vuAWxIWAawrInMM4GRYSExHrgTGGRmvQnOQL3QxGvrW1/vur5tjeQfFGYsb2DdHgRn2QBw9wpgPcGZsjqrEqYbOwZAH4Kzem/VnZkCngiX1/kPkAvMD4vcZnH3rcDpBGfmVprZf81s7yZecmt4jOvC+cFAfvjaumx/Izj72CbCs4K/Ab7b2Dbu/kJY4G8EvgcMJTgzKCIiSaTCTUQkWvWH9r2E4IzTQe7elU+6IDbW/bEtrAR6mlnHhGWDGts4LPDeIvhH+7vuXgm8AvwAWOTu65KQcVmYsXsD6z4iKGqAj4uPXsCKZuy3/ue/jqB74JiEM1PdEgeYITgLOg/ob2ZnNrGvzx7M/Ul3P4bgbOd7wN8b2s7MLifornlBwuJlBCOS9k7I1tXdxzSyj7PrXStX/9FQV8kRBGfTXrTgusuHwve5qokukU5yv58iIoIKNxGRdNOFoHDYaGY9gZ8l+4Du/iHwJnCVmRWY2SHAibt42fPARXzSLbKk3nxDVgPDWphxJfA48GcLBnDJN7O6ovY+4DwzG29m7Qi6Nr7m7kuasevVwEALhr7H3WsJiqnfm1lfADMbYGbHhtNHAOcB5wBfB/5oZgMa2ld9ZlZoZieHheVOgm6htQ1sdzzBGa9T3P3ja8zCz+Ap4Hoz62pmOWY23Mwa7Err7vfUXSvXyKOhrpLvEhTt48PHheH7Gg8sM7Mx4eeca2adgesJCuR5DWUQEZG2o8JNRCS93Ah0IDjz8ypBN71UOBs4hKCL4TUEg2HsbGL75wmKzBcamW/IVcA/wm5+p7Ug42SCa+LeA9YAFwOEt1b4P+BBgrOHw2nkuq8GPAvMAVaZWd2ZwsuAhcCrYXfVGcAoM+tK0C30Indf4e4vEnRnvD28Nq6hfSXKITgr+RGwgeBasW81sN3pBF0z5yWcHftruO4coACYC5QD/yI4e9cm3L3a3VfVPcKcteF8DcH1k/cDmwkGpRkCfMndq9oqg4iINEw34BYRkc8ws/uB99w96Wf8REREZNd0xk1ERDCzA8JudzlmdhxwMsGIhiIiIpIGNHyviIhAcD+1hwgG9VgOfMvdZ0UbSUREROqoq6SIiIiIiEiaU1dJERERERGRNKfCTUREREREJM2pcBMREREREUlzKtxERERERETSnAo3ERERERGRNKfCTUREREREJM2pcBMREREREUlzKtxE2piZLTGzo6POISIikkxhe7fdzCoSHnuE66aa2XwzqzWzc3exn4Fm9qCZrTOzTWb27q5eI5KNVLiJiIiISEud6O6dEx4fhctnA98G3m7GPu4ClgGDgV7AZGB1W4Y0s7y23J9IFFS4iaSAmbUzsxvN7KPwcaOZtQvX9TazR81so5ltMLMXzSwnXHeZma0wsy3hXy6PivadiIiI7Jq73+zuzwA7mrH5AcAd7r7V3avdfZa7P1630swON7NXwnZyWd3ZODPrZmZ3mtlaM/vQzH6a0H6ea2Yvm9nvzWw9cFXYFv/OzJaa2Woz+6uZdUjC2xdJChVuIqnxE+BgYDwwDjgQ+Gm47hJgOdAHKAR+DLiZjQIuAg5w9y7AscCSlKYWERFJvleBm83sDDPbM3GFmQ0GHgf+SNBOjgdKw9V/BLoBw4AjgXOA8xJefhCwmKBtvRb4NTAy3MdewADgyiS8H5GkUOEmkhpnA1e7+xp3Xwv8nKArCEAV0B8Y7O5V7v6iuztQA7QD9jGzfHdf4u6LIkkvIiLSsIfDM2EbzezhFu7j/wEvAv8HfGBmpWZ2QLjuLGCGu98XtpHr3b3UzHKBM4Ar3H2Luy8BrueTthXgI3f/o7tXE5z5mwJ83903uPsW4JfhPkRiQYWbSGrsAXyYMP9huAzgt8BC4CkzW2xmlwO4+0LgYuAqYI2ZTau76FtERCRNfNndu4ePL7dkB+5e7u6Xu/sYgrNjpQQFoQGDgIb+aNkbyOezbeuAhPllCdN9gI7AW3WFJvBEuFwkFlS4iaTGRwQXXdfZM1xG+JfCS9x9GHAS8IO6a9nc/V53Pzx8rQPXpTa2iIhI6rj7OuB3BH/c7ElQfA1vYNN1BD1W6retKxJ3V2/77cCYhEKzm7t3bsv8Ismkwk0kOfLNrH3dA7gP+KmZ9TGz3gR96u8GMLMvmdle4V8WNxF0kaw1s1FmNikcxGQHQYNTG83bERERaT4zKwjbP+OTNrHBf3ea2XVmtq+Z5ZlZF+BbwEJ3Xw/cAxxtZqeF63uZ2Xh3rwEeAK41sy7htXA/IGxb63P3WuDvwO/NrG943AFmdmxbv3eRZFHhJpIcjxEUWnWP9sCbQBnwDsHwyNeE244AZgAVwEzgz+7+HMH1bb8m+CvhKqAvcEXq3oKIiEiLPUXQ/h0KTA2nj2hk247Av4GNBIOJDCbogYK7LwVOIBjIawNBN8px4ev+F9gavuYl4F7gtiYyXUZwacKrZraZoO0d1YL3JhIJC8ZAEBERERERkXSlM24iIiIiIiJpToWbiIiIiIhImlPhJiIiIiIikuZUuImIiIiIiKQ5FW4iIiIiIiJpLi/qAIl69+7tQ4YMadU+tm7dSqdOndomUIrEMTPEM3ccM0M8c8cxM8Qzdxwzv/XWW+vcvU/UOeIiW9tHiGfuOGaGeOaOY2aIZ+44ZoZ45m6sjUyrwm3IkCG8+eabrdpHSUkJxcXFbRMoReKYGeKZO46ZIZ6545gZ4pk7jpnN7MOoM8RJtraPEM/cccwM8cwdx8wQz9xxzAzxzN1YG6mukiIiIiIiImlOhZuIiIiIiEiaU+EmIiIiIiKS5lS4iYiIiIiIpDkVbiIiIiIiImlOhZuIiIiIiEiaU+EmIiIiIiKS5lS4iYiItBEzu83M1pjZu42sNzP7g5ktNLMyM9sv1RlFRCSeVLiJiIi0nTuA45pYfzwwInxMAf6SgkwiIpIB8qIO0FbcnQfmPMDKjSsppjjqOCIikoXc/QUzG9LEJicDd7q7A6+aWXcz6+/uK1OTUKQZ3KG2Cmp3QM1OqNkBteGz14DlAFbvOQeskWWfeq5bb8G+vDZ81AAJ04nP1H5223B5t51lsCa3Dd60QadB0HFQmDFLeG3wc63ZHjyqt0PNtgbn+2+dDQvfByx4rYXPWL1lCcsb3SYxgyfO1A/YxLZNSDhG4bZ58MGyxJWNvMjrHcNbtnzYeZCTnBIrYwo3M+PyZy5naP5QLubiqOOIiIg0ZACQ+C+I5eGyzxRuZjaF4KwchYWFlJSUtOrAFRUVrd5HFOKYOx0y53glnaoW0aXqfTpXvU9+7SZyvIocr8QInuvmc8L5w2orqb03mI+LCQAz2m5/NRSwPW8A2/MGsS1vYPgcTFfndGuz46TqO9J7+0sM2PoQub49/JnvJNd3khM+cqls9r5GAbyetKhJMxpgZuqO98KywdRaQVL2nTGFG0BRYRGzl82OOoaIiEiruftUYCrAxIkTvbi4uFX7KykpobX7iEIcc6c8c/V22FgGG9765LFpDnh1sL6gB3QYALntIKc95HaHnHaQ2z5YltsectqxfOU6Bu45/FPLPvWc2w4sNzzDUBs8ey3gCWfGvN5z4nr/5KwZHuzLchPOzoXTFk6TAznhs+U0uO3ssjLGjRvf+s+wthq2LSV383w6b1lA5y0LYMsrn3yGAAU9oeso6DISuo6ELqOC5857QV6H3TpcSr4j790Eb18JnYdDl+GQ2yF45HX8ZHqX8x2D95bbgZmvz+KQQw4J9v2ps0yJZ54Slnsj0zifPeuVMF//jFz9bT+zvp56Z+Vefe1VDj7o4ITjN/Iaa+isYL3nZiw/osMeu87YQplVuPUt4tH5j7Kjegft89pHHUdERKS+FcCghPmB4TKR5qneBuWzg+Ks/C3Y8HZYpNUE69v1hp77wx4nBM8994dOg5v1D8mFJSUMnFCc1PhtrXx+PvQrTs7Oa6ugYglsWQCb5wfPWxbAqhnwwT8SNjTotGdQ0HUZCUO/Br0Pbmyvyee1MOtH8N71MPAUOPSe3S4sG7Izdzl0HNgGAVNrR96yoHDNABlVuI3rN45aapm7di779ddAXSIiknamAxeZ2TTgIGCTrm+TRlVv/aRIq3tsnpdQpPUJCrMBJ35SpHUclLS/9mednHzoOiJ4DPjip9dVVcCW9z9d1G1eEBR0C/8K434Foy9N/c+iZgfM/DosfQBGXgT73RietZRMkFGFW1FhEQBlq8tUuImISMqZ2X1AMdDbzJYDPwPyAdz9r8BjwAnAQmAbcF40SSXtfXAPvPE/QfEG0L4wKMwGnfJJkdZhgIq0qOR3hp4Tgkeiqs3w6gVQ+iNY+xIcckfQVTUVKsvhhS/Dmhdg/G+iKRwlqTKqcBveYzjtctoxe5WucxMRkdRz9zN3sd6B76QojsRRTSXMuhQW/BH6HgF7XxqeSdsj6mTSHPld4fAHgp/f25fA4/vD5/4Z/AyTaetSKDk+OAt46L0wpMlfRRJTGTXeaW5OLkM7DaVsTVnUUURERER2z/aV8Oyk4B/9e/8AJs2AgSeqaIsbMxj1XTjmxWBwk6cOhff/0vyh7HdX+Wx46mDYtgI+/6SKtgyWUYUbwPBOw5m9ajaerP85RERERNra2pfh8f1gwyw49D7Y7/rgGiuJr94Hw3FvQ+EkeOPb8MrXgmvj2tKqGfD054JRNo95CQo/37b7l7SScYXbsM7DWL99PasqVkUdRURERKRp7jD/jzCjGPK7wLGvwZAzok4lbaV9byj+LxRdA0unwZMHwqa5bbPvD+6C546HzkPgCzOh+75ts19JW5lXuHUaBgQDlIiIiIikreptMHMyvPXdYPj+Y9/QP74zkeXAvj+Bzz8NlevhiQMo3PZ0y/fnDnN+CTPPCa6DPPrFWA7TL7svYwu32as1QImIiIikqS2L4KlDYMm9wdmYI/4NBd2iTiXJ1G8SHDcLek1k9MZfwuv/Ewzfvztqq4Nul7N/AkPOhuLH9b3JIhlXuHXN78rArgN1xk1ERETS04rH4ImJsG0ZFD8WnI2xjPsnmTSk4x4w6RmWdj4TFk4NBi7Zsqh5r63eCi+eGtwnbp/L4ZA7IbcguXklrWTkb4lxheNUuImIiEh68Vp45+fw/JeC65KOewv2OC7qVJJqOXks7joFjvwPbF0CT+wPy/7d9Gt2rIFnJsFH/4WJN8P4X6nYz0IZ+RMvKixi3rp5VNZURh1FREREJLg58vMnwTtXwdBz4JhXoPPQqFNJlAZ8KRh1ssuI4Eza25dAbdVnt9uyMDgzt7EMPvcQjPx26rNKWsjYwq26tpr31r0XdRQRERHJduVlQdfIVU/BAX+Gg2+HvA5Rp5J00HlIMIz/iO/AezcEo4tuW/7J+nWvBddCVm2ESc/CwJMjCirpIGMLN4DZqzRAiYiIiETog3uCmyPX7ICjnocR3wpu0CxSJ7cdHPAnOGxacFbt8Qmw8ilYPh2e+Tzkdw3O0PY5JOqkErG8qAMkw8heI2mX207XuYmIiEg0aqvg7UthwR+CIdsPux869Is6laSzwadDj/Hw4lfhueOCAr/HfnDko9ChMOp0kgYysnDLy8ljTN8xlK1R4SYiIiKpVVCzPhhIYu1LMOr7MOE6yMmPOpbEQddRwU3Y374EqrfAgX+DvE5Rp5I0kZGFGwTdJZ9Y+ETUMURERCSbbHqP/ddOAdsBh94LQ86MOpHETV5HOPAvUaeQNJSR17gBFPUtYlXFKtZsXRN1FBEREckGNZXwylkYNXDsqyraRKRNZW7hFg5QouvcREREJCXe/TmUz2JBt0uh+9io04hIhlHhJiIiItJaa1+Gub+GYeezrsPhUacRkQyUsYVbn0596N+5vwo3ERERSa6qLTDzHOg4GPa/Meo0IpKhMnZwEgjOuqlwExERkaR6+/uwdQkc/QLkd4k6jYhkqKSfcTOzXDObZWaPJvtY9RUVFjFn7RyqaqpSfWgRERHJBssehkW3wujLoM9hUacRkQyWiq6S3wPmpeA4n1FUWERlTSUL1i+I4vAiIiKSybavhte/AT0mwNirok4jIhkuqYWbmQ0EvgjckszjNEYDlIiIiEhSuMNrFwbXtx16N+QWRJ1IRDJcss+43Qj8CKhN8nEatHfvvcnPyVfhJiIiIm1r0d/ho0dh/HXQbZ+o04hIFkja4CRm9iVgjbu/ZWbFTWw3BZgCUFhYSElJSauOW1FR8al9DOowiOfmPUdJXuv2m0z1M8dFHHPHMTPEM3ccM0M8c8cxs0isbVkIb30f+h0No/436jQikiWSOarkYcBJZnYC0B7oamZ3u/vXEjdy96nAVICJEyd6cXFxqw5aUlJC4j4OLT+U5z54jtbuN5nqZ46LOOaOY2aIZ+44ZoZ45o5jZpHYqq2GVyZDTgEcfDtYxt5ZSUTSTNJ+27j7Fe4+0N2HAGcAz9Yv2lKhqG8RK7asYP229ak+tIiIiGSaOb+C9a/CAX+BjgOjTiMiWSTj/0w0rt84AN5Z807ESURERCTW1r8B7/4cBp8FQ86IOo2IZJmUFG7uXuLuX0rFserTyJIiIiLSatXbYOZk6NAfDvhT1GlEJAsl8xq3tFDYqZA+HfuocBMREZGWm/Uj2DwfJs2Agh5RpxGRLJTxXSXNjKLCImavnh11FBEREYmjj56A92+GUd+HfkdFnUZEslTGF24QdJd8d8271NTWRB1FREQynJkdZ2bzzWyhmV3ewPrBZvaMmZWZWYmZaYSLdLZzPbx2PnQbA+N/GXUaEcliWVG4jSscx47qHSzcsDDqKCIiksHMLBe4GTge2Ac408zq3535d8Cd7l4EXA38KrUppdnc4fVvws51cOjdkNs+6kQiksWyonDTACUiIpIiBwIL3X2xu1cC04CT622zD/BsOP1cA+slXSy5G5b9C4p+AT3GR51GRLJcVhRuo/uMJtdyVbiJiEiyDQCWJcwvD5clmg2cGk6fAnQxs14pyCa7Y+uH8OZF0OdzsPelUacREcn8USUB2ue1Z1TvURqgRERE0sGlwJ/M7FzgBWAF8JmLsM1sCjAFoLCwkJKSklYdtKKiotX7iEIkub2G8et/QOfqat60b7PjhRd36+X6rFMnjpkhnrnjmBnim7shWVG4QdBdcuaymVHHEBGRzLYCGJQwPzBc9jF3/4jwjJuZdQa+4u4b6+/I3acCUwEmTpzoxcXFrQpWUlJCa/cRhUhyz/sdrCyDg2/n4GG7f6NtfdapE8fMEM/cccwM8c3dkKzoKgnBACUfbvqQTTs2RR1FREQy1xvACDMbamYFwBnA9MQNzKy3mdW1v1cAt6U4ozSlvAxm/wQGnQpDvx51GhGRj2VN4VY3QMk7a96JOImIiGQqd68GLgKeBOYBD7j7HDO72sxOCjcrBuab2QKgELg2krDyWTU7YObXghtsH/A3MIs6kYjIx7KqqyQEI0sevufhEacREZFM5e6PAY/VW3ZlwvS/gH+lOpc0Q9n/wcZ34Mj/QvveUacREfmUrDnjNqDLAHq078HsVRqgREREROpZXQLzrocR34IBJ0SdRkTkM7KmcDMzigqLKFujWwKIiIhIgp3r4ZWvQZcRMOG3UacREWlQ1hRuEAxQ8s7qd6j12qijiIiISDpwh1fPh51r4bBpkNcp6kQiIg3KqsKtqLCIrVVb+aD8g6ijiIiISDpYcDOsmA7jfwM9J0SdRkSkUVlXuAG6EbeIiIhA+WyYdSns8UUY9d2o04iINCmrCrcxfceQYzmUrdZ1biIiIlmteiu8fAa06wkH366h/0Uk7WXN7QAAOuZ3ZETPESrcREREst1b34PN82HSDGjfJ+o0IiK7lFVn3CDoLqnCTUREJIt9eD8suhXGXAH9JkWdRkSkWbKycFtUvoiKyoqoo4iIiEiqVXwAr0+B3ofA2KuiTiMi0mxZWbgBvLP6nYiTiIiISErVVsHLZwIGh94LOflRJxIRabasLdzUXVJERCTLlF0J61+Dg/4OnYdEnUZEZLdkXeE2uNtgurbrqsJNREQkm6yaAXOvg+HfgD3/X9RpRER2W9YVbmYWDFCyRoWbiIhIVtixBl6ZDF33hv1vjDqNiEiLZF3hBlDUNxhZ0t2jjiIiIiLJ5LUw81yoLIfD74e8jlEnEhFpkews3AqL2LxzMx9u+jDqKCIiIpJM82+ClY/DfjdA97FRpxERabGsLdxAA5SIiIhktA1vQellMPDLMOJbUacREWmVrCzcxhYGf3FT4SYiIpKhqrbAS2dA+0I46FYwizqRiEir5EUdIAqdCzozvMdwFW4iIiKZ6s2LYOtiOOo5aNcz6jQiIq2WlWfcIOguqcJNREQkA31wN3xwJ4z5P+h7RNRpRETaRFYXbu9veJ9tVduijiIiIiJtZfP78Ma3oM/nYN+fRp1GRKTNZHXhVuu1zFkzJ+ooIiIi0hZqKuGVMyEnHw69B3Ky8ooQEclQWVu4jSscB2iAEhERkYwx+8fBSJIH3QadBkWdRkSkTWVt4Ta0x1A65XdS4SYiIpIJPnoc3rseRnwbBn056jQiIm0uawu3HMthbOFYytaocBMREYm17atg5teDG2xP+F3UaUREkiJrCzeAor5FzF41G3ePOoqIiIi0hNfCzMlQXQGHTYO8DlEnEhFJiuwu3AqLKN9RzootK6KOIiIiIi3x3g2wagbsfxN02yfqNCIiSZPVhdu4fhqgREREJLa2r4Syn8HAk2H4hVGnERFJqqwu3Mb2HQuocBMREYmlsivBq2DC9WAWdRoRkaTK6sKtW/tuDO42WIWbiIhI3Gx8BxbfBiMugi7Do04jIpJ0WV24QXCd2+zVs6OOISIiGcLMjjOz+Wa20Mwub2D9nmb2nJnNMrMyMzshipyxN+tHkNcV9v1p1ElERFIi6wu3cYXjmL9uPjuqd0QdRUREYs7McoGbgeOBfYAzzaz+iBk/BR5w9wnAGcCfU5syA6x8GlY+ERRt7XpGnUZEJCWyvnArKiyixmuYt3Ze1FFERCT+DgQWuvtid68EpgEn19vGga7hdDfgoxTmi7/aGph1KXQaAiMvijqNiEjKqHArLAI0QImIiLSJAcCyhPnl4bJEVwFfM7PlwGPA/6YmWoZYchdsLIPxv4bcdlGnERFJmbyoA0Rtr5570T6vva5zExGRVDkTuMPdrzezQ4C7zGxfd69N3MjMpgBTAAoLCykpKWnVQSsqKlq9jygk5s6p3cFBa37Izvy9eXtxX/igJNJsjcmEzzou4pgZ4pk7jpkhvrkbkvWFW25OLvv23Vdn3EREpC2sAAYlzA8MlyW6ADgOwN1nmll7oDewJnEjd58KTAWYOHGiFxcXtypYSUkJrd1HFD6V+91rYdU62k16iOK+n4s0V1My4rOOiThmhnjmjmNmiG/uhiStq6SZtTez181stpnNMbOfJ+tYrTWucByzV8/G3aOOIiIi8fYGMMLMhppZAcHgI9PrbbMUOArAzEYD7YG1KU0ZR9tXw9xfw8BTII2LNhGRZEnmNW47gUnuPg4YDxxnZgcn8XgtVlRYxLpt61i9dXXUUUREJMbcvRq4CHgSmEcweuQcM7vazE4KN7sE+IaZzQbuA851/eVw1965Cmp2BNe2iYhkoaR1lQwboYpwNj98pGXDlDhASb/O/SJOIyIicebujxEMOpK47MqE6bnAYanOFWub5sGiv8OIb0HXkVGnERGJRFJHlTSzXDMrJei3/7S7v5bM47XU2L5jAZi9SgOUiIiIpJ3SyyCvE+x75a63FRHJUEkdnMTda4DxZtYd+Hc4ata7iduky6hZvQt683TZ0xxQdUCrjt8ScR3tJo6545gZ4pk7jpkhnrnjmFmkubrvnAXr/xN0kWzfJ+o4IiKRScmoku6+0cyeIxhF691669Ji1KwDPzqQ5ZuXRzLqTFxHu4lj7jhmhnjmjmNmiGfuOGYWaRavZfjmv0DHPWHkd6NOIyISqWSOKtknPNOGmXUAjgHeS9bxWquobxHz1s6jsqYy6igiIiICsOReulS9D+OuhbwOUacREYlUMq9x6w88Z2ZlBMMjP+3ujybxeK1SVFhEVW0V89fNjzqKiIiIVG+H2T9hS/4IGHJW1GlERCKXzFEly4AJydp/W6sbWXL26tmMLRwbcRoREZEst+APsG0pi3rdwHhL6lhqIiKxkJJr3OJgZK+RFOQWULa6LOooIiIi2W3HWpjzS9jjS2yMz9+ARUSSSn/CCuXn5jOmzxgVbiIiIlF792qo3goTfhN1EhGRtKHCLUFRYZEKNxERkShtXgDv/xWGfwO6jY46jYhI2lDhlqCosIiVFStZu3Vt1FFERESyU+nlkNsexl4VdRIRkbSiwi1B3QAlOusmIiISgTUvwvJ/wz6XQYfCqNOIiKQVFW4JVLiJiIhExB1mXQod9oC9fxB1GhGRtKNRJRP07dSXfp37UbZGhZuIiEhKLX0A1r8OB98OeR2jTiMiknZ0xq0eDVAiIiKSYjU7g2vbuo+DIZOjTiMikpZUuNVT1LeIOWvmUF1bHXUUERGR7LDgT7B1CUz4LeTkRp1GRCQtqXCrp6iwiJ01O1mwfkHUUURERDLfzg3w7jXQ/zjof0zUaURE0pYKt3rG9RsHaIASERGRlHj3GqjeHJxtExGRRqlwq2fv3nuTl5Onwk1ERCTZtiyC9/8Ew86H7vtGnUZEJK2pcKunILeA0b1Hq3ATERFJttlXgOVD0dVRJxERSXsq3Bpw0ICDeOHDF9hetT3qKCIiIplp7UxY+k8Y/UPo0D/qNCIiaU+FWwPOHHsmWyq3MH3+9KijiIiIZKbSH0H7fjD60qiTiIjEggq3BhQPKWZg14Hc/c7dUUcRERHJPOWlsPYl2OcyyO8cdRoRkVhQ4daAHMvhrH3P4omFT7B269qo44iIiGSWRbdBTgEM1c22RUSaS4VbIyaPm0x1bTXT3p0WdRQREZHMUbMDltwNA0+Bdr2iTiMiEhsq3Bqxb999Gd9vPHeV3RV1FBERkcyx/BGoLIfh50edREQkVlS4NWFy0WTe+OgN5q+bH3UUERGRzLDoNui4JxQeFXUSEZFYUeHWhDP3PZMcy+HuMg1SIiIi0mpbl8Kqp2HYuZCTG3UaEZFYUeHWhP5d+nP0sKO5+527qfXaqOOIiIjE2+I7AIdh50WdREQkdlS47cLkosks2biEl5e+HHUUERGR+PJaWHx70EWy85Co04iIxI4Kt104Ze9T6JTfSYOUiIhIs5jZcWY238wWmtnlDaz/vZmVho8FZrYxgpipt/o52LoEhl8QdRIRkVhqduFmZh3MbFQyw6SjTgWdOHX0qTww5wF2VO+IOo6IiKTQ7rZ9ZpYL3AwcD+wDnGlm+yRu4+7fd/fx7j4e+CPwUBtGTl+LboP87jDwy1EnERGJpWYVbmZ2IlAKPBHOjzez6UnMlVa+VvQ1Nu3cxH8X/DfqKCIikiItbPsOBBa6+2J3rwSmASc3sf2ZwH1tEDe9VZbDsgdhyNmQ1yHqNCIisdTcM25XETRGGwHcvRQYmpREaeiooUfRv3N/dZcUEckuV7H7bd8AYFnC/PJw2WeY2eBwf8+2LmYMLLkPanfq3m0iIq2Q18ztqtx9k5klLvMk5ElLuTm5nDX2LP7w2h9Yv209vTr2ijqSiIgkX7LbvjOAf7l7TUMrzWwKMAWgsLCQkpKSVh2soqKi1ftoqf3X3gh5e/FW2WZg9zJEmbul4pgZ4pk7jpkhnrnjmBnim7shzS3c5pjZWUCumY0Avgu8krxY6Wdy0WSun3k9D8x5gG8d8K2o44iISPK1pO1bAQxKmB8YLmvIGcB3GtuRu08FpgJMnDjRi4uLmxm7YSUlJbR2Hy1SXgqPvw/7/4HiUbt//Mhyt0IcM0M8c8cxM8QzdxwzQ3xzN6S5XSX/FxgD7ATuBTYBFycpU1oqKixi3777qrukiEj2aEnb9wYwwsyGmlkBQXH2mevizGxvoAcwsy0Dp6VFt0FOQXB9m4iItNguz7iFI2T9190/D/wk+ZHSk5kxuWgyl824jIUbFrJXz72ijiQiIknS0rbP3avN7CLgSSAXuM3d55jZ1cCb7l5XxJ0BTHP3zL7soGYHLLkbBp4C7XpGnUZEJNZ2ecYt7Htfa2bdUpAnrZ019iwM4+6yu6OOIiIiSdSats/dH3P3ke4+3N2vDZddmVC04e5Xuftn7vGWcZY/EowoqXu3iYi0WnOvcasA3jGzp4GtdQvd/btJSZWmBnYdyKShk7i77G5+duTPqHfBuoiIZBa1fa216DbouCf0OyrqJCIisdfcwu0hsuUGobvwtaKvcd4j5/Hq8lc5ZNAhUccREZHkUdvXGls/hFVPw75XgjX3knoREWlMswo3d/9HeJH1yHDRfHevSl6s9PWV0V/h2//9NneV3aXCTUQkg6nta6XF/wieh50baQwRkUzRrD+BmVkx8D5wM/BnYIGZHZG8WOmrS7sufHnvL3P/nPuprKmMOo6IiCSJ2r5W8FpYfHvQRbLzkKjTiIhkhOb2Xbge+IK7H+nuRwDHAr9PXqz0NrloMhu2b+Cx9x+LOoqIiCSP2r6WWv0cbF0Cw86POomISMZobuGW7+7z62bcfQGQn5xI6e+Y4cfQt1NfjS4pIpLZ1Pa11KJbIb87DPxy1ElERDJGcwu3N83sFjMrDh9/B95MZrB0lpeTx5n7nsl/FvyH8u3lUccREZHkUNvXEpXlsOyh4IbbeR2iTiMikjGaW7h9C5gLfDd8zA2XZa3JRZOprKnkn3P/GXUUERFJDrV9LbHkXqjdCcPVTVJEpC0193YAecBN7n4DgJnlAu2SlioG9uu/H6N7j+ausruYsv+UqOOIiEjbU9vXEotugx7joed+UScREckozT3j9gyQ2N+hAzCj7ePEh5kxuWgyLy19iQ/KP4g6joiItD21fburvBTK39agJCIiSdDcwq29u1fUzYTTHZMTKT7OGnsWAPe8c0/ESUREJAnU9u2uRbdBTkFwfZuIiLSp5hZuW83s4z4PZjYR2J6cSPExuPtgjhx8JHeV3YW7Rx1HRETaltq+3VGzA5bcDQNPgXY9o04jIpJxmnuN28XAP83so3C+P3B6UhLFzOSiyVz4nwt546M3OHDAgVHHERGRtnMxavuab/kjwYiSwy+IOomISEZq8oybmR1gZv3c/Q1gb+B+oAp4AtCFXcBX9/kq7XLb6Z5uIiIZQm1fCy26FTruCf2OijqJiEhG2lVXyb8BleH0IcCPgZuBcmBqUy80s0Fm9pyZzTWzOWb2vVanTUPd2nfjpFEnMe3daVTVVEUdR0REWq/FbV/W2vohrJoBw84Da+5VGCIisjt29ds11903hNOnA1Pd/UF3/z9gr128thq4xN33AQ4GvmNm+7QubnqaXDSZtdvW8uSiJ6OOIiIirdeati87Lb4jeB52bpQpREQy2i4LNzOruw7uKODZhHVNXh/n7ivd/e1wegswDxjQ0qDp7Li9jqN3x97cVXZX1FFERKT1Wtz2ZSWvhcW3B10kOw+JOo2ISMbaVeF2H/C8mT1CMJLWiwBmthewqbkHMbMhwATgtZbFTG/5ufmcMeYMHnnvETbtaPbHIiIi6alN2r6ssfrZoKuk7t0mIpJUuzprdq2ZPUMwktZT/smY9znA/zbnAGbWGXgQuNjdNzewfgowBaCwsJCSkpLmp29ARUVFq/fREqOrR7OzZifX/vtaTuh/wm69NqrMrRXH3HHMDPHMHcfMEM/cccycztqi7csqi26D/O4w6JSok4iIZLRddvlw91cbWLagOTs3s3yCou0ed3+okf1PJbzYe+LEiV5cXNycXTeqpKSE1u6jJY70I7nxwxt5o/INflP8m916bVSZWyuOueOYGeKZO46ZIZ6545g53bWm7csqleWw7CEYfiHkto86jYhIRkva0E9mZsCtwDx3vyFZx0kXZsbkosmULClh6aalUccRERFJviX3Qu1O3btNRCQFkjlm72HAZGCSmZWGj93rQxgzZxedDcA9ZfdEnERERCQFFt0KPcZDzwlRJxERyXhJK9zc/SV3N3cvcvfx4eOxZB0vHQzrMYzDBh3GXWV38cklESIiIhlowywonwXDdLZNRCQVdJfMNja5aDLz1s1j1qpZUUcRERFJnsW3QU47GHJW1ElERLKCCrc2dtqY0yjILeCu2bqnm4iIZKiaHbDknmAkyXY9o04jIpIVVLi1sR4devClkV/ivnfvo7q2Ouo4IiIibW/5I8GIkrp3m4hIyqhwS4LJRZNZvXU1MxbPiDqKiIhI21t0K3TcE/odFXUSEZGsocItCY7f63h6tO/BXWXqLikiIhlm64ewagYMOw9M/4wQEUkV/cZNgnZ57Th9zOn8e96/2bJzS9RxRERE2s7iO4Ln4edFGkNEJNuocEuSyeMms716Ow/NeyjqKCIiIm3Da2Hx7UEXyU6Do04jIpJVVLglySEDD2FYj2HqLikikmXM7Dgzm29mC83s8ka2Oc3M5prZHDO7N9UZW2xjWdBVcsjXok4iIpJ1VLgliZlxTtE5PPvBs7y75t2o44iISAqYWS5wM3A8sA9wppntU2+bEcAVwGHuPga4ONU5W6y8NHjudVCkMUREspEKtyS66MCL6NKuCz9+5sdRRxERkdQ4EFjo7ovdvRKYBpxcb5tvADe7ezmAu69JccaWKy+F3A7QZUTUSUREso4KtyTq1bEXlx12Gf9Z8B9eWvpS1HFERCT5BgDLEuaXh8sSjQRGmtnLZvaqmR2XsnStVV4K3YsgJzfqJCIiWScv6gCZ7nsHfY8/vf4nLp9xOS+e9yJmFnUkERGJVh4wAigGBgIvmNlYd9+YuJGZTQGmABQWFlJSUtKqg1ZUVLRuH+4cvvZN1nSYxIJWZtkdrc4dgThmhnjmjmNmiGfuOGaG+OZuiAq3JOtU0ImfHfkzvvnfb/Logkc5cdSJUUcSEZHkWQEMSpgfGC5LtBx4zd2rgA/MbAFBIfdG4kbuPhWYCjBx4kQvLi5uVbCSkhJatY+tH8IjW9ljzAnsMaJ1WXZHq3NHII6ZIZ6545gZ4pk7jpkhvrkboq6SKXD+hPMZ0XMEVzxzBTW1NVHHERGR5HkDGGFmQ82sADgDmF5vm4cJzrZhZr0Juk4uTmHGlqkbmKTH+ChTiIhkLRVuKZCfm8+1k65lzto53F12d9RxREQkSdy9GrgIeBKYBzzg7nPM7GozOync7ElgvZnNBZ4Dfuju66NJvBvKSwGD7mOjTiIikpXUVTJFvrrPV5m4x0SuLLmS0/c9nfZ57aOOJCIiSeDujwGP1Vt2ZcK0Az8IH/FRXgpdR0Jep6iTiIhkJZ1xSxEz49dH/Zqlm5bylzf+EnUcERGR3VNeCt3HR51CRCRrqXBLoaOGHcUxw47h2hevZdOOTVHHERERaZ7KjbB1ia5vExGJkAq3FPv10b9m/fb1/O6V30UdRUREpHnKZwfPKtxERCKjwi3F9uu/H6ePOZ0bXr2BVRWroo4jIiKyaxpRUkQkcircInDNpGuorKnkF8//IuooIiIiu7axFNoXQod+UScREclaKtwisFfPvfjGft9g6ttTWbhhYdRxREREmlZeqrNtIiIRU+EWkSuPvJKC3AJ++uxPo44iIiLSuJpK2DRHhZuISMRUuEWkX+d+fP/g73P/nPuZv2V+1HFEREQatnke1FbpVgAiIhFT4RahHx76Q3p16MUtH9wSdRQREZGGaWASEZG0oMItQt3ad+PHn/sxb5a/yTOLn4k6joiIyGeVl0JuB+gyIuokIiJZTYVbxL59wLfp264vlz9zOe4edRwREZFPKy+F7kWQkxt1EhGRrKbCLWLt89pz3pDzePOjN/nX3H9FHUdEROQT7hpRUkQkTahwSwPHFB7DmD5j+MmzP6GqpirqOCIiIoFtS6Fqowo3EZE0oMItDeRaLr886pe8v+F9bpt1W9RxREREAhqYREQkbahwSxMnjjyRwwYdxs+f/znbqrZFHUdERCQs3Ay6j406iYhI1lPhlibMjF8f/WtWVqzkpldvijqOiIhIULh1HQl5naJOIiKS9VS4pZHD9zycL438Ete9fB0btm+IOo6IiGS78lLdeFtEJE2ocEszv5z0Szbv3MyvXvxV1FFERCSbVW6ErUt0fZuISJpQ4ZZmxhaO5Zxx5/DH1//Isk3Loo4jIiLZqnx28KzCTUQkLahwS0M/L/45jnNVyVVRRxERkWylESVFRNKKCrc0NLj7YL5zwHe4Y/YdzF07N+o4IiKSjTaWQvtC6NAv6iQiIoIKt7T148/9mM4FnfnJsz+JOoqIiGSj8lKdbRMRSSMq3NJU7469+eGhP+Th9x5m5rKZUccREZFsUlMJm+ZA93FRJxERkZAKtzT2/YO/T2GnQi6bcRnuHnUcERHJFpvnQW2VzriJiKQRFW5prFNBJ6488kpeXPoij73/WNRxREQkW2hgEhGRtKPCLc19Y79vMLzHcK545gpqamuijiMiItmgvBRyO0CXkVEnERGRkAq3NJefm8+1k67lnTXvcNNrN0UdR0REssHG2dB9LOTkRp1ERERCKtxi4LQxp3HyqJO54pkrKF1VGnUcERHJZO4aUVJEJA2pcIsBM+OWk26hV4denPngmWyr2hZ1JBERyVTblkFluQo3EZE0o8ItJnp37M0/vvwP3lv3Hpc8eUnUcUREpBFmdpyZzTezhWZ2eQPrzzWztWZWGj4ujCJno+oGJuk+PsoUIiJST9IKNzO7zczWmNm7yTpGtjlm+DFccsgl/PWtvzJ9/vSo44iISD1mlgvcDBwP7AOcaWb7NLDp/e4+PnzcktKQu1JeClhwjZuIiKSNZJ5xuwM4Lon7z0rXTrqW8f3Gc8H0C1i5ZWXUcURE5NMOBBa6+2J3rwSmASdHnGn3lJdClxGQ3znqJCIikiBphZu7vwBsSNb+s1W7vHbc95X72Fq5la8//HVqvTbqSCIi8okBwLKE+eXhsvq+YmZlZvYvMxuUmmjNpIFJRETSUl7UAWT37d17b35/7O/55n+/yY2v3sgPDvlB1JFERKT5/gPc5+47zex/gH8Ak+pvZGZTgCkAhYWFlJSUtOqgFRUVu9xHXm0Fh2/9gMU5R7G0lcdrK83JnW7imBnimTuOmSGeueOYGeKbuyHm7snbudkQ4FF337eJbRIbpv2nTZvWqmNWVFTQuXO8une0JLO7839z/o/XN7zOn/f7M3t13itJ6RqXLZ91Oohj7jhmhnjmjmPmz3/+82+5+8Soc7Q1MzsEuMrdjw3nrwBw9181sn0usMHduzW134kTJ/qbb77ZqmwlJSUUFxc3vdGaF2DGkVD8GOxxfKuO11aalTvNxDEzxDN3HDNDPHPHMTPEM7eZNdhGRn7Gzd2nAlMhaJha+8HG8YfT0swPH/QwRX8p4voPr+etKW/RMb9j24drQjZ91lGLY+44ZoZ45o5j5gz2BjDCzIYCK4AzgLMSNzCz/u5ed5HyScC81EZsQt2IkuoqKSKSdnQ7gBjr3bE3d55yp24RICKSJty9GrgIeJKgIHvA3eeY2dVmdlK42XfNbI6ZzQa+C5wbTdoGlJdC+77Qvl/USUREpJ5k3g7gPmAmMMrMlpvZBck6VjY7etjRXHrIpfz1rb/yyHuPRB1HRCTruftj7j7S3Ye7+7XhsivdfXo4fYW7j3H3ce7+eXd/L9rECcpLg/u3mUWdRERE6knmqJJnunt/d89394HufmuyjpXtrj3qWib0m8AF0y/goy0fRR1HRETiqKYSNs1RN0kRkTSlrpIZoCC3gHu/ci/bqrbpFgEiItIym9+D2koVbiIiaUqFW4bYu/fe3HjcjcxYPIPfz/x91HFERCRuNDCJiEhaU+GWQb6x3zc4Ze9TuOKZK5i1clbUcUREJE7KSyG3A3QZGXUSERFpgAq3DGJm/P3Ev9OnUx/OeugstlVtizqSiIjExcZS6D4WcnKjTiIiIg1Q4ZZhenXsxZ1fvpP56+bzgyd/EHUcERGJA/fgjJu6SYqIpC0VbhnoqGFHcemhl/K3t/7Gw+89HHUcERFJd9uWQWW5CjcRkTSmwi1DXTPpGvbrvx8XTr9QtwgQEZGm1Q1M0n18lClERKQJKtwyVEFuAfeeei/bq7dzzr/P0S0CRESkceWlgAXXuImISFpS4ZbBRvUexY3H3sgzHzzDDTNviDqOiIikq/JS6DIC8jtHnURERBqhwi3DXbjfhZyy9yn8+Jkf8/bKt6OOIyIi6UgDk4iIpD0Vbhmu7hYBfTv15awHz2Jr5daoI4mISDqp3AhbP1DhJiKS5lS4ZYFeHXtx5yl3smD9At0iQEREPm1jWfCswk1EJK2pcMsSk4ZO4oeH/pCpb0/lnrJ7oo4jIiLpom5ESRVuIiJpTYVbFvnFpF9wxOAj+PrDX9f93UREJFBeCu37Qvt+UScREZEmqHDLIgW5BTx65qNM3GMip/3zNB5///GoI4mISNTKS4P7t5lFnURERJqgwi3LdGnXhSe+9gT79t2XUx84lWc/eDbqSCIiEpWaStg0R90kRURiQIVbFurevjtPTX6K4T2Gc+J9J/Ly0pejjiQiIlHY/B7UVqpwExGJARVuWap3x97MOGcGA7sO5Ph7jueNFW9EHUlERFJNA5OIiMSGCrcs1q9zP5455xl6d+zNsXcfy+xVs6OOJCIiqbRxNuR2gC4jo04iIiK7oMItyw3sOpBnznmGTgWdOOauY5i7dm7UkUREJFXKS6H7WMjJjTqJiIjsggo3YWiPoTxzzjPk5uRy9J1Hs3DDwqgjiYhIsrkHhZu6SYqIxIIKNwFgZK+RzJg8g6raKib9YxIfbvww6kgiIpJM25ZD5QYVbiIiMaHCTT42pu8YnvraU2yp3MKkOyexYvOKqCOJiEiy1A1M0n18lClERKSZVLjJp0zoP4Enzn6CtVvXctSdR7G6YnXUkUREJBnKSwELrnETEZG0p8JNPuOggQfx37P+y7LNyzjmrmNYv2191JFERKStbSyFLiMgv3PUSUREpBlUuEmDPjf4czxyxiMsWL+AY+8+lo07NkYdSURE2pIGJhERiRUVbtKoo4cdzYOnPUjZ6jJOuOcEtuzcEnUkERFpC5WboGKxCjcRkRhR4SZN+uLILzLtq9N4fcXrnDTtJLZVbYs6koiItNbGsuBZhZuISGyocJNdOnX0qdx5yp08v+R5Trn/FHZW74w6kohI2jKz48xsvpktNLPLm9juK2bmZjYxlfmAT0aUVOEmIhIbKtykWc4aexa3nHQLTy16itP+dRpVNVVRRxIRSTtmlgvcDBwP7AOcaWb7NLBdF+B7wGupTRgqL4X2faF9v0gOLyIiu0+FmzTb+RPO50/H/4np86dz9kNnU+M1UUcSEUk3BwIL3X2xu1cC04CTG9juF8B1wI5UhvtYeWlw/zazSA4vIiK7Ly/qABIv3znwO+yo3sGlT1/K0p5LGbX/KAZ2HRh1LBGRdDEAWJYwvxw4KHEDM9sPGOTu/zWzH6YyHAC1VbDpXRj1vZQfWkREWk6Fm+y2Sw69hPZ57bnkyUsY8+cx/PaY33LhfheSYzqBKyLSFDPLAW4Azm3GtlOAKQCFhYWUlJS06tgVFRWUlJTQqWoxB9RWMndlAWs2tW6fqVCXO07imBnimTuOmSGeueOYGeKbuyEq3KRFvnPgd+i+vju3rLmF/3n0f7jv3fv4+4l/Z6+ee0UdTUQkSiuAQQnzA8NldboA+wIlFnRT7AdMN7OT3P3NxB25+1RgKsDEiRO9uLi4VcFKSkooLi6GD5bBWtjnsLPYp9tnLr9LOx/njpE4ZoZ45o5jZohn7jhmhvjmbohOkUiLDegwgGfPeZapX5rK2yvfZuxfxvK7V35HdW111NFERKLyBjDCzIaaWQFwBjC9bqW7b3L33u4+xN2HAK8Cnynakqq8FHLbQ5eRKTukiIi0ngo3aRUz4xv7f4O5357LF4Z/gR8+/UMOufUQylaXRR1NRCTl3L0auAh4EpgHPODuc8zsajM7Kdp0ofJS6DYWctTpRkQkTlS4SZsY0HUAD5/+MPd/9X4+3Pgh+0/dnyufu1L3fBORrOPuj7n7SHcf7u7XhsuudPfpDWxbnNKzbe5B4ab7t4mIxI4KN2kzZsZpY05j3nfmcea+Z/KLF37BhL9NYOaymVFHExERgG3LoXKDCjcRkRhS4SZtrlfHXtx5yp08dtZjVFRWcNhth3HxExdTUVkRdTQRkexWXho8q3ATEYkdFW6SNMePOJ45357Dtw/4Nje9dhP7/nlfnlr0VNSxRESyV3kpYNB9bNRJRERkN+nKZEmqLu268KcT/sTpY07nwv9cyLF3H8u548/lhi/cQI8OPaKOJ7vp5aUvc93L1/H04qfpXNCZHu170KNDD7q37x5Mtw+nO/Rgzco1rJu77pN14Xbd2nUjNyc36rcikp02lkKXvSC/S9RJRERkN6lwk5T43ODPMfubs7n6+av5zcu/4YmFT3DzCTdz6uhTo44mu1DrtTy64FGue/k6Xln2Cr069OKCCRdQ67WU7yinfHs5G3dsZMnGJZRvL6d8R/knt4RY0PA+u7XrRvf23enbqS8njzqZc8efy4CuA1L3pkSyVXkp9Nw/6hQiItICKtwkZdrnteeXR/2S/7fP/+OC6RfwlQe+wqmjT+Wm425iYNeBUceTeiprKrn3nXv57Su/Ze7auQzpPoQ/Hv9Hzht/Hp0KOjX6OndnW9U2HnvuMfYevzcbd2z8uMAr3xEUeXXTi8sX89PnfsqVJVdy7PBjuWDCBZw46kQKcgtS+E5FskNubQVULIbhF0QdRUREWkCFm6TchP4TeO3C17hh5g38rORnPDTvIUb1GsVhgw7jsD0P4/A9D2dEzxGYWdRRs9KWnVuY+tZUfv/q71mxZQXjCsdxz6n3cNqY08hrxn2fzIxOBZ3o064PYwt3fR3Nog2LuL30du4ovYOv/vOr9OnYh8lFk7lgvwvYp88+bfGWRAToXLU4mOg+PtIcIiLSMircJBL5uflcdvhlfHWfr/Kvuf/i5WUv8/D8h7mt9DYA+nTsw6GDDv24mNu///60y2sXcerMtrpiNX947Q/8+c0/s3HHRj4/5PPcctItHDv82KQW0cN7DueaSdfw8+Kf8+SiJ7l11q384fU/cMOrN3DQgIO4YMIFnL7v6XRt1zVpGUSyQeeqhcGERpQUEYklFW4SqeE9h3PZ4ZcBwbVU89fN5+VlL/Pyspd5aelLPDL/EQDa5bbjgAEHBIXcoMM4dNCh9OrYK8roGWPhhoX87pXfcUfpHVTWVHLq6FP50WE/4sABB6Y0R25OLieMOIETRpzAmq1ruLvsbm6ddStTHp3CxU9ezGljTuP88edz+J6H62ysSAt0rl4I7fpAh/5RRxERkRZIauFmZscBNwG5wC3u/utkHk/iLcdyGN1nNKP7jObC/S4EgrNAryx75eNi7oaZN3Ddy9cBsHfvvTl80OEctmdQzO3Vcy/9g343vPnRm1z38nU8OPdB8nPzOXfcuVxy6CWM7DUy6mj07dSXHxzyA75/8Pd5bcVr3Pr2rUybM407Su9gZK+RnD/+fL4+/uv069wv6qgisdG5aiH0Gg/6PSkiEktJK9zMLBe4GTgGWA68YWbT3X1uso4pmaewcyGnjD6FU0afAsD2qu288dEbvLw0KOQenPcgt8y6BYCu7brSvX13urbrSpeCLsFzuy50LQif23VlzfI1zH9z/sfzn9ounG+f1z5jC0B35+nFT3Pdy9fx7AfP0q1dNy477DK+d/D30rIIMjMOHngwBw88mN8f93v+Nfdf3DrrVi5/5nJ+8uxP+OLIL3L++PM5YcQJ5OfmRx1XJH3VVtGpagn0+HLUSUREpIWSecbtQGChuy8GMLNpwMmACjdpsQ75HThi8BEcMfgIIOheOW/tPF5e9jLvrnmXzTs3s6VyC5t3bmbjjo0s3bT04/ktO7fgOCxu+hi5lktBbgF5OXkfP/Jz8z+Zzslv9rrcnFxyLKfhBw0vb+g1S5cu5YnqJ4Cg+ErkfDK/q3XPLXmOWatmsUeXPfjtMb9lyv5TYnPtWOeCzpw7/lzOHX8u89fN57ZZt/GP2f9g+vzpFHYqZK+ee1GQW0BBbgHt8tp9PF2QW0BBTsGn5xMe9bedv2Y+K99Z2eb56/8xwLBG1ze1riFz1s5h3dx1u5cn4RhNZWso196992ZU71G7dTyJ2Ob3yKFK17eJiMRYMgu3AcCyhPnlwEFJPJ5koRzLYUzfMYzpO2aX29Z6LU8++yTjDhzHlp1bPlXk1c1v3rmZisoKqmqrqK6tpqomeK6urabaPz1ft83H8zVV7Kze+al1tV7b7EdNbU3Dy70Gr3VyPsr5+L009x/a9dcN7TGUW0+6lbPHnh3rwV5G9R7FdcdcxzWTruHxhY9z7zv3sm7bOnbW7GRr1VYqayp3+WjSvNS8jzaVwj+J/eLzv+CnR/w0dQeU1iufHTyrcBMRia3IBycxsynAFIDCwkJKSkpatb+KiopW7yPV4pgZ4pm7ZnsNC9765K7QOeTQPfzvY7nhI41UVFTQuXPnttnZJpj50sy22VcTUvX96EpXvtn7m7v1Gnen2sNivLbq4+eq2ioqtlXQsWPHJKX95Pifmk88M4rX33yXtm3btluZE49f/3iNHT/xNb129Ird//tZb9ApvP3+n9ivS/TXsIqISMsks3BbAQxKmB8YLvsUd58KTAWYOHGiFxcXt+qgJSUltHYfqRbHzBDP3HHMDPHMHcfMEM/cccwsKZbXic0FY6AZ92IUEZH0lLPrTVrsDWCEmQ01swLgDGB6Eo8nIiIiIiKSkZL2pzd3rzazi4AnCTqe3ebuc5J1PBERERERkUyV1D4T7v4Y8FgyjyEiIiIiIpLpktlVUkRERERERNqACjcREREREZE0p8JNREREREQkzalwExERERERSXMq3ERERERERNKcCjcREREREZE0p8JNREREREQkzZm7R53hY2a2FviwlbvpDaxrgzipFMfMEM/cccwM8cwdx8wQz9xxzDzY3ftEHSIusrh9hHjmjmNmiGfuOGaGeOaOY2aIZ+4G28i0Ktzagpm96e4To86xO+KYGeKZO46ZIZ6545gZ4pk7jpkl9eL6PYlj7jhmhnjmjmNmiGfuOGaG+OZuiLpKioiIiIiIpDkVbiIiIiIiImkuEwu3qVEHaIE4ZoZ45o5jZohn7jhmhnjmjmNmSb24fk/imDuOmSGeueOYGeKZO46ZIb65PyPjrnETERERERHJNJl4xk1ERERERCSjxLZwM7PjzGy+mS00s8sbWN/OzO4P179mZkMiiJmYZ5CZPWdmc81sjpl9r4Ftis1sk5mVho8ro8han5ktMbN3wkxvNrDezOwP4WddZmb7RZEzIc+ohM+w1Mw2m9nF9bZJi8/azG4zszVm9m7Csp5m9rSZvR8+92jktV8Pt3nfzL4ecebfmtl74c//32bWvZHXNvldSqZGcl9lZisSvgcnNPLaJn/fpDjz/Ql5l5hZaSOvjeyzlmjFrX0MM8WyjYxb+xhmUhuZ+sxp3UbGsX0Mj519baS7x+4B5AKLgGFAATAb2KfeNt8G/hpOnwHcH3Hm/sB+4XQXYEEDmYuBR6P+fBvIvgTo3cT6E4DHAQMOBl6LOnO978oqgvthpN1nDRwB7Ae8m7DsN8Dl4fTlwHUNvK4nsDh87hFO94gw8xeAvHD6uoYyN+e7FEHuq4BLm/EdavL3TSoz11t/PXBlun3WekT3iGP7GOaIZRsZ5/Yx4fuiNjL5mdO6jYxj+9hY7nrrM66NjOsZtwOBhe6+2N0rgWnAyfW2ORn4Rzj9L+AoM7MUZvwUd1/p7m+H01uAecCAqPK0sZOBOz3wKtDdzPpHHSp0FLDI3Vt749qkcPcXgA31Fid+d/8BfLmBlx4LPO3uG9y9HHgaOC5ZORM1lNndn3L36nD2VWBgKrLsjkY+6+Zozu+bpGgqc/j77DTgvlRkkdiIXfsIGd1GpnP7CGoj21wc28g4to+QnW1kXAu3AcCyhPnlfPYX/MfbhP+zbAJ6pSTdLoTdUiYArzWw+hAzm21mj5vZmNQma5QDT5nZW2Y2pYH1zfl5ROUMGv+fNh0/a4BCd18ZTq8CChvYJp0/8/MJ/sLckF19l6JwUdh95bZGutyk62f9OWC1u7/fyPp0/Kwl+WLdPkLs2sg4t4+gNjIKcWoj49o+Qoa2kXEt3GLLzDoDDwIXu/vmeqvfJuiuMA74I/BwiuM15nB33w84HviOmR0RdaDmMLMC4CTgnw2sTtfP+lM8OJ8fm6FfzewnQDVwTyObpNt36S/AcGA8sJKgW0VcnEnTf0lMt89aZJdi2EbG9v8ztZGpF7M2Ms7tI2RoGxnXwm0FMChhfmC4rMFtzCwP6AasT0m6RphZPkGDdI+7P1R/vbtvdveKcPoxIN/Meqc45me4+4rweQ3wb4JT44ma8/OIwvHA2+6+uv6KdP2sQ6vrutKEz2sa2CbtPnMzOxf4EnB22Jh+RjO+Synl7qvdvcbda4G/N5InHT/rPOBU4P7Gtkm3z1pSJpbtY5gldm1kjNtHUBuZUnFrI+PaPkJmt5FxLdzeAEaY2dDwL0ZnANPrbTMdqBtF6KvAs439j5IKYV/bW4F57n5DI9v0q7vOwMwOJPj5RF1sdjKzLnXTBBfYvltvs+nAORY4GNiU0I0hSo3+tSUdP+sEid/drwOPNLDNk8AXzKxH2H3hC+GySJjZccCPgJPcfVsj2zTnu5RS9a41OYWG8zTn902qHQ285+7LG1qZjp+1pEzs2keIZxsZ8/YR1EamTBzbyBi3j5DJbWRzRzFJtwfBSE0LCEaz+Um47GqC/ykA2hOc/l8IvA4Mizjv4QSn88uA0vBxAvBN4JvhNhcBcwhG5XkVODQNPudhYZ7ZYba6zzoxtwE3hz+Ld4CJaZC7E0Ej0y1hWdp91gSN5kqgiqBv+AUE15o8A7wPzAB6httOBG5JeO354fd7IXBexJkXEvRzr/tu141YtwfwWFPfpYhz3xV+Z8sIGpv+9XOH85/5fRNV5nD5HXXf5YRt0+az1iPaR0PfV9K4fQwzxa6NbOz/M9K8fQxzqY1Mbea0biMbyZzW7WNjucPld5ChbaSFb0BERERERETSVFy7SoqIiIiIiGQNFW4iIiIiIiJpToWbiIiIiIhImlPhJiIiIiIikuZUuImIiIiIiKQ5FW4ibcTMasysNOFxeRvue4iZxeMeIyIiIgnUPoq0jbyoA4hkkO3uPj7qECIiImlG7aNIG9AZN5EkM7MlZvYbM3vHzF43s73C5UPM7FkzKzOzZ8xsz3B5oZn928xmh49Dw13lmtnfzWyOmT1lZh0ie1MiIiKtpPZRZPeocBNpOx3qdQU5PWHdJncfC/wJuDFc9kfgH+5eBNwD/CFc/gfgeXcfB+wHzAmXjwBudvcxwEbgK0l9NyIiIm1D7aNIGzB3jzqDSEYwswp379zA8iXAJHdfbGb5wCp372Vm64D+7l4VLl/p7r3NbC0w0N13JuxjCPC0u48I5y8D8t39mhS8NRERkRZT+yjSNnTGTSQ1vJHp3bEzYboGXaMqIiLxp/ZRpJlUuImkxukJzzPD6VeAM8Lps4EXw+lngG8BmFmumXVLVUgREZEUU/so0kz6i4RI2+lgZqUJ80+4e92Qxz3MrIzgr4Jnhsv+F7jdzH4IrAXOC5d/D5hqZhcQ/OXwW8DKZIcXERFJErWPIm1A17iJJFnYh3+iu6+LOouIiEi6UPsosnvUVVJERERERCTN6YybiIiIiIhImtMZNxERERERkTSnwk1ERERERCTNqXATERERERFJcyrcRERERERE0pwKNxERERERkTSnwk1ERERERCTN/X8sJzFy2dlwMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35275733, -2.1040199 ,  1.7610503 , ..., -0.9410262 ,\n",
       "        -9.405142  , -5.3998246 ],\n",
       "       [ 6.1720667 ,  1.3604237 ,  0.52581483, ..., -3.6934588 ,\n",
       "        -5.6055727 ,  3.2385771 ],\n",
       "       [-0.7165133 , -0.7889339 ,  4.1446905 , ..., -3.7276871 ,\n",
       "        -0.8792903 ,  0.71734476],\n",
       "       ...,\n",
       "       [ 1.307783  , -0.26407596, -3.7720563 , ...,  2.347596  ,\n",
       "        -2.108782  ,  2.4457908 ],\n",
       "       [ 2.8201857 ,  0.35596347,  3.2829628 , ...,  0.93782216,\n",
       "        -4.3886576 ,  6.6973114 ],\n",
       "       [ 2.9525733 ,  1.0710117 , -0.23509821, ...,  1.9975362 ,\n",
       "        -2.5879712 ,  2.5029154 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0706,  0.2561,  0.2478,  ...,  0.0673,  0.0031, -0.4901],\n",
       "                      [ 0.1736, -0.1573, -0.0235,  ..., -0.1654, -0.2212,  0.0766],\n",
       "                      [ 0.1346, -0.2606, -0.0259,  ..., -0.0471,  0.3315, -0.2251],\n",
       "                      ...,\n",
       "                      [ 0.1141, -0.3636,  0.2988,  ..., -0.1634, -0.0124, -0.2063],\n",
       "                      [ 0.2902, -0.0515,  0.2353,  ...,  0.0161,  0.3826, -0.3515],\n",
       "                      [-0.0950,  0.1758,  0.0781,  ..., -0.1532,  0.0128, -0.0768]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.0006, -0.0300, -0.1543,  ..., -0.0460,  0.1362,  0.1719],\n",
       "                      [ 0.0598, -0.0329, -0.1208,  ...,  0.1009,  0.0883, -0.1755],\n",
       "                      [-0.2247,  0.0433,  0.1569,  ...,  0.0601, -0.0521, -0.0093],\n",
       "                      ...,\n",
       "                      [-0.0572,  0.0920, -0.2104,  ..., -0.1109, -0.0088,  0.0843],\n",
       "                      [ 0.0428, -0.1226,  0.1813,  ..., -0.0673,  0.1975, -0.1576],\n",
       "                      [ 0.1154, -0.1161,  0.1301,  ..., -0.1765, -0.1297, -0.2094]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-1.8459e-01, -1.7168e-01, -2.0571e-01, -1.7298e-01, -2.0531e-01,\n",
       "                      -8.6969e-02, -1.3057e-01, -1.2225e-01, -7.9841e-02, -2.1315e-01,\n",
       "                      -1.6678e-01, -1.6754e-01, -1.5200e-01, -7.0899e-02, -1.7539e-01,\n",
       "                      -6.2965e-02, -1.2318e-01, -1.4214e-01, -2.0514e-01, -9.5880e-02,\n",
       "                      -1.7223e-01, -2.0700e-01,  2.8908e-02, -1.5095e-01, -7.7546e-02,\n",
       "                      -1.8993e-01, -1.4427e-01, -3.2866e-01, -1.7190e-01, -1.5691e-01,\n",
       "                      -6.4726e-02, -2.1250e-01, -2.4891e-02, -1.6025e-01, -1.7888e-01,\n",
       "                      -1.8907e-01, -1.8781e-01, -1.3460e-01, -8.7262e-02, -1.8273e-01,\n",
       "                      -1.2065e-01, -2.2641e-01,  3.0720e-03, -1.1999e-01, -1.0662e-01,\n",
       "                      -1.8322e-01, -2.0519e-01, -5.5277e-02, -8.8383e-02, -1.1070e-01,\n",
       "                      -2.7953e-01, -1.5284e-01, -1.2044e-01, -1.7754e-01, -1.5616e-01,\n",
       "                       1.5021e-02, -1.0809e-01, -1.3335e-01, -9.7514e-02, -7.7588e-02,\n",
       "                      -1.7400e-01, -1.8111e-01, -1.5125e-01, -6.3800e-02, -4.9685e-02,\n",
       "                      -2.2027e-01, -2.3890e-01, -1.6676e-01, -1.8750e-01, -1.1482e-01,\n",
       "                      -1.3009e-01, -1.0958e-01, -6.2149e-02, -9.3656e-02, -1.4453e-01,\n",
       "                      -1.0088e-01, -7.7121e-02, -1.4053e-01, -3.7359e-03, -1.1832e-01,\n",
       "                      -2.4297e-01, -2.0067e-01, -1.8005e-01, -2.6422e-01, -8.8794e-02,\n",
       "                      -1.4138e-01, -1.2304e-01, -2.0689e-01, -1.7391e-01, -7.5803e-02,\n",
       "                      -1.3787e-01, -2.3171e-01, -1.1690e-01, -8.8827e-02, -1.6594e-01,\n",
       "                      -1.2032e-01, -7.2553e-02, -1.7373e-01, -8.6703e-02, -2.1500e-02,\n",
       "                       6.9731e-03, -8.4064e-02, -1.5695e-01, -1.0305e-01, -7.5675e-02,\n",
       "                      -1.2664e-02,  4.3475e-03, -1.9966e-01, -8.8352e-02, -1.6563e-01,\n",
       "                      -2.6919e-02, -1.5096e-01, -1.7390e-01, -1.4664e-01, -6.4999e-02,\n",
       "                      -1.0524e-01, -1.9006e-01, -1.9282e-02, -5.5658e-02, -2.0705e-01,\n",
       "                      -1.0678e-01, -1.3182e-01, -3.7193e-02, -5.0964e-02, -1.7329e-01,\n",
       "                      -1.8161e-01, -1.3744e-01, -3.8287e-02, -1.7412e-01, -2.3751e-01,\n",
       "                      -7.9413e-02,  1.6216e-02, -4.3149e-02, -6.0321e-02, -9.4472e-02,\n",
       "                      -5.0627e-02, -2.1360e-01, -6.8534e-02, -1.4576e-01, -1.1735e-01,\n",
       "                      -2.6762e-01, -4.3688e-02, -1.4834e-01, -1.0575e-01, -2.8206e-01,\n",
       "                      -2.1122e-01, -3.1124e-02, -2.0038e-01, -1.3363e-01, -8.2664e-02,\n",
       "                      -5.9258e-02, -6.7870e-02, -2.8630e-01, -8.6969e-02, -1.0901e-01,\n",
       "                      -7.2514e-02, -1.7051e-01, -1.3408e-01, -2.4173e-01, -1.7736e-01,\n",
       "                      -5.3439e-02, -1.4134e-01, -1.1709e-01, -1.4886e-01, -1.6196e-01,\n",
       "                      -1.5868e-01, -3.0972e-02, -1.0511e-01, -1.0187e-01, -1.2647e-01,\n",
       "                      -5.1982e-02, -7.7757e-02,  5.6237e-02, -1.0491e-01, -1.5300e-01,\n",
       "                      -1.1689e-01, -2.1496e-01, -1.7428e-01, -2.4827e-01, -1.6312e-01,\n",
       "                      -5.0715e-02, -1.3162e-01, -1.8046e-02, -1.7191e-01, -4.9392e-02,\n",
       "                      -1.2556e-01, -1.0668e-01,  9.7134e-03, -1.0643e-02, -1.6310e-02,\n",
       "                      -4.7035e-02, -5.6430e-02, -2.1924e-02, -2.7987e-01, -1.3426e-01,\n",
       "                      -1.0781e-01, -6.3777e-03, -1.7795e-01, -1.1477e-01, -9.8420e-02,\n",
       "                      -1.6197e-01, -7.5585e-02, -1.5328e-01, -1.3683e-01, -1.2000e-02,\n",
       "                      -1.2506e-01, -1.1462e-01, -8.3624e-02, -1.5205e-01, -2.0746e-01,\n",
       "                      -1.7125e-01, -1.8418e-01, -8.4925e-02, -2.1221e-01, -7.7514e-02,\n",
       "                      -1.8305e-01, -5.7807e-02, -1.8032e-01, -1.2355e-01, -1.8491e-01,\n",
       "                      -1.5804e-01, -1.3306e-01, -2.2971e-01, -1.2554e-01, -1.2133e-01,\n",
       "                      -1.4779e-01, -7.3400e-02, -1.6383e-01,  4.0720e-04, -6.3646e-02,\n",
       "                      -9.1711e-02, -1.7170e-01, -2.7401e-01, -7.4370e-02, -3.7146e-04,\n",
       "                      -2.1476e-02, -2.2189e-01, -1.7403e-01, -1.2083e-01, -1.3289e-01,\n",
       "                      -1.3747e-01, -9.9743e-02, -2.3087e-02, -8.1233e-02, -1.0046e-01,\n",
       "                      -1.1576e-01, -6.9496e-02, -1.4701e-01, -1.3106e-01, -2.2297e-01,\n",
       "                      -1.1650e-01, -1.4112e-01, -1.3982e-01, -1.3268e-01, -2.0529e-01,\n",
       "                      -9.6101e-02, -2.4938e-02,  4.8909e-02,  4.0378e-02, -7.9510e-02,\n",
       "                       5.2525e-02,  4.4226e-03, -1.6937e-02,  3.6586e-02,  2.8599e-02,\n",
       "                       1.1151e-02, -6.5602e-02,  2.6909e-02, -2.2859e-02, -5.0207e-02,\n",
       "                      -1.2896e-02, -6.3050e-02, -8.6819e-02,  6.1184e-02,  8.7417e-02,\n",
       "                      -3.0254e-02,  9.9473e-02,  3.4171e-02,  3.1841e-02, -3.4617e-02,\n",
       "                       2.0938e-02, -3.9862e-02, -6.5614e-03,  2.9229e-02,  7.9038e-03,\n",
       "                      -5.3181e-02, -5.6309e-02, -1.3039e-02, -2.3837e-02,  3.8343e-02,\n",
       "                      -1.2320e-02, -1.0488e-02,  6.3426e-02, -5.3723e-02,  2.4753e-02,\n",
       "                      -1.4926e-02, -3.7102e-02, -1.0191e-01,  4.5423e-02, -4.8495e-02,\n",
       "                      -4.3834e-02, -5.0058e-02, -7.6607e-03,  6.6512e-02, -3.5687e-02,\n",
       "                      -3.2044e-02, -1.9183e-02,  5.1476e-03,  8.6459e-02, -1.2833e-02,\n",
       "                      -1.2299e-01, -4.7638e-02, -1.1373e-02,  1.5960e-02,  5.4219e-02,\n",
       "                       4.6523e-03,  9.3684e-03,  3.9680e-02,  2.7253e-02, -1.0906e-01,\n",
       "                      -6.1935e-02,  3.5700e-03, -9.1084e-02, -6.2085e-02, -3.5744e-02,\n",
       "                       6.9425e-03,  1.2910e-01, -2.6124e-02, -1.0120e-01, -4.9964e-02,\n",
       "                       1.2540e-01,  7.3372e-02,  1.6247e-02,  3.5208e-02, -6.2058e-02,\n",
       "                       1.0772e-02, -7.8205e-03, -2.4480e-02, -6.2170e-02,  4.8079e-02,\n",
       "                      -9.6372e-02,  1.1333e-02,  1.5396e-02,  2.9176e-02,  1.1685e-02,\n",
       "                       4.4908e-02,  4.4205e-02, -9.7675e-02,  4.0970e-02,  4.2851e-02,\n",
       "                       2.6134e-02,  1.7915e-04, -9.5571e-02,  5.2694e-02, -1.6265e-02,\n",
       "                       4.7521e-02, -1.1434e-02,  3.0423e-02, -3.2208e-02, -4.2845e-02,\n",
       "                       3.1254e-02, -6.3912e-02, -4.9433e-02, -9.8816e-02,  9.7852e-02,\n",
       "                       1.6558e-01, -3.3131e-02, -4.6179e-02, -7.3281e-02,  1.5801e-01,\n",
       "                      -4.5197e-02, -5.1470e-03,  3.3588e-02,  3.7760e-02,  2.3700e-02,\n",
       "                      -1.5362e-02, -1.3864e-02, -3.5402e-02,  1.7157e-02,  9.2403e-02,\n",
       "                      -3.8465e-02,  5.6772e-02,  5.1219e-02,  5.5640e-02, -1.5153e-01,\n",
       "                      -1.0519e-01, -2.2335e-01, -1.0530e-01, -4.8631e-03, -1.2291e-01,\n",
       "                      -1.2058e-01, -3.2148e-02, -1.3064e-01, -2.3488e-02, -3.5548e-02,\n",
       "                      -1.7119e-01, -1.0745e-01, -4.0103e-02,  5.6920e-02, -1.3862e-01,\n",
       "                      -6.0551e-03, -2.5458e-01, -3.6551e-02, -5.2580e-02, -1.4314e-01,\n",
       "                      -2.4621e-01, -3.9045e-02, -8.2217e-02, -3.3476e-01, -1.9149e-01,\n",
       "                      -1.8752e-01, -9.4601e-02, -1.5779e-01, -2.0250e-01, -6.9563e-02,\n",
       "                      -1.7337e-01, -2.1161e-02, -1.9350e-01, -1.0823e-01, -1.4476e-01,\n",
       "                      -8.6210e-02, -1.1840e-01, -5.7231e-02, -8.7406e-02, -1.0149e-01,\n",
       "                      -1.5431e-01, -2.0481e-01, -8.1662e-02, -3.0529e-02, -1.9614e-01,\n",
       "                      -2.0554e-01, -1.5390e-01, -1.7945e-01, -8.2026e-02, -1.6684e-01,\n",
       "                      -1.5428e-01, -9.5660e-02, -3.4270e-01, -2.1603e-01, -1.6774e-01,\n",
       "                      -2.1027e-01, -8.6681e-02, -1.8035e-01, -1.9322e-01, -1.3833e-01,\n",
       "                      -6.7304e-02, -1.0161e-01, -5.9339e-02,  7.5386e-03,  2.3928e-02,\n",
       "                      -1.3528e-01, -3.2385e-01, -1.1483e-02, -6.6509e-02, -2.0340e-01,\n",
       "                      -6.0352e-02, -1.8588e-01, -9.5525e-02, -2.0745e-01, -6.7005e-02,\n",
       "                      -1.3220e-01, -2.6303e-01, -7.0311e-02, -1.2220e-01, -9.7303e-02,\n",
       "                      -1.5609e-01, -1.4190e-01, -1.2749e-01, -1.8445e-01, -1.0482e-02,\n",
       "                      -1.4145e-01, -1.3260e-01, -2.0267e-01, -2.7414e-01, -2.6539e-01,\n",
       "                      -1.1151e-01, -4.5833e-02, -2.1213e-01, -1.6691e-01, -1.7044e-01,\n",
       "                      -2.0949e-01, -1.5773e-01, -1.6887e-01, -1.1612e-01, -1.7616e-01,\n",
       "                      -7.7402e-02, -1.5378e-01,  6.4259e-03,  7.9377e-04, -1.0186e-01,\n",
       "                      -1.5206e-01, -1.0685e-01, -8.8348e-02, -2.0941e-01, -9.7752e-02,\n",
       "                      -2.1089e-01, -1.9129e-01, -7.1923e-02, -2.1022e-01, -1.1120e-01,\n",
       "                      -1.2903e-01, -2.5801e-01, -2.8461e-02, -5.3548e-02, -4.3330e-02,\n",
       "                      -7.2908e-02, -8.0040e-02, -2.4982e-01, -1.7901e-01, -1.7376e-01,\n",
       "                      -1.8803e-01,  3.9173e-02])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([-0.1896, -0.1400, -0.0748, -0.1094, -0.0803, -0.1173, -0.2006, -0.0643,\n",
       "                      -0.1696, -0.0764, -0.1191, -0.0902, -0.1813, -0.0610, -0.1589, -0.1992,\n",
       "                      -0.2704, -0.1130, -0.0283, -0.0957, -0.2308, -0.1792, -0.0729, -0.2016,\n",
       "                      -0.1434, -0.0476, -0.0399, -0.2326, -0.3422, -0.2905, -0.1223, -0.1040,\n",
       "                      -0.1730, -0.0764, -0.0573, -0.0438, -0.1166, -0.2771, -0.2539, -0.1828,\n",
       "                      -0.1766, -0.1291, -0.0689, -0.0108, -0.0605, -0.0603, -0.1232, -0.1359,\n",
       "                      -0.1122, -0.1972, -0.1450, -0.0560,  0.0286, -0.0966, -0.0573, -0.1438,\n",
       "                      -0.0827, -0.1376, -0.0255, -0.1102, -0.0967, -0.0798, -0.0465, -0.0854,\n",
       "                      -0.0318, -0.0895, -0.2431, -0.1234, -0.2500, -0.1208, -0.2797, -0.2430,\n",
       "                      -0.1090, -0.1451, -0.0483, -0.1924, -0.1318, -0.2404, -0.1739, -0.1541,\n",
       "                      -0.2211, -0.2536, -0.1120, -0.1885, -0.1921, -0.0014, -0.2490, -0.0988,\n",
       "                      -0.1789, -0.0690, -0.1466, -0.1631, -0.1330, -0.1187, -0.2467, -0.2139,\n",
       "                      -0.2280, -0.1817, -0.1888, -0.0239, -0.2328, -0.0929, -0.0362, -0.1605,\n",
       "                      -0.1768,  0.0316, -0.1193, -0.0824, -0.0450, -0.0558, -0.1246, -0.1703,\n",
       "                      -0.0271, -0.0942, -0.1360, -0.1514, -0.2026, -0.2272, -0.1158, -0.1941,\n",
       "                      -0.0077, -0.1322,  0.0109, -0.1477, -0.1713, -0.2598, -0.0989, -0.1612,\n",
       "                      -0.0425, -0.2457, -0.0567, -0.1598, -0.1546, -0.0700, -0.0916, -0.2175,\n",
       "                      -0.2255, -0.1416, -0.1791, -0.0834, -0.1158, -0.0355, -0.0585, -0.1031,\n",
       "                      -0.1131, -0.0716, -0.0811, -0.0588, -0.1802, -0.1336, -0.1467, -0.0191,\n",
       "                      -0.1963, -0.1469,  0.0131, -0.0496, -0.0285, -0.3427, -0.0899, -0.1426,\n",
       "                      -0.0680, -0.0981, -0.1873, -0.0400, -0.1219, -0.1620, -0.0945,  0.0295,\n",
       "                      -0.0437, -0.1025, -0.0525, -0.0770, -0.1223, -0.2220, -0.2289, -0.0669,\n",
       "                      -0.1046, -0.0551, -0.0916, -0.1148, -0.0882, -0.1196, -0.1424, -0.1581,\n",
       "                      -0.0557, -0.1438, -0.1102, -0.2691, -0.1462,  0.0174, -0.0386, -0.1140,\n",
       "                       0.0510, -0.1007, -0.1671, -0.1166, -0.0736, -0.0613, -0.1925, -0.1110,\n",
       "                      -0.0137, -0.0990, -0.1094, -0.1545, -0.1748, -0.1224, -0.0679, -0.1236,\n",
       "                      -0.1915, -0.0812, -0.0913, -0.0994, -0.0968, -0.0718, -0.1766, -0.0927,\n",
       "                      -0.1150, -0.0568, -0.1365, -0.1878, -0.1205, -0.0600, -0.0701, -0.2275,\n",
       "                      -0.2272, -0.0781, -0.0990, -0.1593, -0.0827, -0.1787,  0.0064, -0.1893,\n",
       "                      -0.2120, -0.1590, -0.0242,  0.0334, -0.0936, -0.0871, -0.1273, -0.1101,\n",
       "                       0.0348, -0.1865, -0.1291, -0.1000, -0.1130, -0.1684, -0.0580, -0.0894,\n",
       "                      -0.0823,  0.0357, -0.0379, -0.1190, -0.0771, -0.1415, -0.1196, -0.0665,\n",
       "                       0.0135, -0.0149,  0.0724, -0.0793, -0.1508,  0.0005, -0.0776,  0.0597,\n",
       "                       0.0244, -0.0358, -0.0357, -0.0016, -0.0040, -0.0901, -0.0301,  0.0516,\n",
       "                       0.0344,  0.0078, -0.0540, -0.0443,  0.0020,  0.0526,  0.0849,  0.0280,\n",
       "                      -0.0033, -0.0157, -0.0426, -0.0366, -0.0219,  0.0198, -0.0363,  0.0217,\n",
       "                      -0.0424,  0.0011, -0.0183,  0.0540, -0.0055,  0.0793,  0.0301,  0.0210,\n",
       "                      -0.0748, -0.0235,  0.0685,  0.1169,  0.0178,  0.1278, -0.0367,  0.1223,\n",
       "                       0.0794, -0.0832,  0.0947, -0.0226,  0.0035, -0.0016,  0.0760, -0.1392,\n",
       "                      -0.0885, -0.1162, -0.0847,  0.0094,  0.0240,  0.0104,  0.0497, -0.0170,\n",
       "                      -0.0714, -0.0549, -0.0730, -0.0116,  0.0195, -0.0450, -0.0285, -0.0858,\n",
       "                      -0.0783, -0.0609, -0.0487,  0.0407,  0.0038, -0.0561, -0.0058,  0.1247,\n",
       "                      -0.0660,  0.0435, -0.1174,  0.0232,  0.0662, -0.0542,  0.0269,  0.0850,\n",
       "                      -0.0694, -0.0774, -0.0521, -0.0165, -0.0901, -0.0494,  0.0178,  0.0363,\n",
       "                       0.0417,  0.0817,  0.0248, -0.1251,  0.0856, -0.0142,  0.1029, -0.0548,\n",
       "                      -0.0280,  0.0179, -0.0479,  0.0436, -0.0084, -0.0216,  0.0170, -0.0050,\n",
       "                       0.0295, -0.0996, -0.0104,  0.0522, -0.1134, -0.0242, -0.0623,  0.0098,\n",
       "                      -0.0416,  0.0117, -0.1092, -0.0305,  0.0304,  0.0588,  0.0049,  0.0481,\n",
       "                      -0.1306, -0.0515, -0.2293, -0.0467, -0.1548, -0.0247, -0.1462, -0.1123,\n",
       "                      -0.2161, -0.1584, -0.1359, -0.1131, -0.1762, -0.1353, -0.1396, -0.2080,\n",
       "                      -0.0682, -0.1836, -0.1066, -0.1066, -0.1568, -0.1698, -0.0429,  0.0304,\n",
       "                      -0.2076, -0.0513, -0.1730, -0.0421, -0.1227, -0.1058, -0.2371, -0.1471,\n",
       "                      -0.1942, -0.2483, -0.2036, -0.0908, -0.0045, -0.0799, -0.2089, -0.1740,\n",
       "                      -0.0736, -0.1720, -0.1087, -0.1053, -0.1151, -0.0612, -0.1677, -0.1099,\n",
       "                      -0.0883, -0.2395, -0.1827, -0.1067, -0.0387, -0.0127, -0.3023, -0.1537,\n",
       "                      -0.1970, -0.0835, -0.1136, -0.0412, -0.1922, -0.1507, -0.0659, -0.1019,\n",
       "                      -0.1164, -0.1790, -0.1070, -0.1539, -0.0625, -0.0954, -0.2502, -0.2174,\n",
       "                      -0.0931, -0.1394, -0.1647, -0.1579, -0.1491, -0.0471,  0.0977, -0.2344,\n",
       "                      -0.0727, -0.2117, -0.1824, -0.0810, -0.1223, -0.0433, -0.1216, -0.0376,\n",
       "                      -0.1751, -0.1538, -0.1367, -0.2010, -0.1118, -0.0749, -0.1099, -0.1641,\n",
       "                      -0.1440, -0.1008, -0.1639, -0.1260, -0.1334, -0.1454, -0.1302, -0.1399,\n",
       "                      -0.0438, -0.0610, -0.0877, -0.1864, -0.1906, -0.0841, -0.1863, -0.1735,\n",
       "                      -0.2036, -0.0120, -0.0970, -0.1451, -0.0676, -0.2962, -0.2520, -0.1610,\n",
       "                      -0.1462, -0.0099, -0.2294, -0.1109, -0.0074, -0.1103, -0.1153,  0.0445])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 5.6288e-02,  5.6581e-02, -6.5771e-02,  ...,  1.3153e-01,\n",
       "                        1.9556e-01, -7.0799e-03],\n",
       "                      [ 7.0441e-02,  2.6354e-01, -1.3965e-01,  ..., -6.8802e-02,\n",
       "                       -3.3846e-02,  4.0243e-01],\n",
       "                      [-2.6118e-04, -1.4432e-01, -3.1300e-01,  ...,  1.5382e-01,\n",
       "                        1.0905e-02, -1.6429e-01],\n",
       "                      ...,\n",
       "                      [-2.2287e-01,  8.5685e-02, -3.1721e-02,  ..., -5.4964e-03,\n",
       "                       -8.3940e-02, -8.6472e-02],\n",
       "                      [-7.6390e-01,  2.1944e-01,  1.7447e-01,  ...,  2.6769e-01,\n",
       "                        1.3888e-01, -6.1639e-01],\n",
       "                      [-9.4874e-02, -1.3232e-01,  1.6675e-01,  ...,  1.0850e-01,\n",
       "                       -4.0632e-02,  8.2124e-02]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0248,  0.1096, -0.1936,  ...,  0.0380,  0.1379,  0.1521],\n",
       "                      [ 0.2346,  0.1034, -0.2066,  ...,  0.4086,  0.0910,  0.0735],\n",
       "                      [ 0.1328, -0.0323, -0.1454,  ...,  0.0813, -0.1120,  0.2856],\n",
       "                      ...,\n",
       "                      [-0.2486, -0.1513,  0.3054,  ...,  0.0754, -0.1643, -0.3457],\n",
       "                      [-0.1628,  0.2259,  0.1904,  ...,  0.1522,  0.0207,  0.1625],\n",
       "                      [-0.0069,  0.0180, -0.0362,  ..., -0.0544,  0.0689, -0.1153]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([-1.9932e-02,  8.3735e-02, -6.4037e-02, -3.1704e-02,  6.8114e-02,\n",
       "                       2.9470e-02,  2.1795e-01, -2.5859e-02, -7.7984e-02, -6.3588e-02,\n",
       "                      -6.0064e-02,  4.4527e-03,  1.1084e-01,  2.7701e-01, -8.2709e-02,\n",
       "                       2.2506e-01, -1.5978e-02,  5.4561e-02, -8.1181e-02,  5.5630e-02,\n",
       "                       1.6155e-01,  3.8957e-02,  5.4267e-02,  1.1832e-01,  2.0628e-01,\n",
       "                       5.9647e-02, -3.8402e-02, -2.3174e-03,  2.1002e-01, -1.9770e-03,\n",
       "                       3.0069e-02,  1.0967e-01,  1.4016e-01, -2.7443e-02,  1.2404e-01,\n",
       "                      -6.8058e-02,  8.5796e-02, -6.6361e-02,  4.7876e-02,  6.3072e-02,\n",
       "                       1.0080e-01,  1.8121e-02,  1.2042e-01,  1.2130e-01,  9.6472e-02,\n",
       "                       7.8747e-02, -5.9917e-02,  8.9941e-02,  7.5345e-02,  1.2613e-01,\n",
       "                       1.1194e-01,  1.4916e-01, -1.8021e-03, -3.0721e-02, -1.2486e-01,\n",
       "                       1.0687e-02,  1.0678e-01,  1.5755e-03, -1.1425e-02, -2.3504e-02,\n",
       "                      -1.9763e-03,  8.2587e-02, -1.4839e-01, -1.2220e-01, -1.0433e-01,\n",
       "                      -9.0213e-02,  5.4326e-02, -7.7984e-02, -1.7309e-02,  1.1832e-01,\n",
       "                       8.4933e-02,  3.3106e-03, -1.2785e-01,  1.8525e-01, -2.7086e-03,\n",
       "                      -4.2529e-02,  7.3244e-02,  1.4857e-01,  7.4677e-02,  1.6206e-01,\n",
       "                       1.0503e-01,  1.4963e-01, -7.4538e-02, -1.0031e-01,  7.4571e-02,\n",
       "                       6.4709e-02, -5.5386e-02,  6.8264e-02,  3.0162e-02,  6.6026e-02,\n",
       "                       8.0165e-02,  1.3074e-01, -2.8980e-02, -3.7818e-02,  7.5877e-03,\n",
       "                       5.9447e-02, -8.3348e-02, -4.1646e-02,  1.3752e-01,  8.8481e-02,\n",
       "                      -3.9599e-02,  1.1338e-01, -4.5377e-02,  2.6569e-02, -7.8971e-03,\n",
       "                       8.0465e-03,  4.0538e-02,  4.5157e-02,  6.7874e-03, -4.6101e-02,\n",
       "                      -1.8362e-02, -2.8570e-02, -1.1950e-01, -4.2953e-03, -9.1850e-02,\n",
       "                       1.8702e-02,  1.2883e-01, -4.5420e-02,  1.0433e-02, -2.2897e-02,\n",
       "                       8.7579e-02,  7.6956e-02, -7.9922e-03, -8.5746e-02,  8.7395e-02,\n",
       "                       1.3137e-01, -9.4798e-03, -2.1290e-02, -6.5325e-02, -7.2425e-02,\n",
       "                      -1.5921e-02, -8.1175e-02,  8.0049e-02, -2.6087e-02, -3.9160e-02,\n",
       "                       1.5593e-01, -1.4148e-02, -2.2137e-01, -6.9021e-02, -6.7311e-02,\n",
       "                      -4.0595e-02, -7.0424e-02, -5.6691e-02, -9.2284e-02, -4.4161e-02,\n",
       "                      -8.1609e-02, -1.0018e-01, -5.8787e-02, -6.3798e-02, -7.6644e-02,\n",
       "                       1.6108e-03, -1.2883e-01, -8.5269e-02, -1.7836e-01,  6.5401e-02,\n",
       "                      -1.7239e-02,  5.6969e-03, -2.4750e-01,  1.0622e-01,  4.6059e-03,\n",
       "                      -2.2061e-01,  3.2525e-02, -3.1070e-01, -1.0784e-01, -5.1469e-02,\n",
       "                       5.7356e-02,  3.8811e-02, -1.7880e-01, -1.5345e-01, -7.3794e-02,\n",
       "                       1.3865e-01, -7.7296e-02, -1.5187e-01,  1.8602e-02,  4.8447e-02,\n",
       "                      -4.8203e-02, -1.1746e-01, -8.8623e-02, -1.8599e-01, -1.3213e-01,\n",
       "                       2.0229e-03, -1.7643e-01, -1.2568e-01, -5.5499e-02,  8.5189e-02,\n",
       "                      -4.6500e-02, -5.0698e-04,  2.3104e-03,  1.3086e-02, -1.2838e-01,\n",
       "                       9.2826e-02, -6.6201e-02,  8.6703e-02, -8.5179e-02, -2.0831e-01,\n",
       "                      -1.1144e-01, -2.0588e-02,  2.4048e-02, -4.6395e-02, -1.0273e-01,\n",
       "                       8.7730e-02, -3.0923e-02, -4.6680e-02, -1.2937e-01, -8.4059e-02,\n",
       "                       5.5021e-03, -5.6822e-02,  2.0172e-03, -2.1504e-01, -1.1968e-01,\n",
       "                      -2.2944e-01, -7.4887e-02, -1.6389e-03, -1.4422e-02, -1.1475e-01,\n",
       "                       1.0713e-02, -1.4576e-01,  5.4616e-02, -1.0076e-03, -5.6314e-02,\n",
       "                       1.8090e-02,  4.6784e-02, -7.0432e-02, -4.4323e-02, -1.3929e-01,\n",
       "                       9.6579e-02,  1.1688e-01, -1.3255e-01, -1.0690e-01, -9.6012e-02,\n",
       "                      -2.9296e-02, -1.5239e-02,  2.4658e-02,  9.1739e-02,  5.8999e-02,\n",
       "                      -1.0708e-01, -5.4104e-02, -3.5511e-02, -1.9843e-01,  4.4199e-02,\n",
       "                      -1.0815e-01, -6.5832e-02,  9.7683e-02, -6.0503e-02, -1.7995e-01,\n",
       "                       3.1798e-02, -1.1847e-02,  9.2934e-02, -2.3866e-02,  4.3316e-03,\n",
       "                       7.2700e-02, -1.2393e-02,  1.1012e-01, -9.2339e-02, -1.4449e-01,\n",
       "                       6.2041e-03, -4.5435e-02, -1.0552e-01,  6.7125e-02, -4.2889e-02,\n",
       "                      -9.7378e-02, -1.4765e-01,  5.3711e-02, -9.6912e-03,  2.3664e-02,\n",
       "                       8.5829e-02,  7.3530e-02,  7.5810e-02,  2.2063e-02,  4.8264e-02,\n",
       "                       1.3828e-01,  6.2599e-02, -1.3213e-02,  4.7801e-03, -2.7468e-02,\n",
       "                       7.8140e-03, -4.4026e-02, -3.9363e-02, -3.7678e-02,  2.7539e-03,\n",
       "                       8.9006e-02,  2.1206e-02,  3.9740e-02,  7.2445e-02, -3.2396e-02,\n",
       "                      -3.7106e-02,  1.6899e-02,  1.0439e-01,  6.1844e-02, -1.4258e-02,\n",
       "                       4.8135e-02,  3.1072e-02,  1.0732e-01, -7.5635e-02, -7.3686e-02,\n",
       "                      -4.0998e-02,  5.0457e-03,  1.4440e-02,  3.7491e-02,  2.5626e-02,\n",
       "                       3.2292e-02,  3.3571e-03, -7.3896e-02,  4.9227e-02,  1.3583e-02,\n",
       "                      -9.4405e-02,  4.9395e-02,  4.4154e-02, -7.7837e-02, -1.7915e-02,\n",
       "                       1.1504e-01,  3.6699e-02, -3.2893e-02, -4.8422e-02, -4.3834e-02,\n",
       "                       5.1490e-05, -2.6227e-02,  1.7045e-02, -4.7354e-03, -7.7355e-03,\n",
       "                      -6.4425e-02,  1.1199e-01, -3.0489e-02,  1.2401e-02, -2.3993e-02,\n",
       "                       1.3841e-02, -5.4075e-02,  2.3509e-02,  3.7844e-02, -4.9406e-02,\n",
       "                      -1.8465e-02,  7.9489e-02,  8.1221e-03,  3.6541e-02,  1.1538e-01,\n",
       "                       1.5842e-02, -3.5867e-02, -5.0781e-02, -5.4112e-02, -7.0139e-02,\n",
       "                      -1.7190e-02, -5.5816e-02, -4.5232e-02, -2.3721e-02,  2.7647e-02,\n",
       "                       1.0641e-02,  3.6086e-02,  4.6789e-02,  1.0104e-01,  7.6065e-03,\n",
       "                       6.2950e-02, -3.2900e-02,  7.3582e-02,  4.6680e-03,  1.5030e-02,\n",
       "                      -1.3486e-01, -1.1091e-01, -6.7852e-04, -6.8582e-02,  6.2815e-02,\n",
       "                      -6.0279e-02, -7.2406e-03,  1.4971e-02, -9.2615e-02, -1.9322e-02,\n",
       "                       4.0713e-02,  5.6641e-02,  1.1404e-01, -2.5677e-02,  2.8208e-02,\n",
       "                       1.6025e-02, -1.1927e-01, -1.9237e-02, -3.0723e-02, -1.5004e-02,\n",
       "                       6.1050e-02, -3.0417e-02, -2.4663e-02,  6.0109e-02,  2.0192e-02,\n",
       "                       7.5733e-03, -4.8319e-02,  3.1279e-02, -2.4789e-02,  6.7939e-02,\n",
       "                      -5.5919e-02, -6.6998e-02,  1.5325e-02,  1.8610e-01,  1.3512e-01,\n",
       "                       1.8623e-01, -1.4742e-02,  4.0560e-02, -2.0034e-02, -2.8219e-02,\n",
       "                       9.1257e-02, -8.6651e-02,  6.2126e-02,  3.7483e-02,  3.8677e-03,\n",
       "                       3.4029e-02,  3.4744e-01, -1.0513e-02,  1.9436e-01,  7.7928e-02,\n",
       "                       4.8832e-03, -9.0134e-02,  6.4820e-02,  9.3800e-02,  1.0452e-01,\n",
       "                      -1.5673e-01, -1.4953e-01,  2.1947e-01, -7.9284e-02, -5.4808e-02,\n",
       "                       6.8873e-02,  8.6207e-03,  4.8322e-02,  9.4003e-02,  7.0949e-02,\n",
       "                       6.5280e-02,  2.5584e-03, -2.2860e-02,  3.1101e-02,  1.3545e-01,\n",
       "                       1.2845e-02,  1.1626e-01,  1.2852e-01,  1.4371e-01,  3.4176e-02,\n",
       "                      -7.8229e-02,  8.5251e-02,  1.0757e-01,  1.8612e-01,  1.1172e-01,\n",
       "                       5.1513e-02,  9.7335e-02,  1.2132e-01, -3.2320e-02,  8.7722e-02,\n",
       "                       1.1372e-01,  3.1554e-02,  7.7522e-02,  6.3857e-02, -1.0834e-02,\n",
       "                       2.2628e-02, -3.3860e-03, -9.1267e-02, -3.9214e-02,  2.8170e-02,\n",
       "                       7.7947e-02, -6.8053e-02,  7.8478e-03,  6.6322e-02, -2.9630e-02,\n",
       "                      -3.4324e-03, -9.9398e-03,  2.9201e-01,  1.6586e-02,  6.3370e-02,\n",
       "                       1.5758e-01,  3.6979e-02, -2.4299e-02,  2.1273e-02, -4.8387e-02,\n",
       "                       1.4161e-02, -6.1831e-02, -1.2436e-03,  1.1189e-01,  1.0331e-01,\n",
       "                       1.7148e-01,  1.3253e-01,  1.3716e-01,  1.1670e-01,  5.4742e-02,\n",
       "                       2.1794e-01,  1.3683e-01,  9.1873e-02,  1.4113e-01,  2.8235e-01,\n",
       "                      -1.0492e-02,  2.7431e-02,  7.9436e-02,  2.4869e-01, -1.1697e-01,\n",
       "                       1.6388e-01, -1.5806e-02,  8.9488e-02,  1.3876e-01, -4.3831e-02,\n",
       "                      -3.6086e-02,  2.0657e-01,  4.7867e-02,  1.0500e-01, -1.5214e-03,\n",
       "                      -8.4684e-02, -1.1028e-01, -4.8523e-02, -1.9760e-02,  1.0750e-01,\n",
       "                       1.3531e-01,  1.3871e-01,  1.7566e-03,  1.4145e-01,  4.5245e-02,\n",
       "                       6.2747e-02,  7.7957e-02,  6.8021e-02,  1.1991e-01,  1.4493e-01,\n",
       "                      -1.1788e-02, -6.3280e-02])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 0.0937,  0.0219,  0.0253,  0.1195,  0.1033,  0.0608,  0.1729,  0.0641,\n",
       "                       0.0443, -0.0434, -0.0022,  0.0031,  0.0592,  0.0782,  0.0621,  0.1483,\n",
       "                       0.1283,  0.2330,  0.0453,  0.1313,  0.1765,  0.0769, -0.0815,  0.0161,\n",
       "                       0.1224,  0.1318, -0.0788,  0.1061,  0.2185,  0.1035, -0.0183,  0.1573,\n",
       "                       0.1046,  0.0416,  0.0246,  0.0364,  0.0342, -0.0311, -0.0462,  0.0569,\n",
       "                       0.1510,  0.1238,  0.0861, -0.0216,  0.1935,  0.1986, -0.0026, -0.0232,\n",
       "                       0.1657,  0.0821, -0.0291,  0.0320,  0.0754,  0.1373, -0.0917, -0.0269,\n",
       "                       0.1609,  0.0931, -0.1092, -0.1614, -0.0150,  0.0061, -0.0377, -0.0201,\n",
       "                       0.0966, -0.0824,  0.1091, -0.0022,  0.0074,  0.0534, -0.0451,  0.0098,\n",
       "                      -0.0288,  0.3169,  0.1180,  0.0731,  0.1824,  0.1695,  0.0308,  0.1247,\n",
       "                       0.0034,  0.1852,  0.0230,  0.0319,  0.0294,  0.0643, -0.0064,  0.0206,\n",
       "                      -0.0478,  0.0341,  0.1471,  0.2235, -0.0326, -0.0005,  0.0156,  0.1547,\n",
       "                       0.0225,  0.0578,  0.0764,  0.1936, -0.1436,  0.1177, -0.0983,  0.1161,\n",
       "                       0.0874,  0.0061,  0.0596,  0.2097, -0.1368, -0.0237,  0.0474, -0.0493,\n",
       "                      -0.0462, -0.0970, -0.0430,  0.0710,  0.1342,  0.0426,  0.0488,  0.0910,\n",
       "                      -0.0112,  0.0676, -0.0850, -0.0337, -0.0129,  0.1838, -0.0392,  0.0105,\n",
       "                      -0.1391, -0.0679, -0.0171, -0.0456,  0.1165, -0.0707, -0.1040, -0.1541,\n",
       "                      -0.1493, -0.0566, -0.0918, -0.1104, -0.0049, -0.0166, -0.0336, -0.0145,\n",
       "                      -0.0444, -0.1741, -0.0562,  0.0020,  0.0227,  0.0990,  0.0137,  0.0641,\n",
       "                      -0.0312, -0.2370, -0.0836,  0.0076, -0.0510, -0.0700, -0.0174,  0.0079,\n",
       "                      -0.1897,  0.0119, -0.0657, -0.0059, -0.0864, -0.1440,  0.1156, -0.0915,\n",
       "                      -0.1207, -0.2388,  0.1351, -0.0739, -0.2446,  0.1591,  0.0328,  0.0468,\n",
       "                      -0.1347, -0.0648, -0.1341, -0.1936, -0.0014, -0.1424, -0.0900, -0.0483,\n",
       "                      -0.0624, -0.0835, -0.1229,  0.0573,  0.0099, -0.0307,  0.0467,  0.0213,\n",
       "                       0.0186, -0.0037, -0.0131, -0.0763,  0.0295, -0.0121, -0.0267, -0.1018,\n",
       "                       0.0972, -0.1041,  0.0122, -0.1776, -0.0926, -0.1043, -0.0146, -0.0793,\n",
       "                      -0.1082, -0.0978, -0.0053,  0.0632,  0.0495, -0.1171, -0.0079, -0.0250,\n",
       "                      -0.0241,  0.0432,  0.0113, -0.0035,  0.0689, -0.1102, -0.1865, -0.0430,\n",
       "                      -0.2501,  0.1926, -0.0169, -0.0318, -0.0978, -0.2074,  0.0382, -0.0560,\n",
       "                      -0.0773,  0.0372,  0.0684, -0.0283, -0.0283, -0.0440,  0.0889, -0.0213,\n",
       "                       0.0156, -0.0391, -0.0814,  0.0048, -0.1752, -0.0324,  0.1156,  0.0866,\n",
       "                      -0.1151, -0.0548, -0.0140, -0.0654,  0.0054,  0.0559, -0.2877,  0.1585,\n",
       "                       0.1065,  0.0422, -0.0019,  0.0475,  0.0404,  0.0686,  0.0016,  0.0934,\n",
       "                       0.0350,  0.0014, -0.0804, -0.0944,  0.0996, -0.0330, -0.0210,  0.0664,\n",
       "                      -0.0593, -0.0225, -0.0079,  0.0217,  0.0989, -0.0799, -0.0357, -0.0027,\n",
       "                      -0.0255, -0.0060,  0.0689,  0.0520,  0.1154, -0.0154,  0.0076, -0.0248,\n",
       "                      -0.0497, -0.0667, -0.0242, -0.0758, -0.1256,  0.0056, -0.0233, -0.0092,\n",
       "                      -0.0028,  0.1072, -0.0525, -0.0615,  0.0019,  0.0615, -0.0054, -0.0288,\n",
       "                      -0.0694, -0.0050, -0.0087,  0.1067, -0.0727,  0.0843,  0.0263, -0.0094,\n",
       "                       0.0904, -0.0635,  0.0019, -0.0435,  0.1037, -0.0709,  0.1200,  0.0671,\n",
       "                      -0.0075, -0.0515, -0.0111,  0.0093,  0.0567, -0.0304,  0.0194,  0.0753,\n",
       "                       0.0052, -0.0308,  0.0235, -0.0698, -0.0114, -0.0089,  0.1383, -0.0028,\n",
       "                      -0.1007,  0.0570,  0.0685,  0.0784,  0.0599,  0.0171, -0.0333, -0.0443,\n",
       "                      -0.1109, -0.1780,  0.0356, -0.0096, -0.0410, -0.0165, -0.0379,  0.1620,\n",
       "                      -0.0730, -0.0098,  0.0282, -0.0023,  0.0393,  0.0059,  0.0405,  0.0027,\n",
       "                      -0.0381, -0.0282, -0.0154, -0.0659,  0.0041,  0.0318,  0.0299,  0.0039,\n",
       "                       0.1089,  0.0477,  0.0266,  0.0171,  0.0969, -0.0849, -0.0299,  0.0594,\n",
       "                      -0.0335,  0.0221,  0.0757,  0.1065,  0.0449,  0.1035,  0.0627,  0.1061,\n",
       "                      -0.0583,  0.0600, -0.0170,  0.0543,  0.0888,  0.2424,  0.1229, -0.0960,\n",
       "                      -0.0861, -0.0746, -0.0582,  0.1194,  0.0427,  0.0136,  0.1314,  0.1112,\n",
       "                       0.0952,  0.2584, -0.0411,  0.0173,  0.1075,  0.1305, -0.0182,  0.1403,\n",
       "                       0.2157, -0.1094, -0.0378, -0.1295,  0.2483,  0.0054, -0.0331,  0.2028,\n",
       "                       0.1108,  0.0574,  0.0432,  0.0799,  0.2029, -0.0122,  0.0701,  0.1105,\n",
       "                       0.2518,  0.2419, -0.0033,  0.1877,  0.0909,  0.0643,  0.0269,  0.0104,\n",
       "                       0.1413,  0.1336, -0.0151,  0.0781,  0.0200,  0.1023,  0.0252, -0.0348,\n",
       "                       0.1458,  0.0438,  0.0644,  0.1585,  0.0796,  0.0695, -0.0451, -0.0562,\n",
       "                      -0.1000, -0.0031,  0.2040,  0.0549,  0.1360,  0.0430,  0.0797,  0.0427,\n",
       "                       0.0167,  0.1025,  0.0450, -0.0390,  0.1009,  0.0503,  0.0130,  0.0653,\n",
       "                      -0.0068,  0.0592, -0.1418,  0.1043,  0.1081,  0.0107,  0.0327,  0.0566,\n",
       "                      -0.0368, -0.0034,  0.0663,  0.0524,  0.0959,  0.1055,  0.0478,  0.2913,\n",
       "                      -0.0644,  0.1386,  0.0490,  0.1320, -0.0937,  0.0704,  0.0117,  0.0487,\n",
       "                      -0.0652, -0.0144,  0.0160,  0.0909,  0.0258, -0.0357,  0.0027,  0.0462,\n",
       "                      -0.0938, -0.0267, -0.0393,  0.1933,  0.1100,  0.0022,  0.1621, -0.0270,\n",
       "                       0.0346,  0.0403, -0.0472,  0.0702,  0.2031,  0.1905,  0.0276, -0.1525])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.3050,  0.1012,  0.0108,  ...,  0.1193,  0.1465,  0.4488],\n",
       "                      [ 0.4518,  0.0136,  0.3288,  ..., -0.2813,  0.4809,  0.1592],\n",
       "                      [-0.2607,  0.0789, -0.0878,  ...,  0.1163, -0.2505,  0.0140],\n",
       "                      ...,\n",
       "                      [ 0.4244, -0.3185, -0.6913,  ..., -0.6083, -0.0765, -0.1445],\n",
       "                      [-0.3566, -0.1565, -0.0905,  ...,  0.3061,  0.0156,  0.1169],\n",
       "                      [ 0.0877, -0.1731, -0.2259,  ..., -0.1858, -0.3341,  0.2257]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.2009, -0.1080,  0.1122,  ...,  0.1692, -0.0771, -0.2411],\n",
       "                      [-0.0127, -0.0107,  0.0532,  ...,  0.1090, -0.1050, -0.1262],\n",
       "                      [ 0.0025,  0.0032, -0.0176,  ..., -0.0525, -0.0761, -0.0530],\n",
       "                      ...,\n",
       "                      [ 0.1176,  0.0903,  0.0239,  ...,  0.1947, -0.0976, -0.1210],\n",
       "                      [ 0.0898, -0.0132,  0.1842,  ..., -0.0366, -0.1285,  0.1472],\n",
       "                      [ 0.0678, -0.1901,  0.0781,  ..., -0.0955, -0.0965, -0.1658]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-1.2787e-01, -8.3490e-02,  4.0282e-02, -1.4540e-01,  3.1356e-02,\n",
       "                      -1.1586e-01, -9.3057e-02, -1.5881e-02,  1.4023e-01, -1.9933e-01,\n",
       "                      -4.4331e-02, -1.5265e-01, -9.3668e-02, -5.0181e-03,  2.4399e-03,\n",
       "                      -1.8419e-01, -1.9322e-01, -1.6734e-01, -2.4275e-01, -1.2968e-01,\n",
       "                      -7.4558e-02, -1.4208e-01,  9.1440e-02, -4.5323e-02, -1.5362e-01,\n",
       "                      -1.0685e-01, -3.3392e-02, -1.4589e-01, -1.4552e-01, -8.0053e-02,\n",
       "                       1.2284e-01, -7.4192e-02, -1.9390e-01, -3.2305e-03, -7.6928e-02,\n",
       "                       1.2445e-02, -1.5180e-01, -2.3243e-02, -3.8159e-03, -1.3270e-01,\n",
       "                      -1.4572e-01,  1.2372e-01, -1.0267e-01, -1.1962e-01,  4.2872e-02,\n",
       "                       3.2719e-02, -2.7461e-01, -1.0414e-01, -1.1848e-01, -6.4607e-02,\n",
       "                      -7.7445e-02, -2.3204e-01, -2.7436e-01, -1.2827e-01, -1.1122e-01,\n",
       "                       1.2644e-02, -1.5393e-01,  2.0798e-02, -1.4044e-02, -1.1403e-01,\n",
       "                      -1.6053e-01, -1.2703e-01,  3.2393e-03, -1.4074e-01, -1.3273e-01,\n",
       "                      -1.5197e-01, -1.0382e-01, -9.7112e-02, -1.2587e-01,  1.0364e-01,\n",
       "                      -1.2858e-01,  9.3291e-02, -8.9345e-02, -2.5049e-01, -7.9617e-02,\n",
       "                      -1.1557e-01, -1.2102e-01, -1.3544e-02, -1.0936e-01, -2.0323e-01,\n",
       "                       1.7380e-02, -3.6103e-03, -1.7094e-01, -1.0117e-01, -9.4451e-02,\n",
       "                      -3.3960e-02,  1.8450e-02, -1.9013e-01, -1.0054e-01,  9.7015e-03,\n",
       "                       3.4908e-03, -1.6288e-01, -1.9121e-01, -8.4069e-02, -3.1150e-03,\n",
       "                       7.9704e-02, -2.3604e-01, -3.7045e-02, -9.0555e-02, -1.0043e-01,\n",
       "                      -5.3893e-02, -2.2092e-02, -1.5108e-01, -3.0051e-02, -6.6812e-02,\n",
       "                      -1.7386e-01, -2.3621e-02,  2.0210e-02, -1.4930e-01, -1.5204e-01,\n",
       "                       2.3195e-01, -1.3505e-01, -1.6570e-01, -1.2284e-01, -1.6220e-01,\n",
       "                      -3.1093e-01, -2.2057e-01,  2.7990e-02, -1.1590e-02, -1.0940e-01,\n",
       "                      -3.0836e-01, -6.0515e-02, -1.4341e-01, -2.8109e-01, -1.8489e-01,\n",
       "                       1.1390e-02, -4.7307e-02, -2.8400e-02, -1.0390e-01, -7.1489e-02,\n",
       "                       2.7920e-02, -1.0145e-01, -3.7346e-02,  1.5656e-02, -1.3126e-03,\n",
       "                       1.2369e-02,  7.7737e-02, -7.4076e-02, -4.2525e-02, -6.2780e-02,\n",
       "                      -1.0034e-01,  4.9950e-02,  3.4500e-02, -5.2288e-02, -8.1795e-03,\n",
       "                      -2.3304e-01, -9.5211e-03, -1.9506e-01, -4.4484e-02, -1.8551e-01,\n",
       "                      -1.2726e-01, -1.0550e-01, -3.1012e-02, -9.3195e-02,  5.4652e-02,\n",
       "                      -1.7125e-01,  1.4776e-02, -1.3729e-01,  3.8813e-03, -1.2343e-01,\n",
       "                      -2.2260e-03, -3.1155e-02,  8.3290e-02, -6.5455e-02,  4.3488e-03,\n",
       "                       1.1106e-01,  1.1183e-01, -4.5682e-02, -1.1066e-02,  2.4570e-02,\n",
       "                       2.8043e-02, -1.2933e-01,  8.0343e-02,  1.7726e-01, -8.9300e-02,\n",
       "                       8.4142e-02, -8.0629e-02,  2.0519e-02, -2.1497e-02, -1.4101e-01,\n",
       "                      -1.1357e-01,  1.2438e-01,  2.5909e-02,  8.8805e-02, -2.3278e-01,\n",
       "                      -4.9469e-02,  5.7925e-02, -1.4069e-01, -2.3233e-01,  1.0913e-02,\n",
       "                       1.7712e-01, -1.7008e-01,  9.7833e-02, -1.6189e-01,  1.4987e-01,\n",
       "                      -6.4622e-02,  5.5425e-02, -7.3772e-02, -9.7868e-02,  3.0556e-01,\n",
       "                      -9.4095e-02, -1.5480e-01,  8.4196e-02, -9.8705e-03,  7.7168e-02,\n",
       "                      -7.7989e-02, -7.5696e-02, -2.4772e-02,  1.1064e-02,  1.9236e-02,\n",
       "                      -1.8064e-01, -1.9880e-02, -1.9577e-02,  2.2888e-01, -4.5252e-02,\n",
       "                       2.9336e-02, -7.9389e-02,  1.0314e-01, -5.2293e-02, -5.9903e-02,\n",
       "                      -9.0525e-02, -6.3646e-02, -8.4501e-02,  5.0450e-02, -1.5963e-01,\n",
       "                       1.3825e-01, -5.1409e-03, -6.7374e-02,  8.8259e-02, -1.5434e-02,\n",
       "                       9.2711e-02,  5.7551e-02, -8.1100e-02, -1.1890e-01,  5.1630e-02,\n",
       "                      -1.3865e-01, -8.1824e-02, -4.5878e-02,  1.4958e-01, -8.9183e-02,\n",
       "                      -6.6599e-02,  2.7879e-02, -2.1817e-01, -3.5094e-02, -8.3155e-02,\n",
       "                       1.8640e-01, -2.6348e-02,  1.6513e-01, -1.9460e-01, -5.4231e-02,\n",
       "                      -6.3751e-02,  7.7591e-02, -1.1596e-01,  9.3567e-02,  4.6322e-02,\n",
       "                       7.7839e-02,  4.5682e-02, -3.6550e-03, -8.0116e-02, -1.0639e-02,\n",
       "                       2.2178e-02,  7.9258e-02,  4.4350e-02,  1.0517e-01, -1.4212e-04,\n",
       "                      -1.3771e-02,  2.1339e-02, -6.5115e-02,  7.5148e-02, -5.5824e-02,\n",
       "                      -1.7925e-02, -5.0363e-02, -5.0154e-02,  6.1021e-02,  2.1674e-02,\n",
       "                       8.7399e-02,  5.7891e-03, -1.2658e-02, -1.8421e-02, -8.9774e-02,\n",
       "                      -3.7141e-02,  2.2764e-03,  7.7626e-02,  4.3175e-02,  1.9220e-02,\n",
       "                       2.0439e-02, -5.6432e-02,  9.7592e-02, -4.2246e-02, -1.4881e-02,\n",
       "                      -6.0549e-02,  5.5491e-02,  1.1158e-02,  1.8355e-03, -1.1650e-01,\n",
       "                       5.9342e-03,  1.8465e-02, -4.5581e-03, -4.7721e-02, -1.2858e-02,\n",
       "                       5.6706e-02,  2.3184e-02, -1.4094e-01,  2.3662e-02,  6.3218e-02,\n",
       "                       2.2956e-03, -6.9671e-02, -1.8276e-02, -5.0185e-02, -2.0616e-02,\n",
       "                       1.4946e-03,  3.0569e-02,  6.8627e-02, -7.0571e-02, -4.1886e-02,\n",
       "                       7.8933e-03,  6.9830e-03,  9.4440e-03, -1.1290e-01, -8.4132e-02,\n",
       "                      -1.7434e-01,  7.6902e-02, -2.0036e-02, -1.4518e-01,  5.2178e-03,\n",
       "                      -6.5560e-02, -1.0713e-02, -4.8932e-02,  6.4888e-02,  6.7032e-02,\n",
       "                       8.8067e-02, -7.1906e-02,  1.6316e-02,  2.4009e-02, -8.1387e-02,\n",
       "                      -3.1867e-05, -1.1804e-01,  5.1437e-02,  6.3433e-02, -1.4213e-02,\n",
       "                       1.6809e-01,  1.1291e-01,  4.2569e-02, -1.3530e-02,  3.7815e-02,\n",
       "                       2.9837e-03, -4.3412e-02, -7.6276e-03, -2.4454e-02,  2.7150e-02,\n",
       "                       1.6489e-02,  7.2503e-03, -3.2974e-02,  4.6199e-02, -1.0848e-02,\n",
       "                       3.8232e-02, -5.4716e-02, -3.3090e-03, -4.1239e-02, -1.7981e-02,\n",
       "                      -2.8654e-02, -5.0898e-02, -1.0198e-01, -5.8317e-02,  2.9202e-02,\n",
       "                       1.1834e-01,  4.4027e-02, -4.1910e-02, -4.8563e-02,  5.1738e-02,\n",
       "                       1.2719e-01,  3.0937e-02,  2.2429e-02, -6.8251e-02, -1.7035e-02,\n",
       "                      -3.9818e-02,  4.9783e-02,  1.5689e-02,  4.1507e-02,  5.2357e-02,\n",
       "                       6.5185e-02,  3.3004e-02,  1.1229e-01, -4.4191e-02, -1.3639e-01,\n",
       "                      -1.3083e-01,  1.3465e-02, -6.2522e-02, -5.2879e-02, -5.5566e-02,\n",
       "                      -9.4385e-02, -1.0534e-01, -4.3527e-02, -1.8128e-01, -1.5950e-01,\n",
       "                      -1.5382e-01, -6.7543e-02, -1.1788e-01, -6.0615e-02, -1.1346e-01,\n",
       "                      -7.1644e-02, -1.1129e-01, -1.5098e-01, -2.3130e-02, -1.3136e-01,\n",
       "                      -2.0528e-01, -8.5650e-02, -4.0516e-02, -1.3550e-01, -1.6449e-01,\n",
       "                       7.6663e-02,  4.2768e-02, -5.5639e-02, -9.2224e-02, -3.4827e-02,\n",
       "                      -4.2829e-02, -6.8078e-02, -1.2093e-01, -1.5350e-01, -3.3414e-02,\n",
       "                      -9.4218e-02, -3.4595e-02, -2.2311e-02, -9.4268e-02, -1.7727e-01,\n",
       "                      -8.9272e-02, -3.7056e-02, -3.5067e-02,  2.6650e-01,  1.2671e-01,\n",
       "                      -2.2185e-02,  1.4844e-01,  8.7237e-03,  5.5187e-02,  5.3357e-03,\n",
       "                      -1.6764e-01, -2.6574e-01, -1.2042e-01, -3.2861e-02,  3.6437e-02,\n",
       "                      -1.5607e-01, -4.0770e-02, -1.7151e-01, -5.3743e-02, -1.5582e-01,\n",
       "                      -1.1340e-01, -2.0672e-02, -1.3838e-01, -7.4146e-02, -4.1461e-02,\n",
       "                       9.6344e-02, -8.8560e-02,  3.0979e-03, -6.9264e-02,  8.9878e-04,\n",
       "                       9.4894e-02, -4.9854e-02, -9.0494e-02, -7.1002e-02, -1.2670e-01,\n",
       "                      -8.0268e-02, -1.2139e-01,  5.6619e-02, -6.8567e-02, -1.5076e-01,\n",
       "                      -2.6395e-01, -4.4273e-02,  4.3072e-02, -2.9363e-01, -5.4474e-02,\n",
       "                      -6.8831e-02, -7.7894e-02, -1.7955e-01, -9.4004e-02, -2.0939e-02,\n",
       "                      -4.4091e-02, -1.5467e-01,  5.7682e-02, -8.7953e-02,  4.4204e-02,\n",
       "                      -1.8091e-01, -1.1402e-02, -5.9059e-02,  3.8146e-03, -9.0675e-03,\n",
       "                      -1.4114e-01, -1.3229e-01,  3.2285e-02, -1.8902e-02, -1.6672e-01,\n",
       "                       1.4865e-01, -6.7413e-02, -5.5325e-02, -1.9887e-01,  6.1742e-02,\n",
       "                      -1.1721e-01, -3.3011e-02, -2.3307e-02, -2.5422e-01, -2.3569e-01,\n",
       "                       1.4602e-02, -4.2531e-02, -1.6359e-01,  4.8900e-02, -1.9330e-01,\n",
       "                      -1.3705e-01, -9.9976e-02, -2.8533e-01, -5.1259e-02,  5.8893e-02,\n",
       "                      -8.7470e-02,  2.7812e-02])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-1.9249e-01, -1.0816e-01, -1.2003e-01, -1.5456e-01,  1.1473e-01,\n",
       "                       1.3668e-02, -5.3098e-02, -3.1537e-02,  2.7286e-02, -1.5981e-01,\n",
       "                      -1.4437e-01, -1.3525e-01, -1.2330e-01, -3.6876e-02, -9.4190e-02,\n",
       "                      -9.6867e-02, -6.5581e-02, -1.1509e-02, -1.3658e-01, -1.5824e-01,\n",
       "                      -9.9626e-02, -1.8542e-01,  9.2277e-02, -1.6298e-01, -1.2712e-01,\n",
       "                      -4.3433e-02,  2.2247e-02, -1.7336e-01, -1.7017e-01, -3.3707e-02,\n",
       "                       8.2780e-02, -9.7115e-02, -5.5054e-02, -5.1921e-02, -1.4925e-01,\n",
       "                      -3.1729e-02, -9.9669e-02,  3.4132e-02, -8.6746e-02, -2.2708e-01,\n",
       "                      -7.9658e-02,  4.8386e-02, -2.6457e-02,  6.7375e-02,  5.6035e-02,\n",
       "                       3.9381e-02, -1.2695e-01, -4.3551e-02, -5.1005e-04, -1.1106e-01,\n",
       "                      -4.3863e-02, -5.3242e-02, -1.5664e-01, -7.3139e-02, -2.3156e-01,\n",
       "                       3.9660e-02, -4.8675e-02, -6.9420e-02, -1.3399e-01, -3.9295e-02,\n",
       "                      -1.2836e-01,  2.6834e-02,  4.7961e-02, -7.4109e-02, -4.8706e-02,\n",
       "                      -4.8906e-02, -3.0985e-02, -8.4504e-02, -9.3346e-02, -3.0355e-02,\n",
       "                       9.6910e-02,  6.2853e-02, -9.2542e-02, -1.8343e-01, -1.4410e-01,\n",
       "                      -9.4369e-02, -8.2589e-02, -4.9112e-03, -1.0261e-01, -1.7905e-01,\n",
       "                      -1.1526e-01,  3.2386e-02,  8.8235e-02, -5.2826e-02, -1.0577e-01,\n",
       "                      -2.6714e-02, -9.0017e-02, -5.9741e-02, -1.9197e-01, -8.3718e-02,\n",
       "                      -6.2915e-02, -1.8229e-03, -1.8728e-01, -1.2194e-01,  3.3687e-03,\n",
       "                      -4.5617e-02, -1.9283e-01, -7.0972e-02, -1.0087e-02, -1.5046e-01,\n",
       "                      -4.5675e-02, -6.3631e-02, -8.2916e-02, -2.7794e-02, -1.2279e-01,\n",
       "                      -1.6746e-01,  6.2606e-02, -5.4523e-02, -7.2893e-02, -2.3697e-01,\n",
       "                       1.7013e-01, -8.8448e-02,  4.1627e-02, -3.3019e-02, -1.5245e-01,\n",
       "                      -2.0409e-01, -1.7957e-01,  1.3882e-02, -6.2854e-02, -2.2962e-02,\n",
       "                      -3.4115e-01, -7.7491e-02, -1.0853e-01, -2.1865e-01, -1.2534e-01,\n",
       "                      -4.2495e-02, -5.4980e-02, -1.4753e-01, -1.0174e-01, -1.4047e-01,\n",
       "                      -4.8342e-02, -9.8344e-02, -4.5375e-02, -1.2893e-01,  3.7898e-02,\n",
       "                       7.3090e-02,  7.0962e-02, -5.3542e-02, -5.1725e-02, -6.9583e-02,\n",
       "                      -6.7963e-02, -6.6267e-02,  7.3591e-02, -6.6135e-02, -3.4659e-02,\n",
       "                      -1.1604e-01,  3.8516e-02,  1.2873e-02, -3.2199e-02, -3.3718e-02,\n",
       "                      -3.0032e-02,  1.4728e-02, -9.1613e-02, -5.6856e-02,  7.4502e-02,\n",
       "                      -2.7668e-02,  9.7680e-02, -9.0832e-02, -3.7196e-02, -1.8819e-01,\n",
       "                      -1.3705e-01,  3.7416e-03, -1.7497e-02, -1.3870e-01,  8.6764e-02,\n",
       "                       5.2086e-02,  3.6760e-02,  9.5526e-02, -6.8254e-02, -3.8973e-02,\n",
       "                       5.8678e-02, -1.2062e-01,  6.9816e-02,  1.4118e-02, -8.4258e-02,\n",
       "                      -7.2814e-02, -4.8847e-02,  1.9025e-02,  6.2688e-02, -7.7832e-02,\n",
       "                      -1.6373e-01,  3.7820e-02, -8.5013e-02, -5.4037e-03, -1.6175e-01,\n",
       "                      -8.6080e-02, -1.5552e-01, -4.2842e-02, -4.4763e-02,  9.9228e-03,\n",
       "                       8.8524e-02, -1.2293e-01,  1.8665e-01, -8.7513e-02,  5.6946e-02,\n",
       "                      -9.6636e-02,  1.2291e-01,  1.4033e-01, -6.2030e-02,  6.5572e-02,\n",
       "                       5.3267e-02, -1.9236e-01,  3.5055e-02, -3.1331e-02, -5.7512e-02,\n",
       "                       1.1399e-02, -9.7591e-02, -1.3548e-01, -2.0716e-02, -7.4558e-03,\n",
       "                      -2.0266e-02,  6.4670e-02, -1.2910e-01,  2.2538e-01, -6.5304e-02,\n",
       "                       1.9376e-02, -1.3906e-01, -1.4753e-02, -4.0642e-02, -7.6174e-02,\n",
       "                      -1.0211e-01,  7.4770e-02, -8.6147e-02, -1.0163e-01, -9.6709e-02,\n",
       "                       1.0147e-01,  3.0011e-02, -6.4181e-02,  1.2886e-01, -8.2028e-02,\n",
       "                       8.2235e-02, -6.5436e-02, -5.2456e-02, -1.1033e-01, -7.6922e-02,\n",
       "                      -1.2133e-01, -1.5538e-01, -1.0424e-01,  2.2958e-01, -1.8083e-02,\n",
       "                      -4.8402e-02, -1.1585e-01, -7.5814e-02, -1.6684e-01, -7.4886e-02,\n",
       "                      -7.8624e-03, -2.2346e-02, -3.9841e-02, -1.2207e-01, -7.7765e-02,\n",
       "                      -1.8817e-01, -1.5704e-02,  8.1068e-03,  2.0984e-01,  1.1246e-01,\n",
       "                       1.2562e-01,  6.8683e-02,  9.3540e-02,  6.2183e-02,  8.4977e-02,\n",
       "                      -5.8543e-02,  7.7014e-02,  8.1322e-03,  9.0813e-02,  7.7412e-02,\n",
       "                      -1.1258e-01,  2.4218e-02,  1.1227e-01,  1.5803e-04, -6.6191e-02,\n",
       "                      -2.1542e-02, -1.1126e-01, -8.7914e-02, -6.6853e-02, -1.3905e-03,\n",
       "                       3.9669e-03, -9.6510e-02, -9.5087e-02, -3.0922e-02, -3.3876e-02,\n",
       "                       3.3573e-02,  1.4700e-02,  7.5515e-02,  7.7514e-02, -2.1603e-02,\n",
       "                       7.0257e-03,  1.4065e-01,  3.8025e-02,  1.3315e-01,  4.2011e-03,\n",
       "                      -1.7829e-02,  5.2125e-02,  1.0961e-01, -3.1801e-02,  5.0520e-03,\n",
       "                      -5.0392e-02, -1.1526e-02, -9.5069e-04, -1.2978e-01, -6.5907e-02,\n",
       "                       1.2321e-01, -3.5653e-02,  8.4980e-03,  3.9410e-02, -1.2757e-01,\n",
       "                       1.1478e-01,  7.7518e-02,  7.7755e-02,  1.0756e-01, -2.2879e-02,\n",
       "                      -3.4154e-02, -8.7534e-03,  8.2487e-02, -3.0829e-02,  1.6082e-02,\n",
       "                       9.9800e-02,  2.5944e-02,  9.0558e-03,  3.7214e-02, -4.1553e-02,\n",
       "                      -2.0125e-02,  5.9909e-02, -3.7221e-02, -2.1270e-02, -5.5063e-03,\n",
       "                       6.4245e-03, -2.1888e-02,  4.5400e-02, -1.1198e-02, -3.0744e-02,\n",
       "                       5.6695e-02, -7.6006e-02,  2.3159e-02, -1.7967e-02, -6.9590e-02,\n",
       "                       1.2979e-01, -4.3360e-02,  1.0686e-03,  9.7294e-02, -6.5004e-02,\n",
       "                       3.1060e-02,  5.9119e-02,  2.3853e-02, -1.3433e-02,  2.9802e-02,\n",
       "                      -3.4714e-03, -6.2702e-02,  4.5604e-03, -7.5195e-02,  8.9196e-02,\n",
       "                      -4.2545e-02, -9.4981e-03,  2.8753e-02, -1.0078e-01,  1.0804e-02,\n",
       "                      -8.0887e-02,  1.4521e-02,  1.3064e-02, -4.9591e-03,  1.2388e-01,\n",
       "                       1.0587e-01, -5.1118e-02,  3.6511e-02,  5.4452e-02,  2.1496e-02,\n",
       "                       3.0880e-04, -2.6287e-02,  3.1654e-02,  1.3087e-01,  7.9317e-02,\n",
       "                      -4.5807e-02,  1.2060e-01, -1.4978e-02,  8.4113e-02,  1.4973e-02,\n",
       "                       1.1455e-01,  7.9035e-02,  2.9232e-02, -4.1673e-02,  1.0573e-01,\n",
       "                      -3.5181e-02, -8.3337e-02, -7.8730e-02, -6.3226e-02, -1.9269e-01,\n",
       "                      -1.6822e-01, -7.4577e-02, -8.3275e-02,  2.2810e-02, -9.3269e-03,\n",
       "                      -1.9926e-01, -2.2276e-02, -4.2565e-02, -8.0757e-02, -1.3310e-01,\n",
       "                      -1.7146e-01, -1.5116e-01, -3.4771e-02, -1.1393e-01, -2.4554e-03,\n",
       "                      -1.1115e-01, -3.2622e-02, -8.9061e-02,  1.4203e-01, -1.4830e-01,\n",
       "                       4.3561e-03, -8.3171e-02, -1.4428e-01, -5.7074e-02, -1.5998e-01,\n",
       "                      -6.9066e-02,  1.2477e-01, -1.0272e-01, -9.2465e-02,  1.1878e-01,\n",
       "                      -1.4330e-01, -6.3027e-02, -1.9371e-01, -2.0732e-01, -6.5503e-02,\n",
       "                       1.7801e-02, -3.4914e-02,  1.2267e-01, -9.1702e-02, -2.1081e-01,\n",
       "                      -6.0807e-02, -3.7810e-02, -1.2031e-01,  5.4763e-02,  6.8370e-02,\n",
       "                      -1.6591e-01,  3.3703e-02,  1.0008e-02,  1.2018e-01, -5.1088e-02,\n",
       "                      -1.4803e-01, -2.4924e-01, -4.6958e-02, -7.4791e-02,  7.1878e-02,\n",
       "                      -3.0686e-02, -8.4289e-03, -9.2631e-02, -1.0899e-01, -8.1218e-04,\n",
       "                      -1.8911e-01, -1.0041e-02, -1.1797e-01, -8.8387e-02, -4.0843e-02,\n",
       "                      -1.1111e-02, -1.1695e-01,  8.7982e-02,  3.9287e-02, -4.1385e-03,\n",
       "                       5.6036e-02, -1.7260e-05, -2.8868e-01, -9.5890e-02, -1.3654e-01,\n",
       "                      -5.8718e-02, -1.0179e-02, -7.5450e-02,  9.8713e-03, -1.2133e-01,\n",
       "                      -8.7382e-02, -4.0046e-02, -3.7082e-02, -2.3111e-01,  5.5695e-02,\n",
       "                      -2.7601e-02,  2.2366e-02, -2.0675e-01, -2.8101e-02, -3.4900e-02,\n",
       "                      -1.1993e-01, -5.4484e-02, -1.4551e-02, -2.9191e-02,  4.4785e-02,\n",
       "                      -1.6149e-01, -1.9043e-01, -6.1831e-02, -4.3436e-02, -1.2834e-01,\n",
       "                      -6.3044e-02,  3.0324e-02, -3.5985e-02, -2.8626e-02, -9.0897e-02,\n",
       "                      -7.4158e-02, -1.6194e-02, -7.6193e-02, -1.4227e-01,  1.0036e-01,\n",
       "                      -1.6466e-01, -2.2078e-02, -1.3676e-01, -1.3596e-01, -6.8189e-02,\n",
       "                      -2.3900e-01, -1.2190e-01,  3.0469e-02,  1.0191e-01, -1.8536e-01,\n",
       "                      -5.6252e-03, -1.2373e-01,  6.6374e-04, -2.4046e-01, -9.2997e-03,\n",
       "                      -7.7379e-02,  9.1151e-02])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[-0.0212, -0.4192, -0.1758,  ..., -0.4264, -0.2189, -0.0503],\n",
       "                      [-0.0638, -0.1676,  0.4306,  ..., -0.0375,  0.1056,  0.2623],\n",
       "                      [-0.8867, -0.0341, -0.4421,  ..., -0.0371, -0.1339,  0.0799],\n",
       "                      ...,\n",
       "                      [ 0.3099, -0.2788,  0.6680,  ..., -0.3159,  0.3500,  0.7757],\n",
       "                      [-0.2640,  0.0895,  0.1060,  ...,  0.1799,  0.3169, -0.1472],\n",
       "                      [ 0.0496, -0.3594, -0.4260,  ...,  0.1613,  0.3132, -0.2896]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1811, -0.2386, -0.0856,  ...,  0.0396,  0.0398, -0.2161],\n",
       "                      [ 0.0279,  0.1887,  0.0059,  ..., -0.1355,  0.2428, -0.1416],\n",
       "                      [-0.0958,  0.1055, -0.1559,  ...,  0.2621,  0.1516,  0.0774],\n",
       "                      ...,\n",
       "                      [-0.0272, -0.0793,  0.0722,  ..., -0.1005,  0.1289, -0.0725],\n",
       "                      [ 0.2411, -0.1567, -0.0566,  ..., -0.0202,  0.1221, -0.0854],\n",
       "                      [-0.1488, -0.2758, -0.1779,  ...,  0.0893, -0.1828,  0.0067]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 1.0788e-01,  6.1872e-02,  1.0561e-02,  8.9283e-02,  1.7886e-01,\n",
       "                       1.7696e-01,  1.6445e-01,  1.0077e-01,  8.5397e-02,  2.2037e-02,\n",
       "                       4.3998e-01,  6.9254e-02,  2.0319e-01,  1.7390e-01,  1.0298e-01,\n",
       "                       1.0568e-01,  1.7604e-02,  2.5407e-01, -8.4058e-02,  1.5631e-01,\n",
       "                       1.7261e-01,  2.1796e-01,  2.0330e-01,  1.7788e-01,  1.7818e-01,\n",
       "                       3.3491e-02,  3.5220e-01,  1.2914e-01,  1.8433e-01,  1.1430e-03,\n",
       "                       9.2350e-02,  2.9718e-01, -1.0380e-01,  8.2823e-02,  1.1857e-01,\n",
       "                       2.1151e-02,  2.1123e-01,  8.1851e-02,  1.2934e-01,  2.2561e-01,\n",
       "                       2.5136e-02,  1.3697e-01,  6.6752e-02, -3.0468e-02,  1.1190e-02,\n",
       "                      -3.1465e-02,  2.9951e-01,  2.1403e-01,  1.7341e-01,  2.7957e-01,\n",
       "                       1.5323e-01,  9.1646e-03,  1.4924e-01,  3.2184e-01,  1.6734e-01,\n",
       "                       1.9832e-01,  2.9606e-01, -7.9373e-02,  1.4005e-01,  1.7876e-01,\n",
       "                       1.1218e-01,  1.6724e-02,  2.9983e-02,  3.5036e-02,  4.9497e-02,\n",
       "                       1.8794e-01,  1.2503e-01,  9.4367e-03,  1.5738e-01,  9.6737e-02,\n",
       "                       6.2235e-02,  1.1481e-01,  1.9947e-01,  2.2446e-01, -2.3023e-02,\n",
       "                      -4.9872e-02,  2.1929e-01, -1.3321e-02,  1.2289e-01, -2.5407e-02,\n",
       "                       1.9064e-01,  2.6807e-01,  2.4289e-01,  8.3314e-02,  5.1817e-02,\n",
       "                       1.8000e-01,  3.2560e-01, -3.3499e-02,  1.8988e-01,  2.1343e-01,\n",
       "                       1.1204e-01,  6.6511e-02, -4.4606e-02,  2.3765e-01,  9.9294e-02,\n",
       "                       1.8029e-01,  2.8524e-02,  1.1218e-01,  2.5051e-03,  2.8574e-02,\n",
       "                       1.8150e-02,  1.1204e-01,  4.0092e-02,  4.5766e-02,  2.1101e-01,\n",
       "                       2.8654e-01,  1.3418e-01,  1.1422e-01,  2.2241e-01,  1.8932e-01,\n",
       "                       1.2251e-01,  1.8956e-01,  2.6498e-01,  1.6800e-01,  2.9018e-01,\n",
       "                       1.3213e-01,  1.6559e-01,  2.0167e-01,  2.6514e-01,  7.0558e-02,\n",
       "                       4.4561e-02,  1.6111e-01,  2.0749e-01,  2.7439e-01,  5.7745e-03,\n",
       "                       2.6544e-01,  1.2537e-01,  1.1924e-01,  1.1305e-02,  2.0343e-01,\n",
       "                       5.0007e-02, -9.6855e-02,  2.6530e-02,  6.0654e-02,  1.6191e-02,\n",
       "                       2.3011e-02, -8.5602e-02, -6.4835e-03, -1.1673e-01, -4.1687e-02,\n",
       "                      -3.5306e-03, -1.2433e-02,  1.3129e-01,  3.2184e-02, -2.1092e-02,\n",
       "                       1.8899e-02,  3.2476e-02,  1.0992e-02,  1.2877e-02,  8.6209e-02,\n",
       "                       6.4360e-02,  7.7569e-02,  4.5746e-02,  1.1767e-01,  1.3588e-02,\n",
       "                       1.7577e-02,  6.5701e-02,  7.1529e-02, -1.5529e-02,  1.0263e-02,\n",
       "                       2.9597e-02,  2.2833e-02,  9.5751e-02,  1.7123e-02,  3.6840e-02,\n",
       "                       3.5682e-02,  1.4545e-01,  9.6899e-02,  4.0865e-02,  2.6850e-02,\n",
       "                      -1.9288e-02,  3.8709e-02,  1.8374e-01,  4.8444e-02, -8.8948e-02,\n",
       "                      -6.5250e-02,  6.6704e-02,  1.5175e-01, -1.2675e-01,  1.5404e-01,\n",
       "                      -5.5816e-02,  6.1098e-02, -1.0595e-02, -6.1021e-02, -6.1459e-02,\n",
       "                       3.5417e-02,  4.2733e-02, -9.3463e-02,  1.0568e-01,  1.7614e-01,\n",
       "                      -6.6605e-02,  6.0659e-02,  9.8362e-02,  9.2219e-02, -4.2789e-04,\n",
       "                       3.2509e-02, -1.4516e-03,  7.4101e-02,  9.1853e-02, -5.0562e-02,\n",
       "                       2.4036e-02,  5.7496e-04, -8.7240e-04,  4.6295e-02,  9.3560e-02,\n",
       "                       1.1583e-01,  1.3910e-02,  4.1965e-02,  7.8969e-02,  5.8777e-02,\n",
       "                      -2.9056e-02,  4.0763e-02, -6.2770e-02,  6.8021e-02, -1.2035e-02,\n",
       "                       2.5349e-02, -1.2484e-01, -6.7815e-04, -1.1362e-03,  1.1126e-01,\n",
       "                      -2.4994e-02, -3.0327e-02,  7.4131e-02, -8.0424e-02, -1.7241e-02,\n",
       "                       1.2235e-01, -1.6422e-02,  1.3714e-01, -3.6731e-02,  3.9572e-02,\n",
       "                       4.3892e-02,  1.0399e-01,  1.2657e-01,  4.1880e-02,  1.2142e-02,\n",
       "                      -7.3102e-02,  1.2423e-01,  3.5988e-02,  1.9581e-02,  1.0980e-01,\n",
       "                      -1.7318e-02,  2.5126e-02, -1.0731e-02,  1.7744e-01,  2.1287e-01,\n",
       "                       8.5503e-02, -1.0429e-01, -5.6648e-02, -2.3281e-02,  8.9181e-02,\n",
       "                      -1.3855e-02, -6.7174e-02, -1.6883e-01, -3.6874e-02,  1.7618e-01,\n",
       "                       5.0891e-02, -6.7713e-02,  9.1489e-02,  4.6310e-02,  9.7202e-02,\n",
       "                      -1.7550e-01, -1.9614e-02, -2.9414e-03,  4.6915e-02, -2.0062e-02,\n",
       "                      -3.2865e-03, -7.6203e-02,  4.5064e-02, -5.8194e-02,  9.1838e-03,\n",
       "                      -1.9182e-02,  1.3210e-01,  3.8799e-03, -1.4727e-02,  1.2915e-03,\n",
       "                      -7.6346e-03, -6.9943e-03,  2.8404e-02,  2.4852e-03, -2.6730e-02,\n",
       "                      -5.3700e-02,  9.5782e-02, -5.3036e-02,  5.9239e-02, -5.3829e-02,\n",
       "                       8.3817e-02, -3.4690e-02,  5.8550e-03,  2.3322e-02, -8.8145e-02,\n",
       "                       2.5588e-02,  4.7731e-02,  1.7397e-01, -5.1034e-02, -7.8776e-02,\n",
       "                      -2.7527e-02,  2.2269e-02, -3.7116e-02, -2.9563e-02, -6.1921e-02,\n",
       "                       4.9676e-02, -4.7193e-02, -2.8788e-02, -1.3694e-01, -1.5967e-02,\n",
       "                      -5.1089e-02, -8.2163e-02, -9.2841e-02,  1.3714e-02,  3.2124e-02,\n",
       "                      -3.7816e-02, -2.8374e-02, -3.5978e-02,  4.6851e-02,  5.4426e-02,\n",
       "                      -9.6526e-02,  8.6183e-02,  2.0390e-02, -1.4512e-02, -1.1318e-01,\n",
       "                      -3.2051e-02, -4.8522e-03,  4.2141e-03, -6.0841e-02, -6.9181e-02,\n",
       "                      -8.2698e-02, -6.2876e-02, -1.2228e-01, -3.6634e-02, -2.5639e-02,\n",
       "                       7.7959e-02,  6.4688e-02, -8.2119e-03,  4.8386e-02,  7.8330e-02,\n",
       "                       5.2118e-02,  1.6580e-02,  1.3169e-02, -5.2109e-02, -5.8656e-02,\n",
       "                      -9.0446e-02,  5.4118e-02,  1.5472e-02,  5.3584e-03, -1.1261e-04,\n",
       "                       2.3520e-02, -4.8631e-02, -1.8575e-02, -2.9952e-02,  4.6637e-02,\n",
       "                       3.1451e-02, -1.2032e-02,  2.1305e-02,  1.8513e-02,  2.1954e-02,\n",
       "                      -1.1506e-01,  1.5242e-02, -3.2770e-02,  1.1594e-01,  2.5212e-03,\n",
       "                       4.3792e-02,  1.0626e-01, -8.5090e-02,  5.2842e-02,  2.8000e-02,\n",
       "                      -8.5624e-02,  9.8916e-02, -3.3200e-02,  1.0225e-02,  6.3468e-02,\n",
       "                       4.7513e-02,  3.8530e-02, -5.3781e-02,  2.1353e-01, -2.1472e-04,\n",
       "                      -6.5684e-02,  2.4547e-02,  4.0139e-02,  7.4886e-02, -5.8947e-02,\n",
       "                      -5.3466e-02, -9.8245e-03,  7.1844e-02,  1.8328e-02,  1.5339e-01,\n",
       "                       4.0127e-01,  6.4340e-02,  2.0917e-01,  1.0743e-01,  1.0969e-01,\n",
       "                       4.3822e-02,  8.8395e-02,  2.8788e-01,  5.0386e-02,  2.8833e-01,\n",
       "                       1.1934e-01,  1.6677e-01,  1.0456e-01,  9.3161e-02,  1.2779e-01,\n",
       "                       1.5526e-01,  1.4574e-01,  1.6124e-01,  1.9337e-01,  1.2346e-01,\n",
       "                       1.7154e-01,  7.5700e-02,  1.2231e-01,  5.1864e-02,  1.3981e-01,\n",
       "                       1.3232e-01,  1.9695e-01,  8.4114e-02,  1.3745e-01,  3.6186e-02,\n",
       "                       3.6306e-01,  6.7389e-02,  1.3917e-01,  2.0993e-01,  2.4425e-01,\n",
       "                       2.2090e-01,  1.0177e-01,  9.7918e-02,  1.3360e-01,  1.1362e-02,\n",
       "                       9.4481e-02,  2.0025e-01,  1.9593e-01,  7.4320e-02,  2.4505e-02,\n",
       "                       2.3067e-01,  2.5962e-01,  3.5301e-01,  2.6943e-01,  1.7281e-01,\n",
       "                      -6.9269e-02,  1.4920e-01,  2.2393e-01,  1.5717e-01,  2.1832e-01,\n",
       "                       2.3768e-01,  1.1104e-01,  2.1859e-01,  1.4347e-01,  1.5783e-01,\n",
       "                      -6.0276e-02,  1.9946e-01,  1.1847e-01,  1.0791e-01, -2.1918e-02,\n",
       "                       6.3513e-02,  9.2835e-02,  1.0003e-01,  1.6403e-01,  1.0802e-01,\n",
       "                       1.1562e-01,  1.4781e-01,  1.9409e-01,  1.5890e-01,  5.0268e-02,\n",
       "                       6.6567e-02,  9.0347e-02,  2.8801e-01,  1.3259e-01,  2.7244e-01,\n",
       "                       2.5421e-01,  2.5309e-01,  8.7880e-02,  1.0701e-01,  7.5814e-02,\n",
       "                       5.5137e-02,  1.0222e-01,  2.7616e-01,  2.6550e-01,  3.0039e-01,\n",
       "                       3.7992e-02, -6.5942e-02,  2.7305e-01,  1.9746e-02,  2.0097e-01,\n",
       "                       6.1306e-02,  9.6384e-02,  1.3120e-01,  4.4882e-02,  5.3250e-02,\n",
       "                       1.7502e-01,  2.3425e-01,  1.3481e-01,  1.2793e-01,  2.1486e-01,\n",
       "                       2.9014e-01,  1.3093e-02,  1.3439e-01,  1.1862e-01,  1.1047e-01,\n",
       "                      -1.0501e-01,  1.4597e-01,  1.3219e-01,  1.7352e-01,  3.7735e-01,\n",
       "                       1.5418e-01,  1.0673e-01,  2.8457e-01, -1.6699e-02,  4.9581e-02,\n",
       "                       2.9031e-01,  2.3926e-01,  3.3792e-01,  1.6653e-01,  7.1748e-02,\n",
       "                       1.4083e-01,  1.4115e-01])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 1.2412e-01,  1.7016e-01,  7.9089e-02,  1.6979e-02, -3.9596e-02,\n",
       "                       1.4005e-01,  1.1228e-01,  1.0023e-01,  8.0656e-02,  1.0010e-01,\n",
       "                       2.0074e-01, -2.1159e-02,  2.0953e-02,  8.6405e-02,  1.9640e-01,\n",
       "                       3.2795e-01, -6.8513e-03,  1.8256e-01,  2.6617e-02,  2.4161e-01,\n",
       "                       2.9929e-01,  2.2319e-01,  2.4900e-01,  1.0816e-01,  1.2192e-01,\n",
       "                       3.2884e-02,  3.1025e-01,  1.1494e-01,  2.9970e-01, -8.7174e-02,\n",
       "                       9.5399e-02,  2.2144e-01,  4.1656e-02,  1.5431e-02,  2.2031e-01,\n",
       "                       1.5730e-02,  1.0778e-01,  1.1029e-01,  1.8812e-01, -4.0615e-03,\n",
       "                       4.8594e-02,  1.7123e-01,  1.3130e-01,  1.5670e-01,  1.4853e-01,\n",
       "                       9.9343e-03,  3.1984e-01,  3.0039e-01,  2.9719e-01,  2.1766e-01,\n",
       "                       2.3019e-01, -1.2531e-02,  1.5634e-01,  2.2958e-01,  1.0040e-01,\n",
       "                       1.1672e-01,  1.1760e-01,  1.1729e-02,  1.9791e-01,  3.5346e-02,\n",
       "                       4.0921e-02,  1.1988e-01,  5.1602e-02,  1.4161e-01,  1.1566e-01,\n",
       "                       1.9616e-01,  1.4545e-01,  8.6232e-02,  3.2877e-02,  1.3208e-01,\n",
       "                       1.8008e-02,  1.2062e-01,  1.4209e-01,  2.2720e-01,  8.2853e-02,\n",
       "                      -8.7612e-02,  1.7956e-01,  4.2784e-03,  1.9189e-01,  1.4676e-02,\n",
       "                       2.0834e-01,  2.2489e-01,  1.8783e-01,  8.1403e-02,  6.4873e-02,\n",
       "                       1.7579e-01,  6.4293e-02,  1.5726e-02,  2.4900e-01,  8.5931e-02,\n",
       "                       1.8225e-01,  4.2584e-02,  1.0152e-01,  8.5307e-02,  8.2735e-02,\n",
       "                       1.1145e-01,  4.3134e-02,  3.6298e-02,  1.0782e-01,  1.8916e-02,\n",
       "                       2.0979e-02,  7.1581e-02,  1.6007e-01, -5.2066e-02,  2.3612e-01,\n",
       "                       1.9332e-01,  2.6607e-01,  1.3923e-01,  1.4743e-01,  2.2526e-01,\n",
       "                       1.5000e-01,  2.3274e-01,  2.6918e-01,  1.1824e-01,  2.8849e-01,\n",
       "                       2.0079e-02,  1.8748e-01,  1.9719e-01,  2.6949e-01,  2.4469e-01,\n",
       "                       1.6246e-01,  1.4549e-01,  2.9400e-01,  2.9997e-01, -7.6649e-02,\n",
       "                       1.1062e-01,  1.0637e-01,  1.4838e-01,  3.8335e-03,  2.7080e-01,\n",
       "                       1.9524e-01, -1.7189e-01,  1.3171e-01, -3.4910e-02,  1.0889e-01,\n",
       "                       5.2954e-02, -7.4826e-03, -6.3654e-02,  8.4935e-02, -1.0376e-02,\n",
       "                       1.6937e-02, -6.9715e-02, -2.5760e-02,  4.0780e-02, -1.1363e-02,\n",
       "                       6.6097e-02, -4.5521e-03, -1.0596e-01, -3.8567e-02,  1.1594e-01,\n",
       "                       4.0396e-02, -2.4159e-02,  4.8602e-02,  1.1194e-01,  1.1075e-01,\n",
       "                       1.5660e-01,  5.0566e-02,  5.6616e-02,  5.2281e-02,  1.2276e-02,\n",
       "                      -6.5238e-03,  4.3494e-02,  2.2075e-01,  6.8469e-02,  7.8859e-02,\n",
       "                      -8.7371e-02,  1.0634e-01,  1.4291e-01,  1.1344e-01, -2.3056e-02,\n",
       "                       4.3134e-02,  1.4441e-01,  7.0317e-02,  6.3248e-02,  2.0924e-02,\n",
       "                      -2.3136e-02,  7.4182e-02,  5.9956e-02, -9.6792e-02, -6.8644e-03,\n",
       "                      -3.0389e-02,  8.0362e-02, -9.0326e-02, -1.2255e-01, -4.0129e-02,\n",
       "                       5.2876e-02,  6.8200e-03, -5.1124e-02,  2.0093e-01, -1.7386e-02,\n",
       "                       4.6440e-03,  6.1297e-02,  6.3660e-02,  8.9712e-02,  4.1195e-02,\n",
       "                      -1.0541e-01, -6.9649e-02,  1.0042e-01, -7.9213e-02, -1.1212e-01,\n",
       "                      -1.1967e-02, -1.9190e-02, -1.1124e-02,  2.1643e-02,  3.7104e-03,\n",
       "                       5.2083e-02, -4.4954e-02, -3.2915e-02,  2.5287e-02,  7.0217e-02,\n",
       "                       1.1063e-01, -2.8046e-02, -4.1370e-02,  7.0346e-02,  1.3942e-01,\n",
       "                      -4.3637e-02, -1.3781e-01, -3.5697e-02, -2.7158e-02,  2.4537e-02,\n",
       "                       1.7110e-01, -2.9346e-02, -2.9739e-02, -1.5992e-02,  7.2469e-02,\n",
       "                      -2.3899e-02,  5.8553e-03,  3.5902e-03,  6.7865e-02, -4.6748e-02,\n",
       "                       2.5989e-02,  6.4000e-03,  7.9133e-02, -8.1634e-03,  9.6850e-02,\n",
       "                       6.4801e-02, -5.8432e-02, -3.8886e-02,  1.8690e-01,  6.2203e-02,\n",
       "                      -3.5200e-02,  3.7428e-02, -2.9753e-02, -1.4787e-01,  1.1745e-01,\n",
       "                       2.2288e-01,  7.9438e-02,  6.4231e-02,  4.8544e-02,  1.2277e-01,\n",
       "                      -4.2701e-02, -9.2229e-02, -3.7354e-02,  2.2880e-02,  1.4822e-01,\n",
       "                      -9.2323e-02, -7.7212e-02, -8.2648e-02, -1.2551e-03,  6.4583e-02,\n",
       "                       4.0704e-02, -6.3553e-02, -6.6022e-02, -1.1326e-01,  3.4400e-02,\n",
       "                      -7.0675e-02, -7.9569e-02, -2.0911e-02, -5.2886e-02, -9.2637e-02,\n",
       "                       1.5771e-01,  8.0320e-02, -5.1111e-02,  9.7133e-02,  5.7217e-02,\n",
       "                      -4.6115e-02, -7.6260e-02,  3.1898e-02, -1.1575e-01,  6.1198e-02,\n",
       "                      -4.1037e-02, -1.1629e-01,  1.2599e-01,  9.3195e-03,  7.9970e-02,\n",
       "                       8.2500e-02, -4.0103e-02,  7.2797e-02, -7.1716e-02,  9.5905e-03,\n",
       "                       6.9542e-02, -5.7207e-02,  6.3101e-02, -1.1619e-02,  8.8884e-03,\n",
       "                      -4.2095e-02, -9.7297e-02,  5.7669e-02,  7.1962e-02,  2.0381e-02,\n",
       "                       5.1474e-02,  1.2065e-02,  1.5471e-02,  1.6582e-02, -6.2779e-02,\n",
       "                       9.7895e-03, -6.1028e-02, -1.0672e-01, -3.5333e-02, -6.4283e-02,\n",
       "                       1.0157e-01,  4.9648e-02, -3.7914e-02,  1.5139e-01, -8.6743e-02,\n",
       "                      -7.2436e-02, -9.1040e-03,  2.9699e-02, -4.6823e-02, -6.8980e-03,\n",
       "                       4.1133e-02,  1.2915e-01,  1.4847e-01, -4.2428e-02,  6.6977e-02,\n",
       "                      -1.0086e-01,  1.1149e-01,  1.0800e-01, -3.0449e-02, -2.3804e-02,\n",
       "                       2.1113e-02, -3.0218e-02,  1.1080e-01,  2.7014e-02,  1.5679e-02,\n",
       "                       4.3302e-02, -2.9163e-02, -7.8828e-02, -2.9689e-02, -2.1701e-02,\n",
       "                       6.4298e-02,  5.8119e-02, -1.8412e-01,  3.7335e-02, -1.3063e-01,\n",
       "                       8.9703e-02, -1.9965e-02,  1.0658e-01,  6.2801e-02,  5.7027e-02,\n",
       "                       1.3595e-01,  4.2934e-02, -8.8859e-02,  3.9234e-02,  8.0034e-02,\n",
       "                      -7.4121e-05,  9.9863e-03,  2.1894e-03,  1.0885e-01, -3.0726e-02,\n",
       "                      -1.0907e-01, -1.6496e-02, -9.0347e-02, -8.5478e-03, -5.0266e-02,\n",
       "                       9.4829e-03,  4.5846e-02, -5.4647e-02,  1.4935e-01,  6.2585e-02,\n",
       "                      -3.6931e-02, -1.1503e-01, -4.7791e-03, -7.1242e-02,  2.7340e-02,\n",
       "                       3.0536e-02,  1.3402e-02,  4.5347e-02, -4.0411e-02, -3.9966e-02,\n",
       "                      -2.2275e-02,  1.1691e-02,  3.1874e-02,  4.9183e-02,  1.0782e-01,\n",
       "                       2.6073e-01,  1.2368e-01,  1.5984e-01,  2.1187e-01,  3.3719e-01,\n",
       "                       1.5518e-01,  1.2222e-01,  2.9831e-01, -4.5514e-02,  4.4630e-01,\n",
       "                      -9.9010e-03,  4.6806e-02,  2.0831e-01,  2.2908e-01,  1.8849e-01,\n",
       "                       1.1800e-01,  1.1417e-01,  3.3297e-01,  7.5171e-02,  5.3356e-02,\n",
       "                       2.7252e-01,  1.3872e-01,  2.3804e-01, -5.9906e-02,  1.3432e-01,\n",
       "                       8.9345e-02,  2.4451e-01,  1.7892e-01,  2.0714e-01,  9.7685e-02,\n",
       "                       2.9149e-01,  3.7078e-02,  9.9887e-02,  1.7979e-01,  2.3670e-01,\n",
       "                       1.9298e-01,  7.4022e-02,  1.1978e-01,  1.0856e-01,  2.1894e-02,\n",
       "                       2.7196e-01,  1.7807e-01,  1.6453e-01,  1.2934e-01,  8.5273e-02,\n",
       "                       3.5984e-01,  3.5584e-01,  4.2216e-01,  2.4919e-01,  9.4636e-02,\n",
       "                      -1.1414e-01,  5.2679e-02,  2.5126e-01,  1.4809e-01,  1.1877e-01,\n",
       "                       4.7691e-02,  1.0775e-01,  1.5662e-01,  1.2801e-01,  1.6854e-01,\n",
       "                      -6.3559e-04,  7.4642e-02,  2.6573e-02,  2.3598e-01,  7.7337e-03,\n",
       "                       9.4476e-02,  6.6148e-02,  1.7422e-01,  5.7458e-02,  8.5992e-02,\n",
       "                       3.6668e-02,  2.0601e-01,  1.9867e-01,  2.0826e-01,  6.4597e-02,\n",
       "                       8.9309e-02,  1.3195e-01,  1.2367e-01,  1.2791e-01,  2.7866e-01,\n",
       "                       1.7172e-01,  2.3673e-01,  1.3773e-01, -1.3154e-02,  2.2580e-01,\n",
       "                       3.8768e-02,  2.1912e-01,  3.6159e-01,  1.9903e-01,  3.0437e-01,\n",
       "                       1.9545e-01, -6.0087e-03,  2.6050e-01,  9.4504e-02,  8.2179e-02,\n",
       "                      -7.7952e-02, -2.1506e-02,  9.1616e-02,  6.0993e-02,  1.5239e-01,\n",
       "                       9.1845e-02,  1.6820e-01,  1.0543e-01,  2.2878e-01,  2.2702e-01,\n",
       "                       1.1178e-01,  1.8659e-01,  2.5784e-01,  2.5188e-01,  1.4408e-01,\n",
       "                      -1.1154e-01,  2.5636e-01,  9.4931e-02,  1.2754e-01,  1.8152e-01,\n",
       "                       1.3546e-01,  1.7596e-01,  3.0574e-01,  2.8179e-02,  1.7848e-01,\n",
       "                       2.6764e-01,  2.7950e-01,  1.2945e-01,  3.8181e-02,  1.0796e-01,\n",
       "                       1.8814e-01,  1.3145e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0172, -0.0665, -0.0319,  ...,  0.2550, -0.0443,  0.1789],\n",
       "                      [-0.2788,  0.0645, -0.0578,  ..., -0.1391,  0.1473, -0.5784],\n",
       "                      [ 0.3107, -0.1115, -0.3081,  ..., -0.0776, -0.1025,  0.0369],\n",
       "                      ...,\n",
       "                      [ 0.0512, -0.2857,  0.2010,  ..., -0.0708,  0.0659, -0.1273],\n",
       "                      [ 0.1116, -0.3207,  0.1126,  ...,  0.0180,  0.0878, -0.1730],\n",
       "                      [-0.0443, -0.2348, -0.0044,  ..., -0.2183,  0.1050, -0.1647]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0033, -0.1151, -0.0252,  ..., -0.0281,  0.2169, -0.1675],\n",
       "                      [-0.1101, -0.2650,  0.0977,  ..., -0.0095,  0.1006, -0.0582],\n",
       "                      [-0.1193, -0.0915, -0.1221,  ..., -0.1313, -0.1536, -0.0233],\n",
       "                      ...,\n",
       "                      [-0.1442, -0.1316,  0.1492,  ...,  0.0653,  0.0443, -0.2712],\n",
       "                      [-0.0320, -0.1056, -0.2201,  ...,  0.0962,  0.0392, -0.1644],\n",
       "                      [-0.0547,  0.0843, -0.2962,  ..., -0.1236, -0.1220, -0.2647]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([ 0.0046, -0.1802, -0.2726,  0.0165, -0.1034, -0.1841, -0.1319, -0.1191,\n",
       "                      -0.1885, -0.0763, -0.1628, -0.1505, -0.1442, -0.1831, -0.1552, -0.2310,\n",
       "                      -0.0976, -0.0929, -0.1488, -0.1296, -0.0714, -0.0164, -0.0731, -0.1140,\n",
       "                      -0.0992, -0.1439, -0.2123, -0.2694, -0.1826, -0.0441, -0.0731, -0.0973,\n",
       "                      -0.1568, -0.0649, -0.1971, -0.2113, -0.1530, -0.1965, -0.0660, -0.1566,\n",
       "                      -0.1013, -0.1858, -0.1275, -0.0672, -0.1283, -0.2081, -0.0967, -0.1688,\n",
       "                      -0.1176, -0.3200, -0.1067, -0.2438, -0.1748, -0.0838, -0.1952, -0.0382,\n",
       "                      -0.1270, -0.0969, -0.1642, -0.0378, -0.1926, -0.0215, -0.1049, -0.0856,\n",
       "                      -0.1774, -0.1259, -0.1972, -0.0873, -0.0418, -0.1005, -0.1538, -0.0529,\n",
       "                      -0.1522, -0.1065, -0.2153, -0.2094, -0.1193, -0.2439, -0.1642, -0.2356,\n",
       "                      -0.1650, -0.0616, -0.1077, -0.1316, -0.1559,  0.0335, -0.1133, -0.1625,\n",
       "                      -0.1419, -0.0653, -0.1867, -0.2473, -0.1379, -0.1257, -0.0396, -0.1558,\n",
       "                      -0.1672, -0.1430, -0.2426, -0.0620, -0.1735, -0.2300, -0.1382, -0.0946,\n",
       "                      -0.0475, -0.1510, -0.1714, -0.2194, -0.1363, -0.1636, -0.0411, -0.2207,\n",
       "                      -0.1398, -0.0773, -0.2128, -0.1367, -0.1723, -0.1085, -0.1129, -0.0937,\n",
       "                      -0.3016, -0.2426, -0.2024, -0.1384, -0.0945, -0.1504, -0.0470, -0.0580,\n",
       "                      -0.0753, -0.1757, -0.0586,  0.0309, -0.1461, -0.1753, -0.0795, -0.1832,\n",
       "                      -0.2535, -0.1303, -0.1723, -0.1224, -0.1607, -0.1734, -0.1841, -0.1314,\n",
       "                      -0.2558, -0.0293,  0.0068, -0.1462, -0.2006, -0.0580, -0.1154, -0.0180,\n",
       "                      -0.1123, -0.2037, -0.1843, -0.1645, -0.1184, -0.0876, -0.1975, -0.0561,\n",
       "                      -0.1804, -0.0701, -0.1915, -0.2375, -0.2104, -0.1402, -0.1872, -0.0962,\n",
       "                       0.0063, -0.1727, -0.1370, -0.0152, -0.1818, -0.1623, -0.0967, -0.1929,\n",
       "                      -0.1149, -0.0974, -0.0893, -0.1991, -0.0358, -0.0065,  0.0310, -0.1098,\n",
       "                      -0.3146, -0.2009, -0.0443, -0.0978, -0.1826, -0.0114, -0.0292, -0.1499,\n",
       "                      -0.0293, -0.0769, -0.1218, -0.0680, -0.1922, -0.0853, -0.0988, -0.2087,\n",
       "                      -0.1371, -0.1101, -0.1733, -0.1059, -0.1669, -0.1572, -0.1004, -0.1681,\n",
       "                      -0.2043, -0.0398, -0.0515, -0.1260, -0.1273,  0.0256, -0.1027, -0.1291,\n",
       "                      -0.1240, -0.2260,  0.0071, -0.0464, -0.1387, -0.1043, -0.0820, -0.1560,\n",
       "                      -0.0870, -0.0908, -0.0138, -0.0951, -0.2340, -0.1618, -0.1435, -0.0838,\n",
       "                      -0.1898, -0.1468, -0.1694, -0.1330, -0.0632, -0.1450, -0.0601, -0.1049,\n",
       "                      -0.1230, -0.1057, -0.1883, -0.1034, -0.0400, -0.1959, -0.1051, -0.2471,\n",
       "                      -0.1689, -0.2563, -0.0250, -0.0267, -0.1422, -0.0708, -0.1476, -0.0833,\n",
       "                       0.0544, -0.1323, -0.0199, -0.0655, -0.0318,  0.0213,  0.0816,  0.1138,\n",
       "                      -0.1447,  0.0578,  0.0025,  0.0712,  0.0689,  0.0282,  0.0463, -0.0204,\n",
       "                      -0.1186,  0.0156, -0.0761, -0.1039,  0.0034, -0.0557, -0.1068,  0.0206,\n",
       "                       0.1659,  0.0592, -0.0008,  0.0188,  0.0152,  0.1045, -0.0020,  0.0106,\n",
       "                       0.0522, -0.0930, -0.0576, -0.0659, -0.0241, -0.0407, -0.0080,  0.0249,\n",
       "                       0.0378,  0.0453,  0.0080,  0.0400,  0.0361,  0.0762,  0.1408, -0.0477,\n",
       "                      -0.0514,  0.1066, -0.0175,  0.0127, -0.0505, -0.1061,  0.1280,  0.0355,\n",
       "                       0.0189, -0.0554, -0.0100, -0.1273, -0.0322, -0.1009,  0.0197,  0.0059,\n",
       "                      -0.0351, -0.0027, -0.0804,  0.1224, -0.0087,  0.0062, -0.0791,  0.0960,\n",
       "                      -0.0599,  0.0539,  0.0196,  0.1232, -0.0223,  0.0452,  0.0470, -0.0020,\n",
       "                      -0.0443, -0.1524,  0.0079, -0.0099,  0.0377, -0.1029, -0.1357,  0.1000,\n",
       "                       0.0090, -0.1608,  0.0072,  0.0119, -0.0362, -0.0342, -0.0523, -0.0122,\n",
       "                      -0.0277,  0.0201,  0.0778,  0.0135,  0.0363,  0.0662, -0.0573, -0.0277,\n",
       "                       0.0225, -0.0832, -0.0147, -0.0536,  0.0412,  0.0093, -0.0418,  0.0861,\n",
       "                      -0.0332,  0.0289, -0.0024,  0.0126,  0.0620, -0.0443,  0.1287, -0.0931,\n",
       "                       0.0778,  0.0710, -0.0382, -0.0476,  0.0637,  0.0440,  0.0883,  0.0140,\n",
       "                      -0.0875, -0.2151,  0.0588, -0.0049,  0.0043, -0.0965, -0.1390,  0.0224,\n",
       "                      -0.1834, -0.2582, -0.1666, -0.0617, -0.0606, -0.1687, -0.1497, -0.1469,\n",
       "                      -0.1336, -0.1423, -0.2092, -0.1829, -0.0257, -0.1247, -0.1517, -0.1749,\n",
       "                      -0.0797, -0.1599, -0.2959, -0.2507, -0.1610,  0.0076, -0.0958, -0.1193,\n",
       "                      -0.2180, -0.0846, -0.1221, -0.1298, -0.2961, -0.1048, -0.0655, -0.1058,\n",
       "                      -0.0681, -0.1899, -0.1115,  0.0297, -0.1028, -0.0889, -0.0561, -0.1859,\n",
       "                      -0.1396, -0.0988, -0.1136, -0.1531, -0.1287, -0.1610, -0.0634, -0.0278,\n",
       "                      -0.1790, -0.0088, -0.1313, -0.0670, -0.1092, -0.0850, -0.1192, -0.1737,\n",
       "                      -0.1449, -0.0162, -0.2045, -0.1418, -0.1291, -0.0677, -0.1972, -0.0926,\n",
       "                      -0.2084, -0.1318, -0.2301, -0.2228, -0.1968, -0.1876, -0.0677, -0.1549,\n",
       "                      -0.1232, -0.1332, -0.0423, -0.0895, -0.0096, -0.0776, -0.1427, -0.1757,\n",
       "                      -0.2094, -0.0906, -0.0862, -0.0944, -0.1375, -0.0865,  0.0076, -0.0426,\n",
       "                      -0.2346, -0.0498,  0.0830, -0.1730, -0.0470, -0.2449, -0.3423, -0.0652,\n",
       "                      -0.2981, -0.1085, -0.1762, -0.1065, -0.1215, -0.0320,  0.0616, -0.1476,\n",
       "                      -0.0839,  0.0447, -0.1945, -0.3307, -0.0621, -0.0357, -0.0478, -0.1160,\n",
       "                      -0.2029, -0.0864, -0.0128, -0.1496, -0.1803, -0.1718, -0.0656, -0.0879])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-1.9233e-01, -1.6802e-01, -7.2829e-02, -1.7460e-02, -1.6808e-01,\n",
       "                      -5.0212e-02, -7.8543e-02, -1.2615e-02, -1.6741e-01, -2.1027e-01,\n",
       "                      -8.6231e-02, -1.5322e-01, -1.2132e-01, -8.2674e-02, -1.2081e-01,\n",
       "                      -1.6880e-01, -1.8770e-01, -8.7900e-02, -1.1605e-01, -2.0660e-01,\n",
       "                       3.2932e-02,  7.2653e-02, -5.2463e-02, -1.6421e-01, -1.9222e-01,\n",
       "                      -1.4101e-01, -1.0853e-01, -1.7798e-01, -1.0477e-01, -6.8277e-02,\n",
       "                       3.3381e-03, -1.7033e-01, -1.1402e-01, -7.0992e-02, -5.6322e-02,\n",
       "                      -1.7465e-01, -1.3885e-01, -9.5557e-02, -9.4410e-02, -1.4785e-01,\n",
       "                      -2.2856e-01, -1.1216e-01, -9.4678e-02, -1.8687e-04, -1.9692e-01,\n",
       "                      -9.1948e-02,  1.8981e-02, -2.5020e-01, -7.3313e-02, -1.7426e-01,\n",
       "                      -2.0034e-01, -2.0346e-01, -2.5377e-01, -1.6265e-01, -9.0662e-02,\n",
       "                      -2.3553e-02, -2.5166e-01, -2.9447e-02, -1.4714e-01,  2.8872e-02,\n",
       "                      -1.1492e-01, -2.0515e-02, -2.6951e-02, -2.4809e-02, -1.0033e-01,\n",
       "                      -1.6660e-01, -1.2108e-01, -1.0386e-01, -1.0643e-01, -8.1951e-02,\n",
       "                      -1.9250e-02, -6.9098e-02, -1.4397e-01, -1.3995e-01, -1.2752e-01,\n",
       "                      -1.2598e-01, -1.7390e-01, -2.7534e-01, -9.8325e-03, -2.8278e-01,\n",
       "                      -5.9504e-02, -1.4368e-01, -4.4970e-02, -1.4065e-01, -2.6950e-01,\n",
       "                      -1.6061e-01, -1.0210e-01, -2.0484e-01, -1.3412e-01, -1.3589e-01,\n",
       "                      -4.7374e-02, -1.2431e-01, -2.0575e-01, -1.3112e-01, -9.6985e-02,\n",
       "                      -2.1934e-01, -5.2299e-02, -1.5087e-01, -1.1832e-01,  6.1714e-02,\n",
       "                      -8.1487e-02, -2.9222e-01, -2.7321e-01, -1.0636e-01, -2.1759e-01,\n",
       "                      -1.4499e-01, -4.0425e-01, -2.1901e-01, -6.9135e-02, -9.4365e-02,\n",
       "                      -6.7547e-02, -1.9001e-01, -1.7090e-01,  1.9685e-02, -1.4222e-01,\n",
       "                      -2.4472e-01, -1.7567e-01, -1.1252e-01, -1.3886e-01, -7.9765e-02,\n",
       "                      -1.8682e-01, -1.6073e-01, -1.4571e-01, -1.9831e-02, -1.7094e-01,\n",
       "                      -3.0460e-01, -1.2665e-01, -2.2638e-01, -5.1247e-02, -1.9821e-01,\n",
       "                      -3.8939e-02, -1.6266e-01, -1.7023e-01, -1.8158e-01, -6.9702e-02,\n",
       "                      -9.6036e-02, -5.5329e-02, -5.9253e-02, -1.7394e-01, -1.0951e-01,\n",
       "                      -1.7054e-01, -1.2377e-01, -1.9692e-01, -1.6017e-01, -3.2290e-01,\n",
       "                      -1.9042e-01, -6.7691e-02, -1.0715e-01, -1.3641e-01,  6.6033e-02,\n",
       "                      -1.5900e-02, -1.4330e-01,  1.1783e-02, -1.3864e-01, -2.0506e-01,\n",
       "                      -2.3548e-01, -2.3614e-01, -2.2924e-01, -1.0272e-01, -1.4319e-02,\n",
       "                      -1.3852e-01, -1.2913e-01, -1.9310e-01, -1.1312e-01, -1.4342e-01,\n",
       "                      -1.6111e-01, -1.8318e-01, -1.3830e-01, -1.6097e-01, -1.0854e-01,\n",
       "                      -2.7270e-01,  7.4709e-02, -1.7743e-01, -1.5409e-01, -4.4712e-02,\n",
       "                      -9.0078e-02, -7.6522e-02, -1.0586e-01, -1.2939e-01, -1.5706e-01,\n",
       "                      -7.3589e-02,  3.2697e-02, -7.6828e-02, -4.7711e-02, -9.8681e-02,\n",
       "                      -9.3104e-02, -2.2296e-01, -3.0872e-02, -1.9259e-01, -3.1557e-02,\n",
       "                      -3.9859e-02, -1.6596e-01, -5.7435e-02, -1.5770e-01, -1.5562e-01,\n",
       "                      -1.4412e-01, -2.3251e-01, -1.2619e-01, -1.4168e-01, -1.1664e-01,\n",
       "                      -9.3417e-02, -9.2485e-02, -2.2405e-01, -2.6955e-01, -1.2867e-01,\n",
       "                      -2.4893e-01, -1.7773e-01, -1.4662e-01, -9.7371e-03, -1.2898e-01,\n",
       "                      -1.0502e-01, -2.5199e-01, -2.2223e-01, -2.7499e-02, -1.9749e-01,\n",
       "                      -2.1194e-01, -1.1708e-01, -1.3422e-01, -9.8554e-02, -1.1523e-01,\n",
       "                      -1.4863e-01, -7.2180e-02, -1.2610e-01, -1.4726e-01, -9.6517e-02,\n",
       "                      -1.3762e-01,  3.8127e-02, -1.0013e-01, -8.8144e-02, -1.1838e-01,\n",
       "                      -2.9281e-01, -2.1397e-01, -4.0408e-01, -5.8585e-02, -1.7474e-01,\n",
       "                      -1.4092e-01, -1.5927e-01, -1.3582e-02, -6.1595e-02, -2.4378e-01,\n",
       "                      -6.1494e-02, -1.0875e-01, -1.1819e-01, -1.5491e-01, -4.8451e-02,\n",
       "                      -9.9483e-02, -7.0120e-02, -2.4790e-01, -2.9057e-01, -1.6001e-01,\n",
       "                      -6.4992e-02, -1.7060e-01, -1.6581e-01, -1.7100e-01, -1.7528e-01,\n",
       "                      -5.1082e-04,  1.2104e-01,  5.7864e-03,  4.0497e-02,  6.8384e-02,\n",
       "                       1.1131e-02, -2.5660e-02,  1.2493e-01,  1.5331e-01,  2.4376e-02,\n",
       "                       9.4851e-02, -9.5042e-03, -5.8329e-02,  5.7863e-02,  1.3739e-01,\n",
       "                      -4.1698e-02,  4.4319e-02, -5.5002e-02,  4.5071e-02, -8.8881e-02,\n",
       "                       9.5607e-02, -8.1915e-05, -4.4614e-02, -6.3469e-02,  6.0801e-02,\n",
       "                       5.1880e-02, -2.9797e-02,  1.1791e-02,  1.3360e-01, -3.5268e-03,\n",
       "                       1.9818e-02, -4.2061e-02,  7.9372e-02,  1.1111e-01, -6.9276e-02,\n",
       "                       6.4373e-05,  3.3084e-02, -1.4272e-02, -2.5413e-02,  2.8181e-02,\n",
       "                       3.1735e-02,  8.5242e-03, -6.6895e-02, -4.4163e-02,  1.0548e-01,\n",
       "                      -8.5494e-03, -1.6573e-02,  1.6334e-02,  6.0598e-02,  1.0197e-01,\n",
       "                      -2.1195e-02, -9.4776e-02,  1.0047e-01, -5.1149e-02, -3.4338e-02,\n",
       "                       1.0248e-01,  9.5061e-02,  4.9496e-02, -8.8845e-02,  2.9561e-02,\n",
       "                      -5.5929e-02,  3.7079e-02,  2.8939e-02,  4.1908e-02,  5.5218e-02,\n",
       "                       7.9995e-02,  3.9947e-02,  9.5809e-02,  2.6141e-02, -1.0920e-01,\n",
       "                      -3.2947e-02, -9.6285e-02,  9.4897e-02, -2.2436e-02,  7.6244e-02,\n",
       "                      -2.5172e-02,  3.8370e-02,  1.2336e-02,  3.9537e-02,  7.4204e-03,\n",
       "                      -6.4962e-02, -1.3837e-01,  1.0465e-01, -9.1468e-02,  7.1006e-02,\n",
       "                       2.8366e-02,  4.0636e-02, -5.9643e-02, -1.3482e-01,  9.9312e-03,\n",
       "                      -1.1093e-01,  5.7388e-02,  7.4746e-02,  3.4978e-02, -6.3975e-02,\n",
       "                      -3.7883e-02,  9.6808e-02,  4.0197e-02,  4.2631e-02, -9.2471e-02,\n",
       "                      -9.4455e-02,  6.6100e-02, -2.1668e-02, -3.4885e-02, -1.2204e-01,\n",
       "                       3.9641e-03,  1.0838e-01, -1.3074e-02, -1.0137e-01,  1.9187e-01,\n",
       "                       3.0930e-02,  1.1538e-01,  5.6219e-02,  3.7692e-03,  1.0318e-01,\n",
       "                       1.6760e-02,  3.4940e-02, -6.2877e-02,  2.3571e-02,  1.2162e-01,\n",
       "                       6.0144e-02,  5.2607e-03, -1.0532e-01, -8.3522e-02,  3.0740e-02,\n",
       "                       5.8099e-02,  4.9659e-02, -2.7216e-02,  6.8042e-02, -3.1121e-02,\n",
       "                      -1.6607e-01, -5.7953e-02, -5.3080e-02, -4.1560e-02, -1.5295e-01,\n",
       "                      -7.3262e-02, -2.1403e-01, -2.0743e-01, -1.2231e-01, -1.7434e-01,\n",
       "                       1.4089e-02, -1.3911e-01, -2.2664e-01, -1.6160e-01, -1.5357e-01,\n",
       "                      -1.5066e-01, -1.9465e-01, -1.4755e-02, -1.8765e-01, -7.2573e-02,\n",
       "                      -1.4190e-01, -6.7329e-03, -1.2153e-01, -1.6399e-01, -2.2866e-01,\n",
       "                      -1.9935e-01, -2.0679e-01, -8.6040e-02, -1.3714e-01, -6.3973e-02,\n",
       "                      -1.8028e-01, -1.0755e-01, -9.9060e-02, -6.4246e-02, -2.8578e-01,\n",
       "                      -7.2835e-02,  1.6409e-02, -4.4270e-02, -4.4693e-02, -2.4299e-01,\n",
       "                      -7.7373e-02, -6.8316e-03,  5.5202e-02, -5.5159e-02, -1.9769e-01,\n",
       "                      -8.5153e-02, -2.4968e-01, -1.2809e-01, -6.6333e-02, -1.3598e-01,\n",
       "                      -2.3629e-01, -4.5577e-02, -1.0544e-01, -1.6111e-01, -7.2102e-03,\n",
       "                      -1.2214e-01, -5.4379e-02, -2.8600e-01, -1.2507e-01, -2.0087e-01,\n",
       "                      -1.6383e-01, -1.8938e-01, -7.4653e-02, -5.1731e-02, -3.9878e-02,\n",
       "                      -1.4594e-01, -5.9510e-02, -2.2766e-01, -3.9754e-02,  4.6987e-02,\n",
       "                      -1.7252e-01, -9.3566e-02, -9.7673e-02, -8.4137e-02, -1.6758e-01,\n",
       "                      -1.4430e-01, -9.7424e-02, -1.1013e-01, -4.2480e-01, -9.4908e-02,\n",
       "                      -1.1241e-01, -6.5515e-02, -1.8793e-01, -1.4034e-01,  6.4261e-02,\n",
       "                      -1.3846e-01, -2.2673e-01, -1.5247e-01, -3.0827e-03, -1.5564e-02,\n",
       "                      -1.1751e-01, -1.0096e-01, -1.2441e-01, -8.6332e-02, -2.4189e-02,\n",
       "                      -1.8032e-01, -8.4790e-02, -2.1341e-02,  1.6232e-03, -1.4811e-02,\n",
       "                      -1.7289e-01, -1.6000e-01, -7.6682e-02, -4.0525e-02, -1.3019e-01,\n",
       "                      -2.4541e-01, -2.5126e-01,  2.7033e-02, -1.1476e-01, -8.9122e-02,\n",
       "                      -1.5264e-02, -7.6718e-02, -1.0713e-01, -1.4785e-01, -2.3529e-01,\n",
       "                      -1.9421e-01, -1.6739e-01, -1.1271e-01, -1.6949e-01, -1.9841e-01,\n",
       "                      -1.9441e-01, -8.5131e-02, -4.9811e-02, -2.2505e-01, -1.2020e-01,\n",
       "                      -1.7116e-01, -1.7485e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.1250,  0.0740,  0.1102,  ...,  0.0923, -0.0505,  0.0998],\n",
       "                      [ 0.3356, -0.0602,  0.1186,  ..., -0.2904, -0.0272, -0.1005],\n",
       "                      [ 0.0438, -0.1121, -0.0784,  ..., -0.4620,  0.4739, -0.2375],\n",
       "                      ...,\n",
       "                      [ 0.0756,  0.2907,  0.1612,  ...,  0.2675, -0.1427,  0.1890],\n",
       "                      [ 0.0321,  0.0550, -0.0981,  ...,  0.3401,  0.0648, -0.0544],\n",
       "                      [-0.4401, -0.4893,  0.2953,  ..., -0.1614, -0.0649, -0.3063]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0926,  0.2969, -0.0011,  ...,  0.2256, -0.1060,  0.2437],\n",
       "                      [ 0.2377,  0.0689, -0.0346,  ...,  0.2531, -0.1530,  0.1724],\n",
       "                      [-0.0418,  0.1453, -0.1037,  ..., -0.0162, -0.0038,  0.0449],\n",
       "                      ...,\n",
       "                      [-0.0455,  0.1852, -0.1550,  ..., -0.1054, -0.1491,  0.0709],\n",
       "                      [ 0.0158, -0.1380,  0.1892,  ...,  0.0210,  0.1142,  0.1426],\n",
       "                      [ 0.2353,  0.0692, -0.2728,  ..., -0.3196,  0.0538, -0.1582]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0656,  0.1984,  0.1500,  0.0872, -0.0835,  0.1441, -0.0298,  0.1461,\n",
       "                       0.0216,  0.0127,  0.0775,  0.0561,  0.1082,  0.2411,  0.0487,  0.0568,\n",
       "                       0.0654,  0.1067,  0.0599,  0.1998,  0.1149,  0.0377, -0.0452,  0.1741,\n",
       "                      -0.1830,  0.2654, -0.0168,  0.0484, -0.1296,  0.0021, -0.0494, -0.0180,\n",
       "                       0.0309,  0.3219,  0.2406,  0.2022,  0.0616,  0.2589,  0.0436, -0.0334,\n",
       "                       0.1463,  0.0833,  0.0540,  0.0489,  0.0338,  0.1303,  0.2293,  0.0941,\n",
       "                       0.0474,  0.1660,  0.1478,  0.0299,  0.0460, -0.0049,  0.0818,  0.0458,\n",
       "                      -0.0829, -0.0184,  0.0485, -0.0646,  0.1587,  0.1628,  0.0110, -0.0271,\n",
       "                       0.0687,  0.0887,  0.1026,  0.0952, -0.0487,  0.1138, -0.0473,  0.0921,\n",
       "                      -0.0204,  0.1279,  0.1375,  0.0667,  0.0571, -0.0519,  0.1989, -0.0046,\n",
       "                       0.0140,  0.1227,  0.2402,  0.0533,  0.1128,  0.1317,  0.1303,  0.1268,\n",
       "                      -0.0264,  0.0201,  0.0648,  0.0826, -0.0732,  0.0114,  0.0833,  0.1458,\n",
       "                       0.0292,  0.1214, -0.0071,  0.0738,  0.0293,  0.1384,  0.0159,  0.0768,\n",
       "                       0.0095, -0.2163,  0.1023,  0.1194,  0.0099,  0.1374,  0.1469,  0.0876,\n",
       "                       0.0921,  0.0515,  0.0308, -0.0284, -0.0111,  0.0792, -0.0053,  0.0196,\n",
       "                       0.2311,  0.1162,  0.1594, -0.0592,  0.0982,  0.1140,  0.0426,  0.1451,\n",
       "                       0.0361,  0.0267, -0.1194, -0.0518,  0.0223, -0.0783, -0.0147, -0.1777,\n",
       "                       0.0367,  0.0825, -0.0404, -0.0398,  0.1366, -0.1352, -0.1191, -0.0074,\n",
       "                       0.0309, -0.0112, -0.0638, -0.0038, -0.0115, -0.1287, -0.0608,  0.0753,\n",
       "                       0.1226, -0.0919, -0.0091, -0.0729,  0.0469, -0.0933, -0.0078, -0.0011,\n",
       "                       0.0108, -0.1482, -0.0028,  0.0512, -0.0565,  0.0048, -0.0435, -0.0785,\n",
       "                      -0.0479, -0.0049,  0.1171, -0.1574, -0.0844, -0.0695, -0.0287, -0.0351,\n",
       "                      -0.1079,  0.0317, -0.0556,  0.0126,  0.0050,  0.1805, -0.0413, -0.1061,\n",
       "                      -0.1064, -0.1588,  0.0009, -0.1169,  0.0252,  0.1526, -0.1163, -0.1314,\n",
       "                       0.1631,  0.0052,  0.0111, -0.0793, -0.2119, -0.0795,  0.0053, -0.0090,\n",
       "                       0.0705, -0.0649,  0.0604, -0.1190, -0.1059,  0.1666, -0.1967, -0.1188,\n",
       "                       0.0152, -0.0314, -0.0013, -0.1504,  0.0669,  0.0397, -0.1992,  0.0090,\n",
       "                       0.0352, -0.0274, -0.0384, -0.0862, -0.1416, -0.0158, -0.0520, -0.0709,\n",
       "                      -0.0541,  0.0685,  0.0520, -0.0515, -0.0236, -0.0577,  0.0101, -0.0010,\n",
       "                      -0.0644, -0.0439,  0.1095,  0.0233, -0.0149, -0.0226, -0.1100, -0.0539,\n",
       "                      -0.2696, -0.0773, -0.0503,  0.0238,  0.0486,  0.0105, -0.1521, -0.0396,\n",
       "                      -0.0790, -0.0573, -0.1297, -0.0149,  0.0048, -0.1003,  0.0655, -0.0767,\n",
       "                       0.0730,  0.0318,  0.0896,  0.0869,  0.0095,  0.0235,  0.0011,  0.0156,\n",
       "                       0.0759, -0.0172, -0.0712,  0.0664, -0.0834, -0.0117,  0.0281,  0.0951,\n",
       "                      -0.0361,  0.0069,  0.0518, -0.0334,  0.0763,  0.0641, -0.0088,  0.0516,\n",
       "                      -0.0117, -0.0060,  0.0164,  0.0848,  0.0448, -0.0107, -0.0540, -0.0294,\n",
       "                      -0.1898,  0.1141, -0.0056, -0.0538, -0.0480, -0.0752, -0.0006, -0.0548,\n",
       "                       0.0191,  0.0635, -0.0281,  0.0082,  0.0598, -0.1137, -0.0893,  0.1219,\n",
       "                      -0.0053,  0.0388,  0.0260,  0.0307, -0.1162,  0.0181,  0.1402, -0.0109,\n",
       "                      -0.0188,  0.0120, -0.0180, -0.0992, -0.0554, -0.0051,  0.0430, -0.0854,\n",
       "                       0.0369, -0.0446, -0.0126, -0.0598, -0.0362, -0.0451,  0.0341,  0.0101,\n",
       "                       0.0039, -0.0013, -0.1012, -0.1062,  0.0475, -0.0847, -0.0682, -0.0458,\n",
       "                      -0.0182, -0.1068,  0.0538, -0.0103,  0.1068, -0.0788,  0.0623, -0.0289,\n",
       "                       0.1544, -0.0949, -0.1079, -0.0606, -0.0034,  0.0610,  0.0826,  0.0196,\n",
       "                      -0.0270,  0.0140, -0.0919, -0.0166,  0.0135, -0.0825,  0.0408, -0.0166,\n",
       "                       0.0561, -0.1068,  0.1188, -0.0896,  0.0326, -0.0972, -0.0542,  0.1247,\n",
       "                       0.0690, -0.0639, -0.0313,  0.0767, -0.0067, -0.0042,  0.0248, -0.0648,\n",
       "                       0.0438, -0.0385, -0.0148,  0.0479, -0.1093, -0.1309,  0.1072,  0.0705,\n",
       "                       0.0354,  0.2473,  0.1708,  0.0950, -0.0606,  0.0863,  0.0112,  0.2155,\n",
       "                       0.0711,  0.1734,  0.1431,  0.0088,  0.2776,  0.2356, -0.0438,  0.1070,\n",
       "                      -0.0669,  0.1129,  0.0678,  0.2601,  0.1090, -0.0291, -0.0353,  0.2511,\n",
       "                      -0.0559,  0.2325,  0.1499,  0.0827, -0.2254,  0.0234, -0.1198,  0.0471,\n",
       "                       0.0755,  0.2939,  0.2579,  0.1405,  0.1402,  0.1113,  0.1899, -0.0695,\n",
       "                       0.2328,  0.1342,  0.0028,  0.1465, -0.0616,  0.0518,  0.0266,  0.1244,\n",
       "                       0.0268,  0.2153,  0.2177,  0.0748,  0.0775,  0.0407,  0.0920,  0.0325,\n",
       "                       0.1242, -0.0738,  0.1048,  0.0323,  0.1870,  0.1631,  0.1167,  0.1218,\n",
       "                       0.1421,  0.1261,  0.0979,  0.1244, -0.0452,  0.0469, -0.0112,  0.1879,\n",
       "                       0.1304,  0.0869,  0.0921,  0.1283,  0.0627,  0.1012,  0.0186, -0.0346,\n",
       "                       0.0389,  0.0110,  0.2152,  0.1250,  0.0605,  0.1571,  0.0167, -0.0777,\n",
       "                      -0.0428,  0.1000,  0.0149, -0.0798,  0.0974,  0.1136,  0.2094,  0.0076,\n",
       "                       0.0647,  0.0104,  0.0133,  0.0941,  0.0134,  0.1073,  0.0456,  0.1788,\n",
       "                       0.0755,  0.1086,  0.0502,  0.1361,  0.1316,  0.0268,  0.0166,  0.1165,\n",
       "                       0.1884,  0.0921,  0.0046,  0.0179, -0.0197,  0.0044, -0.0417, -0.0261,\n",
       "                       0.1902,  0.1288,  0.1065, -0.0728,  0.0763,  0.0333,  0.0208,  0.0152])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 7.1339e-02,  2.3294e-01, -6.1168e-02,  3.4648e-02, -4.6780e-02,\n",
       "                       9.0256e-02, -2.4965e-02,  8.3945e-02,  1.2972e-01,  1.4714e-02,\n",
       "                       1.3888e-01, -2.4714e-02,  4.6407e-02,  1.6180e-01, -1.5670e-02,\n",
       "                       6.4971e-02,  5.6039e-02,  1.7362e-01,  9.5434e-02,  1.4734e-01,\n",
       "                       1.2421e-01,  6.0851e-02,  3.0823e-03,  1.7540e-01,  1.2852e-02,\n",
       "                       2.2128e-01,  8.0861e-02,  1.2914e-01, -1.3288e-01, -3.1512e-02,\n",
       "                       9.7981e-02,  6.4757e-02,  6.5160e-02,  2.5974e-01,  1.9902e-01,\n",
       "                       9.4747e-02,  8.7697e-02,  1.6828e-01,  2.3443e-01,  6.9976e-02,\n",
       "                       2.1390e-01, -2.4764e-02,  6.5734e-02,  1.6277e-01,  5.5632e-02,\n",
       "                       4.2750e-02,  1.5646e-01,  4.1380e-02,  1.6652e-01,  8.9132e-02,\n",
       "                       1.2140e-01,  5.1242e-02,  8.8961e-02,  1.4070e-02,  1.7914e-01,\n",
       "                       1.2722e-03,  2.0727e-02,  5.5298e-02,  1.6608e-01, -1.0782e-01,\n",
       "                       5.3272e-02,  5.4514e-02,  6.2955e-02, -6.7888e-02,  1.2272e-01,\n",
       "                       6.7779e-02, -3.9085e-02,  2.3880e-01,  1.3735e-01,  1.6747e-01,\n",
       "                      -1.4943e-01,  3.3242e-02,  1.1696e-01,  1.8708e-01,  8.3518e-02,\n",
       "                       3.2985e-02,  3.9036e-02,  2.2017e-02,  1.2461e-01,  4.3116e-02,\n",
       "                       4.8879e-02,  1.0034e-01,  1.6867e-01,  1.6787e-01,  7.0950e-02,\n",
       "                       1.1223e-01,  7.6505e-02,  1.3595e-01, -7.0959e-02, -6.7618e-02,\n",
       "                       1.2928e-01,  1.2608e-01,  3.6346e-02,  1.5921e-02,  1.3713e-01,\n",
       "                       1.6583e-01, -1.0217e-02,  9.9320e-02,  2.4486e-02,  2.2464e-01,\n",
       "                       5.8813e-02,  4.8282e-02, -1.0073e-01,  1.2260e-01,  1.1445e-01,\n",
       "                       7.2969e-02,  5.7685e-02,  1.2097e-01,  8.3412e-02,  1.2035e-01,\n",
       "                       3.3189e-02, -2.9024e-04,  9.7094e-02,  6.4888e-02,  6.7776e-02,\n",
       "                      -6.9165e-03,  6.2720e-02,  1.4324e-01, -8.1528e-02,  1.0426e-02,\n",
       "                       9.4886e-02,  1.8355e-01, -6.2635e-02, -3.5948e-02,  1.6620e-01,\n",
       "                       8.0414e-02, -4.6462e-02,  9.6251e-02, -8.3257e-02, -2.9033e-02,\n",
       "                      -1.5257e-01, -1.0378e-01,  1.3877e-02, -1.6602e-01, -1.4596e-01,\n",
       "                      -8.7590e-02,  1.2231e-01,  1.8416e-01, -4.5632e-02, -3.2113e-02,\n",
       "                      -5.2614e-02, -2.9847e-01, -6.2723e-02, -2.0585e-02,  1.3769e-02,\n",
       "                      -3.0897e-02, -1.7861e-01, -2.6489e-02, -2.9527e-02, -2.1923e-01,\n",
       "                      -5.3768e-02, -5.4851e-02, -8.5116e-02, -1.4743e-01,  2.3374e-02,\n",
       "                      -1.4146e-01, -6.9708e-02,  6.5119e-02, -5.1966e-02,  9.6403e-02,\n",
       "                       1.5094e-01, -1.5754e-01,  3.5513e-03, -3.7350e-02, -4.8408e-03,\n",
       "                      -1.3030e-01, -6.4263e-02, -2.6383e-02, -1.2062e-02,  1.1019e-01,\n",
       "                       1.0418e-01, -6.1322e-02,  1.3050e-02, -5.7132e-02, -1.2457e-01,\n",
       "                      -1.6094e-01, -3.2185e-03, -9.4623e-02, -1.5836e-01, -7.8721e-02,\n",
       "                      -7.2509e-03, -2.3127e-02, -1.6726e-01, -8.0926e-02, -2.4173e-02,\n",
       "                      -1.2934e-01, -9.3032e-02, -3.1188e-02,  2.2974e-03, -3.3320e-03,\n",
       "                      -7.0704e-02, -4.5584e-02,  1.8698e-01, -1.3211e-02, -7.4665e-03,\n",
       "                      -8.9508e-02, -3.8207e-02, -7.2131e-02, -4.2696e-02, -1.5995e-02,\n",
       "                      -2.6883e-02, -1.8895e-01,  2.6781e-02, -9.3511e-02, -3.4867e-02,\n",
       "                       1.5841e-03, -1.4211e-01, -9.6651e-02,  3.7527e-02,  3.9805e-02,\n",
       "                      -1.0494e-02, -1.8041e-01,  3.8187e-02,  1.1863e-01, -1.5368e-01,\n",
       "                      -2.1022e-03,  7.1683e-02,  3.6060e-02,  1.1296e-01,  1.7439e-02,\n",
       "                       9.4356e-03, -4.9618e-02, -9.7463e-02, -1.1318e-03,  1.2827e-02,\n",
       "                      -4.5205e-02,  8.9593e-02, -6.6642e-02, -9.1355e-02, -7.9665e-02,\n",
       "                       2.5476e-02, -7.4379e-02, -4.6161e-02, -6.7872e-02, -4.5738e-02,\n",
       "                      -1.0177e-01,  6.6412e-02, -6.9504e-02, -4.7140e-02, -1.4007e-01,\n",
       "                      -7.8921e-02, -1.3667e-01, -1.3520e-01, -3.3840e-02,  1.5824e-02,\n",
       "                      -3.1878e-02, -1.2366e-01, -5.9424e-02, -1.1009e-01, -1.4170e-02,\n",
       "                      -1.1713e-01, -5.0761e-02,  1.1364e-04, -9.5150e-02,  2.9598e-02,\n",
       "                      -8.5213e-02,  4.5089e-02, -2.6288e-04,  4.3549e-03,  2.9253e-02,\n",
       "                      -1.9411e-02,  1.0197e-03,  5.4173e-02, -1.2155e-01, -4.7918e-02,\n",
       "                       9.2344e-02, -8.5420e-02, -1.0413e-01, -9.5484e-03, -9.9328e-02,\n",
       "                       9.5400e-02, -7.9149e-02,  1.6716e-01,  1.3336e-01, -9.9425e-02,\n",
       "                      -1.0787e-02,  9.8973e-02, -3.9458e-02, -6.2495e-02, -7.2971e-02,\n",
       "                       1.2313e-01,  1.1954e-02,  4.5313e-02, -5.8626e-02, -3.5297e-02,\n",
       "                      -6.7572e-02, -5.1902e-02,  5.4323e-02, -8.1660e-02,  9.2557e-02,\n",
       "                       1.4621e-01, -7.3931e-02, -2.0877e-02, -2.3171e-01, -1.1566e-01,\n",
       "                       5.9398e-02, -8.9130e-03,  9.4960e-02, -1.0238e-01, -7.5069e-02,\n",
       "                       5.0337e-03, -6.0902e-02, -1.0936e-02,  1.0121e-01,  1.7689e-02,\n",
       "                      -1.4261e-03,  8.7020e-02, -3.2197e-02,  3.0940e-02,  1.1447e-02,\n",
       "                      -7.3665e-02, -5.7858e-03,  1.4425e-02,  5.2780e-02, -3.8135e-02,\n",
       "                       2.6549e-03, -3.6950e-03, -3.3083e-02, -2.8742e-02, -6.1754e-02,\n",
       "                       1.1464e-01, -1.9457e-02, -1.6253e-02,  5.6400e-02,  1.8647e-02,\n",
       "                      -1.3790e-01, -3.4026e-02, -3.4588e-02,  5.4857e-04,  4.8570e-02,\n",
       "                      -8.6119e-02,  3.9887e-02, -4.0511e-04,  1.1644e-01,  1.4264e-02,\n",
       "                       3.2771e-02,  1.7321e-01,  1.7431e-02,  7.0208e-02, -7.6614e-02,\n",
       "                       2.5222e-02, -6.5315e-02,  9.8677e-02,  8.6162e-02, -9.8995e-03,\n",
       "                       1.6856e-01, -4.5965e-02, -4.0940e-02,  2.2496e-02, -1.7966e-02,\n",
       "                       5.2777e-03, -3.1522e-02, -2.7095e-02, -6.3148e-02,  7.1465e-02,\n",
       "                      -1.3699e-02,  5.1114e-03,  6.0631e-02,  1.2749e-01, -7.4682e-03,\n",
       "                      -4.1431e-02,  7.6619e-02, -1.1293e-01,  3.4561e-02, -6.4700e-03,\n",
       "                       8.1135e-03, -8.8206e-02,  1.1784e-01,  1.0794e-02,  2.7294e-02,\n",
       "                      -1.2471e-02, -1.9426e-03, -9.0858e-02,  1.0266e-01,  1.1108e-01,\n",
       "                      -1.5035e-01, -1.5206e-02,  3.0367e-02,  1.3690e-02, -3.4810e-02,\n",
       "                      -3.0450e-02, -2.8929e-03,  1.9668e-02,  6.7839e-02,  5.9737e-02,\n",
       "                       2.2458e-01,  1.3493e-01,  1.0656e-01, -3.2124e-02,  1.2456e-01,\n",
       "                       1.8865e-02,  1.2702e-01,  5.7173e-02,  4.4024e-02,  1.4972e-01,\n",
       "                       4.7400e-02,  2.0678e-01,  2.4294e-01,  1.2273e-01,  7.8969e-02,\n",
       "                       9.5741e-02,  3.8852e-02,  1.7376e-01,  1.9933e-01,  8.6024e-02,\n",
       "                      -3.8177e-02, -7.3083e-02,  2.4589e-01, -1.3639e-02,  3.2818e-01,\n",
       "                       7.6233e-02, -4.5980e-02, -9.8719e-02, -5.2590e-03,  1.1581e-02,\n",
       "                       4.0609e-02,  2.0325e-01,  2.5716e-01,  2.3069e-01,  1.4854e-01,\n",
       "                       1.1644e-01,  2.1658e-01,  1.7949e-01, -1.9371e-02,  1.1060e-01,\n",
       "                       1.5987e-01,  2.6583e-02,  1.6036e-01,  1.1222e-01,  2.4827e-02,\n",
       "                       7.3525e-02,  5.2536e-02,  1.8813e-01,  2.3186e-01,  2.8818e-01,\n",
       "                      -2.1792e-02,  5.4869e-02, -7.1611e-02,  1.4144e-01,  9.9756e-02,\n",
       "                       6.3940e-02,  8.1725e-02,  2.5534e-03,  2.9405e-02,  2.1958e-01,\n",
       "                       3.0399e-02, -6.9797e-02,  1.4810e-01,  1.6952e-01,  9.6114e-02,\n",
       "                       1.4688e-01,  1.9344e-01,  8.9552e-02,  5.5567e-02, -1.4667e-02,\n",
       "                       1.6611e-01,  1.5562e-01,  1.4440e-01,  4.0504e-02,  9.2979e-02,\n",
       "                       8.2404e-02,  6.9002e-02,  6.1999e-02, -9.9361e-02,  2.8063e-03,\n",
       "                       5.1338e-02,  2.1055e-01,  1.4995e-01,  3.2589e-02,  8.7212e-02,\n",
       "                       1.9567e-01, -3.3533e-02, -1.3709e-01,  1.0018e-01,  8.6777e-02,\n",
       "                       1.1299e-01, -1.0300e-01,  2.4696e-02,  1.3266e-01,  1.7214e-01,\n",
       "                      -2.3226e-02,  6.3203e-02, -1.4369e-01,  4.0589e-02, -5.4241e-02,\n",
       "                       1.5663e-01,  1.7011e-03,  2.4088e-01,  6.9834e-02, -4.7941e-02,\n",
       "                       1.0629e-01,  3.1421e-02,  5.3690e-02,  1.4696e-01,  2.1938e-01,\n",
       "                       1.4336e-01,  2.4641e-01,  1.0909e-01,  2.3811e-02,  5.6181e-02,\n",
       "                       9.5788e-02, -3.4280e-02, -1.7411e-01,  7.7348e-02,  2.1586e-01,\n",
       "                       1.8968e-01,  4.4294e-02,  4.3112e-02,  1.2541e-01, -3.6464e-02,\n",
       "                       2.1957e-02,  1.5128e-01])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[-0.1045, -0.0036, -0.0888,  ...,  0.2626, -0.4975, -0.1686],\n",
       "                      [ 0.1398, -0.0371,  0.0775,  ...,  0.1893, -0.1870,  0.3223],\n",
       "                      [-0.0047,  0.0698, -0.0488,  ...,  0.2093,  0.1402, -0.0173],\n",
       "                      ...,\n",
       "                      [-0.0480,  0.0258,  0.0374,  ..., -0.0649,  0.4252, -0.0803],\n",
       "                      [ 0.0231,  0.0553, -0.0169,  ...,  0.0597, -0.0710,  0.4590],\n",
       "                      [ 0.0411, -0.0318,  0.0948,  ..., -0.2274, -0.0514, -0.1417]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-0.1741,  0.1686,  0.1035,  0.1262,  0.1140, -0.0468, -0.0871,  0.2417,\n",
       "                      -0.0598, -0.0865, -0.1537, -0.2552,  0.2099, -0.0169,  0.0661,  0.2347,\n",
       "                      -0.0641, -0.0152,  0.2324,  0.0194,  0.0683, -0.0514,  0.1237,  0.2907,\n",
       "                       0.2235, -0.2236,  0.0864, -0.2718,  0.2012,  0.0853,  0.1854,  0.0507,\n",
       "                       0.0102, -0.0732, -0.2183,  0.0111, -0.0056, -0.1922, -0.0057, -0.0370,\n",
       "                       0.0118,  0.2918, -0.0335, -0.0664, -0.0332, -0.3282,  0.1701, -0.2283,\n",
       "                      -0.1130, -0.1418, -0.1093,  0.3727,  0.1265,  0.0444, -0.0042,  0.2247,\n",
       "                       0.1439, -0.0410, -0.1130, -0.0100,  0.0364,  0.1996, -0.3053,  0.0440])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[-0.0567, -0.3867, -0.0986,  ...,  0.0252,  0.0342,  0.2731],\n",
       "                      [-0.2517, -0.2838, -0.0251,  ...,  0.1189, -0.2412,  0.1590],\n",
       "                      [ 0.0863,  0.0638, -0.0719,  ..., -0.0923, -0.0264, -0.0134],\n",
       "                      ...,\n",
       "                      [-0.0433, -0.1842,  0.0073,  ..., -0.1529,  0.0497, -0.0689],\n",
       "                      [-0.2304, -0.0515,  0.1771,  ...,  0.2023, -0.0645,  0.0375],\n",
       "                      [ 0.0056, -0.0262, -0.1104,  ..., -0.1084,  0.3735,  0.2032]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([-0.0699, -0.2606,  0.0816, -0.0230,  0.1732, -0.1813, -0.0527, -0.0536,\n",
       "                      -0.1609, -0.0210, -0.0638, -0.1135,  0.0729,  0.1410, -0.0130,  0.0165,\n",
       "                      -0.0994, -0.3573,  0.0613,  0.2802,  0.0795,  0.0231,  0.3410,  0.1445,\n",
       "                       0.2257, -0.1495, -0.1185,  0.2847,  0.2947, -0.0497,  0.0043,  0.2936,\n",
       "                       0.0279,  0.0928, -0.1009, -0.2739, -0.3208, -0.2340, -0.0661, -0.2385,\n",
       "                       0.1676,  0.0143,  0.1433, -0.1278,  0.1914,  0.3149,  0.1093, -0.0665,\n",
       "                      -0.0656,  0.0969, -0.1186, -0.0382, -0.1286,  0.0575,  0.0377, -0.1773,\n",
       "                      -0.2214, -0.1322,  0.0095,  0.0013,  0.1618, -0.1224, -0.1142,  0.0202])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[ 0.1526,  0.2082, -0.1851,  ..., -0.0820, -0.2823,  0.1668],\n",
       "                      [ 0.3338,  0.2200, -0.3379,  ..., -0.2973, -0.0266,  0.3153],\n",
       "                      [ 0.2597, -0.3816,  0.0890,  ..., -0.0901, -0.0551, -0.0862],\n",
       "                      ...,\n",
       "                      [ 0.0360, -0.0792, -0.2975,  ...,  0.0419,  0.0327,  0.2073],\n",
       "                      [ 0.1466, -0.1567, -0.1348,  ..., -0.0270, -0.1203,  0.0237],\n",
       "                      [ 0.3553, -0.1592,  0.0281,  ...,  0.3252,  0.1102,  0.2248]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.6767, -0.1159, -0.0954,  ..., -0.0915, -0.0401, -0.1089]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
