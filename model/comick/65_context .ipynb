{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc63c4-da08-43a8-9972-2615d377d1fa",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313363b9-24d1-4b87-87cd-a0b7dd5fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pytz\n",
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from polyglot.mapping import Embedding, CaseExpander, DigitExpander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b49405-006a-4a45-ae79-0e22f6a7172e",
   "metadata": {},
   "source": [
    "# Hyperparameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68709d29-24a1-45bf-b549-804952fd1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "     def __init__(\n",
    "        self,\n",
    "        context_size=65,\n",
    "        input_size_left_context=64,\n",
    "        input_size_oov_context=20,\n",
    "        input_size_right_context=64,\n",
    "        batch_size=32,\n",
    "        num_hidden_layer=1,\n",
    "        hidden_size=128,\n",
    "        output_size=3611,\n",
    "        shuffle=True,\n",
    "        lr=0.001,\n",
    "        batch_first=True,\n",
    "        bidirectional=True,\n",
    "        init_wb_with_kaiming_normal=True,\n",
    "        n_epoch=20,\n",
    "        patience=20,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lr = lr\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.init_wb_with_kaiming_normal = init_wb_with_kaiming_normal\n",
    "        self.n_epoch = n_epoch\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        \n",
    "hyperparams = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f750b1-d4a7-4a2a-9fe1-369a61f28a2b",
   "metadata": {},
   "source": [
    "# Prepare Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241f20b-fb34-4db2-b887-1687f137b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left context shape: (16562, 62)\n",
      "OOV context shape: (16562, 28)\n",
      "Right context shape: (16562, 63)\n",
      "Actual lable shape: (16562,)\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_idxs(docs, dict_vocabs):\n",
    "    doc_to_idx = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_to_idx.append([dict_vocabs[token] for token in doc])\n",
    "        \n",
    "    return np.array(doc_to_idx)\n",
    "\n",
    "# Left context\n",
    "left_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_with_pad.pkl\", \"rb\")\n",
    "left_context = pickle.load(left_context)\n",
    "left_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_left_context.pkl\", \"rb\")\n",
    "left_context_to_idx = pickle.load(left_context_to_idx)\n",
    "doc_left_context_to_idx = convert_doc_to_idxs(left_context, left_context_to_idx)\n",
    "\n",
    "# OOV context\n",
    "oov_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/oov_context_with_pad.pkl\", \"rb\")\n",
    "oov_context = pickle.load(oov_context)\n",
    "oov_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_oov_context.pkl\", \"rb\")\n",
    "oov_context_to_idx = pickle.load(oov_context_to_idx)\n",
    "doc_oov_context_to_idx = convert_doc_to_idxs(oov_context, oov_context_to_idx)\n",
    "\n",
    "# Right context\n",
    "right_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_with_pad.pkl\", \"rb\")\n",
    "right_context = pickle.load(right_context)\n",
    "right_context_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/token2idx_right_context.pkl\", \"rb\")\n",
    "right_context_to_idx = pickle.load(right_context_to_idx)\n",
    "doc_right_context_to_idx = convert_doc_to_idxs(right_context, right_context_to_idx)\n",
    "\n",
    "# Actual labels\n",
    "labels_context = open(f\"../../datasets/features/{hyperparams.context_size}_context/lables.pkl\", \"rb\")\n",
    "labels_context = pickle.load(labels_context)\n",
    "labels_to_idx = open(f\"../../datasets/features/{hyperparams.context_size}_context/lable_vocabs.pkl\", \"rb\")\n",
    "labels_to_idx = pickle.load(labels_to_idx)\n",
    "doc_labels_to_idx = convert_doc_to_idxs(labels_context, labels_to_idx).flatten()\n",
    "\n",
    "print(f\"Left context shape: {doc_left_context_to_idx.shape}\")\n",
    "print(f\"OOV context shape: {doc_oov_context_to_idx.shape}\")\n",
    "print(f\"Right context shape: {doc_right_context_to_idx.shape}\")\n",
    "print(f\"Actual lable shape: {doc_labels_to_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe386c1f-3e1d-4589-b18e-42e4bc4055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "left_contexts = torch.LongTensor(doc_left_context_to_idx)\n",
    "oov_contexts = torch.LongTensor(doc_oov_context_to_idx)\n",
    "right_contexts = torch.LongTensor(doc_right_context_to_idx)\n",
    "actual_labels = torch.LongTensor(doc_labels_to_idx)\n",
    "dataset = TensorDataset(left_contexts, oov_contexts, right_contexts, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8806cc8-c1d1-450a-833d-f25375c52949",
   "metadata": {},
   "source": [
    "# Char and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86595b81-a95e-4469-acad-49b9ed2c9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "word_embeddings = Embedding.load(\"../../word_embeddings/polyglot/idn_embeddings.tar.bz2\")\n",
    "word_embeddings.apply_expansion(DigitExpander)\n",
    "word_embeddings.apply_expansion(CaseExpander)\n",
    "\n",
    "left_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/left_context_vocabs.pkl\", \"rb\")\n",
    "left_vocabs = pickle.load(left_vocabs)\n",
    "\n",
    "right_vocabs = open(f\"../../datasets/features/{hyperparams.context_size}_context/right_context_vocabs.pkl\", \"rb\")\n",
    "right_vocabs = pickle.load(right_vocabs)\n",
    "\n",
    "left_word_embedding_dict = {left_context_to_idx[vocab] : word_embeddings[vocab] for vocab in left_vocabs}\n",
    "right_word_embedding_dict = {right_context_to_idx[vocab] : word_embeddings[vocab] for vocab in right_vocabs}\n",
    "\n",
    "# Char Embedding\n",
    "char_embedding_dict = open(\"../../word_embeddings/chars_embedding/char_embeddings.pkl\", \"rb\")\n",
    "char_embedding_dict = pickle.load(char_embedding_dict)\n",
    "\n",
    "# Context embedding\n",
    "left_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(left_word_embedding_dict.values()))), padding_idx=left_vocabs.index(\"<PAD>\"), freeze=True)\n",
    "oov_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(char_embedding_dict.values()))), padding_idx=list(char_embedding_dict.keys()).index(\"PAD\"), freeze=True)\n",
    "right_context_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(np.array(list(right_word_embedding_dict.values()))), padding_idx=right_vocabs.index(\"<PAD>\"), freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ddcfe-1525-41ae-bf02-c9ebf3d957aa",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998fb38d-da64-4b4c-9371-75e06ebfc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=hyperparams.batch_size, shuffle=hyperparams.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5be550-2f41-41b4-9df4-ac67744041c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed852446-c649-4727-b9ff-64375cf7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comick(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_left_context=hyperparams.input_size_left_context,\n",
    "        input_size_oov_context=hyperparams.input_size_oov_context,\n",
    "        input_size_right_context=hyperparams.input_size_right_context,\n",
    "        hidden_size=hyperparams.hidden_size,\n",
    "        num_layers=hyperparams.num_hidden_layer,\n",
    "        output_size=hyperparams.output_size,\n",
    "        batch_first=hyperparams.batch_first,\n",
    "        bidirectional=hyperparams.bidirectional,\n",
    "        init_wb_with_kaiming_normal=hyperparams.init_wb_with_kaiming_normal\n",
    "    ):\n",
    "        super(Comick, self).__init__()\n",
    "        \n",
    "        self.input_size_left_context = input_size_left_context\n",
    "        self.input_size_oov_context = input_size_oov_context\n",
    "        self.input_size_right_context = input_size_right_context\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.bilstm_left_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_left_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_oov_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_oov_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.bilstm_right_context_feature = nn.LSTM(\n",
    "            input_size = self.input_size_right_context,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = self.batch_first,\n",
    "            bidirectional = self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.oov_embedding = nn.Linear(in_features=3 * 64, out_features=64)\n",
    "        \n",
    "        self.embedding = np.empty((output_size, 64), dtype=np.float32)\n",
    "        \n",
    "        self.prob = nn.Sequential(\n",
    "            nn.Linear(64, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "                \n",
    "        if init_wb_with_kaiming_normal:\n",
    "            self.init_wb()\n",
    "            \n",
    "    def init_wb(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LSTM)):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        nn.init.kaiming_normal_(param)\n",
    "                    else:\n",
    "                        nn.init.kaiming_normal_(param.reshape(1, -1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_left_context,\n",
    "        input_oov_context,\n",
    "        input_right_context,\n",
    "        idxs_target,\n",
    "        hidden_left_context=None,\n",
    "        hidden_oov_context=None,\n",
    "        hidden_right_context=None,\n",
    "    ):\n",
    "        # BiLSTM left, oov, and right context\n",
    "        output_left_context, (hidden_left_context, memory_left_context) = self.bilstm_left_context_feature(input_left_context, hidden_left_context)\n",
    "        output_oov_context, (hidden_oov_context, memory_oov_context) = self.bilstm_oov_context_feature(input_oov_context, hidden_oov_context)\n",
    "        output_right_context, (hidden_right_context, memory_right_context) = self.bilstm_right_context_feature(input_right_context, hidden_right_context)\n",
    "                \n",
    "        # Concate hidden (forward and backward hidden BiLSTM)\n",
    "        hidden_left_bidirectional = torch.cat((hidden_left_context[0], hidden_left_context[-1]), dim=1)\n",
    "        hidden_oov_bidirectional = torch.cat((hidden_oov_context[0], hidden_oov_context[-1]), dim=1)\n",
    "        hidden_right_bidirectional = torch.cat((hidden_right_context[0], hidden_right_context[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected\n",
    "        output_left_fc = self.fc(hidden_left_bidirectional)\n",
    "        output_oov_fc = self.fc(hidden_oov_bidirectional)\n",
    "        output_right_fc = self.fc(hidden_right_bidirectional)\n",
    "        \n",
    "        # Concate output left, oov, and right context feature\n",
    "        output = torch.cat((output_left_fc, output_oov_fc, output_right_fc), dim=1)\n",
    "        \n",
    "        # OOV embedding\n",
    "        output = self.oov_embedding(output)\n",
    "                \n",
    "        # save OOV embedding\n",
    "        self.embedding[idxs_target.tolist()] = output.cpu().detach().numpy()\n",
    "        \n",
    "        # Projection OOV embedding\n",
    "        prob = self.prob(output)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb40b-9062-47f1-9703-65e80d6cb07d",
   "metadata": {},
   "source": [
    "# Model, Optimizer, Criterion, Metric, and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d05fc-0bb5-426e-aa7b-eb930ba9f06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comick(\n",
       "  (bilstm_left_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_oov_context_feature): LSTM(20, 128, batch_first=True, bidirectional=True)\n",
       "  (bilstm_right_context_feature): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (oov_embedding): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (prob): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=3611, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Comick().to(hyperparams.device)\n",
    "model.prob[0].requires_grad_ = False # disable gradient for projection layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
    "criterion = nn.NLLLoss(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "metric = F1Score(ignore_index=list(char_embedding_dict.keys()).index(\"PAD\")).to(hyperparams.device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d463d75-dd43-4b45-9eec-8f4aca21b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814,427\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([param.numel() for param in model.parameters() if param.requires_grad_]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494a567d-acbc-46c2-adc8-b9a512366c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0953427e-06,  4.5696343e-41,  1.0953427e-06, ...,\n",
       "         0.0000000e+00, -3.9923045e-17,  4.5694942e-41],\n",
       "       [-1.5472306e-16,  4.5694942e-41,  1.1170030e-40, ...,\n",
       "         4.5694942e-41,  1.1171432e-40,  0.0000000e+00],\n",
       "       [-3.9924210e-17,  4.5694942e-41, -1.5473153e-16, ...,\n",
       "         4.5694942e-41, -1.5473916e-16,  4.5694942e-41],\n",
       "       ...,\n",
       "       [-5.6046516e-18,  4.5694942e-41, -1.5581489e-16, ...,\n",
       "         4.5694942e-41, -1.5582251e-16,  4.5694942e-41],\n",
       "       [ 1.1346174e-40,  0.0000000e+00, -5.6047972e-18, ...,\n",
       "         0.0000000e+00, -5.6049295e-18,  4.5694942e-41],\n",
       "       [-1.5583183e-16,  4.5694942e-41,  1.1347715e-40, ...,\n",
       "         4.5694942e-41,  1.1349116e-40,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39ba3-b103-44da-ae1e-34fd12ad2d2f",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c51a3a6-a4af-425f-aa1f-34ffe9ff8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, criterion, metric, scheduler=None, path_name=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    batch_metric_scores = []\n",
    "    \n",
    "    for batch, (input_left_context, input_oov_context, input_right_context, actual_label) in enumerate(tqdm(dataloader), 1):\n",
    "        # Forward Propagation\n",
    "        prob = model(\n",
    "            left_context_embedding(input_left_context).to(hyperparams.device),\n",
    "            oov_context_embedding(input_oov_context).to(hyperparams.device),\n",
    "            right_context_embedding(input_right_context).to(hyperparams.device),\n",
    "            actual_label.to(hyperparams.device)\n",
    "        )\n",
    "                \n",
    "        loss = criterion(prob, actual_label.to(hyperparams.device))\n",
    "        metric_score = metric(prob.argmax(dim=1), actual_label.to(hyperparams.device))\n",
    "        metric_score = metric.compute()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        batch_metric_scores.append(metric_score)\n",
    "        \n",
    "        if batch % 50 == 0 or batch == len(dataloader):\n",
    "            batch_name = \"Batch-\" + str(batch)\n",
    "            print(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\")\n",
    "            with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "                f.write(f\"{batch_name.ljust(9)}: {str(criterion).split('(')[0]}={(loss.item()):.4f} | {str(metric).split('(')[0]}={(metric_score):.4f}\\n\")\n",
    "\n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return batch_losses, batch_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3043a1-39cb-4b38-8024-54f909a565a9",
   "metadata": {},
   "source": [
    "# Looping Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e139244-b036-4d45-beca-1de5bd7b5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d0586da492407590dddf0a00d5ede4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=6.1076 | F1Score=0.2700\n",
      "Batch-100: NLLLoss=4.8941 | F1Score=0.2944\n",
      "Batch-150: NLLLoss=4.5386 | F1Score=0.3088\n",
      "Batch-200: NLLLoss=4.8116 | F1Score=0.3375\n",
      "Batch-250: NLLLoss=4.5884 | F1Score=0.3567\n",
      "Batch-300: NLLLoss=4.1050 | F1Score=0.3744\n",
      "Batch-350: NLLLoss=4.6339 | F1Score=0.3897\n",
      "Batch-400: NLLLoss=3.8761 | F1Score=0.4051\n",
      "Batch-450: NLLLoss=4.0983 | F1Score=0.4165\n",
      "Batch-500: NLLLoss=3.8109 | F1Score=0.4310\n",
      "Batch-518: NLLLoss=3.0280 | F1Score=0.4360\n",
      "\n",
      "Mean NLLLoss: 4.5275 | Mean F1Score: 0.3509\n",
      "==================================================\n",
      "\n",
      "EPOCH-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3ca8e0f3444c82855e594a3466d87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=3.3666 | F1Score=0.5813\n",
      "Batch-100: NLLLoss=3.2216 | F1Score=0.5903\n",
      "Batch-150: NLLLoss=1.7423 | F1Score=0.5896\n",
      "Batch-200: NLLLoss=3.5033 | F1Score=0.6033\n",
      "Batch-250: NLLLoss=2.4334 | F1Score=0.6048\n",
      "Batch-300: NLLLoss=2.5413 | F1Score=0.6106\n",
      "Batch-350: NLLLoss=2.8187 | F1Score=0.6164\n",
      "Batch-400: NLLLoss=3.0958 | F1Score=0.6233\n",
      "Batch-450: NLLLoss=2.7586 | F1Score=0.6293\n",
      "Batch-500: NLLLoss=2.6579 | F1Score=0.6344\n",
      "Batch-518: NLLLoss=1.3555 | F1Score=0.6368\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 2.7246 | Mean F1Score: 0.6096\n",
      "==================================================\n",
      "\n",
      "EPOCH-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce555ca586140c384cee0a916d05c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=2.1034 | F1Score=0.7294\n",
      "Batch-100: NLLLoss=1.9588 | F1Score=0.7404\n",
      "Batch-150: NLLLoss=1.8145 | F1Score=0.7342\n",
      "Batch-200: NLLLoss=2.0065 | F1Score=0.7319\n",
      "Batch-250: NLLLoss=2.4355 | F1Score=0.7327\n",
      "Batch-300: NLLLoss=1.2409 | F1Score=0.7339\n",
      "Batch-350: NLLLoss=1.0709 | F1Score=0.7394\n",
      "Batch-400: NLLLoss=2.3279 | F1Score=0.7394\n",
      "Batch-450: NLLLoss=1.1376 | F1Score=0.7421\n",
      "Batch-500: NLLLoss=1.8864 | F1Score=0.7429\n",
      "Batch-518: NLLLoss=1.3983 | F1Score=0.7439\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 1.7986 | Mean F1Score: 0.7378\n",
      "==================================================\n",
      "\n",
      "EPOCH-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1407e1391f44aca107e1bba3a80be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=1.2491 | F1Score=0.8256\n",
      "Batch-100: NLLLoss=1.1199 | F1Score=0.8176\n",
      "Batch-150: NLLLoss=1.0702 | F1Score=0.8152\n",
      "Batch-200: NLLLoss=1.0866 | F1Score=0.8168\n",
      "Batch-250: NLLLoss=0.8033 | F1Score=0.8149\n",
      "Batch-300: NLLLoss=1.2970 | F1Score=0.8150\n",
      "Batch-350: NLLLoss=1.5853 | F1Score=0.8161\n",
      "Batch-400: NLLLoss=1.3399 | F1Score=0.8165\n",
      "Batch-450: NLLLoss=2.1013 | F1Score=0.8164\n",
      "Batch-500: NLLLoss=1.6067 | F1Score=0.8168\n",
      "Batch-518: NLLLoss=1.0995 | F1Score=0.8172\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 1.1725 | Mean F1Score: 0.8178\n",
      "==================================================\n",
      "\n",
      "EPOCH-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd5b13900414cdcbd71a3da282df4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.5284 | F1Score=0.9059\n",
      "Batch-100: NLLLoss=0.7162 | F1Score=0.8858\n",
      "Batch-150: NLLLoss=0.7081 | F1Score=0.8778\n",
      "Batch-200: NLLLoss=0.8127 | F1Score=0.8741\n",
      "Batch-250: NLLLoss=0.8391 | F1Score=0.8738\n",
      "Batch-300: NLLLoss=0.6320 | F1Score=0.8728\n",
      "Batch-350: NLLLoss=0.6851 | F1Score=0.8724\n",
      "Batch-400: NLLLoss=0.9056 | F1Score=0.8721\n",
      "Batch-450: NLLLoss=0.6368 | F1Score=0.8730\n",
      "Batch-500: NLLLoss=0.5950 | F1Score=0.8721\n",
      "Batch-518: NLLLoss=1.2556 | F1Score=0.8722\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.7067 | Mean F1Score: 0.8789\n",
      "==================================================\n",
      "\n",
      "EPOCH-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5866db3e7a49918ab68ab27398892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.2107 | F1Score=0.9415\n",
      "Batch-100: NLLLoss=0.2875 | F1Score=0.9461\n",
      "Batch-150: NLLLoss=0.2298 | F1Score=0.9486\n",
      "Batch-200: NLLLoss=0.1406 | F1Score=0.9473\n",
      "Batch-250: NLLLoss=0.4442 | F1Score=0.9439\n",
      "Batch-300: NLLLoss=0.4598 | F1Score=0.9426\n",
      "Batch-350: NLLLoss=0.5337 | F1Score=0.9383\n",
      "Batch-400: NLLLoss=0.3132 | F1Score=0.9364\n",
      "Batch-450: NLLLoss=0.4146 | F1Score=0.9351\n",
      "Batch-500: NLLLoss=0.4400 | F1Score=0.9340\n",
      "Batch-518: NLLLoss=0.2792 | F1Score=0.9337\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.3538 | Mean F1Score: 0.9417\n",
      "==================================================\n",
      "\n",
      "EPOCH-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647ebfefc0ab4946995f468650818805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.1445 | F1Score=0.9878\n",
      "Batch-100: NLLLoss=0.0980 | F1Score=0.9886\n",
      "Batch-150: NLLLoss=0.1426 | F1Score=0.9897\n",
      "Batch-200: NLLLoss=0.1812 | F1Score=0.9895\n",
      "Batch-250: NLLLoss=0.0993 | F1Score=0.9889\n",
      "Batch-300: NLLLoss=0.1124 | F1Score=0.9889\n",
      "Batch-350: NLLLoss=0.1030 | F1Score=0.9879\n",
      "Batch-400: NLLLoss=0.0482 | F1Score=0.9876\n",
      "Batch-450: NLLLoss=0.2992 | F1Score=0.9869\n",
      "Batch-500: NLLLoss=0.1268 | F1Score=0.9859\n",
      "Batch-518: NLLLoss=0.1334 | F1Score=0.9855\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.1231 | Mean F1Score: 0.9880\n",
      "==================================================\n",
      "\n",
      "EPOCH-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6b0dd8ae754d19aed3039c7ce3a377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0276 | F1Score=0.9981\n",
      "Batch-100: NLLLoss=0.0264 | F1Score=0.9983\n",
      "Batch-150: NLLLoss=0.0242 | F1Score=0.9984\n",
      "Batch-200: NLLLoss=0.0324 | F1Score=0.9979\n",
      "Batch-250: NLLLoss=0.0179 | F1Score=0.9982\n",
      "Batch-300: NLLLoss=0.0358 | F1Score=0.9983\n",
      "Batch-350: NLLLoss=0.0215 | F1Score=0.9982\n",
      "Batch-400: NLLLoss=0.0237 | F1Score=0.9982\n",
      "Batch-450: NLLLoss=0.0248 | F1Score=0.9983\n",
      "Batch-500: NLLLoss=0.0213 | F1Score=0.9983\n",
      "Batch-518: NLLLoss=0.0172 | F1Score=0.9983\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0347 | Mean F1Score: 0.9981\n",
      "==================================================\n",
      "\n",
      "EPOCH-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55fa5e9b539475abf599d21434275da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0135 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0092 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0084 | F1Score=0.9994\n",
      "Batch-200: NLLLoss=0.0066 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0065 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0099 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0101 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0091 | F1Score=0.9995\n",
      "Batch-450: NLLLoss=0.0025 | F1Score=0.9995\n",
      "Batch-500: NLLLoss=0.0110 | F1Score=0.9995\n",
      "Batch-518: NLLLoss=0.0100 | F1Score=0.9995\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0118 | Mean F1Score: 0.9993\n",
      "==================================================\n",
      "\n",
      "EPOCH-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8663f47d64fd09f100639ac537785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0055 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0027 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0044 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0079 | F1Score=0.9997\n",
      "Batch-250: NLLLoss=0.0035 | F1Score=0.9998\n",
      "Batch-300: NLLLoss=0.0043 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0039 | F1Score=0.9996\n",
      "Batch-400: NLLLoss=0.0049 | F1Score=0.9996\n",
      "Batch-450: NLLLoss=0.0056 | F1Score=0.9996\n",
      "Batch-500: NLLLoss=0.0066 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0035 | F1Score=0.9996\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0074 | Mean F1Score: 0.9995\n",
      "==================================================\n",
      "\n",
      "EPOCH-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3181f1fe306c4208bc5d7957d2c9ee25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0047 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0019 | F1Score=0.9987\n",
      "Batch-150: NLLLoss=0.1295 | F1Score=0.9975\n",
      "Batch-200: NLLLoss=0.1854 | F1Score=0.9930\n",
      "Batch-250: NLLLoss=0.1770 | F1Score=0.9844\n",
      "Batch-300: NLLLoss=0.1870 | F1Score=0.9794\n",
      "Batch-350: NLLLoss=0.5205 | F1Score=0.9747\n",
      "Batch-400: NLLLoss=0.0427 | F1Score=0.9730\n",
      "Batch-450: NLLLoss=0.2306 | F1Score=0.9716\n",
      "Batch-500: NLLLoss=0.0850 | F1Score=0.9717\n",
      "Batch-518: NLLLoss=0.0828 | F1Score=0.9719\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.1250 | Mean F1Score: 0.9852\n",
      "Patience = 1/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a7dabe8f504a95941415c327f28b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0324 | F1Score=0.9922\n",
      "Batch-100: NLLLoss=0.0537 | F1Score=0.9933\n",
      "Batch-150: NLLLoss=0.0106 | F1Score=0.9937\n",
      "Batch-200: NLLLoss=0.0107 | F1Score=0.9939\n",
      "Batch-250: NLLLoss=0.0854 | F1Score=0.9939\n",
      "Batch-300: NLLLoss=0.0322 | F1Score=0.9942\n",
      "Batch-350: NLLLoss=0.1441 | F1Score=0.9941\n",
      "Batch-400: NLLLoss=0.0252 | F1Score=0.9941\n",
      "Batch-450: NLLLoss=0.0055 | F1Score=0.9944\n",
      "Batch-500: NLLLoss=0.0026 | F1Score=0.9947\n",
      "Batch-518: NLLLoss=0.0100 | F1Score=0.9947\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0395 | Mean F1Score: 0.9933\n",
      "==================================================\n",
      "\n",
      "EPOCH-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35983446f4b43feb22005519c994b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0040 | F1Score=0.9987\n",
      "Batch-100: NLLLoss=0.0036 | F1Score=0.9987\n",
      "Batch-150: NLLLoss=0.0028 | F1Score=0.9992\n",
      "Batch-200: NLLLoss=0.0093 | F1Score=0.9986\n",
      "Batch-250: NLLLoss=0.0028 | F1Score=0.9989\n",
      "Batch-300: NLLLoss=0.0022 | F1Score=0.9990\n",
      "Batch-350: NLLLoss=0.0287 | F1Score=0.9985\n",
      "Batch-400: NLLLoss=0.0082 | F1Score=0.9982\n",
      "Batch-450: NLLLoss=0.0090 | F1Score=0.9981\n",
      "Batch-500: NLLLoss=0.0060 | F1Score=0.9982\n",
      "Batch-518: NLLLoss=0.0170 | F1Score=0.9983\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0114 | Mean F1Score: 0.9986\n",
      "==================================================\n",
      "\n",
      "EPOCH-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a7b395a35c4b8583bb0628e21699b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0014 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0019 | F1Score=0.9989\n",
      "Batch-150: NLLLoss=0.0163 | F1Score=0.9991\n",
      "Batch-200: NLLLoss=0.0037 | F1Score=0.9993\n",
      "Batch-250: NLLLoss=0.0016 | F1Score=0.9994\n",
      "Batch-300: NLLLoss=0.0031 | F1Score=0.9991\n",
      "Batch-350: NLLLoss=0.0011 | F1Score=0.9992\n",
      "Batch-400: NLLLoss=0.0024 | F1Score=0.9992\n",
      "Batch-450: NLLLoss=0.0024 | F1Score=0.9992\n",
      "Batch-500: NLLLoss=0.0024 | F1Score=0.9993\n",
      "Batch-518: NLLLoss=0.0046 | F1Score=0.9993\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0058 | Mean F1Score: 0.9992\n",
      "==================================================\n",
      "\n",
      "EPOCH-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ea66ce89be47ceb62e3429770dc815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0018 | F1Score=0.9994\n",
      "Batch-100: NLLLoss=0.0018 | F1Score=0.9997\n",
      "Batch-150: NLLLoss=0.0002 | F1Score=0.9996\n",
      "Batch-200: NLLLoss=0.0009 | F1Score=0.9995\n",
      "Batch-250: NLLLoss=0.0012 | F1Score=0.9996\n",
      "Batch-300: NLLLoss=0.0012 | F1Score=0.9996\n",
      "Batch-350: NLLLoss=0.0006 | F1Score=0.9997\n",
      "Batch-400: NLLLoss=0.0016 | F1Score=0.9996\n",
      "Batch-450: NLLLoss=0.0010 | F1Score=0.9996\n",
      "Batch-500: NLLLoss=0.0018 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0005 | F1Score=0.9996\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0028 | Mean F1Score: 0.9995\n",
      "==================================================\n",
      "\n",
      "EPOCH-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25af5f2fb63c458a9285bae9662d1f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0006 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0014 | F1Score=0.9994\n",
      "Batch-150: NLLLoss=0.0010 | F1Score=0.9995\n",
      "Batch-200: NLLLoss=0.0009 | F1Score=0.9996\n",
      "Batch-250: NLLLoss=0.0015 | F1Score=0.9997\n",
      "Batch-300: NLLLoss=0.0010 | F1Score=0.9997\n",
      "Batch-350: NLLLoss=0.0004 | F1Score=0.9998\n",
      "Batch-400: NLLLoss=0.0018 | F1Score=0.9997\n",
      "Batch-450: NLLLoss=0.0037 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0012 | F1Score=0.9998\n",
      "Batch-518: NLLLoss=0.0009 | F1Score=0.9998\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0028 | Mean F1Score: 0.9997\n",
      "Patience = 2/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a09a97d2834618b829dfa33c323b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0008 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0010 | F1Score=1.0000\n",
      "Batch-150: NLLLoss=0.0004 | F1Score=1.0000\n",
      "Batch-200: NLLLoss=0.0044 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0008 | F1Score=0.9998\n",
      "Batch-350: NLLLoss=0.0010 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-450: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-500: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-518: NLLLoss=0.0002 | F1Score=0.9998\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0016 | Mean F1Score: 0.9999\n",
      "==================================================\n",
      "\n",
      "EPOCH-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4750dc452dfb4fc5b335492654e20c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0005 | F1Score=1.0000\n",
      "Batch-100: NLLLoss=0.0005 | F1Score=0.9998\n",
      "Batch-150: NLLLoss=0.0006 | F1Score=0.9999\n",
      "Batch-200: NLLLoss=0.0005 | F1Score=0.9998\n",
      "Batch-250: NLLLoss=0.0005 | F1Score=0.9999\n",
      "Batch-300: NLLLoss=0.0007 | F1Score=0.9999\n",
      "Batch-350: NLLLoss=0.0009 | F1Score=0.9999\n",
      "Batch-400: NLLLoss=0.0013 | F1Score=0.9998\n",
      "Batch-450: NLLLoss=0.0016 | F1Score=0.9998\n",
      "Batch-500: NLLLoss=0.0191 | F1Score=0.9996\n",
      "Batch-518: NLLLoss=0.0136 | F1Score=0.9995\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.0045 | Mean F1Score: 0.9999\n",
      "Patience = 3/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83fdd8d49b1431c90a595c23b8c57d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0858 | F1Score=0.9937\n",
      "Batch-100: NLLLoss=0.2885 | F1Score=0.9806\n",
      "Batch-150: NLLLoss=0.1943 | F1Score=0.9563\n",
      "Batch-200: NLLLoss=0.1169 | F1Score=0.9438\n",
      "Batch-250: NLLLoss=0.5961 | F1Score=0.9392\n",
      "Batch-300: NLLLoss=0.1467 | F1Score=0.9401\n",
      "Batch-350: NLLLoss=0.1566 | F1Score=0.9416\n",
      "Batch-400: NLLLoss=0.2437 | F1Score=0.9437\n",
      "Batch-450: NLLLoss=0.1068 | F1Score=0.9465\n",
      "Batch-500: NLLLoss=0.0317 | F1Score=0.9488\n",
      "Batch-518: NLLLoss=0.2553 | F1Score=0.9496\n",
      "\n",
      "Huft 😥! Model not improved.\n",
      "Mean NLLLoss: 0.1928 | Mean F1Score: 0.9553\n",
      "Patience = 4/20❗\n",
      "==================================================\n",
      "\n",
      "EPOCH-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6090f24cb1244ba2afe280c868be5532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-50 : NLLLoss=0.0285 | F1Score=0.9925\n",
      "Batch-100: NLLLoss=0.0046 | F1Score=0.9944\n",
      "Batch-150: NLLLoss=0.0092 | F1Score=0.9948\n",
      "Batch-200: NLLLoss=0.0882 | F1Score=0.9950\n",
      "Batch-250: NLLLoss=0.0115 | F1Score=0.9954\n",
      "Batch-300: NLLLoss=0.0133 | F1Score=0.9957\n",
      "Batch-350: NLLLoss=0.0028 | F1Score=0.9961\n",
      "Batch-400: NLLLoss=0.0019 | F1Score=0.9963\n",
      "Batch-450: NLLLoss=0.0150 | F1Score=0.9964\n",
      "Batch-500: NLLLoss=0.0076 | F1Score=0.9965\n",
      "Batch-518: NLLLoss=0.0018 | F1Score=0.9965\n",
      "\n",
      "Yeah 🎉😄! Model improved.\n",
      "Mean NLLLoss: 0.0183 | Mean F1Score: 0.9950\n",
      "==================================================\n",
      "\n",
      "TRAINING SUMMARY\n",
      "Best NLLLoss      : 0.0183\n",
      "Best F1Score      : 0.9950\n",
      "Training duration : 30.621 minutes.\n",
      "Training date     : 2022-10-11 17:31:59.987034+08:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def looping_step(dataloader, model, optimizer, criterion, metric, n_epoch=hyperparams.n_epoch, patience=hyperparams.patience, monitor=\"loss\"):    \n",
    "    start_time = time()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_metric_scores = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    now = datetime.now(pytz.timezone(\"Asia/Ujung_Pandang\"))\n",
    "    path_name = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(f\"HYPERPARAMETERS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        for name, value in vars(hyperparams).items():\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nTRAINING PROGRESS\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "    \n",
    "    # Training Progress\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(f\"EPOCH-{epoch}\")\n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH-{epoch}\\n\")\n",
    "        \n",
    "        batch_losses, batch_metric_scores = training_step(dataloader, model, optimizer, criterion, metric, path_name=path_name)\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "\n",
    "        epoch_loss = torch.mean(torch.FloatTensor(batch_losses))\n",
    "        epoch_losses.append(epoch_loss.item())\n",
    "\n",
    "        epoch_metric_score = torch.mean(torch.FloatTensor(batch_metric_scores))\n",
    "        epoch_metric_scores.append(epoch_metric_score.item())\n",
    "        \n",
    "        with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "            if monitor == \"loss\":\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_losses[-1] < epoch_losses[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")                    \n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "            else:\n",
    "                if epoch == 1:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(f\"\\nMean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                elif epoch_metric_scores[-1] > epoch_metric_scores[-2]:\n",
    "                    best_state_dict = model.state_dict()\n",
    "                    best_loss = (epoch_loss)\n",
    "                    best_metric = (epoch_metric_score)\n",
    "                    \n",
    "                    print(\"\\nYeah 🎉😄! Model improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\")\n",
    "                    \n",
    "                    f.write(\"\\nYeah 🎉😄! Model improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                    print(\"\\nHuft 😥! Model not improved.\")\n",
    "                    print(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    print(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "                    f.write(\"\\nHuft 😥! Model not improved.\\n\")\n",
    "                    f.write(f\"Mean {str(criterion).split('(')[0]}: {(epoch_loss):.4f} | Mean {str(metric).split('(')[0]}: {(epoch_metric_score):.4f}\\n\")\n",
    "                    f.write(f\"Patience = {patience_counter}/{patience}❗\\n\")\n",
    "                    \n",
    "            print(\"=\" * 50, end=\"\\n\\n\")\n",
    "            f.write(f\"{'=' * 50}\\n\\n\")\n",
    "            \n",
    "            if patience_counter == patience:\n",
    "                print(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                f.write(f\"Early stopping, patience = {patience_counter}/{patience}❗\\n\")\n",
    "                break\n",
    "        \n",
    "        metric.reset()\n",
    "    \n",
    "    finish_time = time()\n",
    "    \n",
    "    # Training plot \n",
    "    fig, (ax_loss, ax_metric_score) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Training with context size = {hyperparams.context_size}\")\n",
    "\n",
    "    ax_loss.set_title(\"Loss\")\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Score\")\n",
    "    ax_loss.plot(epoch_losses, \"green\")\n",
    "    ax_loss.grid()\n",
    "\n",
    "    ax_metric_score.set_title(\"F1 Score\")\n",
    "    ax_metric_score.set_xlabel(\"Epoch\")\n",
    "    ax_metric_score.set_ylabel(\"Score\")\n",
    "    ax_metric_score.plot(epoch_metric_scores, \"orange\")\n",
    "    ax_metric_score.grid()\n",
    "\n",
    "    plt.savefig(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_plot.jpg\", dpi=200)                        \n",
    "    \n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    criterion_name = \"Best \" + str(criterion).split('(')[0]\n",
    "    metric_name = \"Best \" + str(metric).split('(')[0]\n",
    "    \n",
    "    print(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\")\n",
    "    print(f\"{metric_name.ljust(18)}: {best_metric:.4f}\")\n",
    "    print(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\")\n",
    "    print(f\"{'Training date'.ljust(18)}: {now}\")\n",
    "    \n",
    "    with open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/training_history.txt\", \"a\") as f:\n",
    "        f.write(\"\\nTRAINING SUMMARY\\n\")\n",
    "        f.write(f\"{'-' * 50}\\n\")\n",
    "        f.write(f\"{criterion_name.ljust(18)}: {best_loss:.4f}\\n\")\n",
    "        f.write(f\"{metric_name.ljust(18)}: {best_metric:.4f}\\n\")\n",
    "        f.write(f\"{'Training duration'.ljust(18)}: {((finish_time - start_time) / 60):.3f} minutes.\\n\")\n",
    "        f.write(f\"{'Training date'.ljust(18)}: {now}\\n\")\n",
    "    \n",
    "    # Save epoch losses, epoch metric scores, model, state dict, and oov embedding dict\n",
    "    filename_model_params = f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/model_params.pth\"\n",
    "    filename_oov_embedding_dict = open(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/oov_embedding_dict.pkl\", \"ab\")\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_losses) + 1)),\n",
    "            \"loss\": epoch_losses\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_losses.csv\", index=False)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"epoch\": list(range(1, len(epoch_metric_scores) + 1)),\n",
    "            \"f1_score\": epoch_metric_scores\n",
    "        }\n",
    "    ).to_csv(f\"../../logs/comick/{hyperparams.context_size}_contexts/{path_name}/epoch_metric_scores.csv\", index=False)\n",
    "    \n",
    "    torch.save(best_state_dict, filename_model_params)\n",
    "    pickle.dump({token : embedding for token, embedding in zip(list(labels_to_idx.keys()), model.embedding)}, filename_oov_embedding_dict)\n",
    "    \n",
    "    return epoch_losses, epoch_metric_scores\n",
    "\n",
    "epoch_losses, epoch_metric_scores = looping_step(dataloader, model, optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef50c-e71c-4288-a9f3-6fd8831033b0",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92debcb3-d6ea-4299-a3e9-37be5b63d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.8923348 ,  4.542819  , -2.51839   , ...,  4.852777  ,\n",
       "        -2.2821963 , -1.7584354 ],\n",
       "       [ 4.6935143 , -3.779116  , -5.0280275 , ..., -1.2798173 ,\n",
       "         1.7461338 ,  3.7477336 ],\n",
       "       [ 2.8594537 , -3.7452493 , -8.464219  , ...,  0.20333871,\n",
       "        -2.8956873 ,  5.2649093 ],\n",
       "       ...,\n",
       "       [ 7.3508425 ,  7.6894984 , -1.7009499 , ..., -1.3672771 ,\n",
       "        -3.99291   , -3.6716337 ],\n",
       "       [-3.6385963 ,  1.6200545 ,  1.4191867 , ...,  2.7713852 ,\n",
       "         1.6803381 ,  2.3960505 ],\n",
       "       [ 1.4085296 , -4.7929792 ,  0.70994484, ..., -7.570637  ,\n",
       "        -1.1925044 ,  0.99271935]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ce78c3-ac2c-4a84-a8e8-4d8e85d26563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bilstm_left_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.0669, -0.2860,  0.1029,  ..., -0.2803,  0.3240, -0.6295],\n",
       "                      [-0.1879, -0.2298,  0.0050,  ..., -0.1224,  0.5650, -0.4005],\n",
       "                      [ 0.4350, -0.4043,  0.0774,  ..., -0.3503,  0.1721, -0.1651],\n",
       "                      ...,\n",
       "                      [ 0.3961, -0.5096, -0.2302,  ..., -0.2529,  0.2428, -0.2204],\n",
       "                      [ 0.1716, -0.0670,  0.3009,  ...,  0.1891, -0.0538,  0.0121],\n",
       "                      [ 0.3298,  0.0397,  0.1945,  ..., -0.4660,  0.1667,  0.0145]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0703, -0.0643,  0.1357,  ..., -0.0996, -0.1409, -0.0676],\n",
       "                      [-0.0014,  0.2026,  0.0278,  ..., -0.0498, -0.1678, -0.0183],\n",
       "                      [-0.1489,  0.0262, -0.0059,  ...,  0.2182, -0.1733,  0.1204],\n",
       "                      ...,\n",
       "                      [ 0.0086, -0.1253, -0.0009,  ..., -0.1089, -0.0938,  0.0948],\n",
       "                      [-0.0952,  0.2762,  0.1344,  ..., -0.0721, -0.1305, -0.2981],\n",
       "                      [ 0.2372,  0.0962,  0.0424,  ..., -0.0547,  0.0243, -0.0008]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0',\n",
       "              tensor([-1.9735e-01, -1.6534e-01, -2.2197e-01, -6.7598e-02, -2.8364e-01,\n",
       "                      -1.5014e-01, -1.2063e-01, -8.6828e-03, -1.3569e-01, -9.0021e-02,\n",
       "                      -1.4201e-01, -2.5248e-01, -2.2558e-01, -2.9093e-01, -1.6058e-01,\n",
       "                      -2.1904e-01, -8.6692e-02, -2.9443e-01, -8.3734e-02, -1.7303e-01,\n",
       "                      -1.7110e-01, -2.3755e-01, -2.7147e-01, -1.8124e-01, -1.0689e-01,\n",
       "                      -1.4932e-01, -1.1013e-01, -1.3622e-01, -1.6575e-01, -1.6427e-01,\n",
       "                      -2.0872e-01, -3.6588e-02, -2.9091e-01, -2.6222e-01, -6.6613e-02,\n",
       "                      -1.3861e-01, -2.0322e-01, -1.8402e-01, -2.4377e-01, -3.1597e-01,\n",
       "                      -6.7067e-02, -2.4211e-01, -1.3656e-01, -1.2002e-01, -2.5096e-01,\n",
       "                      -3.5301e-01, -2.0116e-01, -2.0142e-01, -1.5453e-01, -1.1899e-01,\n",
       "                      -2.9349e-01, -3.6136e-01, -8.9361e-02, -2.8599e-01, -1.2275e-01,\n",
       "                      -1.9167e-01, -1.8583e-01, -1.0970e-01, -2.2149e-01, -3.5078e-01,\n",
       "                      -1.0402e-01, -1.6514e-01, -1.3414e-01, -2.4562e-01, -1.5415e-01,\n",
       "                      -2.9892e-01, -9.3303e-02, -2.8891e-01, -8.4017e-02, -3.1094e-01,\n",
       "                      -1.4825e-01, -1.3125e-02, -1.1922e-01, -2.4667e-01, -1.9141e-01,\n",
       "                      -3.3533e-01, -1.7350e-01, -1.7081e-01, -1.0030e-01,  5.4065e-02,\n",
       "                      -2.1867e-01, -1.1520e-01, -2.3929e-01, -2.5504e-01, -1.7221e-01,\n",
       "                      -3.2958e-01, -1.4949e-01, -2.4629e-01, -2.4162e-01, -1.5020e-01,\n",
       "                      -2.1977e-01, -1.6026e-01, -2.4354e-01, -2.9757e-01, -1.9404e-02,\n",
       "                      -1.3565e-01, -3.3753e-01, -1.7183e-01, -1.4644e-01, -1.9723e-01,\n",
       "                      -2.4522e-01, -1.7588e-01, -1.4958e-01, -2.8002e-01, -1.1025e-01,\n",
       "                      -1.8507e-01, -1.0918e-01, -1.8698e-01, -2.5415e-01, -2.5088e-01,\n",
       "                      -1.8148e-01, -2.0240e-01, -2.8642e-01, -2.7925e-01, -2.0357e-01,\n",
       "                      -1.8564e-01, -2.6766e-01, -8.3184e-02, -2.5936e-01, -1.9691e-01,\n",
       "                      -1.0580e-01, -1.3393e-01, -2.4257e-01, -2.7674e-01, -2.5304e-01,\n",
       "                      -1.8751e-01, -6.4484e-02, -1.3387e-01, -2.0077e-01, -2.3818e-01,\n",
       "                      -1.6099e-01, -2.1072e-01, -2.2709e-01, -1.5276e-01, -1.7514e-01,\n",
       "                      -1.2453e-01, -9.0088e-02, -9.1928e-02, -1.3643e-01, -1.9167e-01,\n",
       "                      -2.0593e-01, -7.1700e-02, -2.3818e-01, -1.3214e-01, -1.2725e-01,\n",
       "                      -1.3099e-01,  2.9856e-02, -1.4311e-01, -1.0871e-01, -3.4535e-01,\n",
       "                      -7.1544e-02, -1.7584e-01, -6.4797e-02, -1.5301e-01, -2.1795e-01,\n",
       "                      -1.0803e-01, -1.1828e-01, -2.1443e-02, -2.8576e-01, -8.4087e-02,\n",
       "                      -1.6629e-01, -1.3135e-01, -1.2626e-01, -1.6174e-01, -2.0843e-01,\n",
       "                      -1.5960e-01, -2.1864e-01, -2.2080e-01, -6.2954e-02, -1.9906e-01,\n",
       "                      -1.2293e-01, -1.5814e-01, -1.3995e-01, -1.7613e-01, -1.3544e-01,\n",
       "                      -2.6407e-01, -1.4101e-01, -1.0815e-01, -1.3851e-01, -1.9534e-01,\n",
       "                      -1.3337e-01, -1.5936e-01, -1.3410e-01, -1.9013e-01, -2.1576e-01,\n",
       "                      -1.5052e-01, -1.4680e-01, -1.3268e-01, -1.3368e-01, -1.6889e-01,\n",
       "                      -8.8934e-02, -3.3456e-01, -8.6704e-02, -2.5586e-01, -7.4695e-02,\n",
       "                      -3.6711e-01, -9.7289e-02, -3.4445e-01, -6.0329e-02, -9.7559e-02,\n",
       "                      -1.2118e-01, -2.7230e-01, -5.9506e-02, -1.4580e-01, -2.1884e-01,\n",
       "                      -1.2350e-01, -8.3835e-02, -1.2174e-01, -2.0631e-02, -1.3181e-01,\n",
       "                      -1.6979e-01, -1.5292e-01, -2.4280e-01, -1.8664e-01, -2.2369e-01,\n",
       "                      -2.0849e-01, -1.9860e-01, -7.2990e-02, -1.0223e-01, -2.8353e-01,\n",
       "                      -2.3916e-01, -1.6416e-01, -1.7795e-01, -1.7393e-01, -1.8573e-01,\n",
       "                      -1.4166e-01, -5.2333e-02, -1.4658e-01, -1.3776e-01, -2.5766e-01,\n",
       "                      -4.6199e-02, -5.5316e-02, -4.1438e-01, -1.5799e-01, -2.5554e-01,\n",
       "                      -1.2321e-01, -1.2648e-01, -1.3310e-01, -1.5985e-01, -1.9928e-01,\n",
       "                      -2.1278e-01, -1.2651e-01, -1.0054e-01, -9.8410e-02, -4.8773e-03,\n",
       "                      -4.3347e-02, -1.9385e-01, -2.2913e-01, -1.6424e-01, -1.8673e-01,\n",
       "                      -1.6438e-01, -2.2708e-01, -2.2787e-01, -2.6045e-01, -1.3043e-01,\n",
       "                      -1.9302e-01,  4.2395e-02, -1.9223e-03, -3.5260e-03,  1.5643e-02,\n",
       "                       6.0904e-02,  3.9846e-02,  3.2188e-02,  6.4657e-02,  4.5216e-02,\n",
       "                       6.9371e-02,  3.5962e-02, -7.4954e-02, -8.9692e-02,  9.5715e-02,\n",
       "                      -7.1283e-02, -9.0281e-04,  2.8496e-02,  2.8150e-02, -6.9492e-02,\n",
       "                       4.7984e-02,  3.9553e-02, -8.4697e-02, -1.1101e-01,  7.5470e-02,\n",
       "                       4.1478e-02,  2.8154e-03,  1.9760e-02,  2.5808e-02,  2.8140e-03,\n",
       "                      -6.8109e-02,  4.0390e-02,  8.6946e-02,  1.0683e-01,  7.1713e-02,\n",
       "                       6.0025e-03,  1.4447e-02, -1.2798e-01, -1.4640e-01, -1.3891e-01,\n",
       "                       2.4779e-02,  4.5626e-02, -6.9643e-02, -1.8938e-02,  5.4667e-03,\n",
       "                      -5.1585e-03, -2.3422e-02, -7.2923e-02,  1.0045e-01,  1.3744e-02,\n",
       "                      -4.9602e-02, -1.3190e-02,  8.9651e-02,  1.4143e-02, -8.0130e-02,\n",
       "                       9.0695e-03, -2.4258e-02, -4.4972e-02,  6.2470e-02,  1.3200e-01,\n",
       "                      -4.7765e-02, -6.9128e-03, -7.1778e-02, -1.8623e-02, -3.4067e-04,\n",
       "                      -6.3762e-02, -5.3291e-02, -1.9166e-02, -1.4042e-02,  5.9692e-02,\n",
       "                       5.4483e-02, -4.0983e-02,  3.2056e-02,  1.4454e-02, -5.0135e-02,\n",
       "                       5.4005e-03, -8.4246e-03,  7.3871e-02,  3.1725e-02,  7.8172e-03,\n",
       "                      -3.0625e-02, -7.9731e-03,  6.8594e-02, -1.2329e-01, -5.7334e-02,\n",
       "                      -2.6373e-02, -2.0387e-02, -3.2736e-02, -1.9919e-02,  6.2703e-02,\n",
       "                       1.1711e-01, -1.6319e-01, -6.8154e-03, -1.1410e-01, -9.1295e-02,\n",
       "                       4.7893e-02,  1.0110e-02,  1.2337e-01, -6.4896e-03,  4.1876e-02,\n",
       "                       7.0030e-02, -1.4376e-01,  9.5617e-02, -1.5981e-01,  8.6104e-02,\n",
       "                       7.1708e-02,  4.2340e-02, -1.3818e-02,  2.5294e-02,  8.5260e-02,\n",
       "                      -2.2973e-02,  5.9277e-02, -3.6731e-02,  5.4215e-02, -6.3439e-02,\n",
       "                       1.8531e-02, -9.6455e-03, -4.7853e-02, -4.3392e-02, -4.0495e-02,\n",
       "                       6.3803e-03,  7.9897e-02, -1.1621e-01,  1.9282e-02,  6.8463e-02,\n",
       "                      -4.0944e-02, -4.1743e-02,  7.5844e-02, -2.3550e-03, -1.7566e-01,\n",
       "                      -2.0415e-01, -1.8994e-01, -1.8119e-01, -1.2550e-01, -1.9831e-01,\n",
       "                      -1.1339e-01, -4.9106e-02, -2.8133e-01, -2.1623e-01, -1.0984e-01,\n",
       "                      -3.5527e-01, -2.2403e-01, -1.3709e-01, -1.2689e-01, -2.8535e-01,\n",
       "                      -2.1008e-01, -2.8465e-01, -2.1181e-01, -1.9750e-01, -2.0480e-01,\n",
       "                      -2.3636e-01, -2.1914e-01, -1.2534e-01, -1.8911e-01, -1.4068e-01,\n",
       "                      -1.8248e-01, -2.0151e-01, -1.8907e-01, -3.1554e-01, -1.6405e-01,\n",
       "                      -1.0829e-01, -2.6395e-01, -7.7965e-02, -7.8804e-02, -2.8131e-01,\n",
       "                      -8.1649e-02, -1.9983e-01, -2.5239e-01, -2.0868e-01, -2.9603e-01,\n",
       "                      -1.9701e-01, -2.4943e-01, -2.3522e-01, -1.9880e-01, -1.1732e-01,\n",
       "                      -1.2444e-01, -2.3879e-01, -1.3420e-01, -1.8980e-01, -3.2422e-01,\n",
       "                      -3.4574e-01, -1.8447e-01, -2.6053e-01, -2.1257e-01, -3.2424e-01,\n",
       "                      -1.1913e-01, -1.4322e-01, -3.0040e-01, -2.9640e-01, -9.0297e-02,\n",
       "                      -2.4308e-01, -1.1879e-01, -3.1030e-01, -2.3505e-01, -2.5290e-01,\n",
       "                      -1.1516e-01, -2.6392e-01, -1.1935e-01, -2.9265e-01, -1.7198e-01,\n",
       "                      -2.1273e-01, -1.9926e-01, -2.6306e-01, -1.9871e-01, -1.8602e-01,\n",
       "                      -1.9269e-01, -1.6593e-01, -1.3683e-01, -2.7072e-01, -2.7486e-01,\n",
       "                      -1.1393e-01, -2.8278e-01, -3.0600e-01, -1.6323e-01, -3.1446e-01,\n",
       "                      -1.0549e-01, -1.1800e-01, -2.1781e-01, -2.2243e-01, -2.3434e-01,\n",
       "                      -2.4033e-01, -1.9230e-01, -2.1354e-01, -2.7165e-01, -1.0079e-01,\n",
       "                      -2.4043e-01, -1.0052e-01, -2.6839e-01, -1.5704e-01, -1.8117e-01,\n",
       "                      -3.0885e-01, -2.1660e-01, -1.3899e-01, -2.8749e-01, -1.5521e-01,\n",
       "                      -2.8177e-01, -1.9256e-01, -6.2059e-02, -1.5047e-01, -9.8928e-02,\n",
       "                      -2.0011e-01, -1.4976e-01, -1.4428e-01, -1.4969e-01, -1.1221e-01,\n",
       "                      -1.2979e-01, -2.3779e-01, -2.4707e-01, -9.0042e-02, -1.6941e-01,\n",
       "                      -1.5415e-01, -2.2386e-01, -2.0951e-01, -1.7976e-01, -2.8993e-01,\n",
       "                      -1.0290e-01, -2.3594e-01])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0',\n",
       "              tensor([-0.1776, -0.2037, -0.2878, -0.2934, -0.1439, -0.1177,  0.0077, -0.1078,\n",
       "                      -0.2813, -0.1151, -0.2528, -0.1515, -0.2074, -0.0886, -0.2300, -0.2192,\n",
       "                      -0.1636, -0.2304, -0.1544, -0.1269, -0.2018, -0.3703, -0.2459,  0.0345,\n",
       "                      -0.3618, -0.2062, -0.1931, -0.2408, -0.1776, -0.2788, -0.2070, -0.1324,\n",
       "                      -0.1551, -0.2262, -0.1956, -0.1491, -0.1247, -0.1448, -0.1839, -0.1960,\n",
       "                      -0.2301, -0.1853, -0.1419, -0.2394, -0.2480, -0.2133, -0.1315, -0.1822,\n",
       "                      -0.1736, -0.2379, -0.2520, -0.2722, -0.1194, -0.2980, -0.1335, -0.1709,\n",
       "                      -0.3070, -0.1520, -0.2491, -0.2675, -0.2634, -0.2088, -0.3451, -0.2680,\n",
       "                      -0.2352, -0.1410, -0.0551, -0.1034, -0.0667, -0.1459, -0.0831, -0.1023,\n",
       "                      -0.1090, -0.2016, -0.1968, -0.2826, -0.1670, -0.2496, -0.1612, -0.1533,\n",
       "                      -0.2010, -0.1563, -0.3038, -0.3144, -0.2321, -0.3922, -0.1711, -0.0399,\n",
       "                      -0.1389, -0.3360, -0.1090, -0.3060, -0.2926, -0.1986, -0.3105, -0.1515,\n",
       "                      -0.2021, -0.2093, -0.1939, -0.1678, -0.1590, -0.2949, -0.1666, -0.2394,\n",
       "                      -0.2432, -0.2168, -0.2427, -0.1163, -0.1758, -0.1689, -0.1300, -0.2138,\n",
       "                      -0.2195, -0.2883, -0.1918, -0.2044, -0.3051, -0.0842, -0.1807, -0.1667,\n",
       "                      -0.0947, -0.1562, -0.2976, -0.3327, -0.3692, -0.2498, -0.0739, -0.2272,\n",
       "                      -0.1597, -0.1540, -0.2696, -0.2623, -0.2250, -0.0844, -0.1345, -0.1528,\n",
       "                      -0.1465, -0.0642, -0.1063, -0.2525, -0.1370, -0.1017, -0.1025, -0.2132,\n",
       "                      -0.1079, -0.2545, -0.2065, -0.2260, -0.0592, -0.2487, -0.1569, -0.3260,\n",
       "                      -0.2179, -0.1836, -0.1745, -0.1548, -0.1512, -0.1242,  0.0210, -0.2328,\n",
       "                      -0.1996, -0.1156, -0.1938, -0.0708, -0.2168, -0.1474, -0.2596, -0.1796,\n",
       "                      -0.1708, -0.1571, -0.1160, -0.3290, -0.2551, -0.1005, -0.1307, -0.1102,\n",
       "                      -0.0640, -0.1829, -0.2031, -0.2641, -0.2075, -0.2580, -0.1672, -0.1747,\n",
       "                      -0.1415, -0.1444, -0.2337, -0.2662, -0.1639, -0.2343, -0.2544, -0.2287,\n",
       "                      -0.1287, -0.1939, -0.1795, -0.0696, -0.1033, -0.1771, -0.1462, -0.0635,\n",
       "                      -0.0915, -0.2383, -0.0611, -0.1702, -0.2269, -0.1661, -0.0237, -0.0904,\n",
       "                      -0.1800, -0.0985, -0.1035, -0.3104, -0.1530, -0.1780, -0.0804, -0.1052,\n",
       "                      -0.0946, -0.1688, -0.0479, -0.1777, -0.2661, -0.0602, -0.1031, -0.1467,\n",
       "                      -0.1881, -0.0555, -0.1381, -0.1405, -0.2362, -0.0480, -0.1808, -0.1210,\n",
       "                      -0.0234, -0.2382, -0.1554, -0.3044, -0.0551, -0.0758, -0.2128, -0.1849,\n",
       "                      -0.2319, -0.2347, -0.1583, -0.1887, -0.2100, -0.1226, -0.1662, -0.1269,\n",
       "                      -0.1647, -0.1419, -0.0884, -0.2617, -0.2568, -0.2553, -0.0635, -0.1094,\n",
       "                      -0.0326,  0.0629, -0.0401,  0.0244, -0.1791,  0.0014,  0.1160, -0.1518,\n",
       "                       0.0237, -0.0221, -0.0571,  0.0199, -0.0052, -0.0208,  0.0308,  0.0778,\n",
       "                      -0.0146,  0.0582,  0.0518, -0.0104, -0.1010, -0.0291,  0.0526, -0.0537,\n",
       "                      -0.0622,  0.0693, -0.0567,  0.0364, -0.0849,  0.0523, -0.0335, -0.0342,\n",
       "                       0.0184, -0.0589,  0.0328,  0.1263,  0.0230, -0.0460,  0.0559, -0.0947,\n",
       "                      -0.0346, -0.0072,  0.0349, -0.0032,  0.0224, -0.0760,  0.0058, -0.0015,\n",
       "                       0.1105,  0.0735,  0.0405,  0.0748,  0.0811,  0.1080, -0.0165,  0.0410,\n",
       "                      -0.0243,  0.0157, -0.0722,  0.0848, -0.0116,  0.0649, -0.0807, -0.0469,\n",
       "                       0.0273, -0.0386, -0.0592, -0.1171,  0.1645, -0.0303, -0.0970,  0.0407,\n",
       "                       0.0667,  0.0762,  0.0970,  0.0958, -0.0331,  0.0713, -0.0716, -0.0080,\n",
       "                      -0.1027, -0.0019,  0.0082, -0.0247,  0.0835,  0.0440,  0.0984,  0.0417,\n",
       "                       0.0671, -0.0167,  0.1029,  0.0640,  0.0996, -0.0360,  0.0201, -0.0059,\n",
       "                       0.0025, -0.0528,  0.0717, -0.0464, -0.0480,  0.0431, -0.0629, -0.0566,\n",
       "                      -0.0978, -0.0290,  0.0419,  0.0187, -0.0048, -0.0526,  0.0576, -0.0194,\n",
       "                       0.0719,  0.0553, -0.0085,  0.0485, -0.0020,  0.0880, -0.0396, -0.0222,\n",
       "                       0.0064,  0.0484,  0.1120, -0.0314, -0.0152, -0.0475,  0.0931,  0.0472,\n",
       "                      -0.2332, -0.2285, -0.2473, -0.1342, -0.2246, -0.1993, -0.1461, -0.1058,\n",
       "                      -0.0892, -0.0652, -0.1300, -0.2612, -0.1828, -0.3108, -0.2078, -0.1958,\n",
       "                      -0.2374, -0.2492, -0.0735, -0.2400, -0.1970, -0.2235, -0.1891, -0.1991,\n",
       "                      -0.1085, -0.1356, -0.1841, -0.1613, -0.0286, -0.2538, -0.2019, -0.0992,\n",
       "                      -0.2432, -0.0840, -0.1220, -0.0728, -0.1642, -0.1537, -0.1770, -0.2662,\n",
       "                      -0.1801, -0.0682, -0.1881, -0.2906, -0.2576, -0.1774, -0.1079, -0.2563,\n",
       "                      -0.2627, -0.0815, -0.3210, -0.3468, -0.1741, -0.3596, -0.1932, -0.2323,\n",
       "                      -0.1071, -0.0780, -0.2322, -0.1905, -0.0993, -0.2071, -0.2763, -0.1871,\n",
       "                      -0.1763, -0.3365, -0.2055, -0.1966, -0.1214, -0.1703, -0.2299, -0.0358,\n",
       "                      -0.0887, -0.3015, -0.2179, -0.2069, -0.4014, -0.2922, -0.2412, -0.1628,\n",
       "                      -0.2305, -0.1403, -0.2181, -0.2692, -0.2021, -0.2598, -0.2216, -0.0985,\n",
       "                      -0.2234, -0.0927, -0.0708, -0.2471, -0.1787, -0.2943, -0.2642, -0.2706,\n",
       "                      -0.2921, -0.0475, -0.1901, -0.2468, -0.1864, -0.2435, -0.1341, -0.0590,\n",
       "                      -0.1824, -0.2220, -0.2199, -0.2075, -0.1089, -0.1937, -0.0695, -0.2228,\n",
       "                      -0.2622, -0.2455, -0.2129, -0.1861, -0.2196, -0.2742, -0.1259, -0.2747,\n",
       "                      -0.1903, -0.1954, -0.1918, -0.2113, -0.2518, -0.2566, -0.0782, -0.3071])),\n",
       "             ('bilstm_left_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.1979, -0.5257,  0.0780,  ...,  0.0652,  0.1355, -0.0319],\n",
       "                      [ 0.2835, -0.1675,  0.2014,  ...,  0.1085, -0.0330, -0.1720],\n",
       "                      [ 0.1988,  0.0730,  0.0067,  ..., -0.3809,  0.2570,  0.4089],\n",
       "                      ...,\n",
       "                      [-0.0833, -0.0324,  0.2632,  ..., -0.0727, -0.1769, -0.3424],\n",
       "                      [ 0.1111,  0.0567, -0.2139,  ...,  0.2930, -0.0668, -0.3689],\n",
       "                      [-0.1816,  0.1265, -0.2754,  ..., -0.0571, -0.2012, -0.0314]])),\n",
       "             ('bilstm_left_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0839,  0.0288, -0.1467,  ..., -0.0420,  0.0094, -0.2827],\n",
       "                      [ 0.1971,  0.1332,  0.1919,  ...,  0.1352, -0.0154, -0.3985],\n",
       "                      [ 0.2220,  0.0406, -0.1587,  ...,  0.0247,  0.5620,  0.2921],\n",
       "                      ...,\n",
       "                      [-0.3552, -0.1954,  0.2658,  ...,  0.0051,  0.0731,  0.0066],\n",
       "                      [-0.0402, -0.1310,  0.1722,  ..., -0.0167, -0.2399,  0.1823],\n",
       "                      [ 0.0281,  0.0758,  0.1122,  ..., -0.0274, -0.0755, -0.2324]])),\n",
       "             ('bilstm_left_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 1.0775e-01,  4.7329e-02,  9.1982e-03, -1.3358e-01,  1.9693e-01,\n",
       "                       4.4988e-02,  1.7712e-02,  3.9214e-02,  1.4556e-01,  1.4179e-01,\n",
       "                      -9.0932e-02,  1.6056e-02,  1.4379e-01,  5.2456e-02,  5.6620e-03,\n",
       "                      -2.6827e-03,  1.6369e-02, -4.9681e-02,  2.2439e-02,  3.7089e-02,\n",
       "                      -9.2496e-02,  7.4428e-02,  5.2293e-02, -4.8842e-02,  4.6267e-02,\n",
       "                       2.8769e-01, -4.9742e-02,  3.3517e-03,  1.4063e-02, -1.0947e-01,\n",
       "                       7.9219e-02,  1.1171e-01, -4.6944e-02,  1.9863e-01, -9.0652e-02,\n",
       "                       4.0079e-02, -1.3048e-02,  7.9173e-02,  1.7389e-02,  1.6627e-01,\n",
       "                       1.2414e-01,  1.7791e-01, -5.9243e-02, -1.7482e-02, -9.7447e-02,\n",
       "                       1.6982e-02,  2.2246e-01, -4.6744e-02, -5.3348e-02, -7.9717e-02,\n",
       "                       1.4247e-01,  2.7731e-01,  7.5838e-02,  8.3521e-02,  1.2478e-01,\n",
       "                       1.5177e-01, -1.5501e-01, -1.1594e-02,  8.0189e-02, -5.4027e-02,\n",
       "                      -4.3784e-02,  5.0319e-02, -6.6323e-03, -1.5030e-02,  3.3543e-02,\n",
       "                       1.2544e-02,  1.0248e-01,  9.2557e-02, -5.2983e-03,  3.9442e-02,\n",
       "                       3.3459e-02, -1.3748e-01,  1.1850e-03, -9.1579e-02,  1.4805e-02,\n",
       "                      -9.3014e-02,  1.3688e-01, -8.4733e-03, -1.8004e-02,  5.2774e-02,\n",
       "                      -3.9546e-02,  5.1636e-02, -8.9390e-02,  1.7547e-01, -5.5753e-02,\n",
       "                      -5.9461e-02,  9.7149e-02,  1.7762e-02, -7.4592e-02, -7.9742e-02,\n",
       "                      -1.0830e-01, -8.0626e-02,  3.4040e-02,  1.5211e-01, -1.1659e-03,\n",
       "                      -3.1656e-03,  7.5102e-02,  7.3476e-02,  2.7101e-02,  2.2827e-02,\n",
       "                       1.8012e-01,  7.1751e-02,  7.2040e-02, -1.6154e-02,  4.7229e-02,\n",
       "                       7.3118e-02,  2.9942e-05, -1.4711e-01,  1.7513e-01,  3.5130e-02,\n",
       "                      -4.3696e-02,  1.4034e-01,  1.5709e-01, -1.7534e-01,  1.4872e-02,\n",
       "                       4.9024e-02,  1.0030e-01,  1.1034e-01, -7.6912e-02, -6.4436e-02,\n",
       "                       3.6177e-02,  4.0023e-02,  7.1031e-02,  6.7601e-02,  9.6391e-02,\n",
       "                       3.1599e-02, -3.7345e-02, -3.3027e-02, -1.7215e-01, -5.9554e-02,\n",
       "                       3.7911e-02, -2.6569e-02, -6.3200e-02, -7.1906e-02,  1.5030e-02,\n",
       "                      -3.8386e-02, -1.2482e-01,  5.4969e-02, -1.3488e-01,  5.0580e-03,\n",
       "                      -1.5510e-01,  3.6357e-02,  9.4454e-02, -1.4381e-01, -3.0199e-02,\n",
       "                      -1.2613e-01, -3.1421e-02,  1.6295e-02,  2.5965e-02, -3.2577e-02,\n",
       "                      -1.9347e-01, -1.9777e-01, -1.1585e-02, -4.7977e-02, -8.2413e-02,\n",
       "                      -7.0311e-02, -9.3657e-02, -4.9941e-02, -3.6409e-02,  7.4002e-03,\n",
       "                      -7.0672e-02, -5.4431e-02, -5.2951e-02,  1.5961e-02, -6.9065e-02,\n",
       "                      -1.1281e-01, -1.2505e-01, -2.2825e-01, -4.1528e-02, -1.9921e-01,\n",
       "                      -1.2589e-01,  4.7848e-02, -2.1402e-01, -1.2086e-01, -1.8352e-01,\n",
       "                       8.5382e-02, -2.4179e-01, -1.5226e-01, -3.1574e-02, -1.2355e-01,\n",
       "                      -5.7981e-02,  1.2882e-02,  2.6475e-02,  1.0168e-01, -3.4357e-02,\n",
       "                      -1.4861e-01,  1.8547e-02, -3.0694e-02, -1.5492e-01,  6.4813e-02,\n",
       "                      -5.0899e-02, -2.0928e-01, -5.2452e-02, -1.7732e-01,  7.8123e-02,\n",
       "                      -2.0469e-01, -3.4742e-02,  4.0206e-02, -1.6683e-01, -1.4923e-01,\n",
       "                       8.9566e-02, -1.3807e-01,  3.5897e-02, -3.4014e-02,  5.6561e-02,\n",
       "                      -1.7417e-01,  5.3080e-02, -6.0666e-02, -3.7857e-03,  3.4731e-03,\n",
       "                      -1.9845e-01, -8.2309e-02, -4.2394e-02,  4.7465e-02,  1.4942e-02,\n",
       "                       3.3819e-02,  1.2118e-01, -3.5337e-02, -1.1493e-01, -1.4877e-01,\n",
       "                       5.1283e-02, -1.6400e-01, -3.6604e-02,  7.2332e-03, -1.7178e-01,\n",
       "                       4.1165e-04, -1.5804e-02,  3.0854e-02, -6.0516e-02, -1.1588e-01,\n",
       "                       3.8693e-03, -8.9880e-02, -9.1513e-03, -9.9678e-02,  6.1897e-02,\n",
       "                      -8.9108e-03, -9.3534e-02, -9.7384e-02, -8.9518e-02, -3.8773e-02,\n",
       "                      -1.1142e-01, -6.0063e-02, -2.3683e-02, -3.1667e-02,  3.1616e-02,\n",
       "                      -2.4977e-02, -3.2199e-02,  1.1691e-01, -2.2533e-01,  6.9610e-02,\n",
       "                      -5.1574e-02, -5.1176e-02, -1.6731e-02, -1.8320e-01,  9.6773e-02,\n",
       "                      -2.4280e-01,  7.7714e-02,  8.7408e-02,  1.3000e-01,  4.2829e-02,\n",
       "                      -9.6101e-02,  2.4220e-02, -1.2524e-02, -4.0961e-02, -1.1422e-01,\n",
       "                      -5.4739e-03, -5.8879e-02,  6.5493e-02,  1.4406e-01, -9.3661e-03,\n",
       "                       3.0183e-02,  6.8900e-02,  1.1839e-01, -3.4327e-02,  3.4700e-02,\n",
       "                      -6.7812e-02, -3.4968e-02, -4.5810e-02,  8.6622e-03, -4.5501e-02,\n",
       "                      -5.5225e-03,  5.3922e-02, -8.6864e-03, -5.0417e-02,  8.2626e-02,\n",
       "                      -4.1475e-02, -6.9745e-02,  4.0875e-03,  5.6402e-02,  7.1039e-02,\n",
       "                      -3.3660e-02,  3.4927e-02,  3.8871e-02, -2.5453e-03, -9.6892e-02,\n",
       "                      -6.5886e-02, -2.3830e-02,  2.9558e-02,  4.7721e-02,  1.6525e-02,\n",
       "                      -1.5272e-01, -1.4360e-02,  7.9684e-02, -2.1130e-02, -5.6377e-02,\n",
       "                      -3.1495e-02, -2.3191e-03,  2.7341e-02, -1.3719e-02, -8.3198e-02,\n",
       "                      -1.0776e-01, -7.4179e-02,  4.1537e-02, -3.3025e-02, -7.4657e-02,\n",
       "                       7.7298e-02,  8.7638e-02,  1.4632e-01, -2.2838e-02,  9.7841e-02,\n",
       "                       6.9392e-02, -4.4822e-02, -1.1988e-02, -8.0845e-04,  3.2419e-02,\n",
       "                       1.7084e-02,  3.4245e-02, -6.8181e-02,  9.7564e-02, -3.9450e-02,\n",
       "                       6.1729e-02,  5.2336e-02,  8.7157e-04,  2.5504e-02,  1.3450e-02,\n",
       "                       6.5640e-02,  3.8166e-02,  2.2382e-02,  6.2661e-02, -6.0999e-03,\n",
       "                       9.2694e-04,  5.4120e-02,  5.8427e-02, -5.4871e-02,  3.6221e-02,\n",
       "                      -3.5178e-02, -1.0178e-01,  3.8118e-02, -1.0415e-03,  2.4898e-02,\n",
       "                      -7.0001e-02, -5.1794e-02,  3.6645e-02, -9.4481e-03,  1.0821e-01,\n",
       "                       1.0657e-02, -1.5804e-01,  7.2199e-02,  6.8243e-03,  5.8152e-02,\n",
       "                       3.7329e-02, -3.3918e-04,  8.5646e-02,  2.1517e-03,  5.1912e-02,\n",
       "                      -5.9828e-05, -8.4064e-03,  7.5610e-02, -7.0365e-02, -4.7334e-02,\n",
       "                      -6.9891e-03,  6.4517e-02, -1.4188e-01, -1.2061e-02,  9.6583e-02,\n",
       "                       6.1109e-02, -8.3252e-02, -3.2118e-02, -2.7623e-02,  4.6574e-02,\n",
       "                       3.1804e-03, -9.2358e-02, -2.2317e-02, -7.0624e-02,  1.2745e-01,\n",
       "                      -7.2248e-02, -9.2460e-02,  1.5378e-01,  1.8176e-01,  9.2874e-02,\n",
       "                       6.6566e-03, -5.6171e-03,  1.6620e-02,  1.9849e-01, -1.3377e-01,\n",
       "                       7.1914e-02,  2.0170e-01, -5.2259e-02,  1.6582e-02,  1.7293e-02,\n",
       "                      -4.9594e-02, -1.1277e-01,  7.0361e-03, -1.9529e-02, -3.9291e-02,\n",
       "                      -8.2223e-03,  9.0748e-03,  1.3775e-01, -4.5608e-02,  1.3570e-01,\n",
       "                      -1.1563e-01,  1.9225e-02,  1.7802e-02, -1.3193e-01,  4.4022e-02,\n",
       "                       2.7906e-01,  7.0437e-02, -1.9543e-02,  1.3078e-01,  8.9833e-02,\n",
       "                       1.6750e-02,  1.4743e-01,  8.3981e-02,  1.4496e-01,  2.4663e-02,\n",
       "                       2.1601e-01,  1.2988e-01,  3.4218e-02, -7.4346e-02, -5.4650e-04,\n",
       "                       2.5639e-01,  1.8977e-01,  9.2603e-02, -1.2413e-03,  1.4812e-02,\n",
       "                       1.0022e-01,  4.4153e-02,  3.0307e-02,  2.8435e-02,  9.9876e-02,\n",
       "                      -1.2409e-01, -7.2309e-02, -3.4463e-02, -7.5132e-03, -1.1495e-01,\n",
       "                       7.3468e-02,  1.6149e-03, -1.3575e-01,  8.9848e-02,  1.6947e-01,\n",
       "                       6.8869e-02,  1.4077e-01, -3.1128e-02,  1.8061e-01, -7.1800e-02,\n",
       "                       1.8431e-02, -2.7953e-02, -8.1318e-02,  3.0277e-02, -9.9362e-02,\n",
       "                       1.1249e-01, -4.8932e-02, -1.7172e-02,  6.0793e-02, -4.2839e-02,\n",
       "                       3.8058e-02, -4.2143e-02,  1.1027e-01,  4.6218e-02,  8.9061e-02,\n",
       "                       5.2153e-02,  1.5261e-01, -4.3608e-02,  6.6740e-02, -1.0816e-01,\n",
       "                       9.0170e-02,  5.2942e-02,  7.4719e-03, -4.9898e-02,  1.0349e-01,\n",
       "                       4.8078e-02, -3.1113e-02,  9.5535e-02,  3.0895e-02,  1.7332e-01,\n",
       "                       1.1832e-01,  1.7682e-01, -9.5786e-02,  1.0342e-01,  1.0427e-01,\n",
       "                       8.1113e-02, -5.2169e-02,  3.8973e-02,  1.0289e-01,  3.3646e-02,\n",
       "                      -3.3747e-02,  7.6714e-02, -6.4275e-02,  2.6674e-02, -1.4026e-01,\n",
       "                       3.5141e-02, -9.2387e-02,  7.7852e-02,  8.0425e-04, -4.9457e-02,\n",
       "                      -3.0275e-02,  1.7364e-03, -3.5633e-02, -1.1986e-02,  5.5113e-02,\n",
       "                       2.2497e-02, -5.9414e-03])),\n",
       "             ('bilstm_left_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([ 1.5864e-01,  1.2459e-01,  6.6697e-02, -5.0744e-03,  1.2377e-01,\n",
       "                      -4.8489e-03, -4.4207e-02, -9.4656e-02, -9.1694e-02,  2.9401e-01,\n",
       "                      -9.0141e-02,  6.8011e-02,  5.1123e-02, -2.3271e-02, -2.1163e-02,\n",
       "                      -1.2006e-01,  2.0632e-02, -1.6035e-01,  4.8880e-02, -4.0955e-02,\n",
       "                      -9.1404e-02,  6.7296e-02,  7.3402e-02, -6.7688e-02,  7.0150e-02,\n",
       "                       1.3248e-01, -1.0990e-02, -2.3645e-02, -2.7770e-02, -6.4187e-02,\n",
       "                      -2.9187e-03,  2.4922e-02, -1.0755e-01,  1.3434e-01, -8.6713e-03,\n",
       "                       1.3618e-03, -4.4408e-02,  1.6746e-02,  5.2601e-02,  1.8696e-02,\n",
       "                       1.4170e-01,  1.6817e-01, -9.1488e-02, -1.4140e-01, -6.8777e-02,\n",
       "                       7.4205e-02,  1.0735e-01, -3.2574e-05, -9.5138e-02, -1.4319e-01,\n",
       "                       8.7667e-02,  1.1166e-01,  3.9304e-02,  9.2468e-02,  1.6800e-01,\n",
       "                      -5.6612e-02, -2.1071e-01, -6.3096e-02, -1.7744e-02, -3.8289e-03,\n",
       "                      -1.2182e-01,  6.0563e-02, -1.1172e-01, -1.4847e-01, -1.1688e-01,\n",
       "                       5.5549e-02, -8.3947e-02,  5.2037e-02, -1.6286e-01,  1.7621e-02,\n",
       "                       5.0943e-02, -9.4143e-02, -2.5868e-02, -3.2401e-02,  1.1335e-01,\n",
       "                      -1.2684e-01,  7.6113e-02, -6.1697e-02, -8.8688e-02, -3.4006e-02,\n",
       "                       4.4631e-02,  1.2304e-01, -1.0894e-02,  2.0839e-02, -6.9682e-03,\n",
       "                      -8.7868e-02,  2.0883e-01, -4.1571e-02, -7.5403e-02,  5.9975e-02,\n",
       "                       2.6878e-02, -2.8347e-02,  6.7920e-02,  4.7097e-02,  7.1772e-02,\n",
       "                       4.4134e-02,  5.5991e-02,  1.1203e-01,  3.3079e-02, -5.9785e-02,\n",
       "                       1.2928e-01, -2.3202e-02,  1.1294e-02, -1.5221e-02,  6.5565e-02,\n",
       "                       2.4979e-02,  3.1704e-04,  8.6096e-03,  9.2397e-02, -6.3618e-02,\n",
       "                      -2.8536e-02,  4.9206e-03,  9.8686e-02, -1.7657e-02,  5.7335e-02,\n",
       "                      -1.3245e-03,  8.7707e-03, -2.2472e-02, -1.1749e-02, -1.0557e-02,\n",
       "                       4.2186e-03, -3.6293e-02,  3.6854e-02,  2.3252e-01, -1.7639e-02,\n",
       "                      -1.6418e-02, -7.2385e-02, -6.4477e-02,  6.7243e-02, -5.7758e-02,\n",
       "                       4.5003e-02, -2.8260e-02, -1.7112e-01, -2.8138e-02,  6.3916e-02,\n",
       "                      -1.7292e-02, -1.2461e-01, -8.9622e-02, -5.3280e-02,  6.0300e-02,\n",
       "                      -7.4913e-02, -1.0512e-01,  2.3130e-01, -1.9045e-01,  5.7254e-02,\n",
       "                      -9.8907e-03, -2.0325e-02, -9.7205e-02, -1.1306e-01,  3.2823e-02,\n",
       "                      -8.3931e-02, -2.2185e-01,  1.0804e-01, -7.2564e-02,  1.3746e-03,\n",
       "                       3.5042e-02, -2.3406e-01, -1.2176e-01, -6.9488e-02,  3.4186e-02,\n",
       "                      -2.4743e-02, -1.8137e-02, -9.0446e-02, -1.4496e-02, -8.8496e-02,\n",
       "                      -6.8958e-02, -8.7697e-02, -2.2908e-01, -9.8174e-02, -5.0033e-02,\n",
       "                      -3.2475e-02,  1.6795e-02, -1.3216e-01, -1.5613e-01, -1.8267e-01,\n",
       "                       3.5238e-02, -1.8363e-01, -1.5987e-01, -2.6437e-02, -1.6138e-01,\n",
       "                      -3.1589e-02,  7.4882e-02,  1.1146e-01,  1.9771e-01, -1.3863e-01,\n",
       "                      -6.2192e-02, -5.1368e-02, -1.5787e-01, -1.2049e-01, -4.1521e-02,\n",
       "                      -1.6744e-01, -9.4958e-02, -1.2590e-01, -2.0230e-01,  5.7577e-02,\n",
       "                      -1.9718e-01, -7.5132e-02, -3.6214e-02, -1.0580e-01, -9.1544e-03,\n",
       "                       1.1005e-02, -1.1329e-01, -1.7902e-01,  9.8070e-02, -2.9601e-02,\n",
       "                      -1.0668e-02, -7.0048e-02, -1.8193e-01, -6.1076e-02, -9.7587e-02,\n",
       "                      -1.9036e-01,  9.5458e-03, -1.9845e-02, -1.6918e-02,  8.2230e-03,\n",
       "                      -3.1228e-02,  1.2407e-02, -4.7090e-02, -1.0805e-01,  1.8894e-02,\n",
       "                      -4.0553e-02, -1.4833e-01, -4.5578e-03, -4.3363e-02,  9.0404e-02,\n",
       "                       6.8285e-03, -2.9436e-02, -1.3777e-01, -7.9881e-02, -1.6415e-01,\n",
       "                      -8.0334e-02, -6.4935e-03, -8.0387e-02, -1.3441e-01,  8.1090e-02,\n",
       "                       1.1818e-02, -9.4230e-02, -9.6202e-02, -2.5156e-01,  7.6770e-02,\n",
       "                       6.9585e-02, -4.2852e-02, -4.1191e-02, -4.8570e-02,  1.2100e-01,\n",
       "                       1.0479e-01, -7.6678e-02,  7.3844e-02, -8.6129e-02,  1.4595e-01,\n",
       "                       3.5130e-02, -9.2951e-02,  2.7292e-02, -3.3084e-02, -4.4398e-02,\n",
       "                      -2.1491e-01, -5.0553e-02,  2.3613e-02, -2.2267e-02,  8.8926e-03,\n",
       "                      -8.8387e-02,  6.0864e-02, -3.4352e-02,  7.1565e-02,  1.3011e-01,\n",
       "                       1.4223e-01, -5.0542e-02,  7.1891e-02, -4.7516e-02, -9.2117e-02,\n",
       "                      -9.6127e-02, -3.7003e-02,  1.1179e-02,  6.0139e-02, -3.5618e-02,\n",
       "                      -6.9413e-02,  1.8830e-02, -2.7094e-02, -7.9748e-02, -4.8026e-03,\n",
       "                      -8.2868e-02,  8.1702e-02,  6.5283e-03, -1.1871e-02, -2.3737e-02,\n",
       "                       3.5665e-02,  3.6476e-02, -5.8451e-02,  7.0061e-02,  7.0167e-02,\n",
       "                       1.9350e-02, -3.5116e-02,  1.4734e-01, -4.5402e-02,  7.3395e-02,\n",
       "                      -8.5189e-02,  1.3787e-02, -1.2668e-01,  5.9647e-02,  3.0307e-02,\n",
       "                      -3.5947e-02,  6.4340e-03, -7.0993e-03,  8.2561e-02, -3.3396e-02,\n",
       "                       3.0814e-02,  7.5833e-02,  9.4605e-02,  2.7499e-02,  3.8716e-02,\n",
       "                      -1.3405e-02,  5.4998e-02, -1.5446e-02, -1.0000e-02,  9.7841e-03,\n",
       "                       6.1510e-03, -7.7718e-02,  1.0593e-01,  2.3099e-03, -4.8470e-02,\n",
       "                       2.1583e-02,  1.0095e-01, -6.9275e-02,  5.6756e-02, -3.4767e-02,\n",
       "                       7.6681e-02,  8.1267e-02,  6.3843e-02,  1.4280e-01, -1.0568e-02,\n",
       "                      -3.3283e-02, -3.7939e-02,  6.4591e-02, -5.1580e-02,  1.3764e-02,\n",
       "                       3.0533e-02, -1.0056e-01, -2.1044e-02,  4.4365e-02, -5.6231e-02,\n",
       "                       5.2785e-02, -9.7160e-03,  5.1152e-03,  2.4778e-03,  8.1069e-02,\n",
       "                      -9.2064e-02, -1.4017e-02, -2.3311e-02, -8.3410e-02,  8.0864e-02,\n",
       "                      -6.7224e-02, -8.2671e-02,  7.8805e-03,  5.5849e-02, -1.0785e-01,\n",
       "                       4.8779e-02,  2.0496e-02, -7.0721e-02,  9.4556e-02,  9.7567e-03,\n",
       "                       1.2082e-02,  1.5355e-02, -4.8432e-02,  4.5446e-03,  1.3588e-01,\n",
       "                       8.8134e-02, -4.9207e-03,  1.5307e-01, -3.7391e-02, -3.8147e-02,\n",
       "                       1.3030e-02,  7.0389e-03,  9.7310e-03,  1.4847e-01,  6.8804e-02,\n",
       "                       3.1614e-02,  1.0196e-01, -2.5842e-02, -1.1858e-02,  9.1323e-03,\n",
       "                       5.3513e-03,  2.1012e-03, -2.5979e-02, -2.1980e-02,  8.9393e-02,\n",
       "                       1.1520e-01, -1.4564e-01,  1.1484e-01,  2.0117e-01, -2.8721e-02,\n",
       "                       2.3165e-02, -7.1055e-02,  1.3279e-01,  1.3118e-01, -4.9413e-02,\n",
       "                      -3.0636e-02,  1.3942e-01, -6.6494e-02,  1.3861e-01,  1.4031e-02,\n",
       "                       3.8002e-02, -1.5392e-01,  6.1467e-02, -2.6064e-02,  1.0789e-02,\n",
       "                      -2.3396e-02,  8.4543e-03, -1.0402e-02, -6.1262e-02,  1.4297e-01,\n",
       "                      -9.3229e-02, -3.3034e-03,  1.6786e-02, -1.0504e-01, -7.2858e-02,\n",
       "                       2.2520e-01, -1.3507e-01,  5.6423e-02,  3.0999e-02,  4.4196e-02,\n",
       "                       1.2889e-01,  1.6918e-01,  8.1305e-02,  1.3407e-01,  2.1555e-01,\n",
       "                       1.3274e-01, -3.6018e-02,  2.5699e-02, -1.1409e-01,  1.0210e-02,\n",
       "                       2.4986e-01,  1.3081e-01, -2.7000e-02, -7.3718e-03,  7.5196e-02,\n",
       "                       8.6631e-02,  6.1188e-02,  1.1140e-02,  6.5879e-02,  1.0414e-01,\n",
       "                      -1.5340e-01,  1.4098e-02, -2.3097e-02,  1.9563e-02, -4.8463e-02,\n",
       "                      -1.2218e-01,  5.7761e-02,  6.9018e-02,  7.4830e-02,  3.2819e-02,\n",
       "                       5.8995e-02,  1.2997e-01,  7.7985e-02,  6.1218e-02,  5.0229e-02,\n",
       "                      -5.4169e-02, -2.5397e-02, -3.5596e-02,  5.7639e-02, -7.9541e-02,\n",
       "                       1.1091e-01, -9.3174e-02,  9.8832e-03,  1.1452e-01, -7.7268e-02,\n",
       "                       1.4669e-02,  1.0742e-02,  9.1897e-02,  5.7359e-02,  3.6993e-02,\n",
       "                       5.2404e-02,  1.1717e-01, -1.0963e-01,  1.1244e-01, -1.4288e-01,\n",
       "                       6.8574e-02, -3.7673e-02,  3.3858e-02, -8.0970e-02, -1.0806e-01,\n",
       "                       4.0276e-02,  1.4879e-02,  1.1115e-01,  3.2419e-02,  1.6639e-01,\n",
       "                      -5.5230e-03,  1.7326e-01,  3.5394e-03, -7.6871e-02,  2.0649e-01,\n",
       "                      -5.3046e-02,  7.9860e-04,  5.4417e-02, -3.2646e-02, -5.5462e-02,\n",
       "                       4.2214e-02,  1.2351e-01, -7.9479e-02,  4.7926e-02, -2.8321e-02,\n",
       "                       1.3902e-01,  1.1629e-01,  2.8748e-02, -1.5003e-01,  8.5150e-02,\n",
       "                      -1.1312e-01,  1.1797e-01, -8.7411e-02,  1.6039e-01, -8.7562e-02,\n",
       "                      -1.3275e-01, -6.7853e-02])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0',\n",
       "              tensor([[ 0.2119,  0.4753, -0.3138,  ..., -0.3048, -0.8631, -0.4735],\n",
       "                      [ 0.2176,  0.1499,  0.1162,  ...,  0.5102,  0.1522,  0.1051],\n",
       "                      [-0.0423,  0.2737,  0.3270,  ...,  0.2886, -0.0931,  0.0407],\n",
       "                      ...,\n",
       "                      [ 0.0863,  0.7157,  0.3823,  ..., -0.4245, -0.0618,  0.1681],\n",
       "                      [ 0.5970, -0.4135,  0.1440,  ..., -0.2439, -0.0929, -0.1120],\n",
       "                      [ 0.6235,  0.1934, -0.2447,  ...,  0.0583,  0.0627,  0.1145]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0',\n",
       "              tensor([[-0.0804,  0.1280,  0.0354,  ...,  0.1430, -0.0162,  0.0431],\n",
       "                      [-0.3925,  0.0958, -0.1531,  ..., -0.0843, -0.0740,  0.2163],\n",
       "                      [ 0.3762, -0.1499, -0.2586,  ...,  0.4474, -0.0296,  0.0882],\n",
       "                      ...,\n",
       "                      [ 0.2013, -0.2059,  0.1509,  ..., -0.0803, -0.2052, -0.1112],\n",
       "                      [-0.1127,  0.0487,  0.0515,  ...,  0.0265,  0.0649, -0.1203],\n",
       "                      [-0.1010,  0.0206, -0.0234,  ...,  0.3299, -0.1012, -0.0298]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0',\n",
       "              tensor([-3.1722e-01, -1.5700e-01, -8.0650e-02, -1.9704e-02, -2.8350e-02,\n",
       "                      -9.6873e-02, -2.7088e-02, -9.9404e-02, -2.0025e-02, -7.6860e-02,\n",
       "                       2.2250e-02, -9.7315e-02, -2.1505e-01, -1.1553e-01, -7.8908e-03,\n",
       "                      -1.2129e-01, -2.2534e-01, -1.5153e-01, -1.8518e-01, -1.0026e-01,\n",
       "                      -1.5731e-01, -2.5177e-01, -2.3211e-01, -1.4784e-01, -1.3655e-01,\n",
       "                      -2.1778e-01, -1.0338e-01,  1.9666e-02, -1.5554e-01, -5.1180e-02,\n",
       "                      -1.8906e-01, -2.0773e-01, -1.0591e-01, -1.5031e-01, -8.2751e-02,\n",
       "                      -2.3108e-01, -5.0603e-02, -9.2853e-02, -8.7210e-03, -1.4533e-01,\n",
       "                      -1.4623e-01, -1.0304e-01, -3.8612e-02, -2.1841e-01, -1.7221e-01,\n",
       "                      -3.2234e-01, -1.1879e-01, -8.8584e-02, -1.6492e-01, -1.9105e-01,\n",
       "                       9.2087e-02, -1.6509e-01, -1.7884e-01, -3.3781e-02, -1.3255e-01,\n",
       "                      -2.5413e-01, -9.4393e-02, -1.5789e-01, -9.7860e-02, -2.3604e-01,\n",
       "                      -1.4598e-01, -1.3311e-01, -1.1220e-01, -4.0515e-02, -4.7403e-02,\n",
       "                      -2.3201e-01, -9.9105e-02, -8.2519e-02,  8.2495e-04, -7.3161e-02,\n",
       "                      -1.2891e-01, -1.3832e-01, -4.1354e-02, -3.1237e-01, -3.2993e-01,\n",
       "                      -2.1606e-01, -2.6421e-01,  2.8436e-03,  2.0107e-02, -1.2953e-01,\n",
       "                      -1.3728e-01, -1.4103e-02, -1.0522e-01, -6.1205e-02,  6.8341e-04,\n",
       "                      -3.7671e-02, -9.9999e-02, -2.6894e-01, -1.6058e-01, -1.7590e-01,\n",
       "                      -1.8709e-01, -1.4476e-01, -1.4221e-01, -4.3887e-02, -1.5051e-01,\n",
       "                      -1.3429e-01, -1.4228e-01, -1.1074e-01,  1.1136e-02, -1.8014e-04,\n",
       "                      -9.1441e-02, -3.3481e-02, -1.4385e-01, -1.9920e-01, -1.2830e-01,\n",
       "                      -1.9239e-01, -1.5156e-01, -5.7191e-02, -5.5291e-03, -1.2021e-01,\n",
       "                      -1.4306e-01, -1.0001e-01, -1.1460e-01,  6.1416e-02, -6.6799e-02,\n",
       "                      -8.4281e-02, -1.9500e-01,  3.6189e-02, -1.7276e-01, -1.5950e-01,\n",
       "                      -1.2899e-01, -2.7950e-01, -2.2217e-01, -1.7433e-01, -1.1352e-01,\n",
       "                      -9.1617e-02,  5.8169e-02, -6.6395e-02,  4.9932e-03, -5.5511e-02,\n",
       "                      -2.1289e-01, -1.6329e-01,  1.5528e-02, -1.1728e-01, -9.6590e-02,\n",
       "                       1.0489e-02, -2.0404e-01, -4.9270e-02, -1.4677e-01, -4.0078e-02,\n",
       "                      -4.9630e-02, -2.0018e-01, -1.1526e-01,  3.9790e-02, -1.5990e-01,\n",
       "                      -1.2754e-01, -1.7671e-01,  3.6432e-02, -1.9915e-01, -1.1933e-01,\n",
       "                      -3.8052e-02, -1.9255e-01, -2.0959e-01, -2.5388e-01, -2.1868e-01,\n",
       "                       6.6006e-02, -1.2299e-02,  6.2766e-02, -1.6123e-01, -6.9508e-02,\n",
       "                      -1.0581e-02, -3.5802e-02, -6.2333e-02, -1.0609e-01, -8.5358e-02,\n",
       "                       1.3717e-02,  1.7440e-01, -8.6434e-02,  1.3643e-02, -1.8051e-01,\n",
       "                       2.5765e-02, -1.4425e-01, -8.6444e-02, -2.0827e-01, -1.0058e-01,\n",
       "                      -8.6348e-04, -2.8971e-02, -2.1897e-01,  8.2151e-02, -7.0178e-02,\n",
       "                      -1.2290e-01, -2.0010e-01, -2.6256e-01,  1.6199e-02, -1.7371e-02,\n",
       "                      -1.0071e-01, -2.8127e-02, -7.2304e-02, -2.1476e-01, -1.1513e-01,\n",
       "                       7.1613e-02, -6.7330e-02,  4.1711e-03, -9.7746e-02, -7.9809e-02,\n",
       "                      -2.0803e-02, -1.3174e-02, -1.3728e-01, -1.0803e-01, -8.9489e-02,\n",
       "                       9.5654e-02, -6.5111e-02, -2.5261e-02, -7.1928e-02, -2.9253e-01,\n",
       "                       1.4505e-01,  1.1569e-01, -9.2164e-02,  1.3646e-02, -1.3172e-01,\n",
       "                       1.2690e-01, -7.6966e-03, -3.8525e-02, -5.4456e-02, -2.3157e-01,\n",
       "                      -1.3880e-01, -1.8749e-01, -6.2661e-02, -2.2616e-01, -9.6613e-02,\n",
       "                      -1.5552e-01, -9.9355e-02,  1.2861e-02, -7.5572e-02, -1.3818e-01,\n",
       "                      -1.4939e-01, -9.9933e-02, -3.2502e-02, -8.8147e-03,  5.6382e-02,\n",
       "                       8.1216e-02, -1.6704e-01, -9.2430e-02,  6.8435e-02,  5.9265e-02,\n",
       "                      -3.8333e-02, -6.2607e-02, -5.4207e-02, -7.4813e-02,  5.8680e-02,\n",
       "                      -7.1757e-02, -2.5133e-02, -1.6741e-01,  3.2808e-02, -1.4357e-01,\n",
       "                       1.2275e-02, -1.2486e-01, -2.0831e-01, -2.0693e-01, -1.9238e-01,\n",
       "                       6.8416e-04, -3.7625e-02,  1.5775e-01, -2.9557e-01,  1.5079e-02,\n",
       "                      -2.4266e-01,  1.1844e-01, -6.9486e-02,  2.6641e-03,  3.4346e-03,\n",
       "                      -9.3231e-02,  4.8079e-03,  5.9122e-02,  3.5269e-02, -8.1729e-02,\n",
       "                       1.3946e-01,  1.7267e-01,  3.9849e-02, -1.1883e-01,  3.1849e-02,\n",
       "                      -1.1615e-03,  6.4619e-02, -4.1171e-02,  2.9200e-02,  2.7530e-03,\n",
       "                      -8.2592e-02, -6.5782e-03, -1.1996e-01,  4.0153e-02,  7.2725e-02,\n",
       "                       6.1384e-02, -4.1019e-02, -7.5431e-02, -4.3521e-02, -4.4274e-03,\n",
       "                       6.9168e-02, -1.7259e-01,  7.3305e-02,  3.8626e-02, -1.2682e-02,\n",
       "                       5.5698e-02, -7.8255e-02, -1.1773e-01, -1.2911e-01, -1.6824e-02,\n",
       "                       4.6992e-02,  6.4188e-02, -1.1759e-01,  2.3320e-02, -6.0969e-02,\n",
       "                      -2.4465e-02, -9.2223e-02, -6.4077e-02,  3.8374e-02, -8.5890e-03,\n",
       "                       6.2314e-02,  1.2996e-03,  1.8072e-01, -2.8941e-02, -5.8140e-02,\n",
       "                       2.2264e-02, -1.6771e-01,  5.9324e-02, -6.6154e-02,  5.2068e-02,\n",
       "                       7.8536e-02,  1.1999e-02,  7.4143e-02,  1.7976e-02, -1.1332e-02,\n",
       "                      -1.1513e-01,  4.6876e-02, -5.5140e-03, -9.1258e-02,  1.1127e-01,\n",
       "                      -1.1349e-01,  2.5664e-02,  1.6427e-01,  2.2811e-02,  8.0228e-02,\n",
       "                       2.5055e-02,  7.8163e-03, -1.0593e-01, -3.3342e-02, -1.2970e-02,\n",
       "                       1.9276e-02,  3.1270e-02,  1.2221e-01,  8.1908e-03,  9.4970e-03,\n",
       "                      -3.1632e-02,  6.8619e-02,  9.4446e-03,  3.9358e-02, -8.1213e-03,\n",
       "                       2.5192e-02, -7.4977e-02,  6.1906e-02,  8.9908e-02,  1.0034e-02,\n",
       "                       5.7579e-02, -5.3366e-02,  5.4552e-02,  1.2547e-01,  5.9152e-02,\n",
       "                      -1.3215e-01,  1.2338e-01,  1.9878e-02, -4.3322e-03, -6.3109e-02,\n",
       "                      -9.9642e-02,  1.4064e-01, -3.4222e-02, -7.7981e-02, -1.3811e-01,\n",
       "                       1.6478e-02,  6.0255e-03, -7.3768e-02, -6.5878e-02, -7.5449e-02,\n",
       "                       1.5338e-02,  1.1608e-02, -5.3640e-02,  9.0274e-03,  8.1261e-02,\n",
       "                       2.8884e-02, -1.5475e-02,  1.3072e-01, -1.1854e-02,  5.4521e-02,\n",
       "                      -4.1375e-02, -2.2975e-02,  7.6230e-02,  1.0146e-01, -1.1915e-01,\n",
       "                      -1.0342e-01, -3.1944e-01, -2.5977e-01, -2.0847e-02, -1.0525e-01,\n",
       "                      -1.8495e-01, -5.9894e-02, -4.9551e-02,  1.6634e-02, -1.1065e-01,\n",
       "                       9.9596e-03, -2.7672e-01, -1.2371e-01, -4.4470e-02, -1.2120e-01,\n",
       "                      -2.3659e-01, -2.6133e-02, -2.1296e-01, -1.3620e-01, -2.2193e-01,\n",
       "                      -3.1970e-01, -2.7124e-02, -2.3204e-01, -1.6623e-01, -2.5181e-01,\n",
       "                      -2.3914e-01, -3.0939e-02, -2.2728e-01, -1.1943e-01, -1.5739e-01,\n",
       "                      -1.2133e-01, -1.0116e-01, -1.6865e-01, -1.9189e-01, -2.1120e-01,\n",
       "                      -2.0046e-01, -4.1631e-02, -4.8204e-02, -4.2624e-02, -2.0698e-01,\n",
       "                      -9.1380e-02, -1.4683e-01, -2.7089e-01, -1.2552e-01, -2.2782e-01,\n",
       "                      -1.3031e-01, -1.1986e-01, -1.1041e-01, -3.0380e-01, -1.0369e-01,\n",
       "                      -1.9383e-01, -8.3598e-02, -6.6977e-02, -1.9498e-01, -1.6504e-01,\n",
       "                      -1.6959e-01, -4.8371e-02, -1.9467e-01, -1.2848e-01, -1.6381e-01,\n",
       "                      -4.3918e-02,  4.1019e-03, -2.3409e-02,  7.2330e-02, -1.6496e-01,\n",
       "                      -1.4601e-01, -1.4807e-01,  1.0809e-01, -9.9641e-02, -1.4703e-01,\n",
       "                      -1.4639e-01, -1.6735e-01, -2.3079e-01, -3.0545e-01, -2.1204e-01,\n",
       "                      -1.6903e-01,  6.3733e-04, -1.5578e-01, -2.5383e-01, -1.5784e-01,\n",
       "                      -1.8892e-01, -8.4379e-02, -2.6505e-01, -9.6821e-02, -8.5637e-02,\n",
       "                      -1.3062e-01, -1.4897e-01, -7.4214e-02, -1.0065e-01, -1.4043e-01,\n",
       "                      -1.4619e-01, -8.0598e-02, -6.0845e-02, -1.0703e-01, -1.2874e-01,\n",
       "                      -8.2411e-02, -1.9812e-01, -1.4554e-01, -1.4504e-01, -1.1248e-01,\n",
       "                      -1.1942e-01, -7.1513e-02, -2.4258e-01, -8.9592e-02, -9.8587e-02,\n",
       "                      -1.8365e-01, -1.2910e-01, -4.8723e-02,  1.3065e-01, -8.4597e-02,\n",
       "                      -5.4506e-02, -1.3119e-01, -1.7029e-01,  1.7294e-02, -1.3365e-01,\n",
       "                      -1.7369e-01, -2.0164e-01, -2.3595e-01, -1.5630e-01, -3.1721e-01,\n",
       "                      -1.7086e-01, -4.3195e-02,  3.2961e-02,  5.0023e-02,  2.7505e-03,\n",
       "                       1.6996e-01, -1.4449e-02])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0',\n",
       "              tensor([-0.2243, -0.1138, -0.0440, -0.2271, -0.0337, -0.2077, -0.1327, -0.2526,\n",
       "                      -0.0652, -0.1116, -0.0298,  0.0449, -0.0162, -0.2252, -0.0152, -0.0621,\n",
       "                      -0.1234, -0.2049, -0.1148, -0.1117, -0.1491, -0.0509, -0.1841, -0.1831,\n",
       "                      -0.1172, -0.1610, -0.1745, -0.1648, -0.1736, -0.1268, -0.2895, -0.1982,\n",
       "                      -0.1657, -0.2915, -0.1066, -0.1731, -0.0992, -0.0419, -0.0928, -0.0776,\n",
       "                      -0.1934, -0.1858, -0.0415, -0.0466, -0.1431, -0.2604, -0.1934, -0.0195,\n",
       "                      -0.1183, -0.2350,  0.0733, -0.2595, -0.1060, -0.1989, -0.1892, -0.2122,\n",
       "                      -0.1889, -0.2176, -0.1468, -0.2314, -0.1263, -0.1064, -0.0378, -0.1900,\n",
       "                       0.0138, -0.0957, -0.0655, -0.1897, -0.0131, -0.0811, -0.0501, -0.1811,\n",
       "                      -0.0281, -0.1866, -0.2598, -0.1908, -0.1915, -0.0162,  0.0486, -0.2500,\n",
       "                      -0.0572, -0.0530, -0.1546, -0.0671, -0.0788, -0.0724, -0.1637, -0.1197,\n",
       "                      -0.1312, -0.0140, -0.1741, -0.1688, -0.1390, -0.0993, -0.1532, -0.1128,\n",
       "                      -0.3273, -0.2260, -0.0886, -0.0981, -0.1604, -0.1714, -0.1800, -0.1347,\n",
       "                      -0.0960, -0.1792, -0.0312, -0.0020, -0.1096, -0.0859, -0.0512, -0.1024,\n",
       "                      -0.1673, -0.1902, -0.1273, -0.1568, -0.1993, -0.0513, -0.1451, -0.0766,\n",
       "                      -0.2250, -0.1460, -0.1866, -0.0771, -0.1945, -0.0349,  0.1958, -0.1330,\n",
       "                      -0.0251, -0.0391, -0.1584, -0.2154,  0.0557, -0.0249, -0.0470, -0.1127,\n",
       "                      -0.2110, -0.1366, -0.0896, -0.0218, -0.1285, -0.0780, -0.0546,  0.0662,\n",
       "                      -0.1332, -0.0811, -0.1610, -0.1315, -0.2425, -0.1422, -0.1530, -0.2527,\n",
       "                      -0.1269, -0.1876, -0.1768,  0.0873, -0.0965,  0.0454, -0.2936, -0.0939,\n",
       "                       0.0076,  0.0482, -0.1435, -0.1812, -0.0106,  0.1142,  0.1398, -0.1732,\n",
       "                      -0.0780, -0.2195, -0.0325,  0.0300,  0.0184, -0.1835, -0.2511,  0.1160,\n",
       "                      -0.1488, -0.1626, -0.0259, -0.2266,  0.0349, -0.1478, -0.0470, -0.1185,\n",
       "                      -0.0977,  0.0354, -0.0899, -0.0966, -0.1002, -0.1137,  0.1193, -0.0369,\n",
       "                      -0.0326, -0.0501, -0.0426, -0.0575, -0.1084, -0.0182, -0.0361, -0.2859,\n",
       "                       0.1127, -0.1377, -0.0344, -0.0782, -0.1661,  0.1523,  0.0026, -0.0542,\n",
       "                       0.0286, -0.1832,  0.0801,  0.0260, -0.0381, -0.0936, -0.1483,  0.0219,\n",
       "                      -0.1073, -0.0223, -0.2175, -0.0857, -0.0583,  0.0141, -0.1060, -0.0189,\n",
       "                      -0.1973, -0.1905, -0.0824, -0.1355,  0.0242,  0.1118,  0.0540, -0.2221,\n",
       "                       0.0539,  0.0207, -0.0365, -0.0215,  0.0756, -0.0927, -0.0476, -0.0253,\n",
       "                      -0.0433, -0.0371, -0.0138, -0.0237, -0.0727, -0.0452, -0.1318, -0.2338,\n",
       "                      -0.1036, -0.1960,  0.0029, -0.1073,  0.0869, -0.3284,  0.0079, -0.1467,\n",
       "                      -0.0102,  0.0268,  0.0494, -0.0266,  0.0872, -0.0616, -0.0352, -0.0138,\n",
       "                      -0.0185,  0.0622, -0.0113, -0.0886, -0.0193,  0.1013,  0.0576,  0.0874,\n",
       "                       0.0072,  0.0655,  0.1408, -0.0625, -0.0225,  0.0056,  0.0146,  0.0509,\n",
       "                      -0.0424, -0.0249,  0.0836, -0.0806, -0.0232, -0.0525,  0.0019,  0.0789,\n",
       "                       0.0289,  0.0503,  0.0117, -0.0246,  0.0322, -0.1729,  0.0710, -0.0533,\n",
       "                      -0.0012,  0.0576, -0.0910,  0.0478, -0.0790, -0.0090,  0.0372, -0.0063,\n",
       "                       0.0463,  0.1572, -0.0432, -0.0528,  0.0518, -0.0081, -0.1429, -0.1618,\n",
       "                      -0.0389, -0.0822,  0.0096,  0.0075, -0.0028, -0.0179, -0.0381,  0.0921,\n",
       "                      -0.1424,  0.0481,  0.0598, -0.0528,  0.0239, -0.0197,  0.0057,  0.0174,\n",
       "                       0.0202, -0.0583,  0.0310,  0.0091,  0.0732,  0.0257,  0.0538, -0.0496,\n",
       "                       0.0747, -0.0618,  0.0429, -0.0101, -0.0941,  0.0480, -0.0560,  0.1725,\n",
       "                       0.0539,  0.1306,  0.0308, -0.0897,  0.0265, -0.1215, -0.0120,  0.0038,\n",
       "                       0.0403,  0.0075, -0.1094,  0.0204,  0.0112, -0.0295,  0.0444,  0.0687,\n",
       "                       0.0537,  0.0647,  0.0635,  0.0044,  0.1743, -0.0039,  0.0319,  0.0533,\n",
       "                      -0.0413,  0.0688, -0.0364,  0.0074,  0.1139,  0.1181,  0.1350,  0.0180,\n",
       "                      -0.0497, -0.0503, -0.1352,  0.0115,  0.0373, -0.0801,  0.1400, -0.0582,\n",
       "                      -0.0779, -0.1869, -0.1441, -0.1766, -0.1604, -0.1163, -0.1872, -0.1495,\n",
       "                      -0.0963,  0.0586, -0.0129,  0.0545, -0.1709, -0.2002, -0.0693, -0.2525,\n",
       "                      -0.1644, -0.0959, -0.0849, -0.1361, -0.1692, -0.2889, -0.1354, -0.2184,\n",
       "                      -0.2136, -0.1436, -0.2093, -0.0265, -0.1766,  0.1129, -0.2638, -0.1479,\n",
       "                      -0.0287,  0.0300, -0.0687, -0.2513, -0.1644,  0.0221, -0.0851, -0.1488,\n",
       "                      -0.1955, -0.2539, -0.0645, -0.2229, -0.2891, -0.2693, -0.1369, -0.1502,\n",
       "                      -0.1559, -0.3022,  0.0128, -0.1688, -0.1076, -0.1133, -0.1924, -0.1054,\n",
       "                      -0.2059, -0.1270, -0.1114, -0.1490, -0.2127, -0.1500,  0.0258, -0.1006,\n",
       "                       0.1429, -0.0741, -0.1311, -0.2499,  0.0448, -0.0409, -0.0710, -0.0782,\n",
       "                      -0.0437, -0.0795, -0.2063, -0.2743, -0.0229,  0.0071, -0.0735, -0.2157,\n",
       "                      -0.0760, -0.1898,  0.0037, -0.1947, -0.0304, -0.0315, -0.0927, -0.1145,\n",
       "                      -0.2013, -0.1771, -0.1610, -0.0266, -0.1443, -0.1520,  0.0249, -0.0444,\n",
       "                      -0.2309, -0.0989, -0.1182, -0.0761, -0.1996, -0.1009, -0.1600, -0.3331,\n",
       "                      -0.0430, -0.0219, -0.1104, -0.1871, -0.2079,  0.0583, -0.0656, -0.1417,\n",
       "                      -0.1743, -0.0940,  0.0137, -0.0179, -0.1806, -0.0871, -0.2261, -0.1541,\n",
       "                      -0.1440, -0.2496, -0.0119, -0.1677, -0.0620,  0.0774,  0.1372, -0.1419])),\n",
       "             ('bilstm_oov_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.4073, -0.0781, -0.3088,  ..., -0.2006, -0.1600,  0.0642],\n",
       "                      [ 0.0668,  0.1498, -0.3204,  ...,  0.0816,  0.3512, -0.0159],\n",
       "                      [-0.2935, -0.2093, -0.0276,  ...,  0.5115, -0.1076, -0.2996],\n",
       "                      ...,\n",
       "                      [-0.5922, -0.8132,  0.1628,  ..., -0.1720, -0.0263, -0.0062],\n",
       "                      [-0.1181,  0.4240, -0.0928,  ..., -0.1635,  0.0023,  0.3789],\n",
       "                      [ 0.0425, -0.2201, -0.4310,  ..., -0.3667,  0.0123, -0.1409]])),\n",
       "             ('bilstm_oov_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0773, -0.1319, -0.1620,  ...,  0.3111, -0.0837,  0.0960],\n",
       "                      [ 0.1273,  0.2797,  0.1714,  ..., -0.0199,  0.1709,  0.4082],\n",
       "                      [ 0.0770,  0.0249, -0.0523,  ..., -0.0773,  0.1363,  0.1501],\n",
       "                      ...,\n",
       "                      [ 0.1251,  0.0454,  0.0735,  ...,  0.0071, -0.2613,  0.3278],\n",
       "                      [ 0.2284, -0.0436,  0.0234,  ..., -0.0383, -0.0713, -0.0305],\n",
       "                      [ 0.0044, -0.1780,  0.1834,  ...,  0.3649,  0.0431, -0.2401]])),\n",
       "             ('bilstm_oov_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([ 0.0607,  0.2202, -0.0889,  0.3657,  0.2064, -0.0685,  0.2799,  0.1539,\n",
       "                       0.1953,  0.2835,  0.1057,  0.0910,  0.1669,  0.2342, -0.1134,  0.1021,\n",
       "                      -0.0381,  0.0023, -0.0037,  0.3583,  0.2652,  0.0116,  0.1488,  0.1320,\n",
       "                      -0.0640,  0.1826,  0.2906, -0.0467, -0.0389,  0.1781,  0.1971,  0.1978,\n",
       "                       0.0396, -0.1615,  0.0723,  0.2761, -0.2327,  0.2378,  0.1421,  0.0362,\n",
       "                       0.0478,  0.0660, -0.0121,  0.2333,  0.1677,  0.2359,  0.2438,  0.0604,\n",
       "                       0.1110,  0.0625,  0.1230,  0.1076,  0.2696,  0.1110,  0.1521,  0.0691,\n",
       "                       0.0364,  0.1599,  0.1417,  0.1405,  0.1194,  0.1092, -0.0339, -0.0220,\n",
       "                       0.1014,  0.1744, -0.0356,  0.0622,  0.1244,  0.2216,  0.1370,  0.1788,\n",
       "                       0.1624,  0.1058,  0.0797,  0.1994,  0.2247,  0.0603,  0.1160,  0.2454,\n",
       "                       0.3485,  0.0619,  0.0462,  0.2677,  0.3067,  0.0117, -0.0520,  0.0045,\n",
       "                       0.3750,  0.0768,  0.0361,  0.1781,  0.1576,  0.2944,  0.1424,  0.0798,\n",
       "                       0.0783,  0.3580,  0.1717,  0.1629,  0.1746,  0.0474,  0.1178,  0.1680,\n",
       "                      -0.0715,  0.2085,  0.2001, -0.0035,  0.2036,  0.0667, -0.0010,  0.0632,\n",
       "                       0.1614,  0.1133,  0.0263, -0.1429,  0.2570,  0.2682,  0.1613,  0.0423,\n",
       "                       0.0531,  0.1359,  0.1599,  0.0864,  0.1804,  0.3116,  0.1429,  0.0089,\n",
       "                       0.1576,  0.1161,  0.0346,  0.0487,  0.0704,  0.0040, -0.0984,  0.0763,\n",
       "                      -0.0416, -0.0506,  0.2203,  0.1109,  0.0119,  0.1073,  0.1089,  0.1492,\n",
       "                       0.0114,  0.1180,  0.1526,  0.0962,  0.0059,  0.1425, -0.0270,  0.1013,\n",
       "                      -0.0390,  0.0403, -0.0308,  0.0519,  0.1229,  0.1388, -0.0157, -0.1375,\n",
       "                       0.0128,  0.0472,  0.0316, -0.0651, -0.1342,  0.1717,  0.0708,  0.0550,\n",
       "                      -0.1177, -0.1485,  0.0700,  0.0832, -0.0219, -0.1225,  0.0650,  0.0830,\n",
       "                       0.0554,  0.0738,  0.0845,  0.0193,  0.0812, -0.0406, -0.0634,  0.1937,\n",
       "                       0.0547,  0.0369,  0.1111, -0.0083,  0.0053,  0.0304,  0.0252,  0.0605,\n",
       "                       0.0673, -0.0101, -0.0005, -0.0490,  0.0628,  0.0578,  0.0552,  0.0429,\n",
       "                       0.1644, -0.0195,  0.2482,  0.0105, -0.0861, -0.0212,  0.0791, -0.0909,\n",
       "                       0.0297,  0.1448,  0.0779,  0.0252,  0.1973,  0.0574,  0.0438,  0.0111,\n",
       "                      -0.0866,  0.0465, -0.0711,  0.1590,  0.1144,  0.1123,  0.0012,  0.1470,\n",
       "                       0.0882, -0.0469,  0.0275,  0.0491,  0.1588,  0.0831,  0.0200,  0.1313,\n",
       "                       0.1629,  0.0543,  0.0523,  0.0233,  0.1315,  0.0325,  0.0204,  0.0980,\n",
       "                       0.0910,  0.1670, -0.1421,  0.0430,  0.2241, -0.2383,  0.0241,  0.0062,\n",
       "                       0.0143,  0.1046,  0.2215,  0.0884, -0.0562, -0.0224,  0.2039,  0.0950,\n",
       "                      -0.0219,  0.0394,  0.0065, -0.0726, -0.1034, -0.0770,  0.0066, -0.0593,\n",
       "                       0.0638, -0.0151, -0.0226, -0.2185,  0.0774, -0.0271,  0.0128,  0.1160,\n",
       "                      -0.1024, -0.0431,  0.0651, -0.1296, -0.0362, -0.0552, -0.1026,  0.0047,\n",
       "                      -0.0699, -0.0264, -0.0448, -0.1970, -0.1234, -0.0911,  0.0127,  0.0248,\n",
       "                      -0.0049, -0.0628,  0.0828,  0.0851, -0.1184, -0.0419, -0.0812, -0.1268,\n",
       "                       0.1233, -0.0422, -0.0408,  0.0749,  0.0311, -0.0158, -0.0037,  0.0711,\n",
       "                      -0.0241,  0.0129, -0.1330, -0.0537, -0.0703, -0.0392, -0.1205,  0.0416,\n",
       "                       0.0425, -0.1246, -0.0610, -0.0281,  0.1380,  0.0709,  0.0413,  0.0225,\n",
       "                       0.0642, -0.0680, -0.0766,  0.0045, -0.0028,  0.0355, -0.0201, -0.1344,\n",
       "                       0.0212, -0.1378, -0.0533, -0.0175, -0.0650,  0.0320, -0.0034,  0.1623,\n",
       "                      -0.0333, -0.0130,  0.0048, -0.0047,  0.0612,  0.0689, -0.0906,  0.0833,\n",
       "                      -0.0352, -0.1672, -0.0097,  0.0180, -0.1120,  0.1028, -0.0680,  0.0466,\n",
       "                      -0.0582,  0.0051,  0.0307, -0.0812,  0.0044,  0.0484, -0.0742,  0.0282,\n",
       "                       0.0246,  0.0198, -0.0396, -0.0312,  0.1030, -0.0332,  0.0010, -0.1849,\n",
       "                       0.0105,  0.0167,  0.0593,  0.0140, -0.1533, -0.0685, -0.0633, -0.1436,\n",
       "                      -0.0096,  0.0563,  0.0191, -0.0229, -0.0509,  0.1026,  0.0118, -0.0049,\n",
       "                       0.1392,  0.2109, -0.0404,  0.1848,  0.1845,  0.1295,  0.2466,  0.0235,\n",
       "                       0.2382,  0.0701,  0.1510,  0.2645,  0.0270,  0.1743,  0.1969,  0.1948,\n",
       "                       0.0376,  0.0273,  0.1955,  0.0485,  0.1248,  0.0689,  0.0677,  0.3042,\n",
       "                       0.1635,  0.0325,  0.1594,  0.1864,  0.1965,  0.0139,  0.1432,  0.2120,\n",
       "                       0.0121, -0.0800,  0.2582,  0.2819, -0.1719,  0.2312,  0.1939, -0.0944,\n",
       "                      -0.0298,  0.1390,  0.0622,  0.1339,  0.0127,  0.0980, -0.0774,  0.1973,\n",
       "                       0.1125,  0.2973,  0.0985,  0.0977,  0.3601,  0.1108,  0.2177,  0.2535,\n",
       "                       0.0071,  0.0061,  0.2561,  0.2940, -0.0645,  0.1293,  0.0100, -0.0451,\n",
       "                       0.0363,  0.1643,  0.2531, -0.0987,  0.0318,  0.1942,  0.3661,  0.0490,\n",
       "                       0.0968, -0.0696,  0.2144,  0.1362,  0.3852,  0.1064,  0.1966,  0.2028,\n",
       "                       0.3950,  0.0818,  0.0532,  0.3065,  0.3066,  0.2339, -0.0108,  0.1599,\n",
       "                       0.2559,  0.0874,  0.1471,  0.0481, -0.0524,  0.1901,  0.0257,  0.2145,\n",
       "                       0.2179,  0.1684,  0.3422,  0.2850,  0.2595,  0.0641,  0.0233,  0.1824,\n",
       "                      -0.0083,  0.2729,  0.0740,  0.0801,  0.3124,  0.0594,  0.2555, -0.0798,\n",
       "                       0.2247, -0.0416,  0.2002,  0.1419,  0.1299,  0.4274,  0.1785,  0.2094,\n",
       "                      -0.0661,  0.2092,  0.3163,  0.0246,  0.2037,  0.1565, -0.0230,  0.0689])),\n",
       "             ('bilstm_oov_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-9.2587e-02,  2.3787e-01,  3.5173e-02,  3.1473e-01,  1.1718e-01,\n",
       "                       7.8955e-02,  1.9590e-01,  9.5716e-02,  3.4066e-01,  2.1308e-01,\n",
       "                       2.6756e-01,  1.3652e-01,  1.4034e-01,  9.6803e-02,  5.6181e-02,\n",
       "                       1.8344e-01, -1.0064e-01, -1.0094e-02,  1.5837e-01,  2.7483e-01,\n",
       "                       3.8755e-02,  1.1858e-02,  1.3357e-01,  2.7179e-01,  1.3281e-01,\n",
       "                       1.4947e-01,  2.1344e-01, -2.7391e-02, -4.0877e-02,  2.6098e-01,\n",
       "                      -3.0670e-02,  2.3115e-01,  1.5847e-01, -3.2687e-02,  1.0771e-01,\n",
       "                       2.8142e-01, -1.0960e-01,  1.2741e-01,  2.3036e-01, -2.7157e-02,\n",
       "                       1.0442e-01,  1.7493e-01,  2.5275e-02,  1.8896e-01, -1.9995e-02,\n",
       "                       7.2710e-02,  1.0664e-01, -1.9988e-02,  1.7951e-01,  1.0127e-01,\n",
       "                       2.6304e-02,  1.6428e-01,  3.3891e-01,  1.0465e-01,  1.9233e-01,\n",
       "                       2.8126e-01,  2.6140e-02,  2.0261e-01,  3.2899e-01,  2.8551e-01,\n",
       "                      -1.0117e-01,  2.4306e-01,  3.1112e-02,  1.4157e-01,  1.4924e-01,\n",
       "                       3.2553e-01,  1.6827e-02,  2.0292e-02,  1.6115e-01,  1.7151e-01,\n",
       "                       2.3254e-01,  2.4033e-01,  7.3056e-02,  1.9269e-01,  1.5678e-01,\n",
       "                       4.0330e-01,  3.2028e-01,  1.6011e-01,  1.9226e-01,  2.1077e-01,\n",
       "                       5.0522e-01,  6.4514e-02,  8.2637e-02,  3.6468e-01,  2.2999e-01,\n",
       "                       1.0126e-01, -8.7452e-03, -2.5729e-02,  3.0263e-01,  1.3910e-01,\n",
       "                      -2.6881e-02,  9.7530e-03,  5.8424e-02,  1.8535e-01,  1.0680e-01,\n",
       "                       1.7525e-01,  2.5168e-01,  3.6271e-01,  1.4346e-01, -2.8573e-02,\n",
       "                       2.0358e-01,  1.8919e-02,  9.7158e-02,  8.0937e-02,  1.1563e-01,\n",
       "                       2.8276e-01,  2.0008e-01,  2.0881e-01,  1.2431e-01,  1.0955e-01,\n",
       "                      -7.7427e-02,  1.1786e-01,  1.2280e-01,  1.3084e-01,  1.1826e-01,\n",
       "                      -3.2140e-02,  2.3137e-01,  4.2558e-01,  1.2680e-01,  1.1009e-01,\n",
       "                      -2.0926e-02,  2.7665e-02,  2.0187e-01,  1.3148e-01,  2.5615e-01,\n",
       "                       3.3924e-01,  3.0159e-02,  1.1356e-01,  4.9197e-02,  8.6954e-02,\n",
       "                       9.3888e-02,  9.3131e-02,  5.2954e-02,  2.6578e-02, -1.5930e-01,\n",
       "                       5.9429e-02, -1.9425e-02,  9.1265e-02, -4.5576e-02,  8.3312e-02,\n",
       "                       1.1469e-01,  1.2333e-01,  3.6053e-02,  2.1715e-02,  9.1538e-02,\n",
       "                       9.6153e-02,  1.2065e-01,  8.8195e-02,  1.8035e-02,  9.4570e-02,\n",
       "                       1.0631e-01,  8.8178e-02, -4.4438e-02,  4.1941e-03,  1.2213e-02,\n",
       "                      -1.3717e-01,  6.5076e-02,  7.6695e-03,  4.3380e-02, -1.1293e-01,\n",
       "                      -7.5093e-02,  3.4162e-02,  1.2439e-02, -1.5347e-01, -5.4973e-02,\n",
       "                       1.2530e-01, -3.3714e-02,  9.8331e-02,  3.9795e-02, -7.3456e-02,\n",
       "                       1.0623e-01, -3.9347e-02,  1.2344e-03, -5.3515e-02,  1.2299e-01,\n",
       "                       9.5999e-02,  2.7028e-02, -5.6597e-02, -2.1675e-02, -2.0462e-02,\n",
       "                      -4.3629e-02, -2.8422e-02, -2.0324e-02,  8.3078e-02,  1.3202e-02,\n",
       "                      -4.4048e-02,  1.0656e-01, -6.1280e-02,  5.6849e-02,  1.4005e-03,\n",
       "                      -8.9951e-02,  3.5521e-02,  1.2978e-01, -1.1888e-01, -3.8423e-02,\n",
       "                       8.3785e-02,  7.8915e-02, -6.7598e-02,  1.2763e-01,  1.4226e-01,\n",
       "                      -6.5983e-02,  1.1275e-01,  2.0739e-01, -7.1223e-02,  4.5944e-02,\n",
       "                       4.2488e-02,  1.7027e-01, -6.9411e-02,  5.6548e-02,  1.5645e-01,\n",
       "                       6.4235e-02,  9.1883e-02, -1.8885e-02,  2.5397e-01,  3.7295e-02,\n",
       "                      -7.4945e-03,  1.2177e-02,  7.3532e-02,  6.9910e-02,  1.7129e-01,\n",
       "                       1.6028e-01,  3.9269e-02,  9.8754e-02, -7.9636e-02,  1.9317e-01,\n",
       "                       9.6373e-02, -9.2329e-03,  3.6751e-02,  6.1169e-02, -2.2583e-02,\n",
       "                      -4.7482e-02, -6.4944e-03,  3.2344e-02,  1.1003e-01,  2.9567e-02,\n",
       "                      -2.5434e-02, -6.2038e-02,  7.9548e-02, -2.6012e-02,  6.7812e-02,\n",
       "                       7.7617e-02, -5.3911e-02, -8.9249e-02,  1.9892e-01,  7.9484e-02,\n",
       "                      -2.7727e-01,  4.3933e-02,  6.9420e-02,  2.5293e-02,  6.4842e-02,\n",
       "                       9.0736e-02,  6.3265e-02, -6.9156e-02, -9.0252e-02,  2.0262e-01,\n",
       "                       1.5780e-02,  2.0189e-02, -1.7955e-02,  5.3982e-02, -1.5058e-01,\n",
       "                      -1.5850e-01, -1.5208e-02, -7.0965e-04, -5.2012e-02, -2.8784e-03,\n",
       "                       6.1894e-02, -1.5304e-01,  4.1472e-02, -5.3503e-02, -1.8546e-03,\n",
       "                      -3.6759e-02,  3.3212e-02,  1.5936e-02,  3.1602e-02,  1.0161e-01,\n",
       "                      -5.6685e-02, -5.0270e-02, -1.6811e-02, -1.5514e-01, -2.3597e-02,\n",
       "                       1.1543e-01,  6.0724e-02, -3.4923e-02, -8.0641e-02, -3.4448e-02,\n",
       "                       1.1059e-01, -9.8051e-02,  1.7647e-01, -7.3496e-02,  1.3114e-02,\n",
       "                       1.8602e-02,  1.8789e-06, -1.5202e-02,  1.1571e-01,  3.7162e-02,\n",
       "                       8.7443e-03, -1.8936e-02, -5.2305e-02,  3.7919e-03, -1.3476e-03,\n",
       "                      -2.7290e-02,  3.0204e-02,  1.2036e-02,  7.4479e-02,  6.8218e-02,\n",
       "                       1.0607e-02, -1.7104e-01, -5.2935e-02,  4.0571e-04, -1.1838e-01,\n",
       "                      -5.7481e-02,  1.0508e-03,  6.3964e-02, -3.1605e-02,  8.1229e-02,\n",
       "                       7.8976e-02,  1.8444e-01,  1.9279e-01, -9.5646e-03,  1.8880e-02,\n",
       "                      -5.6226e-02, -1.1531e-01, -5.1048e-02, -1.2082e-01,  6.1784e-02,\n",
       "                       1.2830e-02,  1.1454e-02, -3.8871e-02,  2.6023e-02,  4.5139e-02,\n",
       "                      -7.4639e-02, -7.1600e-02, -2.3292e-02,  7.7052e-02, -1.5380e-01,\n",
       "                       2.3060e-02,  1.4403e-02,  8.5452e-02,  1.9447e-01,  6.3940e-02,\n",
       "                      -8.6033e-02,  5.4692e-02,  3.4073e-02,  8.8235e-02, -7.3440e-02,\n",
       "                      -9.9106e-02, -2.1483e-02,  2.6288e-03, -8.8267e-02,  2.0453e-02,\n",
       "                       2.6479e-02, -8.9583e-02,  4.7292e-02, -1.8436e-02, -1.5882e-02,\n",
       "                      -4.5906e-03,  6.4422e-02,  2.0041e-02, -7.6287e-02, -6.6204e-02,\n",
       "                       6.0638e-02, -8.6090e-02,  1.3629e-02,  1.0450e-01, -9.9233e-02,\n",
       "                      -2.3744e-02, -3.0814e-02, -8.6111e-05,  8.2785e-02, -1.2158e-01,\n",
       "                       1.4411e-01, -1.5571e-01,  2.4669e-02, -5.4205e-02, -2.4283e-02,\n",
       "                       4.6017e-02, -2.0647e-01,  2.8911e-03,  4.7247e-03,  3.8928e-02,\n",
       "                       4.7190e-04, -1.2333e-01,  7.9431e-03,  3.2619e-02, -1.4687e-02,\n",
       "                       2.5841e-01, -4.8121e-02,  1.8116e-01,  8.3946e-04,  1.3239e-01,\n",
       "                       4.3422e-01,  8.5298e-02,  2.1760e-01,  8.3009e-02,  1.9594e-01,\n",
       "                       1.0897e-01,  1.4565e-02,  1.6986e-01,  7.1857e-02,  2.8804e-02,\n",
       "                       8.9130e-02, -4.9824e-03,  1.4663e-01,  1.2790e-01,  3.9202e-02,\n",
       "                       5.2511e-02,  2.2686e-01,  3.2396e-01,  1.0388e-01,  1.1035e-01,\n",
       "                       1.3053e-01,  1.4591e-01,  1.3891e-01,  1.6640e-02,  2.7427e-01,\n",
       "                       1.4265e-01,  4.6130e-02, -2.0429e-02,  2.1573e-01,  2.6456e-01,\n",
       "                      -4.3599e-02,  1.7186e-02,  2.2315e-01, -6.1358e-02, -2.2090e-02,\n",
       "                       1.4168e-01,  1.0999e-01,  1.5730e-01,  4.9863e-02,  1.6391e-01,\n",
       "                      -7.1830e-02,  1.5983e-01,  4.4077e-02,  3.0308e-01,  1.3061e-01,\n",
       "                       6.3459e-02,  3.1403e-01,  1.8588e-01,  2.8387e-01,  3.1353e-01,\n",
       "                       6.1004e-02,  3.7691e-02,  1.2655e-01,  1.3664e-01,  2.6247e-02,\n",
       "                       2.8041e-02,  6.6091e-02, -5.8943e-03,  8.6503e-02,  1.8674e-01,\n",
       "                       1.3337e-01, -9.1207e-02, -1.3833e-02,  2.3174e-01,  3.7111e-01,\n",
       "                       7.4263e-02, -3.4722e-02,  3.3066e-02,  8.5941e-02,  2.0254e-01,\n",
       "                       4.3592e-01,  7.6549e-02,  1.1254e-01,  7.1225e-02,  3.6565e-01,\n",
       "                       1.4162e-01, -8.3443e-02,  2.4803e-01,  3.1050e-01,  3.9691e-01,\n",
       "                       1.1593e-02,  1.7372e-01,  2.2370e-01,  1.7631e-01,  1.8680e-01,\n",
       "                      -2.2536e-02, -4.4812e-02,  2.5190e-01, -2.9050e-02,  7.3008e-02,\n",
       "                       8.1944e-02,  1.6207e-01,  8.7630e-02,  2.0657e-01,  2.2195e-01,\n",
       "                       1.3805e-01,  2.9270e-02,  2.5532e-01,  1.2420e-01,  2.7202e-01,\n",
       "                       2.0299e-01,  1.1929e-01,  2.2810e-01,  3.0765e-02,  1.6599e-01,\n",
       "                      -3.3183e-02,  1.2333e-01,  1.1120e-01,  1.9007e-01,  2.3797e-01,\n",
       "                       1.7805e-01,  3.7740e-01,  1.0995e-01,  2.3559e-01,  4.9230e-02,\n",
       "                       2.6266e-01,  2.0461e-01,  1.6657e-02,  2.4108e-01,  1.7168e-01,\n",
       "                       1.6983e-01,  1.3316e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0',\n",
       "              tensor([[-0.1708, -0.1382,  0.0808,  ...,  0.0387,  0.4736,  0.0581],\n",
       "                      [-0.0348, -0.2150, -0.0144,  ...,  0.1127, -0.1460,  0.1860],\n",
       "                      [ 0.2676, -0.2137,  0.0602,  ..., -0.0475, -0.0413, -0.6278],\n",
       "                      ...,\n",
       "                      [ 0.2737, -0.2935,  0.4104,  ..., -0.1883,  0.2279, -0.4752],\n",
       "                      [ 0.1751,  0.3062,  0.3987,  ..., -0.0811,  0.1857,  0.2460],\n",
       "                      [ 0.1255, -0.2119,  0.1372,  ...,  0.1695,  0.2542, -0.5099]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0',\n",
       "              tensor([[ 0.3179,  0.2726, -0.2294,  ..., -0.0513,  0.0289,  0.0924],\n",
       "                      [-0.0630,  0.0046,  0.0489,  ...,  0.1737, -0.0682,  0.0034],\n",
       "                      [ 0.2465, -0.0065, -0.0421,  ...,  0.0802, -0.4537,  0.1573],\n",
       "                      ...,\n",
       "                      [ 0.2964,  0.2447,  0.0712,  ...,  0.0442, -0.1673,  0.0348],\n",
       "                      [-0.0774,  0.0462,  0.2990,  ...,  0.1649, -0.2883, -0.0388],\n",
       "                      [ 0.0980,  0.1227,  0.0494,  ..., -0.2425, -0.2125,  0.0573]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0',\n",
       "              tensor([-0.1700, -0.0796, -0.3537, -0.1270,  0.0242, -0.1805, -0.1306, -0.2399,\n",
       "                      -0.3003, -0.1404, -0.1096, -0.0731, -0.0920, -0.1673, -0.2635, -0.2108,\n",
       "                      -0.1267, -0.2557, -0.0362, -0.2012, -0.1035, -0.2020, -0.1845, -0.1200,\n",
       "                      -0.2230, -0.0763, -0.2509, -0.2036, -0.1219, -0.1005, -0.1176, -0.1991,\n",
       "                      -0.1561, -0.1563, -0.2075, -0.2597, -0.1178, -0.0235, -0.2496, -0.2831,\n",
       "                      -0.1929, -0.0522, -0.1696, -0.1924, -0.0807, -0.1702, -0.1074, -0.3015,\n",
       "                      -0.0235, -0.1970, -0.0711, -0.1325, -0.2554, -0.1651, -0.3140, -0.1945,\n",
       "                      -0.0856, -0.1647, -0.1134, -0.2066, -0.1880, -0.1598, -0.1574, -0.1989,\n",
       "                      -0.1427, -0.1651, -0.0415, -0.0602, -0.1951, -0.2372, -0.0791, -0.1301,\n",
       "                      -0.1137, -0.1769, -0.1997, -0.1134, -0.1379, -0.1209, -0.1826, -0.0950,\n",
       "                      -0.0745, -0.2413, -0.2821, -0.1144, -0.2478, -0.1620, -0.3201, -0.3188,\n",
       "                      -0.1923, -0.2250, -0.1565, -0.1206, -0.1662, -0.2314, -0.1858, -0.1484,\n",
       "                      -0.1980, -0.0356, -0.2334, -0.2679, -0.1706, -0.1842, -0.2110, -0.3240,\n",
       "                      -0.0648, -0.1295, -0.2249, -0.0279, -0.2555, -0.1737, -0.2194, -0.1578,\n",
       "                      -0.1983, -0.1281, -0.1257, -0.1526, -0.1500, -0.1742, -0.1381, -0.0735,\n",
       "                      -0.1701, -0.3074, -0.3419, -0.2022, -0.1831, -0.2354, -0.1253, -0.2113,\n",
       "                      -0.0516, -0.0177, -0.1658, -0.0816,  0.0462, -0.2614, -0.1235, -0.2152,\n",
       "                      -0.1415, -0.1793, -0.0921, -0.2180, -0.0328, -0.2662, -0.1130, -0.1718,\n",
       "                      -0.1407, -0.2665, -0.1454, -0.0912, -0.0822, -0.1235, -0.2516, -0.1731,\n",
       "                       0.0127, -0.1238, -0.2245, -0.1767, -0.1385, -0.0304, -0.1388, -0.2238,\n",
       "                      -0.1728, -0.0804, -0.2023, -0.0815, -0.1697, -0.1189, -0.2036, -0.2460,\n",
       "                      -0.1740, -0.1363, -0.0190, -0.1943, -0.0438, -0.2275, -0.1717, -0.2587,\n",
       "                      -0.1454, -0.2157, -0.0544, -0.2806, -0.1825, -0.2273, -0.0051, -0.0765,\n",
       "                      -0.1114,  0.0283, -0.0904, -0.2780, -0.1067, -0.1398, -0.0936, -0.1936,\n",
       "                      -0.1624, -0.2161, -0.1267, -0.1205, -0.1581, -0.1154, -0.1046, -0.0692,\n",
       "                      -0.1321, -0.1947, -0.1875, -0.1316, -0.2297, -0.1419, -0.1419, -0.2684,\n",
       "                      -0.1294, -0.2727, -0.1581, -0.2890, -0.2171, -0.1665, -0.1463, -0.1419,\n",
       "                      -0.1499, -0.1547, -0.2240, -0.3155, -0.1751, -0.1039, -0.1215, -0.1164,\n",
       "                       0.0194, -0.1020, -0.1619, -0.0821, -0.1670, -0.2512, -0.1964, -0.1189,\n",
       "                      -0.1873, -0.0958, -0.1046, -0.0661, -0.1580, -0.1442, -0.1658, -0.0749,\n",
       "                      -0.1242, -0.3009, -0.0641, -0.1677, -0.2316, -0.0908, -0.1387, -0.0544,\n",
       "                      -0.1853, -0.1856, -0.1912, -0.1755, -0.2632, -0.2303, -0.2015, -0.1097,\n",
       "                       0.0086, -0.0698,  0.0259, -0.0137,  0.0497, -0.0153,  0.0937,  0.0394,\n",
       "                       0.0285,  0.0388,  0.0021,  0.1078,  0.0318, -0.0420, -0.0096,  0.0255,\n",
       "                      -0.0238, -0.0385,  0.0365, -0.0400,  0.0900,  0.0739, -0.0710, -0.0076,\n",
       "                      -0.0497,  0.0910, -0.0127,  0.0606,  0.0226, -0.0451,  0.0850,  0.1006,\n",
       "                       0.0269, -0.0054,  0.0173, -0.0240, -0.0348, -0.0797, -0.0310,  0.0560,\n",
       "                      -0.0606, -0.0490, -0.1214, -0.0023, -0.0178,  0.0079, -0.1023, -0.0326,\n",
       "                      -0.0673, -0.0542,  0.0450,  0.0487, -0.0099, -0.0441,  0.0319,  0.0816,\n",
       "                      -0.0349,  0.0402,  0.0597,  0.0091,  0.0381, -0.0270,  0.1538,  0.0599,\n",
       "                       0.0259,  0.0006, -0.0029,  0.0377, -0.0441, -0.0124, -0.0050, -0.0315,\n",
       "                      -0.0558, -0.1620,  0.0386,  0.0487, -0.0200, -0.0106, -0.0297, -0.0092,\n",
       "                      -0.0078, -0.0005, -0.0029, -0.0109, -0.0301, -0.0150, -0.0967, -0.0592,\n",
       "                       0.0145,  0.0352,  0.0064, -0.0388, -0.1484, -0.0814, -0.0359,  0.0100,\n",
       "                      -0.0211,  0.0653, -0.0092, -0.0429,  0.0500,  0.0690,  0.0158,  0.0554,\n",
       "                      -0.0637, -0.0681, -0.0810,  0.1180, -0.1035,  0.0157,  0.0375,  0.0027,\n",
       "                      -0.0165,  0.0462,  0.0261, -0.0153, -0.0191, -0.1032,  0.0712, -0.0193,\n",
       "                       0.0665,  0.0219,  0.0054,  0.0613, -0.0184,  0.0523,  0.0225,  0.0323,\n",
       "                      -0.1682, -0.0540, -0.1615, -0.0738, -0.0352, -0.1829, -0.1713, -0.1898,\n",
       "                      -0.1676, -0.2639, -0.1606, -0.0692, -0.0553, -0.2179, -0.2145, -0.2411,\n",
       "                      -0.2227, -0.1797, -0.1016, -0.1550, -0.0739, -0.2268, -0.1704, -0.1319,\n",
       "                      -0.1893, -0.2192, -0.2929, -0.1577, -0.2654, -0.1458, -0.1503, -0.0686,\n",
       "                      -0.1573, -0.0875, -0.1997, -0.2261, -0.2288, -0.1421, -0.2864, -0.2489,\n",
       "                      -0.1886, -0.1495, -0.1544, -0.2772, -0.0557, -0.1413, -0.2082, -0.1912,\n",
       "                      -0.0861, -0.1779, -0.1314, -0.2116, -0.2440, -0.2079, -0.1966, -0.2645,\n",
       "                      -0.0677, -0.0713, -0.0235, -0.1249, -0.1282, -0.1600, -0.1962, -0.3685,\n",
       "                      -0.1640, -0.2449, -0.0880, -0.1644, -0.1098, -0.1340, -0.0830, -0.1330,\n",
       "                      -0.1183, -0.2081, -0.2002, -0.2574, -0.1890, -0.1084, -0.1812, -0.2460,\n",
       "                      -0.1952, -0.2158, -0.3227, -0.0563, -0.2651, -0.1661, -0.1913, -0.1480,\n",
       "                      -0.3538, -0.0762, -0.2371, -0.1865, -0.0254, -0.2042, -0.1611, -0.1850,\n",
       "                      -0.1125, -0.0924, -0.1985, -0.0973, -0.1735, -0.2024, -0.1762, -0.1451,\n",
       "                      -0.1347, -0.1448, -0.3682, -0.1021, -0.1940, -0.0879, -0.0867, -0.2140,\n",
       "                      -0.1516, -0.2498, -0.1356, -0.1933, -0.2846, -0.1210, -0.1144, -0.1356,\n",
       "                      -0.0207, -0.0709, -0.1320, -0.2795, -0.2842, -0.2377, -0.0637, -0.2107])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0',\n",
       "              tensor([-1.7171e-01, -5.3390e-02, -2.5062e-01, -1.7085e-01, -3.5626e-02,\n",
       "                      -2.4146e-01, -2.2237e-02, -1.9007e-01, -2.1164e-01, -1.5372e-01,\n",
       "                      -4.2891e-02, -1.7262e-01, -1.3310e-02, -1.7803e-01, -2.2948e-01,\n",
       "                      -2.7385e-01, -2.2224e-01, -2.6964e-01, -1.2763e-01, -1.8865e-01,\n",
       "                      -1.3097e-01, -2.7111e-01, -2.5677e-01, -8.8017e-02, -1.1095e-01,\n",
       "                      -1.9329e-01, -1.9710e-01, -1.4017e-01, -2.9515e-01, -4.9823e-02,\n",
       "                      -1.0680e-01, -7.5983e-02, -2.7801e-01, -1.6708e-01, -2.2774e-01,\n",
       "                      -1.7720e-01, -1.5472e-01, -1.0671e-01, -2.5949e-01, -1.5419e-01,\n",
       "                      -1.9280e-01, -2.0795e-01, -1.7202e-01, -2.0863e-01, -1.2853e-01,\n",
       "                      -2.1336e-01, -1.4899e-01, -1.9406e-01, -1.5620e-01, -1.8677e-01,\n",
       "                      -5.0491e-02, -2.6077e-01, -2.2399e-01, -1.2083e-01, -3.2409e-02,\n",
       "                      -2.9619e-01, -1.8352e-01, -1.7728e-01, -1.6423e-01, -1.9051e-01,\n",
       "                      -1.7608e-01, -1.5956e-01, -8.6107e-02, -2.3304e-01, -1.7847e-01,\n",
       "                      -2.4810e-01, -4.4334e-02, -1.1900e-01, -3.9324e-01, -1.0714e-01,\n",
       "                      -1.1078e-01, -5.5598e-02, -1.7087e-01, -2.5647e-01, -2.2389e-01,\n",
       "                      -2.4652e-01, -2.8872e-01, -3.5208e-02, -2.3357e-01, -1.9169e-01,\n",
       "                      -8.9465e-02, -3.3006e-01, -2.9409e-01, -3.2467e-01, -2.1595e-01,\n",
       "                      -1.5221e-01, -2.3332e-01, -2.7493e-01, -2.0068e-01, -2.8720e-01,\n",
       "                      -3.0898e-01, -1.2978e-01, -6.8704e-02, -2.2165e-01, -1.3928e-01,\n",
       "                      -2.1602e-01, -1.5224e-01, -1.1881e-01, -1.7425e-01, -1.1771e-01,\n",
       "                      -2.1152e-01, -1.6033e-01, -1.0721e-01, -2.0074e-01, -9.2013e-02,\n",
       "                      -2.2623e-01, -3.1059e-01, -8.1339e-02, -1.4093e-01, -1.0865e-01,\n",
       "                      -2.5482e-01, -2.0462e-01, -2.2547e-01, -1.7726e-01, -7.0761e-02,\n",
       "                      -1.7875e-01, -1.3878e-01, -1.8922e-01, -2.6180e-01, -2.8636e-01,\n",
       "                      -9.6187e-02, -8.1658e-02, -2.4917e-01, -4.1582e-01, -2.6520e-01,\n",
       "                      -1.9788e-01, -1.2276e-01, -2.9315e-01, -2.0511e-01, -2.1143e-01,\n",
       "                      -2.2353e-01, -1.3156e-01, -7.8885e-02, -7.5941e-02, -1.1965e-01,\n",
       "                      -6.2042e-02, -2.6818e-01, -1.8397e-01, -9.9472e-02, -2.5515e-01,\n",
       "                      -5.2230e-02, -1.9170e-01, -1.4985e-01, -1.6749e-01, -2.2002e-01,\n",
       "                      -2.0970e-01, -1.0402e-01, -8.1352e-02, -1.4941e-01, -2.0586e-01,\n",
       "                      -1.7395e-01, -1.2699e-01, -1.5607e-01, -5.1134e-02, -1.6782e-01,\n",
       "                      -1.0929e-01, -2.8867e-01, -1.0775e-01, -9.0231e-04, -1.4092e-01,\n",
       "                      -8.9765e-02, -7.8338e-02, -1.9718e-01, -2.0436e-01, -1.4867e-01,\n",
       "                      -5.3251e-02, -3.4404e-01, -1.0929e-01, -8.3697e-02, -1.3570e-01,\n",
       "                      -4.4801e-02, -2.0321e-01, -1.1870e-01, -1.4227e-01, -1.2467e-01,\n",
       "                      -1.5961e-01, -2.0413e-01, -1.5957e-01, -2.0508e-01, -1.2341e-01,\n",
       "                      -1.1969e-01, -1.0096e-01, -4.9913e-02, -2.4093e-01, -1.0061e-01,\n",
       "                      -1.3922e-01, -1.2870e-01, -8.2202e-02, -1.4883e-01, -1.0684e-01,\n",
       "                      -5.6906e-02, -2.3480e-01, -9.2825e-02, -2.1377e-01, -8.4076e-02,\n",
       "                      -9.1408e-03, -9.6478e-02, -1.1333e-01, -1.5094e-01, -2.3450e-01,\n",
       "                      -1.3726e-01, -1.2458e-01, -2.0689e-01, -1.7281e-01, -1.1975e-01,\n",
       "                      -1.2384e-01, -1.2003e-01, -2.4574e-01, -2.4295e-02, -2.1059e-01,\n",
       "                      -1.6781e-01, -1.3443e-01, -1.7886e-01, -1.1240e-01, -2.9014e-01,\n",
       "                      -2.3453e-01, -1.4375e-01, -2.2930e-01, -2.4953e-01, -2.6911e-01,\n",
       "                      -1.6197e-01, -1.5323e-01, -1.8472e-01, -2.3772e-01, -5.9530e-02,\n",
       "                      -1.5928e-01, -1.1023e-01, -1.1584e-01, -8.4276e-02, -1.8173e-01,\n",
       "                      -1.2561e-01, -2.3302e-01, -1.5666e-01, -9.0589e-02, -1.7516e-01,\n",
       "                      -1.8573e-01, -2.3414e-01, -1.9866e-01, -1.0340e-01, -1.6365e-01,\n",
       "                      -7.6553e-02, -2.6426e-01, -1.5880e-01, -1.5956e-02, -1.8597e-01,\n",
       "                      -1.9824e-01, -1.8179e-01, -1.1676e-01, -1.7530e-01, -7.6616e-02,\n",
       "                      -5.4462e-03, -2.9463e-01, -1.1155e-01, -2.8664e-01, -8.1663e-02,\n",
       "                      -1.9441e-01,  4.2194e-02, -5.1624e-02, -8.6974e-02, -5.0294e-02,\n",
       "                       7.3012e-02,  5.1944e-02, -4.4842e-02, -4.8538e-02,  2.3750e-02,\n",
       "                       6.3762e-02, -2.2491e-02, -3.5547e-05, -3.3319e-03,  1.1595e-01,\n",
       "                      -1.6263e-02, -8.5059e-02,  1.1492e-01,  2.6391e-02,  8.8155e-02,\n",
       "                       1.2829e-01, -3.3402e-02,  3.2643e-02, -3.5138e-02, -2.4776e-02,\n",
       "                      -7.2037e-02,  3.2821e-02,  2.9610e-02, -2.8520e-02,  6.8768e-02,\n",
       "                      -5.7732e-02, -7.0707e-02,  9.6703e-03,  1.7809e-02,  4.8544e-02,\n",
       "                       4.8599e-03,  1.3085e-01,  7.0430e-02,  4.0774e-02, -5.5144e-02,\n",
       "                       1.2642e-01,  5.9144e-02,  1.1825e-02,  1.9225e-01, -3.0971e-02,\n",
       "                       2.1546e-02,  4.2451e-02,  4.4770e-02, -8.9366e-02,  5.4549e-03,\n",
       "                       1.8307e-01, -1.2318e-01, -1.0095e-01, -3.3547e-02,  6.8423e-02,\n",
       "                      -2.3527e-02, -8.4203e-03, -2.1028e-02, -6.1685e-04,  3.6723e-02,\n",
       "                      -1.2520e-02,  1.0429e-01, -5.5577e-03, -2.0812e-04, -1.4856e-02,\n",
       "                      -5.2768e-02, -5.2898e-02,  2.5843e-02, -4.7878e-03, -4.8754e-02,\n",
       "                      -9.4952e-02, -3.7701e-02, -5.9258e-02,  6.1174e-02,  1.0287e-01,\n",
       "                       1.0600e-02,  8.6745e-02,  1.5985e-01,  1.4977e-02, -9.0135e-02,\n",
       "                       8.0806e-02,  2.1411e-02, -4.0513e-02, -1.6850e-02, -2.0175e-02,\n",
       "                      -3.0539e-03,  9.2767e-02,  3.2217e-02, -3.1582e-02, -8.2897e-03,\n",
       "                      -2.8391e-02, -3.9941e-02,  1.0726e-02, -1.0154e-01, -2.3599e-04,\n",
       "                       4.1770e-02, -2.4292e-03, -1.2255e-01,  1.1295e-01, -3.7929e-03,\n",
       "                      -9.0392e-02,  4.0017e-02,  4.3874e-02,  2.6225e-02,  1.8271e-02,\n",
       "                      -1.5057e-02,  7.7984e-02,  3.1434e-02,  9.7435e-02,  2.3592e-02,\n",
       "                      -3.6132e-02, -7.1426e-02, -9.6761e-03,  1.1503e-02, -2.4338e-02,\n",
       "                      -4.4397e-02, -3.2440e-02,  3.1863e-02,  5.9691e-02,  2.0237e-03,\n",
       "                      -4.8930e-02,  4.9038e-02,  1.3066e-02, -3.7423e-02,  1.0075e-01,\n",
       "                       8.9184e-02, -2.1423e-02,  1.1802e-01,  3.9442e-02, -1.8734e-01,\n",
       "                      -1.1982e-01, -3.2957e-01, -1.3187e-01, -1.6258e-01, -2.7649e-01,\n",
       "                      -2.0813e-01, -1.3671e-01, -1.2983e-01, -2.0198e-01, -5.4784e-02,\n",
       "                      -6.3933e-02, -8.5316e-02, -1.8135e-01, -3.0820e-01, -2.0286e-01,\n",
       "                      -1.0244e-01, -1.7082e-01, -1.6145e-01,  7.7490e-03, -1.9650e-01,\n",
       "                      -1.5246e-01, -2.0037e-01, -8.3315e-02, -1.3030e-01, -2.3931e-01,\n",
       "                      -2.0198e-01, -1.3429e-01, -2.0422e-01, -1.1663e-01, -1.9655e-01,\n",
       "                      -1.0250e-01, -1.1819e-01, -1.4124e-01, -2.1181e-01, -3.1372e-01,\n",
       "                      -2.0653e-01, -1.6705e-01, -2.0439e-01, -2.4125e-01, -1.6150e-01,\n",
       "                      -2.1457e-01, -1.7728e-01, -2.1462e-01, -1.3693e-01, -1.2667e-01,\n",
       "                      -2.5873e-01, -9.5832e-02, -9.4895e-03, -1.2182e-01, -1.5616e-01,\n",
       "                      -2.8523e-01, -6.3463e-02, -1.5964e-01, -1.2362e-01, -2.3376e-01,\n",
       "                      -1.6154e-01, -1.6866e-01, -1.0748e-01, -2.2982e-01, -1.5083e-01,\n",
       "                      -3.3822e-01, -6.6848e-02, -2.7906e-01, -2.2263e-01, -7.4900e-02,\n",
       "                      -1.6481e-01, -1.0406e-01, -3.0045e-01, -1.0714e-01, -1.1156e-01,\n",
       "                      -1.5220e-01, -8.3555e-02, -2.6269e-01, -1.6029e-01, -1.8874e-01,\n",
       "                      -1.9114e-01, -1.2375e-01, -8.9619e-02, -1.9334e-01, -1.6820e-01,\n",
       "                      -2.3330e-01, -2.2150e-01, -1.3082e-01, -2.0873e-01, -2.2302e-01,\n",
       "                      -2.2767e-01, -1.7913e-01, -2.5237e-01, -2.2381e-01, -2.7306e-01,\n",
       "                      -2.5942e-01, -1.0349e-01, -2.5175e-01, -1.4834e-01, -1.1572e-01,\n",
       "                      -1.8869e-04, -6.0223e-02, -2.5702e-01, -7.9089e-02, -1.4073e-01,\n",
       "                      -8.7365e-02, -1.3262e-01, -3.5670e-01, -1.9206e-01, -5.2640e-02,\n",
       "                      -2.6035e-01,  8.2299e-05, -2.5247e-01, -8.4771e-02, -3.1628e-01,\n",
       "                      -1.7839e-01, -3.1140e-01, -2.8839e-01, -1.6104e-01, -1.9216e-01,\n",
       "                      -2.8839e-01, -2.3649e-01, -1.0419e-01, -1.3188e-01, -6.6477e-02,\n",
       "                      -1.5665e-01, -2.1483e-01, -3.1812e-01, -2.8834e-01, -3.3373e-01,\n",
       "                      -4.4371e-02, -3.8421e-01])),\n",
       "             ('bilstm_right_context_feature.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.1392, -0.1523,  0.2607,  ..., -0.0601, -0.0321,  0.0781],\n",
       "                      [ 0.1924, -0.2640,  0.3810,  ...,  0.0702, -0.2510,  0.0446],\n",
       "                      [ 0.0837, -0.0387, -0.0975,  ..., -0.2796, -0.2181, -0.1256],\n",
       "                      ...,\n",
       "                      [-0.2948, -0.4969, -0.0523,  ..., -0.2834, -0.1060, -0.1913],\n",
       "                      [ 0.1531,  0.2934,  0.2855,  ...,  0.1231, -0.0210, -0.1086],\n",
       "                      [-0.1273,  0.0361,  0.0659,  ..., -0.2285, -0.2076, -0.2100]])),\n",
       "             ('bilstm_right_context_feature.weight_hh_l0_reverse',\n",
       "              tensor([[-0.3892, -0.1516, -0.0989,  ...,  0.2289, -0.2184, -0.1029],\n",
       "                      [ 0.2603,  0.2875,  0.1722,  ..., -0.0789, -0.1495, -0.1642],\n",
       "                      [-0.2094, -0.2272,  0.1227,  ..., -0.0344,  0.0400,  0.0635],\n",
       "                      ...,\n",
       "                      [ 0.2053, -0.1962,  0.2873,  ..., -0.2166,  0.4001,  0.1661],\n",
       "                      [ 0.1072, -0.2578, -0.0428,  ...,  0.2254,  0.1507,  0.0464],\n",
       "                      [-0.3885, -0.3400,  0.0455,  ...,  0.2042, -0.2472,  0.0609]])),\n",
       "             ('bilstm_right_context_feature.bias_ih_l0_reverse',\n",
       "              tensor([-1.0472e-01,  5.3327e-02,  2.6046e-02, -8.5705e-02,  1.1553e-01,\n",
       "                      -2.8571e-02,  7.6014e-03,  1.2616e-03, -1.8349e-02, -2.6693e-02,\n",
       "                       7.2375e-02,  1.2382e-02,  1.1635e-01,  1.3198e-01,  1.4756e-01,\n",
       "                      -2.4586e-02,  7.0651e-02,  1.5053e-01, -5.7003e-02,  1.7557e-02,\n",
       "                       1.7014e-01,  1.1711e-02,  9.7734e-02,  2.5801e-02, -8.1445e-02,\n",
       "                       9.3045e-02,  6.8692e-02, -3.1422e-02,  1.8487e-01,  1.5403e-01,\n",
       "                      -2.2959e-02,  6.3377e-02,  8.2461e-02,  5.3899e-02,  1.6438e-01,\n",
       "                       8.1758e-02,  4.4768e-02,  1.7624e-01,  3.0116e-02, -1.5249e-02,\n",
       "                       2.0353e-01, -4.9853e-02, -6.8281e-02, -9.5349e-02, -1.5761e-01,\n",
       "                       2.9892e-01,  9.3358e-02, -7.5436e-03,  2.2968e-02,  1.8198e-01,\n",
       "                       1.2690e-03,  6.9515e-03, -6.2140e-02,  2.8238e-02,  5.9538e-02,\n",
       "                       7.1525e-02,  1.7723e-01,  2.2748e-01,  1.8767e-02,  3.2451e-02,\n",
       "                      -4.4725e-02, -9.6241e-02, -1.5491e-02,  8.5491e-02, -3.8628e-02,\n",
       "                       1.1411e-01,  8.3585e-02,  1.1737e-01, -1.2631e-01, -1.5781e-01,\n",
       "                      -3.9015e-02,  3.8513e-02, -5.9461e-03,  5.6189e-02, -1.2106e-01,\n",
       "                      -5.4932e-02, -9.8498e-02, -1.2228e-01,  4.8075e-02,  4.2420e-02,\n",
       "                       7.1518e-02, -1.1012e-02, -3.5652e-02,  3.5324e-03, -2.4128e-02,\n",
       "                       1.0271e-01,  6.2731e-03, -2.4327e-03, -1.0138e-02,  3.3643e-02,\n",
       "                      -8.3161e-02,  1.8274e-02,  8.5806e-02,  8.3757e-02,  3.2587e-01,\n",
       "                      -4.8377e-02,  1.8512e-01,  1.4588e-01,  5.6477e-02,  3.4593e-03,\n",
       "                      -3.0386e-02,  1.4800e-01,  6.8924e-02,  2.7791e-01,  1.0123e-01,\n",
       "                       1.2470e-01,  1.9680e-02,  2.3161e-01,  4.6528e-02,  6.5184e-02,\n",
       "                       3.4129e-02,  1.5276e-01,  1.0682e-01, -1.5854e-02,  3.2050e-02,\n",
       "                       1.0138e-01,  4.5812e-02,  1.5147e-01,  6.1026e-02,  3.1521e-02,\n",
       "                      -3.9715e-02,  2.2283e-01,  2.1777e-02,  1.8991e-01, -8.5307e-02,\n",
       "                      -6.7349e-02,  3.8653e-02,  2.0472e-01, -3.3701e-02,  6.1740e-02,\n",
       "                       2.7514e-02,  9.1959e-03, -5.4156e-02,  1.5856e-01, -9.0921e-02,\n",
       "                      -1.6903e-01, -8.3834e-02,  1.0154e-01, -7.3197e-02, -1.6667e-01,\n",
       "                      -1.3616e-01, -8.5404e-02, -1.3988e-01, -1.2198e-01,  4.7748e-02,\n",
       "                      -7.9625e-02, -5.5315e-02, -5.2165e-02, -2.5757e-01, -4.4546e-02,\n",
       "                      -2.1426e-02, -5.6451e-02,  2.1124e-01, -1.5381e-01, -9.1652e-02,\n",
       "                      -1.8144e-01, -1.3758e-02, -3.6796e-02, -1.2490e-02,  8.2427e-02,\n",
       "                      -1.4644e-01, -2.0857e-01, -2.0489e-01, -1.0331e-01, -7.7407e-02,\n",
       "                       1.8964e-01, -5.4996e-02, -1.4771e-01, -7.3426e-02,  4.9536e-02,\n",
       "                      -8.2750e-02, -1.7591e-01, -6.8087e-02,  2.8724e-02, -6.5494e-02,\n",
       "                      -1.9910e-01,  5.5124e-02, -5.9462e-02, -2.4853e-01, -7.5152e-03,\n",
       "                       8.0171e-02, -1.7880e-01, -2.6221e-04,  5.1658e-02, -1.0767e-01,\n",
       "                      -2.8267e-02,  2.7314e-02, -1.1419e-01, -1.1086e-01, -4.6972e-02,\n",
       "                      -1.1426e-01, -7.4963e-02, -2.9206e-02, -1.1567e-01,  2.1105e-02,\n",
       "                      -2.1509e-01, -2.3211e-02, -7.7789e-02, -9.0663e-02,  1.4258e-02,\n",
       "                      -1.6671e-02, -1.9933e-01,  1.3026e-02,  6.2946e-02, -1.7932e-01,\n",
       "                      -7.3281e-02,  1.3102e-02,  1.3728e-02,  4.2033e-02,  1.3252e-02,\n",
       "                      -3.7585e-02, -1.0894e-01,  2.2830e-02, -6.4533e-02, -9.2915e-02,\n",
       "                       2.6857e-02,  4.2149e-02, -1.1735e-01, -9.3300e-02, -1.1376e-01,\n",
       "                       5.5774e-02, -7.3619e-02, -2.2809e-01, -5.0912e-02, -7.4772e-03,\n",
       "                      -1.8300e-01,  1.7556e-01, -1.8657e-01, -7.7001e-02, -1.7953e-01,\n",
       "                      -1.7059e-01, -4.0775e-03, -1.8675e-01, -4.7778e-02,  5.6490e-02,\n",
       "                      -2.0426e-03, -3.6061e-02,  3.8368e-02,  6.9601e-02, -7.9576e-02,\n",
       "                      -1.1421e-01, -2.0724e-02, -5.9419e-02, -6.8374e-02,  4.6686e-02,\n",
       "                      -5.1378e-03, -7.8165e-02, -3.7649e-02, -2.0660e-02, -1.7217e-01,\n",
       "                       1.7610e-02, -1.7388e-01,  5.1749e-02, -3.6616e-02, -1.1201e-01,\n",
       "                      -2.5332e-01,  5.4191e-02,  3.0137e-02, -3.3270e-02,  6.8031e-02,\n",
       "                       9.9888e-02, -3.4618e-02,  2.5458e-02,  2.6516e-02,  7.3258e-02,\n",
       "                       4.1283e-02,  1.0869e-01, -5.2355e-03,  3.4428e-02, -2.4277e-02,\n",
       "                      -1.8305e-02, -9.8122e-03, -1.3989e-01, -3.2510e-02, -8.0439e-02,\n",
       "                      -5.0093e-02, -9.7040e-02, -2.4785e-02,  2.7249e-02, -3.1870e-02,\n",
       "                      -1.9882e-02,  2.4315e-02,  4.0044e-03, -8.2442e-02, -2.3861e-02,\n",
       "                      -3.4582e-02,  2.6863e-03, -2.3946e-02, -4.3158e-02,  7.4429e-03,\n",
       "                       7.9561e-02, -1.0593e-01, -9.1279e-02, -6.1174e-02,  8.8857e-02,\n",
       "                      -1.7000e-01,  6.6900e-03, -1.0704e-02, -9.9141e-02, -2.4005e-02,\n",
       "                      -8.2679e-02,  1.6960e-02,  1.0861e-01, -7.2017e-03, -5.6771e-02,\n",
       "                      -4.3504e-02, -5.8749e-02,  1.1277e-02, -6.9543e-03, -1.7018e-03,\n",
       "                       1.9117e-02,  2.1753e-02,  1.5605e-02,  8.6071e-02, -5.5427e-02,\n",
       "                      -1.2274e-01,  2.6527e-02, -2.7139e-02, -9.1319e-03, -1.0305e-04,\n",
       "                       3.8053e-02,  6.1009e-02, -3.5860e-02,  3.7825e-03,  1.2682e-01,\n",
       "                       4.6632e-02, -1.1835e-01, -2.2285e-02, -8.5803e-02, -3.1825e-02,\n",
       "                      -1.7651e-02,  6.6179e-02, -1.0572e-01,  8.3495e-03, -4.3699e-02,\n",
       "                       1.8690e-02, -6.0669e-02,  4.7675e-03,  6.0612e-02,  2.6724e-02,\n",
       "                      -5.8516e-02, -2.4927e-02,  1.2113e-01,  4.7450e-02, -1.7091e-02,\n",
       "                       1.4412e-01, -4.2302e-02,  4.3405e-02, -1.3475e-02,  1.2107e-01,\n",
       "                      -8.9353e-02, -5.7040e-02, -5.2346e-02, -1.2282e-01, -2.1286e-02,\n",
       "                       8.4888e-03,  1.2980e-01, -6.5865e-03,  7.6808e-02,  1.1631e-01,\n",
       "                      -1.5800e-03,  7.9209e-03,  2.6147e-02, -1.3630e-02,  4.9904e-03,\n",
       "                      -1.4739e-02, -5.8871e-02,  1.9373e-03,  6.1636e-02, -2.2054e-02,\n",
       "                      -9.1971e-02, -2.4708e-02, -1.0583e-01, -4.2759e-03, -5.1291e-02,\n",
       "                      -4.1134e-02,  1.1052e-01, -8.1566e-02,  6.0571e-03, -2.2150e-03,\n",
       "                       2.9095e-02,  9.9244e-02,  9.5918e-02,  6.5932e-02, -4.3880e-02,\n",
       "                       9.0633e-03,  1.9218e-01, -3.2454e-03,  1.9945e-01, -5.2899e-02,\n",
       "                      -3.6412e-02,  8.0778e-02, -7.1790e-03, -1.0449e-01,  1.6884e-01,\n",
       "                       8.8237e-02,  1.4936e-01,  2.9419e-02,  1.7834e-01,  7.6693e-02,\n",
       "                       1.7965e-01,  1.4356e-01, -8.4901e-02, -9.2667e-02,  1.6042e-01,\n",
       "                       1.2821e-01,  1.0274e-01, -2.3592e-03,  1.8325e-02,  4.7208e-02,\n",
       "                      -7.7185e-02, -3.5062e-02,  2.1445e-02,  5.0057e-02,  4.1089e-02,\n",
       "                       1.1942e-01,  1.1354e-01,  1.5310e-01,  2.0956e-02,  9.2239e-02,\n",
       "                       1.4995e-01,  9.4856e-02,  1.2699e-01, -4.2906e-02,  2.7235e-01,\n",
       "                      -5.9049e-02, -2.5628e-01, -1.0973e-01,  6.2271e-02,  3.5171e-02,\n",
       "                       1.3427e-01,  4.8124e-02, -4.4359e-02,  2.2876e-01,  1.7982e-01,\n",
       "                      -9.0906e-02,  5.2554e-02, -5.8276e-02,  1.0186e-01,  1.4394e-01,\n",
       "                       8.1742e-02,  2.0717e-01, -6.6375e-03,  1.3585e-01, -4.9791e-02,\n",
       "                       7.0949e-02, -6.8031e-02,  9.6827e-02,  2.9498e-02,  3.3140e-02,\n",
       "                       1.1583e-01,  2.6392e-02,  6.1024e-03, -3.6009e-02,  1.1795e-01,\n",
       "                       1.3965e-02, -7.8256e-02,  6.2477e-02,  5.4958e-02, -7.2271e-03,\n",
       "                       2.1650e-01,  1.1829e-01,  5.0401e-02, -1.1962e-01,  3.3871e-02,\n",
       "                       1.3942e-02,  1.3397e-01, -9.5598e-02,  1.5549e-01,  1.1513e-01,\n",
       "                      -5.5353e-02, -3.1354e-02, -1.4801e-01, -2.0666e-01,  1.0549e-01,\n",
       "                      -1.2926e-01, -2.6957e-02,  1.8974e-01,  2.0673e-01, -1.1752e-01,\n",
       "                       8.0857e-02,  1.9899e-01,  9.8529e-02,  4.9454e-02,  5.8581e-03,\n",
       "                       1.4810e-01,  4.4318e-02,  2.1210e-01,  6.6651e-02,  2.2190e-01,\n",
       "                       1.8802e-02,  2.8085e-01,  1.0856e-01,  1.3460e-01, -8.5405e-03,\n",
       "                       2.3781e-01,  9.4769e-03,  1.1268e-01, -9.3576e-03,  1.0609e-01,\n",
       "                       1.1341e-01,  1.5180e-01, -3.8058e-02,  7.2207e-02,  1.3027e-01,\n",
       "                       1.9047e-01,  9.7645e-02,  1.5657e-01, -1.9364e-02,  3.4693e-02,\n",
       "                       7.0925e-02,  2.0268e-01])),\n",
       "             ('bilstm_right_context_feature.bias_hh_l0_reverse',\n",
       "              tensor([-1.2239e-01,  6.2153e-02,  1.9624e-01, -2.3395e-02,  1.3347e-01,\n",
       "                      -6.1643e-02,  7.2531e-02,  1.6650e-01, -1.2550e-01,  9.0110e-02,\n",
       "                       9.1509e-02,  1.5856e-01,  5.7923e-02,  1.6272e-01,  1.6122e-01,\n",
       "                      -4.1611e-02, -6.0147e-03,  2.0461e-01,  4.1297e-02,  5.2527e-02,\n",
       "                       6.8063e-02,  5.0540e-02,  5.1201e-02,  7.6997e-02,  4.5654e-02,\n",
       "                       1.7474e-01,  5.7172e-03,  1.8985e-02,  1.2162e-01,  7.6864e-02,\n",
       "                       3.0932e-02,  7.6023e-02,  1.1399e-01, -2.1857e-02,  1.4529e-01,\n",
       "                      -1.1466e-02,  2.4645e-01,  4.4196e-02, -9.9387e-02,  1.0162e-02,\n",
       "                       1.9022e-01,  5.1875e-02, -1.1776e-01, -1.4988e-01, -4.7461e-02,\n",
       "                       1.5506e-01,  2.1045e-01,  1.3552e-02, -3.2395e-02,  2.0604e-01,\n",
       "                       9.1833e-02, -2.0970e-02, -8.2566e-02,  5.7324e-02,  5.7456e-02,\n",
       "                       2.0442e-01,  1.9565e-01,  1.9667e-01,  1.9656e-02,  1.4314e-02,\n",
       "                      -6.7683e-02,  7.5421e-02,  1.0988e-01,  8.3791e-02,  1.4630e-01,\n",
       "                      -6.9073e-02,  8.7827e-02,  1.7464e-01, -5.0958e-02, -1.7487e-03,\n",
       "                      -1.9567e-02,  4.5840e-02,  7.1015e-02, -2.9824e-03, -3.7640e-05,\n",
       "                      -7.6076e-02,  5.0749e-02, -2.1173e-01, -4.3232e-02, -1.8695e-02,\n",
       "                       8.4954e-02,  4.3527e-02,  3.5824e-02,  3.9034e-02,  1.4016e-02,\n",
       "                       4.2746e-02,  2.6679e-02, -7.2920e-02, -3.3965e-02,  4.8728e-02,\n",
       "                       1.7003e-01, -3.8649e-02,  1.1750e-02,  1.3292e-01,  2.0593e-01,\n",
       "                      -5.1827e-02,  1.4740e-01,  1.8846e-01,  8.1321e-02,  6.3383e-03,\n",
       "                       1.0841e-01,  6.6463e-02,  1.7799e-02,  1.5437e-01,  7.3937e-02,\n",
       "                       2.5361e-01,  4.9142e-02,  7.6503e-02,  1.0018e-01,  5.4709e-04,\n",
       "                       7.7265e-02,  2.3209e-01,  8.6246e-02,  1.7979e-03,  1.0726e-01,\n",
       "                       1.5029e-01,  9.4389e-02, -2.5215e-04, -5.4658e-03,  3.2587e-02,\n",
       "                      -8.9176e-02,  1.9547e-02, -8.1049e-02,  2.4797e-01,  3.7084e-04,\n",
       "                       2.3074e-02, -4.9820e-02,  1.4492e-01, -7.3258e-02, -8.7374e-02,\n",
       "                      -5.8759e-02, -2.3412e-02, -3.1112e-03, -1.3857e-01, -1.5108e-01,\n",
       "                      -1.7478e-01, -5.8028e-02,  1.2337e-02, -1.5699e-01, -1.6749e-01,\n",
       "                      -2.1699e-01,  2.5968e-02, -1.9542e-01, -1.2283e-01,  1.1446e-01,\n",
       "                      -4.7361e-02,  8.7954e-03,  9.3175e-02, -3.3573e-01,  4.3827e-02,\n",
       "                      -1.5122e-01, -1.9073e-02,  1.8496e-01, -1.3989e-01, -5.9443e-02,\n",
       "                      -1.1682e-01, -9.8324e-02, -1.8579e-02, -1.4659e-01, -3.3504e-02,\n",
       "                      -9.8308e-02, -5.4159e-02,  3.8859e-02, -1.7027e-01, -2.5551e-01,\n",
       "                       4.7781e-02, -3.4532e-02, -2.3183e-02,  3.3232e-02,  8.5752e-02,\n",
       "                      -6.5892e-02, -1.4194e-01, -6.3220e-02, -1.0297e-01, -5.7150e-02,\n",
       "                      -1.2135e-01,  2.0176e-02, -8.4555e-02, -2.7986e-01,  3.6845e-02,\n",
       "                       1.4605e-01, -1.1564e-01, -2.2231e-02,  6.0981e-02, -1.2381e-01,\n",
       "                       1.1565e-01, -6.8352e-02, -3.0993e-02, -2.1170e-01, -4.7915e-02,\n",
       "                      -1.0723e-01,  1.2916e-02, -2.0315e-02, -1.4605e-01, -5.3000e-03,\n",
       "                      -2.5012e-01, -9.0289e-02, -9.6895e-02, -2.2236e-01, -1.7447e-01,\n",
       "                      -6.6369e-02, -4.9245e-02, -5.4882e-02, -7.1456e-02, -1.3015e-01,\n",
       "                      -1.5061e-02, -1.2309e-01, -9.8636e-02,  6.4153e-03, -4.1039e-02,\n",
       "                      -1.6003e-01, -9.7207e-02, -5.6097e-02, -4.5784e-02, -5.4910e-02,\n",
       "                      -4.3359e-02, -8.8907e-02, -1.1088e-02, -5.7679e-02, -1.1941e-01,\n",
       "                      -8.0426e-02, -1.3967e-01, -2.0750e-01, -1.4243e-01,  5.4960e-02,\n",
       "                      -1.1251e-01,  3.1908e-02, -1.4791e-01, -1.9093e-01, -2.9031e-02,\n",
       "                       5.6668e-03, -3.8823e-02, -1.2339e-01, -7.2166e-02, -3.0679e-02,\n",
       "                       3.2137e-02, -7.2956e-02, -5.3350e-02,  2.0059e-02,  6.8190e-02,\n",
       "                       1.2454e-01,  2.6858e-02, -1.1696e-01,  3.4730e-02,  1.0802e-01,\n",
       "                       1.3895e-02,  2.8931e-02,  8.7564e-03, -5.9624e-02, -1.8470e-01,\n",
       "                      -1.0113e-01, -1.2502e-01,  1.4340e-01, -1.1528e-01, -1.0612e-01,\n",
       "                      -1.6411e-01, -1.5670e-02,  8.0793e-02, -9.9811e-02,  8.3100e-02,\n",
       "                       1.2934e-01, -2.4434e-02, -6.7227e-02,  4.6062e-02, -4.4388e-03,\n",
       "                       1.0371e-01, -2.9812e-02, -1.8614e-02, -1.0119e-02,  1.6605e-02,\n",
       "                       5.0558e-02, -1.6862e-02, -9.6343e-02,  4.5022e-03,  1.8608e-02,\n",
       "                      -7.9328e-02, -1.2280e-02, -1.4387e-01,  1.0693e-01, -1.8989e-01,\n",
       "                      -3.6164e-02,  6.9974e-03,  3.0992e-02, -4.7051e-02, -1.0143e-01,\n",
       "                       1.1488e-01, -1.4930e-02,  4.4191e-02, -2.3682e-03, -1.7211e-03,\n",
       "                      -1.3000e-01, -1.2776e-02, -1.6669e-01,  1.5794e-02,  2.5298e-02,\n",
       "                      -1.9129e-03, -4.4186e-04, -1.1310e-01,  8.4808e-02,  2.4691e-02,\n",
       "                      -4.0840e-02, -4.3472e-02,  2.9667e-02, -3.1571e-02,  2.8338e-02,\n",
       "                       2.4830e-02, -7.4380e-02,  5.3441e-02, -1.2861e-01,  1.9248e-02,\n",
       "                      -5.7962e-03, -1.1175e-01, -1.3176e-02, -3.1444e-03, -1.6053e-01,\n",
       "                      -1.2069e-01,  4.9716e-03, -1.2598e-01,  1.5575e-01, -7.6146e-02,\n",
       "                       3.1668e-02,  1.4375e-03, -8.0625e-02,  1.7396e-01, -4.1273e-02,\n",
       "                      -5.9687e-03,  1.3553e-01,  4.6667e-02, -4.4533e-02,  4.2130e-02,\n",
       "                       1.7621e-02,  1.3055e-01, -4.9433e-02, -1.7270e-01,  9.6028e-02,\n",
       "                      -7.2340e-02,  3.4741e-02,  2.4134e-02,  1.2499e-02,  2.4086e-03,\n",
       "                      -5.8462e-03,  9.4253e-02, -1.0617e-01,  8.9049e-02,  1.5850e-01,\n",
       "                       1.8354e-02, -8.4412e-02,  2.2329e-03, -9.2503e-02,  1.0598e-01,\n",
       "                      -2.5388e-02, -1.2257e-01, -1.4114e-04, -3.5870e-02,  1.0946e-01,\n",
       "                      -5.8612e-02, -1.5774e-01,  5.0926e-02,  4.5364e-02,  9.3906e-02,\n",
       "                       6.8877e-02,  1.0896e-01, -9.6959e-02, -1.6823e-02, -2.7180e-03,\n",
       "                       1.1058e-02, -2.2159e-02, -5.5582e-03, -1.3655e-01,  4.3734e-02,\n",
       "                      -1.9052e-02,  6.1822e-02, -2.9894e-02,  2.4628e-02,  4.1858e-02,\n",
       "                      -3.6985e-02,  6.1815e-02, -3.6322e-02, -2.5522e-02,  6.5912e-02,\n",
       "                       1.3518e-01,  1.0697e-02,  8.4157e-02, -2.3427e-02, -1.9981e-01,\n",
       "                       2.2220e-01,  1.4946e-01,  1.3052e-01,  6.5837e-02,  2.7260e-02,\n",
       "                      -1.1724e-01,  2.7370e-04, -1.4018e-02, -3.8491e-02,  2.5410e-01,\n",
       "                       6.7324e-02,  9.7057e-02,  2.6800e-02,  1.6102e-01, -9.1578e-03,\n",
       "                       1.6254e-01,  5.9274e-02, -8.8413e-02, -7.1775e-02,  2.3767e-01,\n",
       "                       1.2125e-01,  8.9342e-02, -3.7577e-02, -8.2882e-02,  3.3545e-02,\n",
       "                      -1.0371e-01,  1.0608e-02,  3.1611e-02, -3.8129e-02, -5.6933e-02,\n",
       "                       2.2406e-01,  1.8468e-01,  1.5730e-01,  9.2282e-02,  1.1776e-01,\n",
       "                       1.5385e-01,  2.3291e-01,  9.1286e-02, -2.6843e-02,  2.3881e-01,\n",
       "                       6.9060e-02, -1.0166e-01,  2.1509e-02,  5.2865e-02,  4.3099e-02,\n",
       "                       1.0503e-01,  1.0055e-01,  1.3413e-02,  2.2210e-01,  1.4685e-01,\n",
       "                      -1.9270e-01,  5.8485e-02,  1.8482e-02,  1.2105e-01,  1.9309e-01,\n",
       "                       1.6662e-01,  5.4478e-02, -7.8345e-02,  4.1962e-02, -1.2202e-02,\n",
       "                       5.0257e-02,  1.0783e-01,  1.1556e-01, -9.2040e-03, -4.4560e-02,\n",
       "                       2.3325e-01,  6.9474e-02, -3.3662e-02, -1.3161e-01, -4.2438e-02,\n",
       "                       1.2350e-01, -5.2660e-02, -3.8425e-03, -7.0421e-02, -4.8295e-02,\n",
       "                       1.0825e-01,  8.9052e-02,  5.9973e-02,  2.4984e-02,  1.1512e-01,\n",
       "                       1.4334e-02,  8.6756e-03,  1.1942e-02,  1.8866e-01,  3.1796e-02,\n",
       "                      -6.5762e-02, -6.7875e-02, -7.9569e-02, -6.4037e-02,  4.0374e-02,\n",
       "                      -1.3600e-01, -1.3869e-01,  5.1386e-02,  1.3877e-01,  1.1302e-02,\n",
       "                       9.7747e-02,  2.0079e-01,  4.4175e-02,  3.1370e-02,  8.1062e-03,\n",
       "                       1.1619e-02,  1.1791e-01,  9.8557e-02,  7.8977e-02,  1.9307e-01,\n",
       "                      -1.1332e-01,  2.9872e-01,  1.4015e-01, -5.7819e-03,  1.5099e-01,\n",
       "                       2.5923e-01, -3.7622e-02,  9.8521e-02,  4.9798e-02,  2.8143e-02,\n",
       "                      -6.2037e-02,  1.3572e-01,  5.7669e-02, -4.8457e-02, -1.2022e-02,\n",
       "                       2.1185e-01,  2.3355e-01,  1.6166e-01, -2.6565e-02,  1.1931e-01,\n",
       "                       1.1019e-01,  2.1542e-01])),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[ 0.0336, -0.1098,  0.1152,  ...,  0.1279, -0.1568,  0.0955],\n",
       "                      [-0.1042,  0.0081,  0.0415,  ...,  0.0563,  0.3779, -0.2720],\n",
       "                      [-0.1080,  0.2072, -0.0197,  ..., -0.5407, -0.3706, -0.2700],\n",
       "                      ...,\n",
       "                      [-0.1017, -0.1147, -0.0131,  ..., -0.0553,  0.2980, -0.0572],\n",
       "                      [ 0.2414, -0.0385,  0.0683,  ...,  0.0305,  0.0931,  0.0600],\n",
       "                      [ 0.1128, -0.2231,  0.0432,  ...,  0.2725,  0.0905, -0.0938]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([-0.0535, -0.1676,  0.0688, -0.2254,  0.0902,  0.3404,  0.0497,  0.1745,\n",
       "                       0.1812, -0.1122,  0.1370, -0.0250,  0.0658,  0.0295,  0.0963, -0.0541,\n",
       "                       0.1806, -0.1471, -0.0642,  0.1461, -0.1008, -0.2933,  0.1893,  0.1932,\n",
       "                      -0.0839,  0.0043,  0.3145, -0.3039,  0.0890,  0.0663,  0.1971,  0.0150,\n",
       "                       0.1613,  0.3166, -0.2651, -0.1290,  0.0464, -0.1904,  0.0013, -0.1432,\n",
       "                       0.3715,  0.0335,  0.1066,  0.2507,  0.3008, -0.0624, -0.0576,  0.0445,\n",
       "                      -0.0898,  0.0704, -0.0062, -0.1850,  0.0046,  0.1603, -0.1529, -0.0032,\n",
       "                      -0.1011, -0.0338,  0.2623, -0.1727,  0.3264, -0.2056,  0.1401,  0.1720])),\n",
       "             ('oov_embedding.weight',\n",
       "              tensor([[-0.1614, -0.2117,  0.0157,  ...,  0.1271, -0.1119,  0.0007],\n",
       "                      [-0.2955,  0.0113, -0.0556,  ..., -0.1153,  0.1801, -0.0444],\n",
       "                      [ 0.1902,  0.1901,  0.1298,  ...,  0.0716, -0.1417,  0.1111],\n",
       "                      ...,\n",
       "                      [ 0.2413, -0.0927,  0.0563,  ..., -0.0392, -0.1799,  0.0026],\n",
       "                      [ 0.3689,  0.1330, -0.1041,  ..., -0.0761, -0.3747, -0.1732],\n",
       "                      [-0.0069,  0.1094,  0.1698,  ..., -0.0102,  0.0509, -0.2581]])),\n",
       "             ('oov_embedding.bias',\n",
       "              tensor([-0.1504,  0.1093,  0.2959, -0.3735, -0.2704, -0.1764, -0.0189, -0.0870,\n",
       "                      -0.4447, -0.3637, -0.0033,  0.0549,  0.1832,  0.0889,  0.0050, -0.0343,\n",
       "                      -0.2606,  0.0748,  0.0175,  0.1587, -0.0605, -0.0740, -0.1319,  0.1397,\n",
       "                      -0.0397,  0.1046, -0.0726,  0.0619,  0.0800,  0.1746,  0.0044, -0.1067,\n",
       "                      -0.1277, -0.1096, -0.1439, -0.0715,  0.3165, -0.3211,  0.1460,  0.1961,\n",
       "                      -0.3230, -0.0093,  0.1524,  0.1342, -0.0619,  0.1672, -0.0476,  0.0487,\n",
       "                       0.1834, -0.0823, -0.0400,  0.1413,  0.2209,  0.3014, -0.1221,  0.0224,\n",
       "                      -0.3569, -0.2362, -0.2139,  0.2355,  0.1764,  0.0444,  0.0119,  0.2131])),\n",
       "             ('prob.0.weight',\n",
       "              tensor([[ 0.0580,  0.1189, -0.1615,  ...,  0.0545, -0.0600,  0.0743],\n",
       "                      [ 0.1480, -0.2152, -0.0874,  ...,  0.0186, -0.0391, -0.2627],\n",
       "                      [ 0.1563, -0.0195, -0.0413,  ..., -0.0802, -0.2566,  0.2526],\n",
       "                      ...,\n",
       "                      [ 0.1697,  0.2172, -0.1258,  ..., -0.2987, -0.1818,  0.0106],\n",
       "                      [-0.1014,  0.0397, -0.0959,  ...,  0.0082, -0.0635,  0.2639],\n",
       "                      [ 0.3664, -0.1912, -0.3599,  ..., -0.1477, -0.2260,  0.2698]])),\n",
       "             ('prob.0.bias',\n",
       "              tensor([-0.7280, -0.1215, -0.2048,  ..., -0.2432, -0.2174, -0.1344]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616de6-2ec1-4a77-980a-addd6d77d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
